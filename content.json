{"meta":{"title":"๑Charles✦ˑ̫✦Vincent๑","subtitle":"๑Charles✦ˑ̫✦Vincent๑","description":"嘿嘿(๑乛◡乛๑),但这也是无可奈何的事情，毕竟世上不可能有那么多十全十美的好事，做人在某些时候总是要有些取舍的。","author":"๑Charles✦ˑ̫✦Vincent๑","url":"http://zhoujinjian.cc"},"pages":[{"title":"404 Page Not Found","date":"2017-08-04T15:36:59.000Z","updated":"2018-03-31T10:12:07.268Z","comments":true,"path":"404.html","permalink":"http://zhoujinjian.cc/404.html","excerpt":"","text":""},{"title":"about","date":"2017-07-28T16:50:51.000Z","updated":"2018-03-31T10:03:00.022Z","comments":true,"path":"about/index.html","permalink":"http://zhoujinjian.cc/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2018-01-04T16:00:00.000Z","updated":"2018-03-27T10:19:32.000Z","comments":true,"path":"categories/index.html","permalink":"http://zhoujinjian.cc/categories/index.html","excerpt":"","text":"title: Androiddate: 2018-01-05 00:00:00 type: “categories” title: Hexodate: 2018-01-05 00:00:00 type: “categories”"},{"title":"search","date":"2018-04-01T09:52:02.000Z","updated":"2018-04-01T09:56:05.356Z","comments":true,"path":"search/index.html","permalink":"http://zhoujinjian.cc/search/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-03-31T16:00:00.000Z","updated":"2018-04-01T09:51:23.239Z","comments":true,"path":"tags/index.html","permalink":"http://zhoujinjian.cc/tags/index.html","excerpt":"","text":""},{"title":"Android","date":"2018-01-04T16:00:00.000Z","updated":"2018-01-12T11:33:56.000Z","comments":true,"path":"categories/Android/index.html","permalink":"http://zhoujinjian.cc/categories/Android/index.html","excerpt":"","text":""},{"title":"Hexo","date":"2018-01-04T16:00:00.000Z","updated":"2018-01-19T09:55:08.000Z","comments":true,"path":"categories/Hexo/index.html","permalink":"http://zhoujinjian.cc/categories/Hexo/index.html","excerpt":"","text":""}],"posts":[{"title":"zhoujinjian.cc-Android系列分析文档【🌀置顶🌀】","slug":"zhoujinjian.cc-Android系列分析文档【🌀置顶🌀】","date":"2088-08-08T00:08:08.080Z","updated":"2018-07-09T12:47:10.886Z","comments":true,"path":"2088/08/08/zhoujinjian.cc-Android系列分析文档【🌀置顶🌀】/","link":"","permalink":"http://zhoujinjian.cc/2088/08/08/zhoujinjian.cc-Android系列分析文档【🌀置顶🌀】/","excerpt":"","text":"Android Multimedia System：Android Video System（5）：Android Multimedia - NuPlayer音视频同步实现分析Android Video System（4）：Android Multimedia - OpenMax实现分析Android Video System（3）：音视频录制Recorder、编码Encoder、混合MediaMuxer源码分析Android Video System（2）：音视频分离MediaExtractor、解码Decoder、渲染Renderer源码分析Android Video System（1）：Video System(视频系统)框架分析 Android Camera System：Android Camera System（2）：Camera System(Camera 系统)startPreview、takePicture、Recorder流程分析Android Camera System（1）：Camera System(Camera 系统)框架、Open()过程分析 Android Display System：Android Display System（5）：Android Display System 系统分析之Display Driver ArchitectureAndroid Display System（4）：Android Display System 系统分析之Gralloc &amp;&amp; HWComposer模块分析Android Display System（3）：Android Display System 系统分析之HardwareRenderer.draw()绘制流程分析Android Display System（2）：Android Display System 系统分析之Android EGL &amp;&amp; OpenGLAndroid Display System（1）：Android 7.1.2 (Android N) Android Graphics 系统 分析 Android Audio System：Android Audio System（3）：Android audio system(音频系统)分析Android Audio System（2）：Linux ALSA音频系统分析Android Audio System（1）：Linux &amp;&amp; Android Audio 系统框架分析 Android Input System：Android Input System（2）：Android 7.1.2 (Android N) Android 输入子系统 - Input System 分析Android Input System（1）：Linux内核（Kernel-3.18） - Linux Input 子系统 分析 Android 基础（AMS &amp; WMS）Android基础 （6）：Android 7.1.2 (Android N) Android WindowManagerService 窗口管理服务 分析Android基础 （5）：Android 7.1.2 (Android N) Activity - Window 加载显示流程 分析Android基础 （4）：Android 7.1.2 (Android N) Activity 启动流程 （AMS）分析Android基础 （3）：Android 7.1.2 (Android N) Android 系统启动流程 分析Android基础 （2）：Android 7.1.2 (Android N) Android Binder 系统分析Android基础 （1）：Android 7.1.2 (Android N) Android消息机制–Handler、Looper、Message 分析","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Video System（5）：Android Multimedia - NuPlayer音视频同步实现分析","slug":"Android Video System（5）：Android Multimedia - NuPlayer音视频同步实现分析","date":"2018-09-11T16:00:00.000Z","updated":"2018-07-09T12:33:48.621Z","comments":true,"path":"2018/09/12/Android Video System（5）：Android Multimedia - NuPlayer音视频同步实现分析/","link":"","permalink":"http://zhoujinjian.cc/2018/09/12/Android Video System（5）：Android Multimedia - NuPlayer音视频同步实现分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料 Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】【特别感谢 - Android MediaCodec ACodec】【特别感谢 - android ACodec MediaCodec NuPlayer flow】 Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) （一）、音视频同步时如何实现的？从Renderer接口层来看，没有任何关于同步处理的接口，仅有有限的几个控制接口flush/pause/resume，以及queueBuffer/queueEOS接口。同步问题的核心就在于ALooper-AHandler机制。其实真正的同步都是在消息循环的响应函数里实现的。先看音频。 1.1、Renderer中的音频同步机制起始位置从音频PCM数据进入开始，处理在Renderer::queueBuffer()中，最终发送了kWhatQueueBuffer消息。这个消息的实际处理函数是Renderer::onQueueBuffer()。实际代码在“音视频原始数据输入——queueBuffer”中有，这里仅针对音频流程解释下。 基本逻辑很简单，保存传入的buffer参数，并通知输出下AudioQueue。 1234567891011121314151617181920[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]void NuPlayer::Renderer::onQueueBuffer(const sp&lt;AMessage&gt; &amp;msg) &#123;...... QueueEntry entry; entry.mBuffer = buffer; entry.mNotifyConsumed = notifyConsumed; entry.mOffset = 0; entry.mFinalResult = OK; entry.mBufferOrdinal = ++mTotalBuffersQueued; if (audio) &#123; Mutex::Autolock autoLock(mLock); mAudioQueue.push_back(entry); postDrainAudioQueue_l(); &#125; else &#123; mVideoQueue.push_back(entry); postDrainVideoQueue(); &#125;......&#125; 下面看看postDrainAudioQueue_l的实现，内部实现逻辑基本上就是边界判断加上发送kWhatDrainAudioQueue消息。 123456789[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]void NuPlayer::Renderer::postDrainAudioQueue_l(int64_t delayUs) &#123; if (mAudioQueue.empty()) return; mDrainAudioQueuePending = true; sp&lt;AMessage&gt; msg = new AMessage(kWhatDrainAudioQueue, this); msg-&gt;setInt32(\"drainGeneration\", mAudioDrainGeneration); msg-&gt;post(delayUs);&#125; 那就继续查看下这个消息如何处理的。 1234567891011121314151617181920212223242526[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp] case kWhatDrainAudioQueue: &#123; mDrainAudioQueuePending = false; if (onDrainAudioQueue()) &#123; uint32_t numFramesPlayed; uint32_t numFramesPendingPlayout = mNumFramesWritten - numFramesPlayed; // 这里是audio sink中缓存了多长的可用于播放的数据 int64_t delayUs = mAudioSink-&gt;msecsPerFrame() * numFramesPendingPlayout * 1000ll; if (mPlaybackRate &gt; 1.0f) &#123; delayUs /= mPlaybackRate; &#125; // 利用一半的延时来保证下次刷新时间（注意时间上有重叠） delayUs /= 2; // 参考buffer大小来估计最大的延时时间 const int64_t maxDrainDelayUs = std::max( mAudioSink-&gt;getBufferDurationInUs(), (int64_t)500000 /* half second */); ALOGD_IF(delayUs &gt; maxDrainDelayUs, \"postDrainAudioQueue long delay: %lld &gt; %lld\", (long long)delayUs, (long long)maxDrainDelayUs); Mutex::Autolock autoLock(mLock); postDrainAudioQueue_l(delayUs); // 这里同一个消息重发了 &#125; break; &#125; 到这里，貌似还是没有同步的机制，不过我们已经知道这个音频播放消息的触发机制了，在queueBuffer和消息处理函数中都会触发，基本上就是定时器。还有最后一个函数onDrainAudioQueue()。下面是代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]bool NuPlayer::Renderer::onDrainAudioQueue() &#123; uint32_t numFramesPlayed; if (mAudioSink-&gt;getPosition(&amp;numFramesPlayed) != OK) &#123; drainAudioQueueUntilLastEOS(); ALOGW(\"onDrainAudioQueue(): audio sink is not ready\"); return false; &#125; uint32_t prevFramesWritten = mNumFramesWritten; while (!mAudioQueue.empty()) &#123; QueueEntry *entry = &amp;*mAudioQueue.begin(); mLastAudioBufferDrained = entry-&gt;mBufferOrdinal; if (entry-&gt;mBuffer == NULL) &#123; // 删除针对EOS的处理代码 &#125; // ignore 0-sized buffer which could be EOS marker with no data if (entry-&gt;mOffset == 0 &amp;&amp; entry-&gt;mBuffer-&gt;size() &gt; 0) &#123; int64_t mediaTimeUs; CHECK(entry-&gt;mBuffer-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;mediaTimeUs)); ALOGV(\"onDrainAudioQueue: rendering audio at media time %.2f secs\", mediaTimeUs / 1E6); onNewAudioMediaTime(mediaTimeUs); &#125; size_t copy = entry-&gt;mBuffer-&gt;size() - entry-&gt;mOffset; ssize_t written = mAudioSink-&gt;write(entry-&gt;mBuffer-&gt;data() + entry-&gt;mOffset, copy, false /* blocking */); if (written &lt; 0) &#123;/* ...忽略异常处理部分代码 */&#125; entry-&gt;mOffset += written; size_t remainder = entry-&gt;mBuffer-&gt;size() - entry-&gt;mOffset; if ((ssize_t)remainder &lt; mAudioSink-&gt;frameSize()) &#123; if (remainder &gt; 0) &#123;// 这是直接凑成完整的一帧音频 ALOGW(\"Corrupted audio buffer has fractional frames, discarding %zu bytes.\", remainder); entry-&gt;mOffset += remainder; copy -= remainder; &#125; entry-&gt;mNotifyConsumed-&gt;post(); mAudioQueue.erase(mAudioQueue.begin()); entry = NULL; &#125; size_t copiedFrames = written / mAudioSink-&gt;frameSize(); mNumFramesWritten += copiedFrames; &#123; Mutex::Autolock autoLock(mLock); int64_t maxTimeMedia; maxTimeMedia = mAnchorTimeMediaUs + (int64_t)(max((long long)mNumFramesWritten - mAnchorNumFramesWritten, 0LL) * 1000LL * mAudioSink-&gt;msecsPerFrame()); mMediaClock-&gt;updateMaxTimeMedia(maxTimeMedia); notifyIfMediaRenderingStarted_l(); &#125; if (written != (ssize_t)copy) &#123; // A short count was received from AudioSink::write() // // AudioSink write is called in non-blocking mode. // It may return with a short count when: // // 1) Size to be copied is not a multiple of the frame size. Fractional frames are // discarded. // 2) The data to be copied exceeds the available buffer in AudioSink. // 3) An error occurs and data has been partially copied to the buffer in AudioSink. // 4) AudioSink is an AudioCache for data retrieval, and the AudioCache is exceeded. // (Case 1) // Must be a multiple of the frame size. If it is not a multiple of a frame size, it // needs to fail, as we should not carry over fractional frames between calls. CHECK_EQ(copy % mAudioSink-&gt;frameSize(), 0); // (Case 2, 3, 4) // Return early to the caller. // Beware of calling immediately again as this may busy-loop if you are not careful. ALOGV(\"AudioSink write short frame count %zd &lt; %zu\", written, copy); break; &#125; &#125; // calculate whether we need to reschedule another write. bool reschedule = !mAudioQueue.empty() &amp;&amp; (!mPaused || prevFramesWritten != mNumFramesWritten); // permit pause to fill buffers //ALOGD(\"reschedule:%d empty:%d mPaused:%d prevFramesWritten:%u mNumFramesWritten:%u\", // reschedule, mAudioQueue.empty(), mPaused, prevFramesWritten, mNumFramesWritten); return reschedule;&#125; 这里面比较主要的更新是onNewAudioMediaTime和mNumFramesWritten字段。剩下的一部分代码是关于异常边界情况下的音视频处理逻辑： 123456789101112131415161718192021222324252627[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]sp&lt;ABuffer&gt; firstAudioBuffer = (*mAudioQueue.begin()).mBuffer; sp&lt;ABuffer&gt; firstVideoBuffer = (*mVideoQueue.begin()).mBuffer; if (firstAudioBuffer == NULL || firstVideoBuffer == NULL) &#123; // 对于一个队列为空的情况，通知另个一队列EOS syncQueuesDone_l(); return; &#125; int64_t firstAudioTimeUs; int64_t firstVideoTimeUs; CHECK(firstAudioBuffer-&gt;meta() -&gt;findInt64(\"timeUs\", &amp;firstAudioTimeUs)); CHECK(firstVideoBuffer-&gt;meta() -&gt;findInt64(\"timeUs\", &amp;firstVideoTimeUs)); int64_t diff = firstVideoTimeUs - firstAudioTimeUs; if (diff &gt; 100000ll) &#123; // 音频数据时间戳比视频数据早0.1s， (*mAudioQueue.begin()).mNotifyConsumed-&gt;post(); mAudioQueue.erase(mAudioQueue.begin()); return; &#125; syncQueuesDone_l(); 1.2、Renderer中的视频同步部分和音频同步类似，入口在在Renderer::queueBuffer()，主要区分在Renderer::onQueueBuffer()中，代码如下： 123// 如果是视频，则将数据存放到视频队列，然后安排刷新mVideoQueue.push_back(entry);postDrainVideoQueue(); 下面按照之前的思路继续分析，接下来是postDrainVideoQueue实现，主要音视频同步逻辑位于这里。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]void NuPlayer::Renderer::postDrainVideoQueue() &#123; if (mVideoQueue.empty()) &#123; return; &#125; QueueEntry &amp;entry = *mVideoQueue.begin(); sp&lt;AMessage&gt; msg = new AMessage(kWhatDrainVideoQueue, this); //这是实际处理视频缓冲区和显示的消息 msg-&gt;setInt32(\"drainGeneration\", getDrainGeneration(false /* audio */)); if (entry.mBuffer == NULL) &#123; // EOS doesn't carry a timestamp. msg-&gt;post(); mDrainVideoQueuePending = true; return; &#125; bool needRepostDrainVideoQueue = false; int64_t delayUs; int64_t nowUs = ALooper::GetNowUs(); int64_t realTimeUs; int64_t mediaTimeUs; CHECK(entry.mBuffer-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;mediaTimeUs)); if (mFlags &amp; FLAG_REAL_TIME) &#123; realTimeUs = mediaTimeUs; &#125; else &#123; &#123; Mutex::Autolock autoLock(mLock); if (mAnchorTimeMediaUs &lt; 0) &#123; // 同步基准未设置的情况下，直接显示 mMediaClock-&gt;updateAnchor(mediaTimeUs, nowUs, mediaTimeUs); mAnchorTimeMediaUs = mediaTimeUs; realTimeUs = nowUs; &#125; else if (!mVideoSampleReceived) &#123; // 第一帧未显示前，直接显示 // Always render the first video frame. realTimeUs = nowUs; &#125; else if (mAudioFirstAnchorTimeMediaUs &lt; 0 // 音频未播放之前，以视频为准 || mMediaClock-&gt;getRealTimeFor(mediaTimeUs, &amp;realTimeUs) == OK) &#123; realTimeUs = getRealTimeUs(mediaTimeUs, nowUs); &#125; else if (mediaTimeUs - mAudioFirstAnchorTimeMediaUs &gt;= 0) &#123; // 视频超前的情况下，等待 needRepostDrainVideoQueue = true; realTimeUs = nowUs; &#125; else &#123; realTimeUs = nowUs; &#125; &#125; // Heuristics to handle situation when media time changed without a // discontinuity. If we have not drained an audio buffer that was // received after this buffer, repost in 10 msec. Otherwise repost // in 500 msec. delayUs = realTimeUs - nowUs; int64_t postDelayUs = -1; if (delayUs &gt; 500000) &#123; postDelayUs = 500000; if (mHasAudio &amp;&amp; (mLastAudioBufferDrained - entry.mBufferOrdinal) &lt;= 0) &#123; postDelayUs = 10000; &#125; &#125; else if (needRepostDrainVideoQueue) &#123; // CHECK(mPlaybackRate &gt; 0); // CHECK(mAudioFirstAnchorTimeMediaUs &gt;= 0); // CHECK(mediaTimeUs - mAudioFirstAnchorTimeMediaUs &gt;= 0); postDelayUs = mediaTimeUs - mAudioFirstAnchorTimeMediaUs; postDelayUs /= mPlaybackRate; &#125; if (postDelayUs &gt;= 0) &#123; msg-&gt;setWhat(kWhatPostDrainVideoQueue); msg-&gt;post(postDelayUs); mVideoScheduler-&gt;restart(); ALOGI(\"possible video time jump of %dms or uninitialized media clock, retrying in %dms\", (int)(delayUs / 1000), (int)(postDelayUs / 1000)); mDrainVideoQueuePending = true; return; &#125; &#125; realTimeUs = mVideoScheduler-&gt;schedule(realTimeUs * 1000) / 1000; int64_t twoVsyncsUs = 2 * (mVideoScheduler-&gt;getVsyncPeriod() / 1000); delayUs = realTimeUs - nowUs; // 上面代码的主要目的是计算这个延时 ALOGW_IF(delayUs &gt; 500000, \"unusually high delayUs: %\" PRId64, delayUs); // post 2 display refreshes before rendering is due msg-&gt;post(delayUs &gt; twoVsyncsUs ? delayUs - twoVsyncsUs : 0); mDrainVideoQueuePending = true;&#125; 这里主要的是发送了一个延时消息kWhatDrainVideoQueue，下面是如何处理的代码： 1234567891011121314[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]case kWhatDrainVideoQueue:&#123; int32_t generation; CHECK(msg-&gt;findInt32(\"drainGeneration\", &amp;generation)); if (generation != getDrainGeneration(false /* audio */)) &#123; break; &#125; mDrainVideoQueuePending = false; onDrainVideoQueue(); postDrainVideoQueue(); // 注意这里相当于定时器的实现了 break;&#125; 直接调用onDrainVideoQueue函数，看看如何实现的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]void NuPlayer::Renderer::onDrainVideoQueue() &#123; if (mVideoQueue.empty()) &#123; return; &#125; QueueEntry *entry = &amp;*mVideoQueue.begin(); if (entry-&gt;mBuffer == NULL) &#123; // ...省略针对EOS 处理 &#125; int64_t nowUs = ALooper::GetNowUs(); int64_t realTimeUs; int64_t mediaTimeUs = -1; if (mFlags &amp; FLAG_REAL_TIME) &#123; CHECK(entry-&gt;mBuffer-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;realTimeUs)); &#125; else &#123; CHECK(entry-&gt;mBuffer-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;mediaTimeUs)); realTimeUs = getRealTimeUs(mediaTimeUs, nowUs); &#125; bool tooLate = false; if (!mPaused) &#123; setVideoLateByUs(nowUs - realTimeUs); tooLate = (mVideoLateByUs &gt; 40000); if (tooLate) &#123; ALOGV(\"video late by %lld us (%.2f secs)\", (long long)mVideoLateByUs, mVideoLateByUs / 1E6); &#125; else &#123; int64_t mediaUs = 0; mMediaClock-&gt;getMediaTime(realTimeUs, &amp;mediaUs); ALOGV(\"rendering video at media time %.2f secs\", (mFlags &amp; FLAG_REAL_TIME ? realTimeUs : mediaUs) / 1E6); if (!(mFlags &amp; FLAG_REAL_TIME) &amp;&amp; mLastAudioMediaTimeUs != -1 &amp;&amp; mediaTimeUs &gt; mLastAudioMediaTimeUs) &#123; // If audio ends before video, video continues to drive media clock. // Also smooth out videos &gt;= 10fps. mMediaClock-&gt;updateMaxTimeMedia(mediaTimeUs + 100000); &#125; &#125; &#125; else &#123; setVideoLateByUs(0); if (!mVideoSampleReceived &amp;&amp; !mHasAudio) &#123; // This will ensure that the first frame after a flush won't be used as anchor // when renderer is in paused state, because resume can happen any time after seek. Mutex::Autolock autoLock(mLock); clearAnchorTime_l(); &#125; &#125; // Always render the first video frame while keeping stats on A/V sync. if (!mVideoSampleReceived) &#123; realTimeUs = nowUs; tooLate = false; &#125; entry-&gt;mNotifyConsumed-&gt;setInt64(\"timestampNs\", realTimeUs * 1000ll); // 上面所有计算的参数在这里使用了 entry-&gt;mNotifyConsumed-&gt;setInt32(\"render\", !tooLate); entry-&gt;mNotifyConsumed-&gt;post(); // 注意这里，实际是向解码器发送消息，用于显示 mVideoQueue.erase(mVideoQueue.begin()); entry = NULL; mVideoSampleReceived = true; if (!mPaused) &#123; // 这里是通知NuPlayer层渲染开始 if (!mVideoRenderingStarted) &#123; mVideoRenderingStarted = true; notifyVideoRenderingStart(); &#125; Mutex::Autolock autoLock(mLock); notifyIfMediaRenderingStarted_l(); &#125;&#125; 到这里，小结下，读完这部分代码发现，NuPlayer::Renderer使用的以视频为基准的同步机制，音频晚了直接丢包，视频需要显示。同步主要位于视频缓冲区处理部分onDrainVideoQueue和音频缓冲区处理部分onDrainVideoQueue中。 （二）、音视频同步时序图 1：OMX component 集成在ACodec中，ACodec（A/V）解完数据后，通知Nulayer；2：NuPlayer通知Render，Render需要A/V的时间同步（另，如果是JPEG的话就不需要这个同步，直接render即可）；3：对于Audio，直接通过AudioSink播放；4：对于Video，通过通知ACodec，让ACodec通过(NativeWindow/Render)发送到界面 Step1:1.1 omx_message::FILL_BUFFER_DONE ===&gt;&gt;&gt;ACodec::onOMXFillBufferDone() OMX component send msg FILL_BUFFER_DONE ACodec call onOMXFillBufferDone to handle1.2 ACodec::onOMXFillBufferDone()::ACodec::kWhatDrainThisBuffer setMessage ===&gt;&gt;&gt; NuPlayer::onMessageReceived()Step2:2.1 NuPlayer::renderBuffer()2.2 NuPlayer::Renderer::queueBuffer() ===&gt;&gt;&gt; send msg kWhatQueueBuffer2.3 NuPlayer::Renderer::onMessageReceived() ===&gt;&gt;&gt; onQueueBuffer()2.4 postDrainAudioQueue() or postDrainVideoQueue() ===&gt;&gt;&gt; send msg kWhatDrainVideoQueue2.5 onMessageReceived() ===&gt;&gt;&gt; onDrainVideoQueue(); postDrainVideoQueue();onDrainVideoQueue():A/V的时间同步,如果慢0.4s,标记too_latepostDrainVideoQueue():A/V的时间同步,如果解码时间快，决定等待的时间，并把消息给renderStep3：3.1 postDrainAudioQueue()===&gt;&gt;&gt;onDrainAudioQueue()===&gt;&gt;&gt;mAudioSink-&gt;write()Step4:4.1 Renderer::onDrainVideoQueue(): entry-&gt;mNotifyConsumed-&gt;setInt32(“render”, !tooLate);4.2 Renderer::postDrainVideoQueue()===&gt;&gt;&gt; send msg kWhatDrainVideoQueue4.2 ACodec::BaseState::onMessageReceived() ===&gt;&gt;&gt; onOutputBufferDrained(msg); 1234567891011121314151617181920Log:...07-03 11:32:58.675 741 5075 V NuPlayerRenderer: rendering video at media time 0.36 secs07-03 11:32:58.676 725 1139 V AudioFlinger: releaseWakeLock_l() AudioOut_2507-03 11:32:58.676 725 1139 V AudioFlinger: thread 0xef883380 type 0 TID 1139 going to sleep07-03 11:32:58.679 725 1131 V AudioFlinger: releaseWakeLock_l() AudioOut_D07-03 11:32:58.682 741 5075 V NuPlayerRenderer: onDrainAudioQueue: rendering audio at media time 0.70 secs07-03 11:32:58.683 741 5075 V AudioTrack: frame adjustment:2400 timestamp:BOOTTIME offset 007-03 11:32:58.683 741 5075 V AudioTrack: ExtendedTimestamp[0] position: 0 time: -107-03 11:32:58.683 741 5075 V AudioTrack: ExtendedTimestamp[1] position: 17280 time: 9647618595007-03 11:32:58.683 741 5075 V AudioTrack: ExtendedTimestamp[2] position: 0 time: -107-03 11:32:58.683 741 5075 V AudioTrack: ExtendedTimestamp[3] position: 0 time: -107-03 11:32:58.683 741 5075 V AudioTrack: ExtendedTimestamp[4] position: 0 time: -107-03 11:32:58.683 741 5075 V AudioSink: getPlayedOutDurationUs(370130) nowUs(96536315) frames(14880) framesAt(96476185)07-03 11:32:58.684 741 5075 V NuPlayerRenderer: onDrainAudioQueue: rendering audio at media time 0.73 secs07-03 11:32:58.694 725 1131 V AudioFlinger: thread 0xf0208880 type 0 TID 1131 going to sleep07-03 11:32:58.701 741 5075 V NuPlayerRenderer: onDrainAudioQueue: rendering audio at media time 0.75 secs07-03 11:32:58.717 741 5075 V NuPlayerRenderer: rendering video at media time 0.41 secs07-03 11:32:58.733 741 5075 V NuPlayerRenderer: onDrainAudioQueue: rendering audio at media time 0.77 secs... （三）、参考资料(特别感谢各位前辈的分析和图示)：④NuPlayer播放框架之Renderer源码分析：音视频同步时如何实现的？android ACodec MediaCodec NuPlayer flow - CSDN博客android MediaCodec ACodec - CSDN博客","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Video System（4）：Android Multimedia - OpenMax实现分析","slug":"Android Video System（4）：Android Multimedia - OpenMax实现分析","date":"2018-09-05T16:00:00.000Z","updated":"2018-07-06T12:27:52.098Z","comments":true,"path":"2018/09/06/Android Video System（4）：Android Multimedia - OpenMax实现分析/","link":"","permalink":"http://zhoujinjian.cc/2018/09/06/Android Video System（4）：Android Multimedia - OpenMax实现分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料 Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】【特别感谢 - Copy Windrunnerlihuan（OpenMax分析）】【特别感谢 - Android MediaCodec ACodec】【特别感谢 - android ACodec MediaCodec NuPlayer flow】 Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) ☯ \\hardware\\qcom\\media\\msm8996\\libstagefrighthw QComOMXPlugin.cpp QComOMXMetadata.h QComOMXPlugin.h ☯ \\hardware\\qcom\\media\\msm8996\\mm-video-v4l2\\vidc ☯ \\hardware\\qcom\\media\\msm8996\\mm-core （一）、OpenMax简介 android中的 NuPlayer就是用OpenMax来做(codec)编解码的，本节就主要科普一下OpenMax和它在Android系统中扮演的角色。 1.1、 OpenMax系统的结构1.1.1、OpenMax总体层次结构 OpenMax是一个多媒体应用程序的框架标准，由NVIDIA公司和Khronos在2006年推出。 OpenMax是无授权费的，跨平台的应用程序接口API，通过使媒体加速组件能够在开发、集成和编程环节中实现跨多操作系统和处理器硬件平台，提供全面的流媒体编解码器和应用程序便携化。 OpenMax的官方网站如下所示： http://www.khronos.org/openmax/OpenMax实际上分成三个层次，自上而下分别是，OpenMax DL（开发层），OpenMax IL（集成层）和OpenMax AL（应用层）。三个层次的内容分别如下所示： 第一层：OpenMax DL（Development Layer，开发层） OpenMax DL定义了一个API，它是音频、视频和图像功能的集合。供应商能够在一个新的处理器上实现并优化，然后编解码供应商使用它来编写更广泛的编解码器功能。它包括音频信号的处理功能，如FFT和filter，图像原始处理，如颜色空间转换、视频原始处理，以实现例如MPEG-4、H.264、MP3、AAC和JPEG等编解码器的优化。 第二层：OpenMax IL（Integration Layer，集成层） OpenMax IL作为音频、视频和图像编解码器能与多媒体编解码器交互，并以统一的行为支持组件（例如，资源和皮肤）。这些编解码器或许是软硬件的混合体，对用户是透明的底层接口应用于嵌入式、移动设备。它提供了应用程序和媒体框架，透明的。编解码器供应商必须写私有的或者封闭的接口，集成进移动设备。IL的主要目的是使用特征集合为编解码器提供一个系统抽象，为解决多个不同媒体系统之间轻便性的问题。 第三层：OpenMax AL（Appliction Layer，应用层） OpenMax AL API在应用程序和多媒体中间件之间提供了一个标准化接口，多媒体中间件提供服务以实现被期待的API功能。 OpenMax API将会与处理器一同提供，以使库和编解码器开发者能够高速有效地利用新器件的完整加速潜能，无须担心其底层的硬件结构。该标准是针对嵌入式设备和移动设备的多媒体软件架构。在架构底层上为多媒体的编解码和数据处理定义了一套统一的编程接口，对多媒体数据的处理功能进行系统级抽象，为用户屏蔽了底层的细节。因此，多媒体应用程序和多媒体框架通过OpenMax IL可以以一种统一的方式来使用编解码和其他多媒体数据处理功能，具有了跨越软硬件平台的移植性。 注：在实际的应用中，OpenMax的三个层次中使用较多的是OpenMax IL集成层，由于操作系统到硬件的差异和多媒体应用的差异，OpenMax的DL和AL层使用相对较少。 1.1.2、OpenMax IL简介OpenMax IL 处在中间层的位置，OpenMAX IL 作为音频，视频和图像编解码器 能与多媒体编解码器交互，并以统一的行为支持组件（例如资源和皮肤）。这些编解码器或许是软硬件的混合体，对用户是 的底层接口应用于嵌入式或 / 和移动设备。它提供了应用程序和媒体框架， 透明的。本质上不存在这种标准化的接口，编解码器供 应商必须写私有的或者封闭的接口，集成进移动设备。 IL 的主要目的 是使用特征集合为编解码器提供一个系统抽象，为解决多个不同媒体系统之间轻便性的问题。 OpenMax IL 的目的就是为硬件平台的图形及音视频提供一个抽象层，可以为上层的应用提供一个可跨平台的支撑。这一点对于跨平台的媒体应用来说十分重要。本人也接触过几家高清解码芯片，这些芯片底层的音视频接口虽然功能上大致相同，但是接口设计及用法上各有不同，而且相差很多。你要想让自己开发的媒体应用完美的运行在不同的硬件厂商平台上，就得适应不同芯片的底层解码接口。这个对于应用开发来说十分繁琐。所以就需要类似于OpenMax IL 这种接口规范。应用假如涉及到音视频相关功能时，只需调用这些标准的接口，而不需要关心接口下方硬件相关的实现。假如换了硬件平台时，只需要把接口层与硬件适配好了就行了。上层应用不需要频繁改动。 你可以把OpenMax IL 看作是中间件中的porting层接口，但是现在中间件大部分都是自家定义自己的。 OpenMax 想做的就是定义一个这样的行业标准，这样媒体应用、硬件厂商都遵循这种标准。硬件厂商将OpenMax 与处理器一并提供，上层的多媒体框架想要用到硬件音视频加速功能时，只需遵循openmax的接口就可以扩平台运行。 可喜的，现在越来越多的多媒体框架及多媒体应用正在遵循openmax标准，包括各种知名的媒体开源软件。越来越多的芯片厂商也在遵循openmax的标准。对于现在的音视频编解码来说，分辨率越来越高，需要芯片提供硬件加速功能是个大的趋势。我相信 接口的标准化是一定要走的。如下图所示， openmax IL在多媒体框架中的应用： OpenMax IL目前已经成为了事实上的多媒体框架标准。嵌入式处理器或者多媒体编解码模块的硬件生产者，通常提供标准的OpenMax IL层的软件接口，这样软件的开发者就可以基于这个层次的标准化接口进行多媒体程序的开发。 OpenMax IL的接口层次结构适中，既不是硬件编解码的接口，也不是应用程序层的接口，因此比较容易实现标准化。OpenMax IL的层次结构如下： 图中的虚线中的内容是OpenMax IL层的内容，其主要实现了OpenMax IL中的各个组件（Component）。对下层，OpenMax IL可以调用OpenMax DL层的接口，也可以直接调用各种Codec实现。对上层，OpenMax IL可以给OpenMax AL 层等框架层（Middleware）调用，也可以给应用程序直接调用。 1.1.3、OpenMax IL结构OpenMax IL主要内容如下所示。 ☯ 客户端（Client）：OpenMax IL的调用者☯ 组件（Component）：OpenMax IL的单元，每一个组件实现一种功能☯ 端口（Port）：组件的输入输出接口☯ 隧道化（Tunneled）：让两个组件直接连接的方式 OpenMax IL的基本运作过程如图所示： OpenMAL IL的客户端，通过调用四个OpenMAL IL组件，实现了一个功能。四个组件分别是Source组件、Host组件、Accelerator组件和Sink组件。Source组件只有一个输出端口；而Host组件有一个输入端口和一个输出端口；Accelerator组件具有一个输入端口，调用了硬件的编解码器，加速主要体现在这个环节上。Accelerator组件和Sink组件通过私有通讯方式在内部进行连接，没有经过明确的组件端口。 OpenMAL IL在使用的时候，其数据流也有不同的处理方式：既可以经由客户端，也可以不经由客户端。图中Source组件到Host组件的数据流就是经过客户端的；而Host组件到Accelerator组件的数据流就没有经过客户端，使用了隧道化的方式；Accelerator组件和Sink组件甚至可以使用私有的通讯方式。 OpenMax Core是辅助各个组件运行的部分，它通常需要完成各个组件的初始化等工作，在真正运行过程中，重点是各个OpenMax IL的组件，OpenMax Core不是重点，也不是标准。 OpenMAL IL的组件是OpenMax IL实现的核心内容，一个组件以输入、输出端口为接口，端口可以被连接到另一个组件上。外部对组件可以发送命令，还进行设置/获取参数、配置等内容。组件的端口可以包含缓冲区（Buffer）的队列。 组件的处理的核心内容是：通过输入端口消耗Buffer，通过输出端口填充Buffer，由此多组件相联接可以构成流式的处理。OpenMAL IL中一个组件的结构如下图所示： 组件的功能和其定义的端口类型密切相关，通常情况下：只有一个输出端口的，为Source组件；只有一个输入端口的，为Sink组件；有多个输入端口，一个输出端口的为Mux组件；有一个输入端口，多个输出端口的为DeMux组件；输入输出端口各一个组件的为中间处理环节，这是最常见的组件。 端口具体支持的数据也有不同的类型。例如，对于一个输入、输出端口各一个组件，其输入端口使用MP3格式的数据，输出端口使用PCM格式的数据，那么这个组件就是一个MP3解码组件。 隧道化（Tunneled）是一个关于组件连接方式的概念。通过隧道化可以将不同的组件的一个输入端口和一个输出端口连接到一起，在这种情况下，两个组件的处理过程合并，共同处理。尤其对于单输入和单输出的组件，两个组件将作为类似一个使用。 1.2、Android中的OpenMax1.2.1、OpenMax在Android中的使用情况 在Android中，OpenMax IL层，通常可以用于多媒体引擎的插件，Android的多媒体引擎OpenCore和StageFright都可以使用OpenMax作为插件，主要用于编解码（Codec）处理。 在Android的框架层，也定义了由Android封装的OpenMax接口，和标准的接口概念基本相同，但是使用C++类型的接口，并且使用了Android的Binder IPC机制。Android封装OpenMax的接口被StageFright使用，OpenCore没有使用这个接口，而是使用其他形式对OpenMax IL层接口进行封装。Android OpenMax的基本层次结构如图： Android系统的一些部分对OpenMax IL层进行使用，基本使用的是标准OpenMax IL层的接口，只是进行了简单的封装。标准的OpenMax IL实现很容易以插件的形式加入到Android系统中。 Android的多媒体引擎OpenCore和StageFright都可以使用OpenMax作为多媒体编解码的插件，只是没有直接使用OpenMax IL层提供的纯C接口，而是对其进行了一定的封装(C++封装)。 在Android2.x版本之后，Android的框架层也对OpenMax IL层的接口进行了封装定义，甚至使用Android中的Binder IPC机制。Stagefright使用了这个层次的接口，OpenCore没有使用。 注：OpenCore使用OpenMax IL层作为编解码插件在前，Android框架层封装OpenMax接口在后面的版本中才引入。 1.2.2、Android OpenMax实现的内容 android中的 AwesomePlayer就是用openmax来做(Codec)编解码,其实在openmax接口设计中，他不光能用来当编解码。通过他的组件可以组成一个完整的播放器，包括sourc、demux、decode、output。但是为什么android只用他来做code呢？应该有如下方面： ☯ 1.在整个播放器中，解码器不得不说是最重要的一部分，而且也是最耗资源的一块。如果全靠软解，直接通过cpu来运算，特别是高清视频。别的事你就可以啥都不干了。所以解码器是最需要硬件提供加速的部分。现在的高清解码芯片都是主芯片+DSP结构，解码的工作都是通过DSP来做，不会在过多的占用主芯片。所有将芯片中DSP硬件编解码的能力通过openmax标准接口呈现出来，提供上层播放器来用。我认为这块是openmax最重要的意义。☯ 2.source 主要是和协议打交道，demux 分解容器部分，大多数的容器格式的分解是不需要通过硬件来支持。只是ts流这种格式最可能用到硬件的支持。因为ts格式比较特殊，单包的大小太小了，只有188字节。所以也是为什么现在常见的解码芯片都会提供硬件ts demux 的支持。☯ 3.音视频输出部分video\\audio output 这块和操作系统关系十分紧密。可以看看著名开源播放器vlc。vlc 在mac、linux、Windows都有，功能上差别也不大。所以说他是跨平台的，他跨平台跨在哪？主要的工作量还是在音视频解码完之后的输出模块。因为各个系统的图像渲染和音频输出实现方法不同，所以vlc需要针对每个平台实现不同的output。这部分内容放在openmax来显然不合适。 Android中使用的主要是OpenMax的编解码功能。虽然OpenMax也可以生成输入、输出、文件解析-构建等组件，但是在各个系统（不仅是Android）中使用的最多的还是编解码组件。媒体的输入、输出环节和系统的关系很大，引入OpenMax标准比较麻烦；文件解析-构建环节一般不需要使用硬件加速。编解码组件也是最能体现硬件加速的环节，因此最常使用。 1.3、初窥适配层接口在Android中实现OpenMax IL层和标准的OpenMax IL层的方式基本，一般需要实现以下两个环节： ☯ 编解码驱动程序：位于Linux内核空间，需要通过Linux内核调用驱动程序，通常使用非标准的驱动程序。☯ OpenMax IL层：根据OpenMax IL层的标准头文件实现不同功能的组件。 Android中还提供了OpenMax的适配层接口（对OpenMax IL的标准组件进行封装适配），它作为Android本地层的接口，可以被Android的多媒体引擎调用。上一篇文章末尾，初始化解码器核心调用的两个方法就是适配层的接口。 ☯ 1.上面已经说过了，android系统中只用openmax来做Codec，所以android向上抽象了一层OMXCodec，提供给上层播放器用。播放器中音视频解码器mVideosource、mAudiosource都是OMXCodec的实例。☯ 2.OMXCodec通过IOMX 依赖binder机制 获得 OMX服务，OMX服务 才是openmax 在android中 实现。☯ 3.OMX把软编解码和硬件编解码统一看作插件的形式管理起来。 （二）、Android中OpenMax的实现(preview)2.1、OpenMax的接口与实现在Android中实现OpenMax IL层和标准的OpenMax IL层的方式基本，一般需要实现以下两个环节。 编解码驱动程序：位于Linux内核空间，需要通过Linux内核调用驱动程序，通常使用非标准的驱动程序。OpenMax IL层：根据OpenMax IL层的标准头文件实现不同功能的组件。 Android中还提供了OpenMax的适配层接口（对OpenMax IL的标准组件进行封装适配），它作为Android本地层的接口，可以被Android的多媒体引擎调用。 OpenMax IL层接口 OpenMax IL层的接口定义由若干个头文件组成，这也是实现它需要实现的内容，位于frameworks/native/include/media/openmax下，它们的基本描述如下所示： OMX_Types.h：OpenMax Il的数据类型定义OMX_Core.h：OpenMax IL核心的APIOMX_Component.h：OpenMax IL 组件相关的 APIOMX_Audio.h：音频相关的常量和数据结构OMX_IVCommon.h：图像和视频公共的常量和数据结构OMX_Image.h：图像相关的常量和数据结构OMX_Video.h：视频相关的常量和数据结构OMX_Other.h：其他数据结构（包括A/V 同步）OMX_Index.h：OpenMax IL定义的数据结构索引OMX_ContentPipe.h：内容的管道定义 提示：OpenMax标准只有头文件，没有标准的库，设置没有定义函数接口。对于实现者，需要实现的主要是包含函数指针的结构体。 其中，OMX_Component.h中定义的OMX_COMPONENTTYPE结构体是OpenMax IL层的核心内容，表示一个组件，其内容如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990[-&gt;frameworks/native/include/media/openmax/OMX_Component.h]typedef struct OMX_COMPONENTTYPE &#123; OMX_U32 nSize; /* 这个结构体的大小 */ OMX_VERSIONTYPE nVersion; /* 版本号 */ OMX_PTR pComponentPrivate; /* 这个组件的私有数据指针. */ /* 调用者（IL client）设置的指针，用于保存它的私有数据，传回给所有的回调函数 */ OMX_PTR pApplicationPrivate; /* 以下的函数指针返回OMX_core.h中的对应内容 */ OMX_ERRORTYPE (*GetComponentVersion)(/* 获得组件的版本*/ OMX_IN OMX_HANDLETYPE hComponent, OMX_OUT OMX_STRING pComponentName, OMX_OUT OMX_VERSIONTYPE* pComponentVersion, OMX_OUT OMX_VERSIONTYPE* pSpecVersion, OMX_OUT OMX_UUIDTYPE* pComponentUUID); OMX_ERRORTYPE (*SendCommand)(/* 发送命令 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_COMMANDTYPE Cmd, OMX_IN OMX_U32 nParam1, OMX_IN OMX_PTR pCmdData); OMX_ERRORTYPE (*GetParameter)(/* 获得参数 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_INDEXTYPE nParamIndex, OMX_INOUT OMX_PTR pComponentParameterStructure); OMX_ERRORTYPE (*SetParameter)(/* 设置参数 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_INDEXTYPE nIndex, OMX_IN OMX_PTR pComponentParameterStructure); OMX_ERRORTYPE (*GetConfig)(/* 获得配置 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_INDEXTYPE nIndex, OMX_INOUT OMX_PTR pComponentConfigStructure); OMX_ERRORTYPE (*SetConfig)(/* 设置配置 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_INDEXTYPE nIndex, OMX_IN OMX_PTR pComponentConfigStructure); OMX_ERRORTYPE (*GetExtensionIndex)(/* 转换成OMX结构的索引 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_STRING cParameterName, OMX_OUT OMX_INDEXTYPE* pIndexType); OMX_ERRORTYPE (*GetState)(/* 获得组件当前的状态 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_OUT OMX_STATETYPE* pState); OMX_ERRORTYPE (*ComponentTunnelRequest)(/* 用于连接到另一个组件*/ OMX_IN OMX_HANDLETYPE hComp, OMX_IN OMX_U32 nPort, OMX_IN OMX_HANDLETYPE hTunneledComp, OMX_IN OMX_U32 nTunneledPort, OMX_INOUT OMX_TUNNELSETUPTYPE* pTunnelSetup); OMX_ERRORTYPE (*UseBuffer)(/* 为某个端口使用Buffer */ OMX_IN OMX_HANDLETYPE hComponent, OMX_INOUT OMX_BUFFERHEADERTYPE** ppBufferHdr, OMX_IN OMX_U32 nPortIndex, OMX_IN OMX_PTR pAppPrivate, OMX_IN OMX_U32 nSizeBytes, OMX_IN OMX_U8* pBuffer); OMX_ERRORTYPE (*AllocateBuffer)(/* 在某个端口分配Buffer */ OMX_IN OMX_HANDLETYPE hComponent, OMX_INOUT OMX_BUFFERHEADERTYPE** ppBuffer, OMX_IN OMX_U32 nPortIndex, OMX_IN OMX_PTR pAppPrivate, OMX_IN OMX_U32 nSizeBytes); OMX_ERRORTYPE (*FreeBuffer)(/*将某个端口Buffer释放*/ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_U32 nPortIndex, OMX_IN OMX_BUFFERHEADERTYPE* pBuffer); OMX_ERRORTYPE (*EmptyThisBuffer)(/* 让组件消耗这个Buffer */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_BUFFERHEADERTYPE* pBuffer); OMX_ERRORTYPE (*FillThisBuffer)(/* 让组件填充这个Buffer */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_BUFFERHEADERTYPE* pBuffer); OMX_ERRORTYPE (*SetCallbacks)(/* 设置回调函数 */ OMX_IN OMX_HANDLETYPE hComponent, OMX_IN OMX_CALLBACKTYPE* pCallbacks, OMX_IN OMX_PTR pAppData); OMX_ERRORTYPE (*ComponentDeInit)(/* 反初始化组件 */ OMX_IN OMX_HANDLETYPE hComponent); OMX_ERRORTYPE (*UseEGLImage)( OMX_IN OMX_HANDLETYPE hComponent, OMX_INOUT OMX_BUFFERHEADERTYPE** ppBufferHdr, OMX_IN OMX_U32 nPortIndex, OMX_IN OMX_PTR pAppPrivate, OMX_IN void* eglImage); OMX_ERRORTYPE (*ComponentRoleEnum)( OMX_IN OMX_HANDLETYPE hComponent, OMX_OUT OMX_U8 *cRole, OMX_IN OMX_U32 nIndex); &#125; OMX_COMPONENTTYPE; ☯ 1）EmptyThisBuffer和FillThisBuffer是驱动组件运行的基本的机制，前者表示让组件消耗缓冲区，表示对应组件输入的内容；后者表示让组件填充缓冲区，表示对应组件输出的内容。☯ 2）UseBuffer，AllocateBuffer，FreeBuffer为和端口相关的缓冲区管理函数，对于组件的端口有些可以自己分配缓冲区，有些可以使用外部的缓冲区，因此有不同的接口对其进行操作。☯ 3）SendCommand表示向组件发送控制类的命令。GetParameter，SetParameter，GetConfig，SetConfig几个接口用于辅助的参数和配置的设置和获取。☯ 4）ComponentTunnelRequest用于组件之间的隧道化连接，其中需要制定两个组件及其相连的端口。☯ 5）ComponentDeInit用于组件的反初始化。 OMX_COMPONENTTYPE结构体实现后，其中的各个函数指针就是调用者可以使用的内容。各个函数指针和OMX_core.h中定义的内容相对应。 提示：OpenMax函数的参数中，经常包含OMX_IN和OMX_OUT等宏，它们的实际内容为空，只是为了标记参数的方向是输入还是输出。 OMX_Component.h中端口类型的定义为OMX_PORTDOMAINTYPE枚举类型，内容如下所示： 123456789typedef enum OMX_PORTDOMAINTYPE &#123; OMX_PortDomainAudio, /* 音频类型端口 */ OMX_PortDomainVideo, /* 视频类型端口 */ OMX_PortDomainImage, /* 图像类型端口 */ OMX_PortDomainOther, /* 其他类型端口 */ OMX_PortDomainKhronosExtensions = 0x6F000000, //为Khronos标准预留宽展 OMX_PortDomainVendorStartUnused = 0x7F000000 //为厂商预留扩展 OMX_PortDomainMax = 0x7ffffff &#125; OMX_PORTDOMAINTY 音频类型，视频类型，图像类型，其他类型是OpenMax IL层此所定义的四种端口的类型。 端口具体内容的定义使用OMX_PARAM_PORTDEFINITIONTYPE类（也在OMX_Component.h中定义）来表示，其内容如下所示： 1234567891011121314151617181920typedef struct OMX_PARAM_PORTDEFINITIONTYPE &#123; OMX_U32 nSize; /* 结构体大小 */ OMX_VERSIONTYPE nVersion; /* 版本*/ OMX_U32 nPortIndex; /* 端口号 */ OMX_DIRTYPE eDir; /* 端口的方向 */ OMX_U32 nBufferCountActual; /* 为这个端口实际分配的Buffer的数目 */ OMX_U32 nBufferCountMin; /* 这个端口最小Buffer的数目*/ OMX_U32 nBufferSize; /* 缓冲区的字节数 */ OMX_BOOL bEnabled; /* 是否使能 */ OMX_BOOL bPopulated; /* 是否在填充 */ OMX_PORTDOMAINTYPE eDomain; /* 端口的类型 */ union &#123; /* 端口实际的内容，由类型确定具体结构 */ OMX_AUDIO_PORTDEFINITIONTYPE audio; OMX_VIDEO_PORTDEFINITIONTYPE video; OMX_IMAGE_PORTDEFINITIONTYPE image; OMX_OTHER_PORTDEFINITIONTYPE other; &#125; format; OMX_BOOL bBuffersContiguous; OMX_U32 nBufferAlignment; &#125; OMX_PARAM_PORTDEFINITIONTYPE; 对于一个端口，其重点的内容如下: ☯ 端口的方向（OMX_DIRTYPE）：包含OMX_DirInput（输入）和OMX_DirOutput（输出）两种☯ 端口分配的缓冲区数目和最小缓冲区数目☯ 端口的类型（OMX_PORTDOMAINTYPE）：可以是四种类型☯ 端口格式的数据结构：使用format联合体来表示，具体由四种不同类型来表示，与端口的类型相对应☯ OMX_AUDIO_PORTDEFINITIONTYPE，OMX_VIDEO_PORTDEFINITIONTYPE，OMX_IMAGE_PORTDEFINITIONTYPE和OMX_OTHER_PORTDEFINITIONTYPE等几个具体的格式类型，分别在OMX_Audio.h，OMX_Video.h，OMX_Image.h和OMX_Other.h这四个头文件中定义。 OMX_Core.h中定义的枚举类型OMX_STATETYPE命令表示OpenMax的状态机，内容如下所示： 123456789101112typedef enum OMX_STATETYPE &#123; OMX_StateInvalid, /* 组件监测到内部的数据结构被破坏 */ OMX_StateLoaded, /* 组件被加载但是没有完成初始化 */ OMX_StateIdle, /* 组件初始化完成，准备开始 */ OMX_StateExecuting, /* 组件接受了开始命令，正在树立数据 */ OMX_StatePause, /* 组件接受暂停命令*/ OMX_StateWaitForResources, /* 组件正在等待资源 */ OMX_StateKhronosExtensions = 0x6F000000, /* 保留for Khronos */ OMX_StateVendorStartUnused = 0x7F000000, /* 保留for厂商 */ OMX_StateMax = 0X7FFFFFFF &#125; OMX_STATETYPE; OpenMax组件的状态机可以由外部的命令改变，也可以由内部发生的情况改变。OpenMax IL组件的状态机的迁移关系如图所示： OMX_Core.h中定义的枚举类型OMX_COMMANDTYPE表示对组件的命令类型，内容如下所示： 1234567891011typedef enum OMX_COMMANDTYPE &#123; OMX_CommandStateSet, /* 改变状态机器 */ OMX_CommandFlush, /* 刷新数据队列 */ OMX_CommandPortDisable, /* 禁止端口 */ OMX_CommandPortEnable, /* 使能端口 */ OMX_CommandMarkBuffer, /* 标记组件或Buffer用于观察 */ OMX_CommandKhronosExtensions = 0x6F000000, /* 保留for Khronos */ OMX_CommandVendorStartUnused = 0x7F000000, /* 保留for厂商 */ OMX_CommandMax = 0X7FFFFFFF &#125; OMX_COMMANDTYPE; OMX_COMMANDTYPE类型在SendCommand调用中作为参数被使用，其中OMX_CommandStateSet就是改变状态机的命令。 2.1.2、OpenMax IL实现的内容 对于OpenMax IL层的实现，一般的方式并不调用OpenMax DL层。具体实现的内容就是各个不同的组件。 OpenMax IL组件的实现包含以下两个步骤： ☯ 组件的初始化函数：硬件和OpenMax数据结构的初始化，一般分成函数指针初始化、私有数据结构的初始化、端口的初始化等几个步骤，使用OMX_Component.h其中的pComponentPrivate成员保留本组件的私有数据为上下文，最后获得填充完成OMX_COMPONENTTYPE类型的结构体。☯ OMX_COMPONENTTYPE类型结构体的各个指针：实现其中的各个函数指针，需要使用私有数据的时候，从其中的pComponentPrivate得到指针，转化成实际的数据结构使用。 端口的定义是OpenMax IL组件对外部的接口。OpenMax IL常用的组件大都是输入和输出端口各一个。对于最常用的编解码（Codec）组件，通常需要在每个组件的实现过程中，调用硬件的编解码接口来实现。在组件的内部处理中，可以建立线程来处理。OpenMax的组件的端口有默认参数，但也可以在运行时设置，因此一个端口也可以支持不同的编码格式。音频编码组件的输出和音频编码组件的输入通常是原始数据格式（PCM格式），视频编码组件的输出和视频编码组件的输入通常是原始数据格式（YUV格式）。 提示：在一种特定的硬件实现中，编解码部分具有相似性，因此通常可以构建一个OpenMax组件的”基类”或者公共函数，来完成公共性的操作。 2.2、Android中OpenMax的适配层 Android中的OpenMax适配层的接口在frameworks/av/include/media/IOMX.h文件定义，其内容如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class IOMX : public IInterface &#123; public: DECLARE_META_INTERFACE(OMX); typedef void *buffer_id; typedef void *node_id; virtual bool livesLocally(pid_t pid) = 0; struct ComponentInfo &#123;// 组件的信息 String8 mName; List&lt;String8&gt; mRoles; &#125;; virtual status_t listNodes(List&lt;ComponentInfo&gt; *list) = 0; // 节点列表 virtual status_t allocateNode( const char *name, const sp&lt;IOMXObserver&gt; &amp;observer, // 分配节点 node_id *node) = 0; virtual status_t freeNode(node_id node) = 0; // 找到节点 virtual status_t sendCommand(// 发送命令 node_id node, OMX_COMMANDTYPE cmd, OMX_S32 param) = 0; virtual status_t getParameter(// 获得参数 node_id node, OMX_INDEXTYPE index, void *params, size_t size) = 0; virtual status_t setParameter(// 设置参数 node_id node, OMX_INDEXTYPE index, const void *params, size_t size) = 0; virtual status_t getConfig(// 获得配置 node_id node, OMX_INDEXTYPE index, void *params, size_t size) = 0; virtual status_t setConfig(// 设置配置 node_id node, OMX_INDEXTYPE index, const void *params, size_t size) = 0; virtual status_t useBuffer(// 使用缓冲区 node_id node, OMX_U32 port_index, const sp&lt;IMemory&gt; ¶ms, buffer_id *buffer) = 0; virtual status_t allocateBuffer(// 分配缓冲区 node_id node, OMX_U32 port_index, size_t size, buffer_id *buffer, void **buffer_data) = 0; virtual status_t allocateBufferWithBackup(// 分配带后备缓冲区 node_id node, OMX_U32 port_index, const sp&lt;IMemory&gt; ¶ms, buffer_id *buffer) = 0; virtual status_t freeBuffer(// 释放缓冲区 node_id node, OMX_U32 port_index, buffer_id buffer) = 0; virtual status_t fillBuffer(node_id node, buffer_id buffer) = 0; // 填充缓冲区 virtual status_t emptyBuffer(// 消耗缓冲区 node_id node, buffer_id buffer, OMX_U32 range_offset, OMX_U32 range_length, OMX_U32 flags, OMX_TICKS timestamp) = 0; virtual status_t getExtensionIndex( node_id node, const char *parameter_name, OMX_INDEXTYPE *index) = 0; virtual sp&lt;IOMXRenderer&gt; createRenderer(// 创建渲染器（从ISurface） const sp&lt;ISurface&gt; &amp;surface, const char *componentName, OMX_COLOR_FORMATTYPE colorFormat, size_t encodedWidth, size_t encodedHeight, size_t displayWidth, size_t displayHeight) = 0; ...... &#125;; IOMX表示的是OpenMax的一个组件，根据Android的Binder IPC机制，BnOMX继承IOMX，实现者需要继承实现BnOMX。IOMX类中，有标准的OpenMax的GetParameter，SetParameter，GetConfig，SetConfig，SendCommand，UseBuffer，AllocateBuffer，FreeBuffer，FillThisBuffer和EmptyThisBuffer等接口。 在IOMX.h文件中，另有表示观察器类的IOMXObserver，这个类表示OpenMax的观察者，其中只包含一个onMessage()函数，其参数为omx_message接口体，其中包含Event事件类型、FillThisBuffer完成和EmptyThisBuffer完成几种类型。 提示：Android中OpenMax的适配层是OpenMAX IL层至上的封装层，在Android系统中被StageFright调用，也可以被其他部分调用。 2.3、TI(Texas Instruments 德州仪器) OpenMax IL的硬件实现2.3.1、TI OpenMax IL实现的结构和机制 Android的开源代码中，已经包含了TI的OpenMax IL层的实现代码，其路径如hardware/ti/omap3/omx下。其中包含的主要目录如下所示： ☯ system：OpenMax核心和公共部分☯ audio：音频处理部分的OpenMax IL组件☯ video：视频处理部分OpenMax IL组件☯ image：图像处理部分OpenMax IL组件 TI OpenMax IL实现的结构如图所示: 在TI OpenMax IL实现中，最上面的内容是OpenMax的管理者用于管理和初始化，中间层是各个编解码单元的OpenMax IL标准组件，下层是LCML层，供各个OpenMax IL标准组件所调用。 （1）TI OpenMax IL实现的公共部分在system/src/openmax_il/目录中，主要的内容如下所示。 ☯ omx_core/src：OpenMax IL的核心，生成动态库libOMX_Core.so☯ lcml/：LCML的工具库，生成动态库libLCML.so （2）I OpenMax IL的视频（Video）相关的组件在video/src/openmax_il/目录中，主要的内容如下所示。 ☯ prepost_processor：Video数据的前处理和后处理，生成动态库libOMX.TI.VPP.so☯ video_decode：Video解码器，生成动态库libOMX.TI.Video.Decoder.so☯ video_encode：Video编码器，生成动态库libOMX.TI.Video.encoder.so （3）TI OpenMax IL的音频（Audio）相关的组件在audio/src/openmax_il/目录中，主要的内容如下所示。 ☯ g711_dec：G711解码器，生成动态库libOMX.TI.G711.decode.so☯ g711_enc：G711编码器，生成动态库libOMX.TI.G711.encode.so☯ g722_dec：G722解码器，生成动态库libOMX.TI.G722.decode.so☯ g722_enc：G722编码器，生成动态库libOMX.TI.G722.encode.so☯ g726_dec：G726解码器，生成动态库libOMX.TI.G726.decode.so☯ g726_enc：G726编码器，生成动态库libOMX.TI.G726.encode.so☯ g729_dec：G729解码器，生成动态库libOMX.TI.G729.decode.so☯ g729_enc：G720编码器，生成动态库libOMX.TI.G729.encode.so☯ nbamr_dec：AMR窄带解码器，生成动态库libOMX.TI.AMR.decode.so☯ nbamr_enc：AMR窄带编码器，生成动态库libOMX.TI.AMR.encode.so☯ wbamr_dec：AMR宽带解码器，生成动态库libOMX.TI.WBAMR.decode.so☯ wbamr_enc：AMR宽带编码器，生成动态库libOMX.TI.WBAMR.encode.so☯ mp3_dec：MP3解码器，生成动态库libOMX.TI.MP3.decode.so☯ aac_dec：AAC解码器，生成动态库libOMX.TI.AAC.decode.so☯ aac_enc：AAC编码器，生成动态库libOMX.TI.AAC.encode.so☯ wma_dec：WMA解码器，生成动态库libOMX.TI.WMA.decode.so （4）TI OpenMax IL的图像（Image）相关的组件在image/src/openmax_il/目录中，主要的内容如下所示。 ☯ jpeg_enc：JPEG编码器，生成动态库libOMX.TI.JPEG.Encoder.so☯ jpeg_dec：JPEG解码器，生成动态库libOMX.TI.JPEG.decoder.so 2.3.2、TI OpenMax IL的核心和公共内容 LCML的全称是”Linux Common Multimedia Layer“，是TI的Linux公共多媒体层。在OpenMax IL的实现中，这个内容在system/src/openmax_il/lcml/目录中，主要文件是子目录src中的LCML_DspCodec.c文件。通过调用DSPBridge的内容， 让ARM和DSP进行通信，然DSP进行编解码方面的处理。DSP的运行还需要固件的支持。 TI OpenMax IL的核心实现在system/src/openmax_il/omx_core/目录中，生成TI OpenMax IL的核心库libOMX_Core.so。 其中子目录src中的OMX_Core.c为主要文件，其中定义了编解码器的名称等，其片断如下所示： 12345678910111213141516171819202122char *tComponentName[MAXCOMP][2] = &#123; &#123;\"OMX.TI.JPEG.decoder\", \"image_decoder.jpeg\"&#125;,/* 图像和视频编解码器 */ &#123;\"OMX.TI.JPEG.Encoder\", \"image_encoder.jpeg\"&#125;, &#123;\"OMX.TI.Video.Decoder\", \"video_decoder.avc\"&#125;, &#123;\"OMX.TI.Video.Decoder\", \"video_decoder.mpeg4\"&#125;, &#123;\"OMX.TI.Video.Decoder\", \"video_decoder.wmv\"&#125;, &#123;\"OMX.TI.Video.encoder\", \"video_encoder.mpeg4\"&#125;, &#123;\"OMX.TI.Video.encoder\", \"video_encoder.h263\"&#125;, &#123;\"OMX.TI.Video.encoder\", \"video_encoder.avc\"&#125;, /* ......省略 ，语音相关组件*/ #ifdef BUILD_WITH_TI_AUDIO /* 音频编解码器 */ &#123;\"OMX.TI.MP3.decode\", \"audio_decoder.mp3\"&#125;, &#123;\"OMX.TI.AAC.encode\", \"audio_encoder.aac\"&#125;, &#123;\"OMX.TI.AAC.decode\", \"audio_decoder.aac\"&#125;, &#123;\"OMX.TI.WMA.decode\", \"audio_decoder.wma\"&#125;, &#123;\"OMX.TI.WBAMR.decode\", \"audio_decoder.amrwb\"&#125;, &#123;\"OMX.TI.AMR.decode\", \"audio_decoder.amrnb\"&#125;, &#123;\"OMX.TI.AMR.encode\", \"audio_encoder.amrnb\"&#125;, &#123;\"OMX.TI.WBAMR.encode\", \"audio_encoder.amrwb\"&#125;, #endif &#123;NULL, NULL&#125;, &#125;; tComponentName数组的各个项中，第一个表示编解码库内容，第二个表示库所实现的功能。 其中，TIOMX_GetHandle()函数用于获得各个组件的句柄，其实现的主要片断如下所示：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647OMX_ERRORTYPE TIOMX_GetHandle( OMX_HANDLETYPE* pHandle, OMX_STRING cComponentName, OMX_PTR pAppData, OMX_CALLBACKTYPE* pCallBacks) &#123; static const char prefix[] = \"lib\"; static const char postfix[] = \".so\"; OMX_ERRORTYPE (*pComponentInit)(OMX_HANDLETYPE*); OMX_ERRORTYPE err = OMX_ErrorNone; OMX_COMPONENTTYPE *componentType; const char* pErr = dlerror(); // ...... 省略错误处理内容 int i = 0; for(i=0; i&lt; COUNTOF(pModules); i++) &#123; // 循环查找 if(pModules[i] == NULL) break; &#125; // ...... 省略错误处理内容 int refIndex = 0; for (refIndex=0; refIndex &lt; MAX_TABLE_SIZE; refIndex++) &#123; // 循环查找组件列表 if (strcmp(componentTable[refIndex].name, cComponentName) == 0) &#123; if (componentTable[refIndex].refCount&gt;= MAX_CONCURRENT_INSTANCES) &#123; // ...... 省略错误处理内容 &#125; else &#123; char buf[sizeof(prefix) + MAXNAMESIZE+ sizeof(postfix)]; strcpy(buf, prefix); strcat(buf, cComponentName); strcat(buf, postfix); pModules[i] = dlopen(buf, RTLD_LAZY | RTLD_GLOBAL); // ...... 省略错误处理内容 // 动态取出初始化的符号 pComponentInit = dlsym(pModules[i], \"OMX_ComponentInit\"); pErr = dlerror(); // ...... 省略错误处理内容 *pHandle = malloc(sizeof(OMX_COMPONENTTYPE)); // ...... 省略错误处理内容 pComponents[i] = *pHandle; componentType = (OMX_COMPONENTTYPE*) *pHandle; componentType-&gt;nSize = sizeof(OMX_COMPONENTTYPE); err = (*pComponentInit)(*pHandle); // 执行初始化工作 // ...... 省略部分内容 &#125; &#125; &#125; err = OMX_ErrorComponentNotFound; goto UNLOCK_MUTEX; // ...... 省略部分内容 return (err); &#125; 在TIOMX_GetHandle()函数中，根据tComponentName数组中动态库的名称，动态打开各个编解码实现的动态库，取出其中的OMX_ComponentInit符号来执行各个组件的初始化。 2.3.3、一个TI OpenMax IL组件的实现 TI OpenMax IL中各个组件都是通过调用LCML来实现的，实现的方式基本类似。主要都是实现了名称为OMX_ComponentInit的初始化函数，实现OMX_COMPONENTTYPE类型的结构体中的各个成员。各个组件其目录结构和文件结构也类似。 以MP3解码器的实现为例，在audio/src/openmax_il/mp3_dec/src目录中，主要包含以下文件： ☯ OMX_Mp3Decoder.c：MP3解码器组件实现☯ OMX_Mp3Dec_CompThread.c：MP3解码器组件的线程循环☯ OMX_Mp3Dec_Utils.c：MP3解码器的相关工具，调用LCML实现真正的MP3解码的功能 OMX_Mp3Decoder.c中的OMX_ComponentInit()函数负责组件的初始化，返回的内容再从参数中得到，这个函数的主要片断如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667OMX_ERRORTYPE OMX_ComponentInit (OMX_HANDLETYPE hComp) &#123; OMX_ERRORTYPE eError = OMX_ErrorNone; OMX_COMPONENTTYPE *pHandle = (OMX_COMPONENTTYPE*) hComp; OMX_PARAM_PORTDEFINITIONTYPE *pPortDef_ip = NULL, *pPortDef_op = NULL; OMX_AUDIO_PARAM_PORTFORMATTYPE *pPortFormat = NULL; OMX_AUDIO_PARAM_MP3TYPE *mp3_ip = NULL; OMX_AUDIO_PARAM_PCMMODETYPE *mp3_op = NULL; MP3DEC_COMPONENT_PRIVATE *pComponentPrivate = NULL; MP3D_AUDIODEC_PORT_TYPE *pCompPort = NULL; MP3D_BUFFERLIST *pTemp = NULL; int i=0; MP3D_OMX_CONF_CHECK_CMD(pHandle,1,1); /* ......省略，初始化OMX_COMPONENTTYPE类型的指针pHandle */ OMX_MALLOC_GENERIC(pHandle-&gt;pComponentPrivate, MP3DEC_COMPONENT_PRIVATE); pComponentPrivate = pHandle-&gt;pComponentPrivate; /* 私有指针互相指向 */ pComponentPrivate-&gt;pHandlepHandle = pHandle; /* ......略，初始化似有数据指针pComponentPrivate */ /* 设置输入端口（OMX_PARAM_PORTDEFINITIONTYPE类型）的默认值 */ pPortDef_ip-&gt;nSize = sizeof(OMX_PARAM_PORTDEFINITIONTYPE); pPortDef_ip-&gt;nPortIndex = MP3D_INPUT_PORT; pPortDef_ip-&gt;eDir = OMX_DirInput; pPortDef_ip-&gt;nBufferCountActual = MP3D_NUM_INPUT_BUFFERS; pPortDef_ip-&gt;nBufferCountMin = MP3D_NUM_INPUT_BUFFERS; pPortDef_ip-&gt;nBufferSize = MP3D_INPUT_BUFFER_SIZE; pPortDef_ip-&gt;nBufferAlignment = DSP_CACHE_ALIGNMENT; pPortDef_ip-&gt;bEnabled = OMX_TRUE; pPortDef_ip-&gt;bPopulated = OMX_FALSE; pPortDef_ip-&gt;eDomain = OMX_PortDomainAudio; pPortDef_ip-&gt;format.audio.eEncoding = OMX_AUDIO_CodingMP3; pPortDef_ip-&gt;format.audio.cMIMEType = NULL; pPortDef_ip-&gt;format.audio.pNativeRender = NULL; pPortDef_ip-&gt;format.audio.bFlagErrorConcealment = OMX_FALSE; /* 设置输出端口（OMX_PARAM_PORTDEFINITIONTYPE类型）的默认值 */ pPortDef_op-&gt;nSize = sizeof(OMX_PARAM_PORTDEFINITIONTYPE); pPortDef_op-&gt;nPortIndex = MP3D_OUTPUT_PORT; pPortDef_op-&gt;eDir = OMX_DirOutput; pPortDef_op-&gt;nBufferCountMin = MP3D_NUM_OUTPUT_BUFFERS; pPortDef_op-&gt;nBufferCountActual = MP3D_NUM_OUTPUT_BUFFERS; pPortDef_op-&gt;nBufferSize = MP3D_OUTPUT_BUFFER_SIZE; pPortDef_op-&gt;nBufferAlignment = DSP_CACHE_ALIGNMENT; pPortDef_op-&gt;bEnabled = OMX_TRUE; pPortDef_op-&gt;bPopulated = OMX_FALSE; pPortDef_op-&gt;eDomain = OMX_PortDomainAudio; pPortDef_op-&gt;format.audio.eEncoding = OMX_AUDIO_CodingPCM; pPortDef_op-&gt;format.audio.cMIMEType = NULL; pPortDef_op-&gt;format.audio.pNativeRender = NULL; pPortDef_op-&gt;format.audio.bFlagErrorConcealment = OMX_FALSE; /* ......省略，分配端口 */ /* 设置输入端口的默认格式 */ pPortFormat = pComponentPrivate-&gt;pCompPort[MP3D_INPUT_PORT]-&gt;pPortFormat; OMX_CONF_INIT_STRUCT(pPortFormat, OMX_AUDIO_PARAM_PORTFORMATTYPE); pPortFormat-&gt;nPortIndex = MP3D_INPUT_PORT; pPortFormat-&gt;nIndex = OMX_IndexParamAudioMp3; pPortFormat-&gt;eEncoding = OMX_AUDIO_CodingMP3; /* 设置输出端口的默认格式 */ pPortFormat = pComponentPrivate-&gt;pCompPort[MP3D_OUTPUT_PORT]-&gt;pPortFormat; OMX_CONF_INIT_STRUCT(pPortFormat, OMX_AUDIO_PARAM_PORTFORMATTYPE); pPortFormat-&gt;nPortIndex = MP3D_OUTPUT_PORT; pPortFormat-&gt;nIndex = OMX_IndexParamAudioPcm; pPortFormat-&gt;eEncoding = OMX_AUDIO_CodingPCM; /* ......省略部分内容 */ eError = Mp3Dec_StartCompThread(pHandle); // 启动MP3解码线程 /* ......省略部分内容 */ return eError; &#125; 这个组件是OpenMax的标准实现方式，对外的接口的内容只有一个初始化函数。完成OMX_COMPONENTTYPE类型的初始化。输入端口的编号为MP3D_INPUT_PORT（==0），类型为OMX_PortDomainAudio，格式为OMX_AUDIO_CodingMP3。输出端口的编号是MP3D_OUTPUT_PORT（==1），类型为OMX_PortDomainAudio，格式为OMXAUDIO CodingPCM。 OMX_Mp3Dec_CompThread.c中定义了MP3DEC_ComponentThread()函数，用于创建MP3解码的线程的执行函数。 OMX_Mp3Dec_Utils.c中的Mp3Dec_StartCompThread()函数，调用了POSIX的线程库建立MP3解码的线程，如下所示： 12nRet = pthread_create (&amp;(pComponentPrivate-&gt;ComponentThread), NULL, MP3DEC_ComponentThread, pComponentPrivate); Mp3Dec_StartCompThread()函数就是在组件初始化函数OMX_ComponentInit()最后调用的内容。MP3线程的开始并不表示解码过程开始，线程需要等待通过pipe机制获得命令和数据（cmdPipe和dataPipe），在适当的时候开始工作。这个pipe在MP3解码组件的SendCommand等实现写操作，在线程中读取其内容。 2.4、Qualcomm(高通) OpenMax IL的硬件实现2.4.1、qcom OpenMax IL实现的结构和机制（1）在AOSP中依然有对高通平台的OpenMax IL层实现代码，位于hardware/qcom/media/mm-core下。这一部分是OpenMax核心和公共部分，主要编译为libOmxCore.so。 （2）e.g. 继续在hardware/qcom/media下，选取mm-video-v4l2目录。即Video4linux2（简称V4L2),是linux中关于视频设备的内核驱动。再次进入vidc，（DivxDrmDecrypt为DRM数字版权相关）主要目录如下： ☯ vdec：视频解码处理，编译成libOmxVdec.so/libOmxVdecHevc.so☯ venc：视频编码处理，编译成libOmxVenc.soqcom OpenMax IL的核心和公共内容 类似于前面介绍的TI，高通平台在OpenMax IL实现也是大同小异，位于hardware/qcom/media/mm-core，生成libOmxCore.so库。 其中qc_omx_core为主要文件，位于hardware/qcom/media/mm-core/omxcore/src/common/下面。和TI的差不多，OMX_GetHandle()函数用户获取各个组件的句柄，其实现的主要片断如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798//编解码器组件集合数组extern omx_core_cb_type core[]; OMX_API OMX_ERRORTYPE OMX_APIENTRYOMX_GetHandle(OMX_OUT OMX_HANDLETYPE* handle, OMX_IN OMX_STRING componentName, OMX_IN OMX_PTR appData, OMX_IN OMX_CALLBACKTYPE* callBacks)&#123; OMX_ERRORTYPE eRet = OMX_ErrorNone; int cmp_index = -1; int hnd_index = -1; DEBUG_PRINT(\"OMXCORE API : Get Handle %p %s %p\\n\", handle, componentName, appData); pthread_mutex_lock(&amp;lock_core); if(handle) &#123; struct stat sd; //组件句柄 *handle = NULL; //获取根据组件名获取相应index cmp_index = get_cmp_index(componentName); if(cmp_index &gt;= 0) &#123; DEBUG_PRINT(\"getting fn pointer\\n\"); // dynamically load the so 动态加载组件的so库 core[cmp_index].fn_ptr = omx_core_load_cmp_library(core[cmp_index].so_lib_name, &amp;core[cmp_index].so_lib_handle); if(core[cmp_index].fn_ptr) &#123; // Construct the component requested // Function returns the opaque handle //根据获取的组件句柄初始化它 void* pThis = (*(core[cmp_index].fn_ptr))(); if(pThis) &#123; //包装一层，忽略 void *hComp = NULL; hComp = qc_omx_create_component_wrapper((OMX_PTR)pThis); if((eRet = qc_omx_component_init(hComp, core[cmp_index].name)) != OMX_ErrorNone) &#123; DEBUG_PRINT(\"Component not created succesfully\\n\"); pthread_mutex_unlock(&amp;lock_core); return eRet; &#125; //设置回调 qc_omx_component_set_callbacks(hComp,callBacks,appData); hnd_index = get_comp_handle_index(componentName); if(hnd_index &gt;= 0) &#123; //保存这个组件句柄 core[cmp_index].inst[hnd_index]= *handle = (OMX_HANDLETYPE) hComp; &#125; else &#123; /*-------下面全是错误处理，忽略------*/ DEBUG_PRINT(\"OMX_GetHandle:NO free slot available to store Component Handle\\n\"); pthread_mutex_unlock(&amp;lock_core); return OMX_ErrorInsufficientResources; &#125; DEBUG_PRINT(\"Component %p Successfully created\\n\",*handle); &#125; else &#123; eRet = OMX_ErrorInsufficientResources; DEBUG_PRINT(\"Component Creation failed\\n\"); &#125; &#125; else &#123; eRet = OMX_ErrorNotImplemented; DEBUG_PRINT(\"library couldnt return create instance fn\\n\"); &#125; &#125; else &#123; eRet = OMX_ErrorNotImplemented; DEBUG_PRINT(\"ERROR: Already another instance active ;rejecting \\n\"); &#125; &#125; else &#123; eRet = OMX_ErrorBadParameter; DEBUG_PRINT(\"\\n OMX_GetHandle: NULL handle \\n\"); &#125; pthread_mutex_unlock(&amp;lock_core); return eRet;&#125; 这里的有个数组：extern omx_core_cb_type core[]，是从别的文件中声明过来的全局变量，其中包含了各种编解码器的名称和一些属性的结构体。结构体定义位于hardware/qcom/media/mm-core/omxcore/src/common/qc_omx_core.h： 123456789typedef struct _omx_core_cb_type&#123; char* name;// Component name 组件名 create_qc_omx_component fn_ptr;// create instance fn ptr 创建实例函数指针 void* inst[OMX_COMP_MAX_INST];// Instance handle 实例句柄 void* so_lib_handle;// So Library handle so库句柄 char* so_lib_name;// so directory so名 char* roles[OMX_CORE_MAX_CMP_ROLES];// roles played 组件扮演的角色&#125;omx_core_cb_type; 但是给omx_core_cb_type core[]这个结构体数组复制的地方要根据不同型号进行选取，我们进入hardware/qcom/media/mm-core/src下面，会看到有许多型号，7627A、7630、8084、8226、8610、8660等等。比如这个8974的，位于hardware/qcom/media/mm-core/src/8974/qc_registry_table_android.c中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162omx_core_cb_type core[] =&#123; //avc/h264解码器 &#123; \"OMX.qcom.video.decoder.avc\", NULL, // Create instance function // Unique instance handle &#123; NULL &#125;, NULL, // Shared object library handle \"libOmxVdec.so\", &#123; \"video_decoder.avc\" &#125; &#125;, //mpeg4解码器&#123; \"OMX.qcom.video.decoder.mpeg4\", NULL, // Create instance function // Unique instance handle &#123; NULL &#125;, NULL, // Shared object library handle \"libOmxVdec.so\", &#123; \"video_decoder.mpeg4\" &#125; &#125;, //wmv解码器 &#123; \"OMX.qcom.video.decoder.wmv\", NULL, // Create instance function // Unique instance handle &#123; NULL &#125;, NULL, // Shared object library handle \"libOmxVdec.so\", &#123; \"video_decoder.vc1\" &#125; &#125;, //hevc/h265编码器 &#123; \"OMX.qcom.video.encoder.hevc\", NULL, // Create instance function // Unique instance handle &#123; NULL &#125;, NULL, // Shared object library handle \"libOmxVencHevc.so\", &#123; \"video_encoder.hevc\" &#125; &#125;, ...太多了，省略... &#125; 上面就是对编解码器相关信息的注册。 在OMX_GetHandle()函数中，根据omx_core_cb_type core[]数组中动态库的名称，动态打开各个编解码实现的动态库,然后进行初始化。 2.4.1、一个qcom OpenMax IL组件的实现 高通平台对于编解码组件的处理都比较集中，不像TI那么分散和细致。一个组件实现都要包含Qc_omx_component.h头文件，位于很多地方，如hardware/qcom/media/mm-core/inc，要实现里面相关纯虚函数。当一个组件被创建后要初始化，就要实现component_init(OMX_IN OMX_STRING componentName)方法。 举个例子，依然以Video4linux2平台，进入hardware/qcom/media/mm-video-v4l2/vidc/vdec/src查看视频解码相关组件。比如我们看看解码组件omx_vdec_hevc.cpp，查看component_init方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413#ifdef VENUS_HEVC#define DEVICE_NAME \"/dev/video/venus_dec\"#else#define DEVICE_NAME \"/dev/video/q6_dec\"#endif/* ====================================================================== FUNCTION omx_vdec::ComponentInit DESCRIPTION Initialize the component. PARAMETERS ctxt -- Context information related to the self. id -- Event identifier. This could be any of the following: 1. Command completion event 2. Buffer done callback event 3. Frame done callback event RETURN VALUE None. ========================================================================== */OMX_ERRORTYPE omx_vdec::component_init(OMX_STRING role)&#123; OMX_ERRORTYPE eRet = OMX_ErrorNone; struct v4l2_fmtdesc fdesc; struct v4l2_format fmt; struct v4l2_requestbuffers bufreq; struct v4l2_control control; unsigned int alignment = 0,buffer_size = 0; int fds[2]; int r,ret=0; bool codec_ambiguous = false; //打开设备文件\"/dev/video/venus_dec\"或\"/dev/video/q6_dec\" OMX_STRING device_name = (OMX_STRING)DEVICE_NAME; ...... drv_ctx.video_driver_fd = open(device_name, O_RDWR); ...... //如果是一个打开成功后，为什么要再次打开？？excuse me ？ if (drv_ctx.video_driver_fd == 0) &#123; drv_ctx.video_driver_fd = open(device_name, O_RDWR); &#125; //打开设备文件失败 if (drv_ctx.video_driver_fd &lt; 0) &#123; DEBUG_PRINT_ERROR(\"Omx_vdec::Comp Init Returning failure, errno %d\", errno); return OMX_ErrorInsufficientResources; &#125; drv_ctx.frame_rate.fps_numerator = DEFAULT_FPS;//帧率分子 drv_ctx.frame_rate.fps_denominator = 1;//帧率分母 //创建一个异步线程，执行async_message_thread函数，对输入端进行设置 ret = pthread_create(&amp;async_thread_id,0,async_message_thread,this); //创建线程失败，则关闭设备文件 if (ret &lt; 0) &#123; close(drv_ctx.video_driver_fd); DEBUG_PRINT_ERROR(\"Failed to create async_message_thread\"); return OMX_ErrorInsufficientResources; &#125; ...... // Copy the role information which provides the decoder kind //将组建角色名字copy进设备驱动上下文结构体的kind属性 strlcpy(drv_ctx.kind,role,128); //如果是mpeg4解码组件 if (!strncmp(drv_ctx.kind,\"OMX.qcom.video.decoder.mpeg4\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.mpeg4\",\\ OMX_MAX_STRINGNAME_SIZE); drv_ctx.timestamp_adjust = true; drv_ctx.decoder_format = VDEC_CODECTYPE_MPEG4; eCompressionFormat = OMX_VIDEO_CodingMPEG4; output_capability=V4L2_PIX_FMT_MPEG4; /*Initialize Start Code for MPEG4*/ codec_type_parse = CODEC_TYPE_MPEG4; m_frame_parser.init_start_codes (codec_type_parse); ...... //如果是mpeg2解码组件 &#125; else if (!strncmp(drv_ctx.kind,\"OMX.qcom.video.decoder.mpeg2\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.mpeg2\",\\ OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_MPEG2; output_capability = V4L2_PIX_FMT_MPEG2; eCompressionFormat = OMX_VIDEO_CodingMPEG2; /*Initialize Start Code for MPEG2*/ codec_type_parse = CODEC_TYPE_MPEG2; m_frame_parser.init_start_codes (codec_type_parse); ...... //如果是h263解码组件 &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.h263\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.h263\",OMX_MAX_STRINGNAME_SIZE); DEBUG_PRINT_LOW(\"H263 Decoder selected\"); drv_ctx.decoder_format = VDEC_CODECTYPE_H263; eCompressionFormat = OMX_VIDEO_CodingH263; output_capability = V4L2_PIX_FMT_H263; codec_type_parse = CODEC_TYPE_H263; m_frame_parser.init_start_codes (codec_type_parse); ...... //如果是divx311... &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.divx311\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.divx\",OMX_MAX_STRINGNAME_SIZE); DEBUG_PRINT_LOW (\"DIVX 311 Decoder selected\"); drv_ctx.decoder_format = VDEC_CODECTYPE_DIVX_3; output_capability = V4L2_PIX_FMT_DIVX_311; eCompressionFormat = (OMX_VIDEO_CODINGTYPE)QOMX_VIDEO_CodingDivx; codec_type_parse = CODEC_TYPE_DIVX; m_frame_parser.init_start_codes (codec_type_parse); eRet = createDivxDrmContext(); if (eRet != OMX_ErrorNone) &#123; DEBUG_PRINT_ERROR(\"createDivxDrmContext Failed\"); return eRet; &#125; //如果是divx4... &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.divx4\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.divx\",OMX_MAX_STRINGNAME_SIZE); DEBUG_PRINT_ERROR (\"DIVX 4 Decoder selected\"); drv_ctx.decoder_format = VDEC_CODECTYPE_DIVX_4; output_capability = V4L2_PIX_FMT_DIVX; eCompressionFormat = (OMX_VIDEO_CODINGTYPE)QOMX_VIDEO_CodingDivx; codec_type_parse = CODEC_TYPE_DIVX; codec_ambiguous = true; m_frame_parser.init_start_codes (codec_type_parse); eRet = createDivxDrmContext(); if (eRet != OMX_ErrorNone) &#123; DEBUG_PRINT_ERROR(\"createDivxDrmContext Failed\"); return eRet; &#125; //如果是divx... &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.divx\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.divx\",OMX_MAX_STRINGNAME_SIZE); DEBUG_PRINT_ERROR (\"DIVX 5/6 Decoder selected\"); drv_ctx.decoder_format = VDEC_CODECTYPE_DIVX_6; output_capability = V4L2_PIX_FMT_DIVX; eCompressionFormat = (OMX_VIDEO_CODINGTYPE)QOMX_VIDEO_CodingDivx; codec_type_parse = CODEC_TYPE_DIVX; codec_ambiguous = true; m_frame_parser.init_start_codes (codec_type_parse); eRet = createDivxDrmContext(); if (eRet != OMX_ErrorNone) &#123; DEBUG_PRINT_ERROR(\"createDivxDrmContext Failed\"); return eRet; &#125; //如果是avc/h264... &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.avc\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.avc\",OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_H264; output_capability=V4L2_PIX_FMT_H264; eCompressionFormat = OMX_VIDEO_CodingAVC; codec_type_parse = CODEC_TYPE_H264; m_frame_parser.init_start_codes (codec_type_parse); m_frame_parser.init_nal_length(nal_length); ...... //如果是hevc/h265... &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.hevc\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.hevc\",OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_HEVC; output_capability=V4L2_PIX_FMT_HEVC; eCompressionFormat = (OMX_VIDEO_CODINGTYPE)QOMX_VIDEO_CodingHevc; codec_type_parse = CODEC_TYPE_HEVC; m_frame_parser.init_start_codes (codec_type_parse); m_frame_parser.init_nal_length(nal_length); ... //如果是vc1... &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.vc1\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.vc1\",OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_VC1; eCompressionFormat = OMX_VIDEO_CodingWMV; codec_type_parse = CODEC_TYPE_VC1; output_capability = V4L2_PIX_FMT_VC1_ANNEX_G; m_frame_parser.init_start_codes (codec_type_parse); //如果是wmv... &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.wmv\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.vc1\",OMX_MAX_STRINGNAME_SIZE); drv_ctx.decoder_format = VDEC_CODECTYPE_VC1_RCV; eCompressionFormat = OMX_VIDEO_CodingWMV; codec_type_parse = CODEC_TYPE_VC1; output_capability = V4L2_PIX_FMT_VC1_ANNEX_L; m_frame_parser.init_start_codes (codec_type_parse); //如果是vp8... &#125; else if (!strncmp(drv_ctx.kind, \"OMX.qcom.video.decoder.vp8\",\\ OMX_MAX_STRINGNAME_SIZE)) &#123; strlcpy((char *)m_cRole, \"video_decoder.vp8\",OMX_MAX_STRINGNAME_SIZE); output_capability=V4L2_PIX_FMT_VP8; eCompressionFormat = OMX_VIDEO_CodingVPX; codec_type_parse = CODEC_TYPE_VP8; arbitrary_bytes = false; // 如果是不认识的解码组件，则报错 &#125; else &#123; DEBUG_PRINT_ERROR(\"ERROR:Unknown Component\"); eRet = OMX_ErrorInvalidComponentName; &#125; //如果错误 if (eRet == OMX_ErrorNone) &#123; //设置视频输出编码格式为YUV的一种 drv_ctx.output_format = VDEC_YUV_FORMAT_NV12; //设置颜色编码 OMX_COLOR_FORMATTYPE dest_color_format = (OMX_COLOR_FORMATTYPE) QOMX_COLOR_FORMATYUV420PackedSemiPlanar32m; if (!client_buffers.set_color_format(dest_color_format)) &#123; DEBUG_PRINT_ERROR(\"Setting color format failed\"); eRet = OMX_ErrorInsufficientResources; &#125; //订阅事件 capture_capability= V4L2_PIX_FMT_NV12; ret = subscribe_to_events(drv_ctx.video_driver_fd); if (ret) &#123; DEBUG_PRINT_ERROR(\"Subscribe Event Failed\"); return OMX_ErrorInsufficientResources; &#125; struct v4l2_capability cap; //设置查询能力标志位 ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_QUERYCAP, &amp;cap); if (ret) &#123; DEBUG_PRINT_ERROR(\"Failed to query capabilities\"); /*TODO: How to handle this case */ &#125; else &#123; DEBUG_PRINT_HIGH(\"Capabilities: driver_name = %s, card = %s, bus_info = %s,\" \" version = %d, capabilities = %x\", cap.driver, cap.card, cap.bus_info, cap.version, cap.capabilities); &#125; ret=0; fdesc.type=V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE; fdesc.index=0; while (ioctl(drv_ctx.video_driver_fd, VIDIOC_ENUM_FMT, &amp;fdesc) == 0) &#123; DEBUG_PRINT_HIGH(\"fmt: description: %s, fmt: %x, flags = %x\", fdesc.description, fdesc.pixelformat, fdesc.flags); fdesc.index++; &#125; fdesc.type=V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE; fdesc.index=0; while (ioctl(drv_ctx.video_driver_fd, VIDIOC_ENUM_FMT, &amp;fdesc) == 0) &#123; DEBUG_PRINT_HIGH(\"fmt: description: %s, fmt: %x, flags = %x\", fdesc.description, fdesc.pixelformat, fdesc.flags); fdesc.index++; &#125; update_resolution(320, 240); fmt.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE; fmt.fmt.pix_mp.height = drv_ctx.video_resolution.frame_height; fmt.fmt.pix_mp.width = drv_ctx.video_resolution.frame_width; fmt.fmt.pix_mp.pixelformat = output_capability; ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_S_FMT, &amp;fmt); if (ret) &#123; /*TODO: How to handle this case */ DEBUG_PRINT_ERROR(\"Failed to set format on output port\"); &#125; DEBUG_PRINT_HIGH(\"Set Format was successful\"); //如果有歧义的解码组件 if (codec_ambiguous) &#123; if (output_capability == V4L2_PIX_FMT_DIVX) &#123; struct v4l2_control divx_ctrl; if (drv_ctx.decoder_format == VDEC_CODECTYPE_DIVX_4) &#123; divx_ctrl.value = V4L2_MPEG_VIDC_VIDEO_DIVX_FORMAT_4; &#125; else if (drv_ctx.decoder_format == VDEC_CODECTYPE_DIVX_5) &#123; divx_ctrl.value = V4L2_MPEG_VIDC_VIDEO_DIVX_FORMAT_5; &#125; else &#123; divx_ctrl.value = V4L2_MPEG_VIDC_VIDEO_DIVX_FORMAT_6; &#125; divx_ctrl.id = V4L2_CID_MPEG_VIDC_VIDEO_DIVX_FORMAT; ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_S_CTRL, &amp;divx_ctrl); if (ret) &#123; DEBUG_PRINT_ERROR(\"Failed to set divx version\"); &#125; &#125; else &#123; DEBUG_PRINT_ERROR(\"Codec should not be ambiguous\"); &#125; &#125; //解码相关参数设置 fmt.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE; fmt.fmt.pix_mp.height = drv_ctx.video_resolution.frame_height; fmt.fmt.pix_mp.width = drv_ctx.video_resolution.frame_width; fmt.fmt.pix_mp.pixelformat = capture_capability; ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_S_FMT, &amp;fmt); if (ret) &#123; /*TODO: How to handle this case */ DEBUG_PRINT_ERROR(\"Failed to set format on capture port\"); &#125; DEBUG_PRINT_HIGH(\"Set Format was successful\"); if (secure_mode) &#123; control.id = V4L2_CID_MPEG_VIDC_VIDEO_SECURE; control.value = 1; DEBUG_PRINT_LOW(\"Omx_vdec:: calling to open secure device %d\", ret); ret=ioctl(drv_ctx.video_driver_fd, VIDIOC_S_CTRL,&amp;control); if (ret) &#123; DEBUG_PRINT_ERROR(\"Omx_vdec:: Unable to open secure device %d\", ret); close(drv_ctx.video_driver_fd); return OMX_ErrorInsufficientResources; &#125; &#125; /*Get the Buffer requirements for input and output ports*/ //获得输入和输出的缓冲条件 drv_ctx.ip_buf.buffer_type = VDEC_BUFFER_TYPE_INPUT; drv_ctx.op_buf.buffer_type = VDEC_BUFFER_TYPE_OUTPUT; if (secure_mode) &#123; drv_ctx.op_buf.alignment=SZ_1M; drv_ctx.ip_buf.alignment=SZ_1M; &#125; else &#123; drv_ctx.op_buf.alignment=SZ_4K; drv_ctx.ip_buf.alignment=SZ_4K; &#125; drv_ctx.interlace = VDEC_InterlaceFrameProgressive; drv_ctx.extradata = 0; drv_ctx.picture_order = VDEC_ORDER_DISPLAY; control.id = V4L2_CID_MPEG_VIDC_VIDEO_OUTPUT_ORDER; control.value = V4L2_MPEG_VIDC_VIDEO_OUTPUT_ORDER_DISPLAY; ret = ioctl(drv_ctx.video_driver_fd, VIDIOC_S_CTRL, &amp;control); drv_ctx.idr_only_decoding = 0; m_state = OMX_StateLoaded;#ifdef DEFAULT_EXTRADATA if (eRet == OMX_ErrorNone &amp;&amp; !secure_mode) enable_extradata(DEFAULT_EXTRADATA, true, true);#endif eRet=get_buffer_req(&amp;drv_ctx.ip_buf); DEBUG_PRINT_HIGH(\"Input Buffer Size =%d\",drv_ctx.ip_buf.buffer_size); get_buffer_req(&amp;drv_ctx.op_buf); //如果解码器格式是h264或者hevc/h265 if (drv_ctx.decoder_format == VDEC_CODECTYPE_H264 || drv_ctx.decoder_format == VDEC_CODECTYPE_HEVC) &#123; h264_scratch.nAllocLen = drv_ctx.ip_buf.buffer_size; h264_scratch.pBuffer = (OMX_U8 *)malloc (drv_ctx.ip_buf.buffer_size); h264_scratch.nFilledLen = 0; h264_scratch.nOffset = 0; if (h264_scratch.pBuffer == NULL) &#123; DEBUG_PRINT_ERROR(\"h264_scratch.pBuffer Allocation failed \"); return OMX_ErrorInsufficientResources; &#125; &#125; //如果解码器格式是h264 if (drv_ctx.decoder_format == VDEC_CODECTYPE_H264) &#123; if (m_frame_parser.mutils == NULL) &#123; m_frame_parser.mutils = new H264_Utils(); if (m_frame_parser.mutils == NULL) &#123; DEBUG_PRINT_ERROR(\"parser utils Allocation failed \"); eRet = OMX_ErrorInsufficientResources; &#125; else &#123; m_frame_parser.mutils-&gt;initialize_frame_checking_environment(); m_frame_parser.mutils-&gt;allocate_rbsp_buffer (drv_ctx.ip_buf.buffer_size); &#125; &#125; //创建一个h264流的解析器 h264_parser = new h264_stream_parser(); if (!h264_parser) &#123; DEBUG_PRINT_ERROR(\"ERROR: H264 parser allocation failed!\"); eRet = OMX_ErrorInsufficientResources; &#125; &#125; //打开一个管道，读写端保存进fds数组 if (pipe(fds)) &#123; DEBUG_PRINT_ERROR(\"pipe creation failed\"); eRet = OMX_ErrorInsufficientResources; &#125; else &#123; int temp1[2]; if (fds[0] == 0 || fds[1] == 0) &#123; if (pipe (temp1)) &#123; DEBUG_PRINT_ERROR(\"pipe creation failed\"); return OMX_ErrorInsufficientResources; &#125; //close (fds[0]); //close (fds[1]); fds[0] = temp1 [0]; fds[1] = temp1 [1]; &#125; //输入/读 m_pipe_in = fds[0]; //输出/写 m_pipe_out = fds[1]; //创建一个工作线程，调用omx开始处理解码，并进行i/o操作 r = pthread_create(&amp;msg_thread_id,0,message_thread,this); if (r &lt; 0) &#123; DEBUG_PRINT_ERROR(\"component_init(): message_thread creation failed\"); eRet = OMX_ErrorInsufficientResources; &#125; &#125; &#125; //没有错误，然后收尾 if (eRet != OMX_ErrorNone) &#123; DEBUG_PRINT_ERROR(\"Component Init Failed\"); DEBUG_PRINT_HIGH(\"Calling VDEC_IOCTL_STOP_NEXT_MSG\"); (void)ioctl(drv_ctx.video_driver_fd, VDEC_IOCTL_STOP_NEXT_MSG, NULL); DEBUG_PRINT_HIGH(\"Calling close() on Video Driver\"); close (drv_ctx.video_driver_fd); drv_ctx.video_driver_fd = -1; &#125; else &#123; DEBUG_PRINT_HIGH(\"omx_vdec::component_init() success\"); &#125; //memset(&amp;h264_mv_buff,0,sizeof(struct h264_mv_buffer)); return eRet;&#125; 这个是解码组件的初始化实现。我们能够看出和TI的差距挺大的。步骤大概如下（maybe wrong）： ☯ 打开media相关设备文件☯ 创建一个异步线程，执行async_message_thread函数，对输入端进行设置☯ 根据解码器role名配置相关属性☯ 对视频解码相关基本配置进行设置☯ 创建一个管道，然后再开一个个工作线程，调用omx开始处理解码，并进行i/o操作 （三）、Android中OpenMax的实现3.1、android MediaCodec ACodec3.1.1、MediaCodecMediaCodec类可用于访问Android底层的媒体编解码器，例如，编码/解码组件。它是Android为多媒体支持提供的底层接口的一部分（通常与MediaExtractor, MediaSync, MediaMuxer, MediaCrypto, MediaDrm, Image, Surface, 以及AudioTrack一起使用）。从广义上讲，一个编解码器通过处理输入数据来产生输出数据。它通过异步方式处理数据，并且使用了一组输入输出buffers。在简单层面，请求（或接收）到一个空的输入buffer，向里面填满数据并将它传递给编解码器处理。这个编解码器将使用完这些数据并向所有空的输出buffer中的一个填充这些数据。最终，请求（或接受）到一个填充了数据的buffer,可以使用其中的数据内容，并且在使用完后将其释放回编解码器。 1、 Data Types编解码器处理三种类型的数据：压缩数据，原始音频数据，原始视频数据。上述三种数据都可以通过ByteBuffers进行处理，但需要为原始视频数据提供一个Surface来提高编解码性能。 压缩缓存（Compressed Buffers）：输入缓冲区(解码器)和输出缓冲区(编码器)包含压缩数据格式的类型。 原始音频缓存（Raw Audio Buffers）：原始音频缓冲区包含整个PCM音频帧数据。 原始视频缓存（Raw Video Buffers）：ByteBuffer模式视频缓冲区根据他们的颜色格式布局。视频编解码器可能支持三种类型的色彩格式：1)、native raw video format：被COLOR_FormatSurface标记，其可与输入或输出Surface一起使用；2)、flexible YUV buffers（如COLOR_FormatYUV420Flexible），可以与输入/输出Surface一起使用, ByteBuffer模式下可以通过调用getInput/OutputImage(int)方法进行使用；3)、通常只在ByteBuffer模式下被支持。由供应商指定，可以使用 getInput/OutputImage(int)方法。 2、States在编解码器的生命周期内有三种理论状态：停止态-Stopped、执行态-Executing、释放态-Released。停止状态（Stopped）包括了三种子状态：未初始化（Uninitialized）、配置（Configured）、错误（Error）。执行状态（Executing）在概念上会经历三种子状态：刷新（Flushed）、运行（Running）、流结束（End-of-Stream）。 使用工厂方法之一创建一个编解码器的时候，是处于Uninitialized状态。首先，需要通过configure(…)方法配置它，以此进入Configured 状态。然后，通过调用start()方法转入Executing 状态。在这个状态下可以通过上述buffer队列操作过程数据。调用start()方法后立即进入Flushed状态，此时编解码器拥有所有的缓存。一旦第一个输入缓存被移出队列，编解码器就转入运行子状态，这种状态占据了编解码器的大部分生命周期。当将一个带有end-of-stream marker标记的输入缓存入队列时，编解码器将转入流结束子状态。可在Executing状态的任何时候通过调用flush()。调用stop()方法返回编解码器的Uninitialized 状态，因此这个编解码器需要再次configured 。当使用完编解码器后，必须调用release()方法释放其资源。 3、Data Processing每一个编解码器包含一组输入和输出缓存，这些缓存在API调用中通过buffer-ID进行引用。当成功调用start()方法后客户端将不会拥有输入或输出buffers。在同步模式下，通过调用dequeueInput/OutputBuffer(…) 方法从编解码器获得一个输入或输出buffer；在异步模式下，可以通过MediaCodec.Callback.onInput/OutputBufferAvailable(…)的回调方法自动地获得可用的buffers。在获得一个输入buffe后，填充数据，利用queueInputBuffer/queueSecureInputBuffer方法将其提交给编解码器，不要提交多个具有相同时间戳的输入bufers（除非它是也被同样标记的codec-specific data，因为codec-specific data缓冲的时间戳无意义）。a. Asynchronous Processing using Buffers 从Android 5.0开始，首选的方法是调用configure之前通过设置回调异步地处理数据。异步模式稍微改变了状态转换方式，因为必须在调用flush()方法后再调用start()方法才能使编解码器的状态转换为Running子状态并开始接收输入buffers。同样，初始调用start方法将编解码器的状态直接变化为Running 子状态并通过回调方法开始传递可用的输入buufers。b. Synchronous Processing using Buffers从Android 5.0开始，即使在同步模式下使用编解码器，也应该通过getInput/OutputBuffer(int) 和/或 getInput/OutputImage(int) 方法检索输入和输出buffers。 3.1.2、ACodec1、ACodec消息机制： ACodec有一个BaseState和派生出来的其他State，如 UninitializedState, LoadedToIdleState, ExecutingState等。当有消息过来时，如果派生类有重写的方法，则会调到重写的方法，如果没有，则会调到BaseState的 ACodec继承自AHierarchicalStateMachine类，该类用于将收到的消息传递给哪个state。 ACodec收到的消息分两种，一种是MediaCodec传过来的，对应onMessageReceived方法；另一种是OMX Component传过来的，对应onOMXMessage方法。而onOMXMessage里面又分了4种情况来调用不同的方法。（EVENT、EMPTY_BUFFER_DONE、FILL_BUFFER_DONE和FRAME_RENDERED） 2、MediaCodec与ACodec的通知： OMX的组件解码之后，ACodec::BaseState:: onOMXFillBufferDone (…)会被回调，去取得解码后的数据。然后会在onOMXFillBufferDone中调用notify通知MediaCodec，发给MediaCodec的消息形如notify-&gt;setInt32(“what”, CodecBase::kWhatDrainThisBuffer); MediaCodec收到ACodec发的消息之后会updateBuffers(kPortIndexOutput, msg) 进行更新，同时调用onOutputBufferAvailable()中通知NuPlayer::Decoder有可用的output buffer。 3、ACodec有三种端口模式状态，其会根据当前处于哪个状态来决定buffer如何处理： KEEP_BUFFERS：当ACodec处于BaseState或者收到OnInputBufferFilled消息但是buffer里面没有填充有效的数据时，ACodec握有的buffer不会送到OMX 组件； RESUBMIT_BUFFERS：当ACodec处于ExecutingState或者处于OutputPortSettingChangedState但是当前是input口的buffer时，ACodec将握有的buffer送给OMX 组件； FREE_BUFFERS：当ACodec处于OutputPortSettingChangedState并且当前是output口的buffer时，ACodec将握有的buffer free。 4、stagefright类的调用关系： OMXNodeInstance负责创建并维护不同的实例，这些实例以node作为唯一标识。这样播放器中每个ACodec在OMX服务端都对应有了自己的OMXNodeInstance实例。 OMXMaster用来维护底层软硬件解码库，根据OMXNodeInstance中想要的解码器来创建解码实体组件。 OMXPluginBase负责加载组件库，创建组件实例，由OMXMaster管理。Android原生提供的组件都是由SoftOMXPlugin类来管理，这个类就是继承自OMXPluginBase。（Android源码提供了一些软件解码和编码的组件，它们被抽象为SoftOMXComponent） OMXClient是客户端用来与OMX IL进行通信的。 内部结构CallbackDispatcher作用是用来接收回调函数的消息 OMXNodeInstance + CallbackDispatcher = 合作完成不同实例的消息处理任务 5、ACodec同OMXNodeInstance的消息传递： ACodec将CodecObserver observer对象通过omx-&gt;allocateNode()传递到OMXNodeInstance。 OMXNodeInstance将kCallbacks(OnEvent,OnEmptyBufferDone,OnFillBufferDone)传递给OMX Component 当OMX Component有消息notify上来时，OMXNodeInstance最先收到，然后调用OMX.cpp。将消息在OMX.cpp里面将OMX Component thread转换到CallbackDispatcher线程中处理。CallbackDispatcher又将消息反调到OMXNodeInstance. 最后调用CodecObserver 的onMessage()回到ACodec中 6、 ACodec与OMX组件的关系 ACodec ，CodecObserver和OMXNodeInstance是一一对应的，简单的可以理解它们3个构成了OpenMAX IL的一个Component，每一个node就是一个codec在OMX服务端的标识。当然还有CallbackDispatcher，用于处理codec过来的消息，通过它的post/loop/dispatch来发起接收，从OMX.cpp发送消息，最终通过OMXNodeInstance::onMessage -&gt; CodecObserver::onMessage -&gt; ACodec::onMessage一路往上，当然消息的来源是因为我们有向codec注册OMXNodeInstance::kCallbacks。 而在OMXPluginBase创建组件实例的时候，需要传递一个callback给组件，这个callback用于接收组件的消息，它的实现是在OMXNodeInstance.cpp中。而kcallbacks是OMXNodeInstance的静态成员变量，它内部的三个函数指针分别指向了OMXNodeInstance的三个静态方法，也即是这三个方法与组件进行着消息传递  对于NuPlayer来说，它并不直接接触解码组件，而是通过创建ACodec来和组件交互。ACode内部有一个id，这个id对应于一个OMXNodeInstance。OMX对象中会对产生的每一个OMXNodeInstance分配一个唯一的node_id。每一个OMXNodeInstance内部又保存着组件实例的指针【OMX_HANDLETYPE mHandle;】，通过这个指针就可以和组件进行交互。交互的流程为：ACodec → OMX → OMXNodeInstance → COMPONENT。 8、组件的管理 对组件的管理可以总结为：通过OMXMaster加载libstagefrighthw.so库文件，创建OMXPluginBase【即创建继承此类的组件对象】，通过这个类来管理组件。 Android源码提供了一些软件解码和编码的组件，它们被抽象为SoftOMXComponent。OMXPluginBase扮演者组件的管理者。它负责加载组件库，创建组件实例。而OMXMaster则管理着OMXPluginBase，Android原生提供的组件都是由SoftOMXPlugin类来管理，这个类就是继承自OMXPluginBase。 对于厂商来说，如果要实现自己的组件管理模块，需要通过继承实现OMXPluginBase，并将之编译为libstagefrighthw.so。在OMXMaster中会加载这个库文件，然后调用其createOMXPlugin方法获得一个OMXPluginBase指针，然后将其加入OMXPluginBase列表以及与组件名相关的map 【mPluginByComponentName】中，后续都会通过OMXPluginBase来管理组件。 3.2、音视频解码数据处理数据处理请参考： 音视频解码数据处理 3.3、高通实现（参考高通文档）OpenMAX Integration Layer Video Encoder for Linux Android（高通文档）OpenMAX Integration Layer Video Decoder for Linux Android（高通文档） （四）、参考资料(特别感谢各位前辈的分析和图示)：Android多媒体开发(五)—-OpenMax简介Android多媒体开发(六)—-Android中OpenMax的实现(preview)Android多媒体开发(七)—-Android中OpenMax的实现android ACodec MediaCodec NuPlayer flow - CSDN博客android MediaCodec ACodec - CSDN博客","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Display System（5）：Android Display System 系统分析之Display Driver Architecture","slug":"Android Display System（5）：Android Display System 系统分析之Display Driver Architecture","date":"2018-08-29T16:00:00.000Z","updated":"2018-06-20T15:15:33.034Z","comments":true,"path":"2018/08/30/Android Display System（5）：Android Display System 系统分析之Display Driver Architecture/","link":"","permalink":"http://zhoujinjian.cc/2018/08/30/Android Display System（5）：Android Display System 系统分析之Display Driver Architecture/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】 【特别感谢 - 高通Android平台-应用空间操作framebuffer dump LCD总结】【特别感谢 - linux qcom LCD framwork】【特别感谢 - msm8610 lcd driver code analysis】 Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) 🌀🌀：专注于Linux &amp;&amp; Android Multimedia（Camera、Video、Audio、Display）系统分析与研究 【Android Display System 系统分析系列】：【Android Display System（1）：Android 7.1.2 (Android N) Android Graphics 系统 分析】【Android Display System（2）：Android Display System 系统分析之Android EGL &amp;&amp; OpenGL】【Android Display System（3）：Android Display System 系统分析之HardwareRenderer.draw()绘制流程分析】【Android Display System（4）：Android Display System 系统分析之Gralloc &amp;&amp; HWComposer模块分析】【Android Display System（5）：Android Display System 系统分析之Display Driver Architecture】 路径：kernel\\msm-3.18\\drivers\\video\\msm\\mdss MDSS driver software block diagram mdss_fb → Top-level IOCTL/native framebufferinterface mdss_mdp.c → MDP resources(clocks/irq/bus-bw/power) mdss_mdp_overlay → Overlay/DMA top-levelAPI mdss_mdp_ctl → Controls the hardware abstraction to club the (LM + DSPP + Ping-pong +interface) mdss_mdp_pipe → SRC pipe related handling mdss_mdp_intf_cmd/mdss_mdp_intf_video/mdss_mdp_intf_writeback → MDP panelinterface relatedhandling mdss_mdp_pp → Postprocessing related implementation mdss_mdp_rotator → Rotator APIs (overlay_set/overlay_playinterface) mdss_mdp_pp.c → Postprocessing relatedmaterial 注：首先说明，由于博主不是kernel开发方向的，可能理解不够透彻，还请看官见谅，主要是为了理解Kernel Display原理。为了加深对显示屏工作原理的理解，首先先看两个操作LCD显示屏的例子绪论（总体架构图）： （一）、直接操作framebuffer显示图像1.1、直接操作framebuffer显示图像1.1.1、源代码panel_test.c 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#include &lt;unistd.h&gt; #include &lt;stdio.h&gt; #include &lt;fcntl.h&gt; #include &lt;linux/fb.h&gt; #include &lt;sys/mman.h&gt; #include &lt;stdlib.h&gt;#include \"yellow_face.zif\"int main() &#123; int fbfd = 0; struct fb_var_screeninfo vinfo; struct fb_fix_screeninfo finfo; struct fb_cmap cmapinfo; long int screensize = 0; char *fbp = 0; int x = 0, y = 0; long int location = 0; int b,g,r; // Open the file for reading and writing fbfd = open(\"/dev/graphics/fb0\", O_RDWR,0); // 打开Frame Buffer设备 if (fbfd &lt; 0) &#123; printf(\"Error: cannot open framebuffer device.%x\\n\",fbfd); exit(1); &#125; printf(\"The framebuffer device was opened successfully.\\n\"); // Get fixed screen information if (ioctl(fbfd, FBIOGET_FSCREENINFO, &amp;finfo)) &#123; // 获取设备固有信息 printf(\"Error reading fixed information.\\n\"); exit(2); &#125; printf(\"\\ntype:0x%x\\n\", finfo.type ); // FrameBuffer 类型,如0为象素 printf(\"visual:%d\\n\", finfo.visual ); // 视觉类型：如真彩2，伪彩3 printf(\"line_length:%d\\n\", finfo.line_length ); // 每行长度 printf(\"\\nsmem_start:0x%lx,smem_len:%u\\n\", finfo.smem_start, finfo.smem_len ); // 映象RAM的参数 printf(\"mmio_start:0x%lx ,mmio_len:%u\\n\", finfo.mmio_start, finfo.mmio_len ); // Get variable screen information if (ioctl(fbfd, FBIOGET_VSCREENINFO, &amp;vinfo)) &#123; // 获取设备可变信息 printf(\"Error reading variable information.\\n\"); exit(3); &#125; printf(\"%dx%d, %dbpp,xres_virtual=%d,yres_virtual=%dvinfo.xoffset=%d,vinfo.yoffset=%d\\n\", vinfo.xres, vinfo.yres, vinfo.bits_per_pixel,vinfo.xres_virtual,vinfo.yres_virtual,vinfo.xoffset,vinfo.yoffset); screensize = finfo.line_length * vinfo.yres_virtual; // Map the device to memory 通过mmap系统调用将framebuffer内存映射到用户空间,并返回映射后的起始地址 fbp = (char *)mmap(0, screensize, PROT_READ | PROT_WRITE, MAP_SHARED,fbfd, 0); if ((int)fbp == -1) &#123; printf(\"Error: failed to map framebuffer device to memory.\\n\"); exit(4); &#125; printf(\"The framebuffer device was mapped to memory successfully.\\n\"); /***************exampel 1**********************/ b = 10; g = 100; r = 100; for ( y = 0; y &lt; 340; y++ ) for ( x = 0; x &lt; 420; x++ ) &#123; location = (x+100) * (vinfo.bits_per_pixel/8) + (y+100) * finfo.line_length; if ( vinfo.bits_per_pixel == 32 ) &#123; // *(fbp + location) = b; // Some blue *(fbp + location + 1) = g; // A little green *(fbp + location + 2) = r; // A lot of red *(fbp + location + 3) = 0; // No transparency &#125; &#125;/*****************exampel 1********************//*****************exampel 2********************/ unsigned char *pTemp = (unsigned char *)fbp; int i, j; //起始坐标(x,y),终点坐标(right,bottom) x = 400; y = 400; int right = 700;//vinfo.xres; int bottom = 1000;//vinfo.yres; for(i=y; i&lt; bottom; i++) &#123; for(j=x; j&lt;right; j++) &#123; unsigned short data = yellow_face_data[(((i-y) % 128) * 128) + ((j-x) %128)]; pTemp[i*finfo.line_length + (j*4) + 2] = (unsigned char)((data &amp; 0xF800) &gt;&gt; 11 &lt;&lt; 3); pTemp[i*finfo.line_length + (j*4) + 1] = (unsigned char)((data &amp; 0x7E0) &gt;&gt; 5 &lt;&lt; 2); pTemp[i*finfo.line_length + (j*4) + 0] = (unsigned char)((data &amp; 0x1F) &lt;&lt; 3); &#125; &#125; /*****************exampel 2********************/ //note：vinfo.xoffset =0 vinfo.yoffset =0 否则FBIOPAN_DISPLAY不成功 if (ioctl(fbfd, FBIOPAN_DISPLAY, &amp;vinfo)) &#123; printf(\"Error FBIOPAN_DISPLAY information.\\n\"); exit(5); &#125; sleep(10); munmap(fbp,finfo.smem_len);//finfo.smem_len == screensize == finfo.line_length * vinfo.yres_virtual close(fbfd); return 0; &#125; Android.mk 12345678910111213141516171819# Copyright 2006-2014 The Android Open Source ProjectLOCAL_PATH:= $(call my-dir)include $(CLEAR_VARS)LOCAL_SRC_FILES:= panel_test.cLOCAL_SHARED_LIBRARIES := $(common_libs) libqdutils libdl liblog libbase libcutilsLOCAL_C_INCLUDES := $(common_includes) $(kernel_includes)LOCAL_ADDITIONAL_DEPENDENCIES := $(common_deps) $(kernel_deps)LOCAL_MODULE := panel_testLOCAL_CFLAGS := -Werrorinclude $(BUILD_EXECUTABLE)include $(call first-makefiles-under,$(LOCAL_PATH)) yellow_face.zifyellow_face.zif 1.1.2、编译测试编译会生成panel_test，然后进行测试。 注意事项：1、adb shell stop 杀掉surfaceflinger 之后在测试；2、设置背光 echo 255 &gt; /sys/class/leds/lcd-backlight/brightness 1、连接adb2、adb push panel_test system/bin3、进入adb shell4、stop5、echo 255 &gt; /sys/class/leds/lcd-backlight/brightness6、system/bin/panel_test 1.1.3、显示效果 1.1.4、视频（加深理解、自☯备☯梯☯子）Android Frame Buffer and Screen Shots TutorialMplayer on Android Through Chroot and Frame Buffer 1.1.5、驱动总体概览图 （二）、FrameBuffer驱动程序分析FrameBuffer通常作为LCD控制器或者其他显示设备的驱动，FrameBuffer驱动是一个字符设备，设备节点是/dev/fbX（Android 设备为/dev/graphics/fb0），主设备号为29，次设备号递增，用户可以将Framebuffer看成是显示内存的一个映像，将其映射到进程地址空间之后，就可以直接进行读写操作，而写操作可以立即反应在屏幕上。这种操作是抽象的，统一的。用户不必关心物理显存的位置、换页机制等等具体细节。这些都是由Framebuffer设备驱动来完成的。Framebuffer设备为上层应用程序提供系统调用，也为下一层的特定硬件驱动提供接口；那些底层硬件驱动需要用到这儿的接口来向系统内核注册它们自己。 Linux中的PCI设备可以将其控制寄存器映射到物理内存空间，而后，对这些控制寄存器的访问变成了对理内存的访问，因此，这些寄存器又被称为”memio”。一旦被映射到物理内存，Linux的普通进程就可以通过mmap将这些内存I/O映射到进程地址空间，这样就可以直接访问这些寄存器了。 FrameBuffer设备属于字符设备，采用了文件层—驱动层的接口方式，Linux为帧缓冲设备定义了驱动层的接口fb_info结构，在文件层上，用户调用file_operations的函数操作，间接调用fb_info中的fb_ops函数集来操作硬件。 2.1、 Framebuffer数据结构2.1.1、 fb_infofb_info是Linux为帧缓冲设备定义的驱动层接口。它不仅包含了底层函数，而且还有记录设备状态的数据。每个帧缓冲设备都与一个fb_info结构相对应。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[-&gt;kernel\\include\\linux\\fb.h]struct fb_info &#123; atomic_t count; int node; int flags; struct mutex lock; /* Lock for open/release/ioctl funcs */ struct mutex mm_lock; /* Lock for fb_mmap and smem_* fields */ struct fb_var_screeninfo var; /* Current var */ struct fb_fix_screeninfo fix; /* Current fix */ struct fb_monspecs monspecs; /* Current Monitor specs */ struct work_struct queue; /* Framebuffer event queue */ struct fb_pixmap pixmap; /* Image hardware mapper */ struct fb_pixmap sprite; /* Cursor hardware mapper */ struct fb_cmap cmap; /* Current cmap */ struct list_head modelist; /* mode list */ struct fb_videomode *mode; /* current mode */ struct file *file; /* current file node */#ifdef CONFIG_FB_DEFERRED_IO struct delayed_work deferred_work; struct fb_deferred_io *fbdefio;#endif struct fb_ops *fbops; struct device *device; /* This is the parent */ struct device *dev; /* This is this fb device */ int class_flag; /* private sysfs flags */#ifdef CONFIG_FB_TILEBLITTING struct fb_tile_ops *tileops; /* Tile Blitting */#endif char __iomem *screen_base; /* Virtual address */ unsigned long screen_size; /* Amount of ioremapped VRAM or 0 */ void *pseudo_palette; /* Fake palette of 16 colors */ #define FBINFO_STATE_RUNNING 0#define FBINFO_STATE_SUSPENDED 1 u32 state; /* Hardware state i.e suspend */ void *fbcon_par; /* fbcon use-only private area */ /* From here on everything is device dependent */ void *par; /* we need the PCI or similar aperture base/size not smem_start/size as smem_start may just be an object allocated inside the aperture so may not actually overlap */ struct apertures_struct &#123; unsigned int count; struct aperture &#123; resource_size_t base; resource_size_t size; &#125; ranges[0]; &#125; *apertures; bool skip_vt_switch; /* no VT switch on suspend/resume required */&#125;; 2.1.2、fb_var_screeninfo fb_var_screeninfo：用于记录用户可修改的显示控制器参数，包括屏幕分辨率、每个像素点的比特数等 12345678910111213141516171819202122232425262728293031[-&gt;\\include\\uapi\\linux\\fb.h]struct fb_var_screeninfo &#123; __u32 xres; /* 行可见像素*/ __u32 yres; /* 列可见像素*/ __u32 xres_virtual; /* 行虚拟像素*/ __u32 yres_virtual; /* 列虚拟像素*/ __u32 xoffset; /* 水平偏移量*/ __u32 yoffset; /* 垂直偏移量*/ __u32 bits_per_pixel;/*每个像素所占bit位数*/ __u32 grayscale; /* 灰色刻度*/ struct fb_bitfield red; /* bitfield in fb mem if true color, */ struct fb_bitfield green; /* else only length is significant */ struct fb_bitfield blue; struct fb_bitfield transp; /* transparency */ __u32 nonstd; /* != 0 Non standard pixel format */ __u32 activate; /* see FB_ACTIVATE_* */ __u32 height; /* 图像高度*/ __u32 width; /* 图像宽度*/ __u32 accel_flags; /* (OBSOLETE) see fb_info.flags */ __u32 pixclock; /* pixel clock in ps (pico seconds) */ __u32 left_margin; /* time from sync to picture */ __u32 right_margin; /* time from picture to sync */ __u32 upper_margin; /* time from sync to picture */ __u32 lower_margin; __u32 hsync_len; /* length of horizontal sync */ __u32 vsync_len; /* length of vertical sync */ __u32 sync; /* see FB_SYNC_* */ __u32 vmode; /* see FB_VMODE_* */ __u32 rotate; /* angle we rotate counter clockwise */ __u32 reserved[5]; /* Reserved for future compatibility */ &#125;; 2.1.3、fb_fix_screeninfofb_fix_screeninfo：记录了用户不能修改的显示控制器的参数，这些参数是在驱动初始化时设置的 123456789101112131415161718192021[-&gt;\\include\\uapi\\linux\\fb.h]struct fb_fix_screeninfo &#123; char id[16]; /* identification string eg \"TT Builtin\" */ unsigned long smem_start; /* Start of frame buffer mem */ /* (physical address) */ __u32 smem_len; /* Length of frame buffer mem */ __u32 type; /* see FB_TYPE_* */ __u32 type_aux; /* Interleave for interleaved Planes */ __u32 visual; /* see FB_VISUAL_* */ __u16 xpanstep; /* zero if no hardware panning */ __u16 ypanstep; /* zero if no hardware panning */ __u16 ywrapstep; /* zero if no hardware ywrap */ __u32 line_length; /* length of a line in bytes */ unsigned long mmio_start; /* Start of Memory Mapped I/O */ /* (physical address) */ __u32 mmio_len; /* Length of Memory Mapped I/O */ __u32 accel; /* Indicate to driver which */ /* specific chip/card we have */ __u16 capabilities; /* see FB_CAP_* */ __u16 reserved[2]; /* Reserved for future compatibility */&#125;; fb_ops是提供给底层设备驱动的一个接口。当我们编写一个FrameBuffer的时候，就要依照Linux FrameBuffer编程的套路，填写fb_ops结构体。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[-&gt;\\include\\linux\\fb.h]struct fb_ops &#123; /* open/release and usage marking */ struct module *owner; int (*fb_open)(struct fb_info *info, int user); int (*fb_release)(struct fb_info *info, int user); /* For framebuffers with strange non linear layouts or that do not * work with normal memory mapped access */ ssize_t (*fb_read)(struct fb_info *info, char __user *buf, size_t count, loff_t *ppos); ssize_t (*fb_write)(struct fb_info *info, const char __user *buf, size_t count, loff_t *ppos); /* checks var and eventually tweaks it to something supported, * DO NOT MODIFY PAR */ int (*fb_check_var)(struct fb_var_screeninfo *var, struct fb_info *info); /* set the video mode according to info-&gt;var */ int (*fb_set_par)(struct fb_info *info); /* set color register */ int (*fb_setcolreg)(unsigned regno, unsigned red, unsigned green, unsigned blue, unsigned transp, struct fb_info *info); /* set color registers in batch */ int (*fb_setcmap)(struct fb_cmap *cmap, struct fb_info *info); /* blank display */ int (*fb_blank)(int blank, struct fb_info *info); /* pan display */ int (*fb_pan_display)(struct fb_var_screeninfo *var, struct fb_info *info); /* Draws a rectangle */ void (*fb_fillrect) (struct fb_info *info, const struct fb_fillrect *rect); /* Copy data from area to another */ void (*fb_copyarea) (struct fb_info *info, const struct fb_copyarea *region); /* Draws a image to the display */ void (*fb_imageblit) (struct fb_info *info, const struct fb_image *image); /* Draws cursor */ int (*fb_cursor) (struct fb_info *info, struct fb_cursor *cursor); /* Rotates the display */ void (*fb_rotate)(struct fb_info *info, int angle); /* wait for blit idle, optional */ int (*fb_sync)(struct fb_info *info); /* perform fb specific ioctl (optional) */ int (*fb_ioctl)(struct fb_info *info, unsigned int cmd, unsigned long arg); /* perform fb specific ioctl v2 (optional) - provides file param */ int (*fb_ioctl_v2)(struct fb_info *info, unsigned int cmd, unsigned long arg, struct file *file); /* Handle 32bit compat ioctl (optional) */ int (*fb_compat_ioctl)(struct fb_info *info, unsigned cmd, unsigned long arg); /* Handle 32bit compat ioctl (optional) */ int (*fb_compat_ioctl_v2)(struct fb_info *info, unsigned cmd, unsigned long arg, struct file *file); /* perform fb specific mmap */ int (*fb_mmap)(struct fb_info *info, struct vm_area_struct *vma); /* get capability given var */ void (*fb_get_caps)(struct fb_info *info, struct fb_blit_caps *caps, struct fb_var_screeninfo *var); /* teardown any resources to do with this framebuffer */ void (*fb_destroy)(struct fb_info *info); /* called at KDB enter and leave time to prepare the console */ int (*fb_debug_enter)(struct fb_info *info); int (*fb_debug_leave)(struct fb_info *info);&#125;; 2.2、Framebuffer驱动注册过程在系统启动时，内核调用所有注册驱动程序的驱动程序初始化函数。 为了帧缓冲区驱动程序，调用mdss_fb_init。 mdss_fb_init注册mdss_fb_driver。驱动在mdss_fb.c文件中注册。 12345678910111213[-\\drivers\\video\\msm\\mdss\\mdss_fb.c]static struct platform_driver mdss_fb_driver = &#123; .probe = mdss_fb_probe, .remove = mdss_fb_remove, .suspend = mdss_fb_suspend, .resume = mdss_fb_resume, .shutdown = mdss_fb_shutdown, .driver = &#123; .name = \"mdss_fb\", .of_match_table = mdss_fb_dt_match, .pm = &amp;mdss_fb_pm_ops, &#125;,&#125;; 在调用init之后，内核调用每个平台驱动程序的探测函数。 在调用mdss_fb_probe时，函数执行资源分配并调用mdss_fb_register。 可以有多个帧缓冲区（fb）设备（节点）。 该驱动程序通过调用mdss_fb_register来注册各个fb设备，后者又调用register_framebuffer。 HDMI和主显示器是各个fb设备的例子。 以下操作已注册： 首先看一下mdss_fb_probe()函数 123456789101112131415161718192021222324252627282930313233343536[-&gt;\\drivers\\video\\msm\\mdss\\mdss_fb.c]static int mdss_fb_probe(struct platform_device *pdev)&#123; struct msm_fb_data_type *mfd = NULL; struct mdss_panel_data *pdata; struct fb_info *fbi; int rc; pdata = dev_get_platdata(&amp;pdev-&gt;dev); /* * alloc framebuffer info + par data */ fbi = framebuffer_alloc(sizeof(struct msm_fb_data_type), NULL); mfd = (struct msm_fb_data_type *)fbi-&gt;par; mfd-&gt;key = MFD_KEY; mfd-&gt;fbi = fbi; mfd-&gt;panel_info = &amp;pdata-&gt;panel_info; mfd-&gt;panel.type = pdata-&gt;panel_info.type; mfd-&gt;panel.id = mfd-&gt;index; mfd-&gt;fb_page = MDSS_FB_NUM; mfd-&gt;index = fbi_list_index; mfd-&gt;mdp_fb_page_protection = MDP_FB_PAGE_PROTECTION_WRITECOMBINE; mfd-&gt;ext_ad_ctrl = -1; ...... platform_set_drvdata(pdev, mfd); rc = mdss_fb_register(mfd); ...... return rc;&#125; 首先调用framebuffer_alloc()函数返回一个fb_info 结构体，然后调用mdss_fb_register(mfd)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132[-&gt;\\drivers\\video\\msm\\mdss\\mdss_fb.c]static int mdss_fb_register(struct msm_fb_data_type *mfd)&#123; int ret = -ENODEV; int bpp; char panel_name[20]; struct mdss_panel_info *panel_info = mfd-&gt;panel_info; struct fb_info *fbi = mfd-&gt;fbi; struct fb_fix_screeninfo *fix; struct fb_var_screeninfo *var; int *id; /* * fb info initialization */ fix = &amp;fbi-&gt;fix; var = &amp;fbi-&gt;var; fix-&gt;type_aux = 0; /* if type == FB_TYPE_INTERLEAVED_PLANES */ fix-&gt;visual = FB_VISUAL_TRUECOLOR; /* True Color */ fix-&gt;ywrapstep = 0; /* No support */ fix-&gt;mmio_start = 0; /* No MMIO Address */ fix-&gt;mmio_len = 0; /* No MMIO Address */ fix-&gt;accel = FB_ACCEL_NONE;/* FB_ACCEL_MSM needes to be added in fb.h */ var-&gt;xoffset = 0, /* Offset from virtual to visible */ var-&gt;yoffset = 0, /* resolution */ var-&gt;grayscale = 0, /* No graylevels */ var-&gt;nonstd = 0, /* standard pixel format */ var-&gt;activate = FB_ACTIVATE_VBL, /* activate it at vsync */ var-&gt;height = -1, /* height of picture in mm */ var-&gt;width = -1, /* width of picture in mm */ var-&gt;accel_flags = 0, /* acceleration flags */ var-&gt;sync = 0, /* see FB_SYNC_* */ var-&gt;rotate = 0, /* angle we rotate counter clockwise */ mfd-&gt;op_enable = false; switch (mfd-&gt;fb_imgType) &#123; case MDP_RGB_565: ...... var-&gt;transp.offset = 0; var-&gt;transp.length = 0; bpp = 2; break; case MDP_RGB_888: ...... var-&gt;transp.offset = 0; var-&gt;transp.length = 0; bpp = 3; break; case MDP_ARGB_8888: ...... var-&gt;transp.offset = 0; var-&gt;transp.length = 8; bpp = 4; break; case MDP_RGBA_8888: ...... var-&gt;transp.offset = 24; var-&gt;transp.length = 8; bpp = 4; break; case MDP_YCRYCB_H2V1: ...... var-&gt;transp.offset = 0; var-&gt;transp.length = 0; bpp = 2; break; default: return ret; &#125; mdss_panelinfo_to_fb_var(panel_info, var); fix-&gt;type = panel_info-&gt;is_3d_panel; if (mfd-&gt;mdp.fb_stride) fix-&gt;line_length = mfd-&gt;mdp.fb_stride(mfd-&gt;index, var-&gt;xres, bpp); else fix-&gt;line_length = var-&gt;xres * bpp; var-&gt;xres_virtual = var-&gt;xres; var-&gt;yres_virtual = panel_info-&gt;yres * mfd-&gt;fb_page; var-&gt;bits_per_pixel = bpp * 8; /* FrameBuffer color depth */ /* * Populate smem length here for uspace to get the * Framebuffer size when FBIO_FSCREENINFO ioctl is called. */ fix-&gt;smem_len = PAGE_ALIGN(fix-&gt;line_length * var-&gt;yres) * mfd-&gt;fb_page; /* id field for fb app */ id = (int *)&amp;mfd-&gt;panel; snprintf(fix-&gt;id, sizeof(fix-&gt;id), \"mdssfb_%x\", (u32) *id); fbi-&gt;fbops = &amp;mdss_fb_ops; fbi-&gt;flags = FBINFO_FLAG_DEFAULT; fbi-&gt;pseudo_palette = mdss_fb_pseudo_palette; mfd-&gt;ref_cnt = 0; mfd-&gt;panel_power_state = MDSS_PANEL_POWER_OFF; mfd-&gt;dcm_state = DCM_UNINIT; if (mdss_fb_alloc_fbmem(mfd)) pr_warn(\"unable to allocate fb memory in fb register\\n\"); ...... ret = fb_alloc_cmap(&amp;fbi-&gt;cmap, 256, 0); if (ret) pr_err(\"fb_alloc_cmap() failed!\\n\"); if (register_framebuffer(fbi) &lt; 0) &#123; fb_dealloc_cmap(&amp;fbi-&gt;cmap); mfd-&gt;op_enable = false; return -EPERM; &#125; snprintf(panel_name, ARRAY_SIZE(panel_name), \"mdss_panel_fb%d\", mfd-&gt;index); mdss_panel_debugfs_init(panel_info, panel_name); pr_info(\"FrameBuffer[%d] %dx%d registered successfully!\\n\", mfd-&gt;index, fbi-&gt;var.xres, fbi-&gt;var.yres); return 0;&#125; 任何一个特定硬件Framebuffer驱动在初始化时都必须向fbmem.c注册，FrameBuffer模块提供了驱动注册接口函数register_framebuffer： 123456789101112[-&gt;\\drivers\\video\\fbdev\\core\\fbmem.c]intregister_framebuffer(struct fb_info *fb_info)&#123; int ret; mutex_lock(&amp;registration_lock); ret = do_register_framebuffer(fb_info); mutex_unlock(&amp;registration_lock); return ret;&#125; 参数fb_info描述特定硬件的FrameBuffer驱动信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[-&gt;\\drivers\\video\\fbdev\\core\\fbmem.c]static int do_register_framebuffer(struct fb_info *fb_info) &#123; int i; struct fb_event event; struct fb_videomode mode; if (fb_check_foreignness(fb_info)) return -ENOSYS; //根据当前注册的fb_info的apertures属性从FrameBuffer驱动数组registered_fb中查询是否存在冲突 do_remove_conflicting_framebuffers(fb_info-&gt;apertures, fb_info-&gt;fix.id, fb_is_primary_device(fb_info)); //判断已注册的驱动是否超过32个FrameBuffer驱动 if (num_registered_fb == FB_MAX) return -ENXIO; //增加已注册的驱动个数 num_registered_fb++; //从数组registered_fb中查找空闲元素，用于存储当前注册的fb_info for (i = 0 ; i &lt; FB_MAX; i++) if (!registered_fb[i]) break; //将当前注册的fb_info在数组registered_fb中的索引位置保存到fb_info-&gt;node fb_info-&gt;node = i; //初始化当前注册的fb_info的成员信息 atomic_set(&amp;fb_info-&gt;count, 1); mutex_init(&amp;fb_info-&gt;lock); mutex_init(&amp;fb_info-&gt;mm_lock); //在/dev目录下创建一个fbx的设备文件，次设备号就是该fb_info在数组registered_fb中的索引 fb_info-&gt;dev = device_create(fb_class, fb_info-&gt;device,MKDEV(FB_MAJOR, i), NULL, \"fb%d\", i); if (IS_ERR(fb_info-&gt;dev)) &#123; printk(KERN_WARNING \"Unable to create device for framebuffer %d; errno = %ld\\n\", i, PTR_ERR(fb_info-&gt;dev)); fb_info-&gt;dev = NULL; &#125; else //初始化fb_info fb_init_device(fb_info); if (fb_info-&gt;pixmap.addr == NULL) &#123; fb_info-&gt;pixmap.addr = kmalloc(FBPIXMAPSIZE, GFP_KERNEL); if (fb_info-&gt;pixmap.addr) &#123; fb_info-&gt;pixmap.size = FBPIXMAPSIZE; fb_info-&gt;pixmap.buf_align = 1; fb_info-&gt;pixmap.scan_align = 1; fb_info-&gt;pixmap.access_align = 32; fb_info-&gt;pixmap.flags = FB_PIXMAP_DEFAULT; &#125; &#125; fb_info-&gt;pixmap.offset = 0; if (!fb_info-&gt;pixmap.blit_x) fb_info-&gt;pixmap.blit_x = ~(u32)0; if (!fb_info-&gt;pixmap.blit_y) fb_info-&gt;pixmap.blit_y = ~(u32)0; if (!fb_info-&gt;modelist.prev || !fb_info-&gt;modelist.next) INIT_LIST_HEAD(&amp;fb_info-&gt;modelist); fb_var_to_videomode(&amp;mode, &amp;fb_info-&gt;var); fb_add_videomode(&amp;mode, &amp;fb_info-&gt;modelist); //将特定硬件对应的fb_info注册到registered_fb数组中 registered_fb[i] = fb_info; event.info = fb_info; if (!lock_fb_info(fb_info)) return -ENODEV; //使用Linux事件通知机制发送一个FrameBuffer注册事件FB_EVENT_FB_REGISTERED fb_notifier_call_chain(FB_EVENT_FB_REGISTERED, &amp;event); unlock_fb_info(fb_info); return 0; &#125; 注册过程就是将指定的设备驱动信息fb_info存放到registered_fb数组中。因此在注册具体的fb_info时，首先要构造一个fb_info数据结构，并初始化该数据结构，该结构用于描述一个特定的FrameBuffer驱动。 （三）、MDP driverMDP也被注册为平台驱动程序。 mdp3_driver_init执行驱动程序init。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149[-&gt;\\drivers\\video\\msm\\mdss\\mdp3.c]static struct platform_driver mdp3_driver = &#123; .probe = mdp3_probe, .remove = mdp3_remove, .suspend = mdp3_suspend, .resume = mdp3_resume, .shutdown = NULL, .driver = &#123; .name = \"mdp3\", .of_match_table = mdp3_dt_match, .pm = &amp;mdp3_pm_ops, &#125;,&#125;;static int __init mdp3_driver_init(void)&#123; int ret; ret = platform_driver_register(&amp;mdp3_driver); if (ret) &#123; pr_err(\"register mdp3 driver failed!\\n\"); return ret; &#125; return 0;&#125;static int mdp3_probe(struct platform_device *pdev)&#123; int rc; static struct msm_mdp_interface mdp3_interface = &#123; .init_fnc = mdp3_init, .fb_mem_get_iommu_domain = mdp3_fb_mem_get_iommu_domain, .panel_register_done = mdp3_panel_register_done, .fb_stride = mdp3_fb_stride, .check_dsi_status = mdp3_check_dsi_ctrl_status, &#125;; struct mdp3_intr_cb underrun_cb = &#123; .cb = mdp3_dma_underrun_intr_handler, .data = NULL, &#125;; pr_debug(\"%s: START\\n\", __func__); if (!pdev-&gt;dev.of_node) &#123; pr_err(\"MDP driver only supports device tree probe\\n\"); return -ENOTSUPP; &#125; if (mdp3_res) &#123; pr_err(\"MDP already initialized\\n\"); return -EINVAL; &#125; mdp3_res = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(struct mdp3_hw_resource), GFP_KERNEL); if (mdp3_res == NULL) return -ENOMEM; pdev-&gt;id = 0; mdp3_res-&gt;pdev = pdev; mutex_init(&amp;mdp3_res-&gt;res_mutex); spin_lock_init(&amp;mdp3_res-&gt;irq_lock); platform_set_drvdata(pdev, mdp3_res); atomic_set(&amp;mdp3_res-&gt;active_intf_cnt, 0); mutex_init(&amp;mdp3_res-&gt;reg_bus_lock); INIT_LIST_HEAD(&amp;mdp3_res-&gt;reg_bus_clist); mdp3_res-&gt;mdss_util = mdss_get_util_intf(); if (mdp3_res-&gt;mdss_util == NULL) &#123; pr_err(\"Failed to get mdss utility functions\\n\"); rc = -ENODEV; goto get_util_fail; &#125; mdp3_res-&gt;mdss_util-&gt;get_iommu_domain = mdp3_get_iommu_domain; mdp3_res-&gt;mdss_util-&gt;iommu_attached = is_mdss_iommu_attached; mdp3_res-&gt;mdss_util-&gt;iommu_ctrl = mdp3_iommu_ctrl; mdp3_res-&gt;mdss_util-&gt;bus_scale_set_quota = mdp3_bus_scale_set_quota; mdp3_res-&gt;mdss_util-&gt;panel_intf_type = mdp3_panel_intf_type; mdp3_res-&gt;mdss_util-&gt;dyn_clk_gating_ctrl = mdp3_dynamic_clock_gating_ctrl; mdp3_res-&gt;mdss_util-&gt;panel_intf_type = mdp3_panel_intf_type; mdp3_res-&gt;mdss_util-&gt;panel_intf_status = mdp3_panel_get_intf_status; rc = mdp3_parse_dt(pdev); if (rc) goto probe_done; rc = mdp3_res_init(); if (rc) &#123; pr_err(\"unable to initialize mdp3 resources\\n\"); goto probe_done; &#125; mdp3_res-&gt;fs_ena = false; mdp3_res-&gt;fs = devm_regulator_get(&amp;pdev-&gt;dev, \"vdd\"); if (IS_ERR_OR_NULL(mdp3_res-&gt;fs)) &#123; pr_err(\"unable to get mdss gdsc regulator\\n\"); return -EINVAL; &#125; rc = mdp3_debug_init(pdev); if (rc) &#123; pr_err(\"unable to initialize mdp debugging\\n\"); goto probe_done; &#125; pm_runtime_set_autosuspend_delay(&amp;pdev-&gt;dev, AUTOSUSPEND_TIMEOUT_MS); if (mdp3_res-&gt;idle_pc_enabled) &#123; pr_debug(\"%s: Enabling autosuspend\\n\", __func__); pm_runtime_use_autosuspend(&amp;pdev-&gt;dev); &#125; /* Enable PM runtime */ pm_runtime_set_suspended(&amp;pdev-&gt;dev); pm_runtime_enable(&amp;pdev-&gt;dev); if (!pm_runtime_enabled(&amp;pdev-&gt;dev)) &#123; rc = mdp3_footswitch_ctrl(1); if (rc) &#123; pr_err(\"unable to turn on FS\\n\"); goto probe_done; &#125; &#125; rc = mdp3_check_version(); if (rc) &#123; pr_err(\"mdp3 check version failed\\n\"); goto probe_done; &#125; rc = mdp3_register_sysfs(pdev); if (rc) pr_err(\"unable to register mdp sysfs nodes\\n\"); rc = mdss_fb_register_mdp_instance(&amp;mdp3_interface); if (rc) pr_err(\"unable to register mdp instance\\n\"); rc = mdp3_set_intr_callback(MDP3_INTR_LCDC_UNDERFLOW, &amp;underrun_cb); if (rc) pr_err(\"unable to configure interrupt callback\\n\"); rc = mdss_smmu_init(mdss_res, &amp;pdev-&gt;dev); if (rc) pr_err(\"mdss smmu init failed\\n\"); mdp3_res-&gt;mdss_util-&gt;mdp_probe_done = true; pr_debug(\"%s: END\\n\", __func__); return rc;&#125; 内核调用MDP探测函数mdp3_probe。 在探测器上，驱动程序从设备树中获取面板信息，并通过调用mdp3_parse_dt来解析信息。 在mdp3_ctrl_on期间，MDP驱动程序调用mdp3_ctrl_res_req_clk并请求MDP和Vsync时钟。 在mdp3_ctrl_off期间，驱动程序请求关闭MDP和Vsync时钟。 （四）、DSI controller driver （lcd驱动 dsi）msm_dsi_v2_driver_init执行驱动程序初始化。 msm_dsi_v2_driver_init调用 msm_dsi_v2_register_driver注册驱动程序。总体时序图： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102[-&gt;\\drivers\\video\\msm\\mdss\\dsi_host_v2.c]static struct platform_driver msm_dsi_v2_driver = &#123; .probe = msm_dsi_probe, .remove = msm_dsi_remove, .shutdown = NULL, .driver = &#123; .name = \"msm_dsi_v2\", .of_match_table = msm_dsi_v2_dt_match, &#125;,&#125;;static int msm_dsi_v2_register_driver(void)&#123; return platform_driver_register(&amp;msm_dsi_v2_driver);&#125;static int msm_dsi_probe(struct platform_device *pdev)&#123; struct dsi_interface intf; char panel_cfg[MDSS_MAX_PANEL_LEN]; struct mdss_dsi_ctrl_pdata *ctrl_pdata = NULL; int rc = 0; struct device_node *dsi_pan_node = NULL; bool cmd_cfg_cont_splash = false; struct resource *mdss_dsi_mres; int i; pr_debug(\"%s\\n\", __func__); rc = msm_dsi_init(); pdev-&gt;id = 0; ctrl_pdata = platform_get_drvdata(pdev); if (!ctrl_pdata) &#123; ctrl_pdata = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(struct mdss_dsi_ctrl_pdata), GFP_KERNEL); platform_set_drvdata(pdev, ctrl_pdata); &#125; ctrl_pdata-&gt;mdss_util = mdss_get_util_intf(); if (mdp3_res-&gt;mdss_util == NULL) &#123; pr_err(\"Failed to get mdss utility functions\\n\"); return -ENODEV; &#125; mdss_dsi_mres = platform_get_resource(pdev, IORESOURCE_MEM, 0); if (!mdss_dsi_mres) &#123; &#125; else &#123; dsi_host_private-&gt;dsi_reg_size = resource_size(mdss_dsi_mres); dsi_host_private-&gt;dsi_base = ioremap(mdss_dsi_mres-&gt;start, dsi_host_private-&gt;dsi_reg_size); &#125; mdss_dsi_mres = platform_get_resource(pdev, IORESOURCE_IRQ, 0); rc = of_platform_populate(pdev-&gt;dev.of_node, NULL, NULL, &amp;pdev-&gt;dev); /* DSI panels can be different between controllers */ rc = dsi_get_panel_cfg(panel_cfg); /* find panel device node */ dsi_pan_node = dsi_find_panel_of_node(pdev, panel_cfg); cmd_cfg_cont_splash = mdp3_panel_get_boot_cfg() ? true : false; rc = mdss_dsi_panel_init(dsi_pan_node, ctrl_pdata, cmd_cfg_cont_splash); rc = dsi_ctrl_config_init(pdev, ctrl_pdata); msm_dsi_parse_lane_swap(pdev-&gt;dev.of_node, &amp;(ctrl_pdata-&gt;dlane_swap)); for (i = 0; i &lt; DSI_MAX_PM; i++) &#123; rc = msm_dsi_io_init(pdev, &amp;(ctrl_pdata-&gt;power_data[i])); &#125; pr_debug(\"%s: Dsi Ctrl-&gt;0 initialized\\n\", __func__); dsi_host_private-&gt;dis_dev = pdev-&gt;dev; intf.on = msm_dsi_on; intf.off = msm_dsi_off; intf.cont_on = msm_dsi_cont_on; intf.clk_ctrl = msm_dsi_clk_ctrl; intf.op_mode_config = msm_dsi_op_mode_config; intf.index = 0; intf.private = NULL; dsi_register_interface(&amp;intf); msm_dsi_debug_init(); msm_dsi_ctrl_init(ctrl_pdata); rc = msm_dsi_irq_init(&amp;pdev-&gt;dev, mdss_dsi_mres-&gt;start, ctrl_pdata); rc = dsi_panel_device_register_v2(pdev, ctrl_pdata); pr_debug(\"%s success\\n\", __func__); return 0;&#125; 内核调用msm_dsi_probe。 面板被检测到。 msm_dsi_probe调用mdss_dsi_panel_init函数。 mdss_dsi_panel_init调用mdss_panel_parse_dt来获取面板参数。MDP驱动程序使用该事件与DSI驱动程序进行通信。 DSI驱动程序具有mdss_dsi_event_handler，这是MDP核心事件的回调处理程序。 mdss_panel.h定义了MDP核心事件。 1234567891011121314151617181920212223242526272829303132[-&gt;\\drivers\\video\\msm\\mdss\\mdss_panel.h]enum mdss_intf_events &#123; MDSS_EVENT_RESET = 1, MDSS_EVENT_LINK_READY, MDSS_EVENT_UNBLANK, MDSS_EVENT_PANEL_ON, MDSS_EVENT_POST_PANEL_ON, MDSS_EVENT_BLANK, MDSS_EVENT_PANEL_OFF, MDSS_EVENT_CLOSE, MDSS_EVENT_SUSPEND, MDSS_EVENT_RESUME, MDSS_EVENT_CHECK_PARAMS, MDSS_EVENT_CONT_SPLASH_BEGIN, MDSS_EVENT_CONT_SPLASH_FINISH, MDSS_EVENT_PANEL_UPDATE_FPS, MDSS_EVENT_FB_REGISTERED, MDSS_EVENT_PANEL_CLK_CTRL, MDSS_EVENT_DSI_CMDLIST_KOFF, MDSS_EVENT_ENABLE_PARTIAL_ROI, MDSS_EVENT_DSC_PPS_SEND, MDSS_EVENT_DSI_STREAM_SIZE, MDSS_EVENT_DSI_UPDATE_PANEL_DATA, MDSS_EVENT_REGISTER_RECOVERY_HANDLER, MDSS_EVENT_REGISTER_MDP_CALLBACK, MDSS_EVENT_DSI_PANEL_STATUS, MDSS_EVENT_DSI_DYNAMIC_SWITCH, MDSS_EVENT_DSI_RECONFIG_CMD, MDSS_EVENT_DSI_RESET_WRITE_PTR, MDSS_EVENT_PANEL_TIMING_SWITCH, MDSS_EVENT_MAX,&#125;; 在msm_dsi_on期间，通过调用msm_dsi_clk_enable打开DSI时钟。 在msm_dsi_off期间，通过调用msm_dsi_clk_disable关闭clks。 （五）、 Panel driver （面板 dsi）MDSS : Multimedia Display sub systemDSI: Display Serial Interface qcom,mdss-dsi-force-clock-lane-hs; // faulse ：clock每帧回lp11ture: clock不回 qcom,mdss-dsi-hfp-power-mode; // data 每行回lp11,对应的hfp要修改成300以上 面板信息位于kernel\\arch\\arm\\boot\\dts\\中的.dtsi文件中。 这包含所有面板特定的命令，例如on，off和reset（mdss-dsi-on-command，mdss-dsi-off-command，mdss-dsi-reset-sequence），BL控制和其他面板 独立参数。例如：msm8610-mdss.dtsi （文件名通常为 msmxxx-mdss.dtsi 指定了mdss 的 mdp 和 dsi） 5.1、.dtsi文件解析1234567891011mdss_mdp: qcom,mdss_mdp@fd900000 &#123; compatible = \"qcom,mdss_mdp3\"; // 对应mdss驱动 mdss_mdp.c---------- mdss_dsi0: qcom,mdss_dsi@fdd00000 &#123; compatible = \"qcom,msm-dsi-v2\"; // 对应dsi解析驱动 dsi_host_v2.c或者 mdss_dsi0: qcom,mdss_dsi_ctrl0@1a94000 &#123; compatible = \"qcom,mdss-dsi-ctrl\"; // 对应dsi解析驱动 mdss_dsi.c 通过下面函数向 mdss_fb.c 注册了fb_info结构 (包含在mdss_dsi_ctrl_pdata结构中) 1234567891011drivers\\video\\msm\\mdss\\dsi_host_v2.c （lcd驱动 dsi）dsi_panel_device_register_v2(struct platform_device *dev,struct mdss_dsi_ctrl_pdata *ctrl_pdata)static const struct of_device_id msm_dsi_v2_dt_match[] = &#123; &#123;.compatible = &quot;qcom,msm-dsi-v2&quot;&#125;, &#123;&#125;&#125;;或者 drivers\\video\\msm\\mdss\\mdss_dsi.c msm8610-asus.dts （指定mdp中的哪一个配置）通常在dts文件的 mdss_dsi0 lab里面通过 qcom,dsi-pref-prim-pan 属性 指定使用哪一个lcd配置 123&amp;mdss_dsi0 &#123; qcom,dsi-pref-prim-pan = &lt;&amp;dsi_fl10802_fwvga_vid&gt;;&#125;; dsi-panel-fl10802-fwvga-video.dtsi 123456789&amp;mdss_mdp &#123; dsi_fl10802_fwvga_vid: qcom,mdss_dsi_fl10802_fwvga_video &#123; qcom,mdss-dsi-panel-name = &quot;fl10802 fwvga video mode dsi panel&quot;; qcom,mdss-dsi-drive-ic = &quot;fl10802&quot;; qcom,mdss-dsi-panel-controller = &lt;&amp;mdss_dsi0&gt;; qcom,mdss-dsi-panel-type = &quot;dsi_video_mode&quot;; qcom,mdss-dsi-panel-destination = &quot;display_1&quot;; ... &#125; 5.2、mdss_mdp 和 mdss_dsi0 的关系mdss_mdp 相当于一个数组，里面定义了很多不同lcd显示屏的配置项包括分辨率等等 面板驱动程序可以根据连接的实际面板数量具有多个节点。mdss_register_panel注册面板驱动程序：msm_dsi_probe→dsi_panel_device_register_v2→mdss_register_panel→of_platform_device_create以下面板控制功能可用并在mdss_dsi_panel_init中初始化：ctrl_pdata→on = mdss_dsi_panel_on;ctrl_pdata→off = mdss_dsi_panel_off;ctrl_pdata→panel_data.set_backlight = mdss_dsi_panel_bl_ctrl; 例如，在mdp3_ctrl.c中，函数mdp3_ctrl_on（）会调用以下DSI处理程序MDSS_EVENT_UNBLANK和MDSS_EVENT_PANEL_ON事件如下所示：rc = panel→event_handler（panel，MDSS_EVENT_UNBLANK，NULL）;rc | =面板→event_handler（面板，MDSS_EVENT_PANEL_ON，NULL）; 5.3、通过内核接口打开和关闭显示器5.3.1、启动 按照上面的部分所述注册设备后：mdss_fb_open→mdss_fb_blank_sub→pdata→on（面板ON功能）注意：对于命令模式面板，如果面板在启动时已打开，则会跳过该启动序列 以避免关闭/ -on的文物。 5.3.2、暂停/恢复挂起/恢复时，调用fb驱动程序挂起/恢复和MDP驱动程序挂起/恢复。 fb驱动程序依次调用面板驱动程序的开/关功能。 5.3.2.1、暂停序列 Kernelcall→mdss_fb_release_all→mdss_fb_blank→mdss_fb_blank_sub→mdp3_ctrl_off→mdp3_ctrl_off发送两个事件： MDSS_EVENT_PANEL_OFF - dsi_event_handler接收事件。 当事件发生时接收到时，调用小组关闭序列。 MDSS_EVENT_BLANK - 该事件由调用的dsi_event_handler处理mdss_dsi_off。 Kernelcall→mdp3_suspend - 未使用5.3.2.2、恢复序列 Kernelcall→mdss_fb_blank→mdss_fb_blank_sub→mdp3_ctrl_on→mdp3_ctrl_on发送两个事件： MDSS_EVENT_UNBLANK - dsi_event_handler接收事件。 当事件发生时收到，DSI-on被调用。 MDSS_EVENT_PANEL_ON - 事件由dsi_event_handler处理，dsi_event_handler发送面板上的序列。Kernelcall→mdp3_resume - 未使用 5.4、图像更新到面板 用户必须确保MSMFB_OVERLAY_SET IOCTL在呼叫之前至少被调用一次 到MSMFB_OVERLAY_PLAY IOCLT（例如ioctl(fd, MSMFB_OVERLAY_PLAY, &amp;od)）。MSMFB_OVERLAY_PLAY队列缓冲区 显示在面板上。 用户使用MSMFB_DISPLAY_COMMIT调用fb IOCLT。这会启动一个呼叫 mdss_fb_display_commit 并安排工作队列。此工作队列处理程序调用 msm_fb_pan_display_ex，然后调用mdp3_ctrl_pan_display。 mdss_fb_ioctl→（MSMFB_DISPLAY_COMMIT）→mdss_fb_display_commit→ mdss_fb_pan_display_ex→计划工作mdss_fb_commit_wq_handler msm_fb_commit_wq_handler→mdp3_ctrl_display_commit_kickoff→mdp3_dmap_update一次呼叫后，可以有多个对PLAY和COMMIT IOCLT的呼叫 MSMFB_OVERLAY_SET IOCTL。面板更新完成并且设备完成后需要进入挂起或关闭状态，用户可以调用MSMFB_OVERLAY_UNSET。 注意：工作队列架构正在被即将发布的线程取代，这些文件未被捕获。视频和命令模式面板的面板更新略有不同。为了在命令模式面板中，mdp3_dmap_update函数会等待，直到前一个图像更新为止完成使用MDP DMA开始新帧更新 if（dma-&gt; output_config.out_sel == MDP3_DMA_OUTPUT_SEL_DSI_CMD）{cb_type = MDP3_DMA_CALLBACK_TYPE_DMA_DONE;如果（intf-&gt; active）wait_for_completion_killable（DMA-&gt; dma_comp）; 对于视频面板，在DMA触发后，mdp3_dmap_update等待vsync。 （六）、Msm8610 lcd driver 内核初始化分析请参考【msm8610 lcd driver code analysis】 （七）、参考资料(特别感谢各位前辈的分析和图示)：(๑乛◡乛๑) 、（ ͡° ͜ʖ ͡°）、（ಡωಡ）！！！累~~~时至今日，终于完整的分析了Android Display System 总体框架流程，不禁感叹计算机世界的博大精深，在这个系列的分析中历练了如何拆解分析一个庞大复杂的模块、学习收获良多，同时也了解了自身知识的欠缺，由于涉及知识较多较广，博主也未能完全吃透，其中分析有误的地方还请各位见谅。所谓路漫漫其修远兮，吾将上下而求索。Todo：专注于Linux &amp;&amp; Android Multimedia（Camera、Video、Audio、Display）系统分析与研究，Multimedia System 任还有许多未解之惑，需恶补Linux内核知识，少年，加油（➽➽➽） （八）、参考资料(特别感谢各位前辈的分析和图示)：Graphics Stack Update高通Android平台-应用空间操作framebuffer dump LCD总结msm8610 lcd driver code analysislinux qcom LCD framworkQualcomm平台 display bring up 过程详解高通8x25平台display模块总结Android 中的 framebufferFrameBuffer驱动程序分析Android Framebuffer介绍及使用Android 图形系统","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Display System（4）：Android Display System 系统分析之Gralloc && HWComposer模块分析","slug":"Android Display System（4）：Android Display System 系统分析之Gralloc && HWComposer模块分析","date":"2018-08-15T16:00:00.000Z","updated":"2018-06-20T15:16:27.801Z","comments":true,"path":"2018/08/16/Android Display System（4）：Android Display System 系统分析之Gralloc && HWComposer模块分析/","link":"","permalink":"http://zhoujinjian.cc/2018/08/16/Android Display System（4）：Android Display System 系统分析之Gralloc && HWComposer模块分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】 【特别感谢 - Android研究 Gralloc &amp;&amp; HWComposer系列分析】【特别感谢 - Android display 系列分析】【特别感谢 - Android图形显示之硬件抽象层Gralloc】 Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) 🌀🌀：专注于Linux &amp;&amp; Android Multimedia（Camera、Video、Audio、Display）系统分析与研究 【Android Display System 系统分析系列】：【Android Display System（1）：Android 7.1.2 (Android N) Android Graphics 系统 分析】【Android Display System（2）：Android Display System 系统分析之Android EGL &amp;&amp; OpenGL】【Android Display System（3）：Android Display System 系统分析之HardwareRenderer.draw()绘制流程分析】【Android Display System（4）：Android Display System 系统分析之Gralloc &amp;&amp; HWComposer模块分析】【Android Display System（5）：Android Display System 系统分析之Display Driver Architecture】 \\hardware\\libhardware\\include\\hardware fb.h \\hardware\\libhardware\\modules\\gralloc framebuffer.cpp gralloc.cpp gralloc_priv.h gr.h mapper.cpp \\hardware\\qcom\\display\\msm8996\\libgralloc alloc_controller.cpp framebuffer.cpp gpu.cpp gralloc.cpp ionalloc.cpp mapper.cpp \\hardware\\qcom\\display\\msm8996\\libgralloc1 gr_adreno_info.cpp gr_allocator.cpp gr_buf_mgr.cpp gr_device_impl.cpp gr_ion_alloc.cpp gr_utils.cpp \\frameworks\\native\\services\\surfaceflinger DisplayDevice.cpp SurfaceFlinger.cpp MonitoredProducer.cpp SurfaceFlingerConsumer.cpp SurfaceFlinger_hwc1.cpp Client.cpp DispSync.cpp EventControlThread.cpp EventThread.cpp Layer.cpp MessageQueue.cpp \\frameworks\\native\\services\\surfaceflinger\\DisplayHardware FramebufferSurface.cpp HWC2.cpp HWC2On1Adapter.cpp HWComposer.cpp HWComposer_hwc1.cpp Linux系统下的显示驱动框架，每个显示屏被抽象为一个帧缓冲区，注册到FrameBuffer模块中，并在/dev/graphics目录下创建对应的fbX设备。Android系统在硬件抽象层中提供了一个Gralloc模块，封装了对帧缓冲区的所有访问操作。用户空间的应用程序在使用帧缓冲区之间，首先要加载Gralloc模块，并且获得一个gralloc设备和一个fb设备。有了gralloc设备之后，用户空间中的应用程序就可以申请分配一块图形缓冲区，并且将这块图形缓冲区映射到应用程序的地址空间来，以便可以向里面写入要绘制的画面的内容。最后，用户空间中的应用程序就通过fb设备来将已经准备好了的图形缓冲区渲染到帧缓冲区中去，即将图形缓冲区的内容绘制到显示屏中去。相应地，当用户空间中的应用程序不再需要使用一块图形缓冲区的时候，就可以通过gralloc设备来释放它，并且将它从地址空间中解除映射。 高通MSM8996 Gralloc模块 实现源码位于：\\hardware\\qcom\\display\\msm8996\\libgralloc每个硬件抽象层模块都必须定义HAL_MODULE_INFO_SYM符号，并且有自己唯一的ID，Gralloc也不例外，Gralloc模块ID定义为： 12345[-&gt;/hardware/libhardware/include/hardware/gralloc.h]/** * The id of this module */#define GRALLOC_HARDWARE_MODULE_ID \"gralloc\" 同时定义了以HAL_MODULE_INFO_SYM为符号的类型为 private_module_t的结构体： 12345678910111213141516171819202122232425262728293031323334hardware\\libhardware\\modules\\gralloc\\gralloc.cpp// HAL module methodsstatic struct hw_module_methods_t gralloc_module_methods = &#123; .open = gralloc_device_open&#125;;// HAL module initializestruct private_module_t HAL_MODULE_INFO_SYM = &#123; .base = &#123; .common = &#123; .tag = HARDWARE_MODULE_TAG, .version_major = 1, .version_minor = 0, .id = GRALLOC_HARDWARE_MODULE_ID, .name = \"Graphics Memory Allocator Module\", .author = \"The Android Open Source Project\", .methods = &amp;gralloc_module_methods, .dso = 0, .reserved = &#123;0&#125;, &#125;, .registerBuffer = gralloc_register_buffer, .unregisterBuffer = gralloc_unregister_buffer, .lock = gralloc_lock, .unlock = gralloc_unlock, .perform = gralloc_perform, .lock_ycbcr = gralloc_lock_ycbcr, &#125;, .framebuffer = 0, .fbFormat = 0, .flags = 0, .numBuffers = 0, .bufferMask = 0, .lock = PTHREAD_MUTEX_INITIALIZER,&#125;; 通过Gralloc模块加载 分析的方法将Gralloc模块加载到内存中来之后，就可以调用函数dlsym来获得它所导出的符号HMI，得到private_module_t的首地址后，由于private_module_t的第一个成员变量的类型为gralloc_module_t，因此也是gralloc_module_t的首地址，由于gralloc_module_t的第一个成员变量类型为hw_module_t，因此也是hw_module_t的首地址，因此只要得到这三种类型中其中一种类型变量的地址，就可以相互转换为其他两种类型的指针。 （一）、Gralloc模块 数据结构在分析Gralloc模块之前，首先介绍Gralloc模块定义的一些数据结构。private_module_t用于描述Gralloc模块下的系统帧缓冲区信息12345678910111213141516[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\fb_priv.h]struct private_module_t &#123; gralloc_module_t base; struct private_handle_t* framebuffer;//指向系统帧缓冲区的句柄 uint32_t fbFormat; uint32_t flags;//用来标志系统帧缓冲区是否支持双缓冲 uint32_t numBuffers;//表示系统帧缓冲区包含有多少个图形缓冲区 uint32_t bufferMask;//记录系统帧缓冲区中的图形缓冲区的使用情况 pthread_mutex_t lock;//一个互斥锁，用来保护结构体private_module_t的并行访问 struct fb_var_screeninfo info;//保存设备显示屏的动态属性信息 struct fb_fix_screeninfo finfo;//保存设备显示屏的固定属性信息 float xdpi;//描述设备显示屏在宽度 float ydpi;//描述设备显示屏在高度 float fps;//用来描述显示屏的刷新频率 uint32_t swapInterval;&#125;; framebuffer_device_t用来描述系统帧缓冲区设备的信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[-&gt;/hardware/libhardware/include/hardware/fb.h]typedef struct framebuffer_device_t &#123; /** * Common methods of the framebuffer device. This *must* be the first member of * framebuffer_device_t as users of this structure will cast a hw_device_t to * framebuffer_device_t pointer in contexts where it's known the hw_device_t references a * framebuffer_device_t. */ struct hw_device_t common; //用来记录系统帧缓冲区的标志 const uint32_t flags; //用来描述设备显示屏的宽度、高度 const uint32_t width; const uint32_t height; //用来描述设备显示屏的一行有多少个像素点 const int stride; //用来描述系统帧缓冲区的像素格式 const int format; //用来描述设备显示屏在宽度上的密度、密度 const float xdpi; const float ydpi; //用来描述设备显示屏的刷新频率 const float fps; //用来设置帧缓冲区交换前后两个图形缓冲区的最小和最大时间间隔 const int minSwapInterval; const int maxSwapInterval; /* Number of framebuffers supported*/ const int numFramebuffers; int reserved[7]; //用来设置帧缓冲区交换前后两个图形缓冲区的最小和最大时间间隔 int (*setSwapInterval)(struct framebuffer_device_t* window, int interval); //用来设置帧缓冲区的更新区域 int (*setUpdateRect)(struct framebuffer_device_t* window, int left, int top, int width, int height); //用来将图形缓冲区buffer的内容渲染到帧缓冲区中去 int (*post)(struct framebuffer_device_t* dev, buffer_handle_t buffer); //用来通知fb设备，图形缓冲区的组合工作已经完成 int (*compositionComplete)(struct framebuffer_device_t* dev); void (*dump)(struct framebuffer_device_t* dev, char *buff, int buff_len); int (*enableScreen)(struct framebuffer_device_t* dev, int enable); void* reserved_proc[6];&#125; framebuffer_device_t; gralloc_module_t用于描述gralloc模块信息 12345678910111213141516171819202122[-&gt;\\hardware\\libhardware\\include\\hardware\\gralloc.h]typedef struct gralloc_module_t &#123; struct hw_module_t common; //映射一块图形缓冲区到一个进程的地址空间去 int (*registerBuffer)(struct gralloc_module_t const* module, buffer_handle_t handle); //取消映射一块图形缓冲区到一个进程的地址空间去 int (*unregisterBuffer)(struct gralloc_module_t const* module, buffer_handle_t handle); int (*lock)(struct gralloc_module_t const* module, buffer_handle_t handle, int usage, int l, int t, int w, int h, void** vaddr); int (*unlock)(struct gralloc_module_t const* module, buffer_handle_t handle); ...... /* reserved for future use */ void* reserved_proc[3];&#125; gralloc_module_t; alloc_device_t用于描述gralloc设备的信息 123456789101112131415[-&gt;\\hardware\\libhardware\\include\\hardware\\gralloc.h]typedef struct alloc_device_t &#123; struct hw_device_t common; int (*alloc)(struct alloc_device_t* dev, int w, int h, int format, int usage, buffer_handle_t* handle, int* stride); int (*free)(struct alloc_device_t* dev, buffer_handle_t handle); void (*dump)(struct alloc_device_t *dev, char *buff, int buff_len); void* reserved_proc[7];&#125; alloc_device_t; 123456789101112[-&gt;/hardware/libhardware/include/hardware/hardware.h]typedef struct hw_module_t &#123; uint32_t tag;//标签 uint16_t version_major;//模块主设备号 uint16_t version_minor;//模块次设备号 const char *id;//模块ID const char *name;//模块名称 const char *author;//模块作者 struct hw_module_methods_t* methods;//模块操作方法 void* dso;//保存模块首地址 uint32_t reserved[32-7];//保留位 &#125; hw_module_t; 模块 设备 作用 private_module_t framebuffer_device_t 将图形缓冲器映射到帧缓冲区 gralloc_module_t alloc_module_t 分配或释放图形缓冲区 hw_module_t hw_module_t 关联设备和模块 硬件抽象层Gralloc模块定义了设备fb和设备gpu： 1234[-&gt;\\hardware\\libhardware\\include\\hardware\\fb.h]#define GRALLOC_HARDWARE_FB0 \"fb0\"[-&gt;/hardware/libhardware/include/hardware/gralloc.h]#define GRALLOC_HARDWARE_GPU0 \"gpu0\" 设备gpu用于分配图形缓冲区，而设备fb用于渲染图形缓冲区；hw_module_t用于描述硬件抽象层Gralloc模块，而hw_device_t则用于描述硬件抽象层Gralloc设备，通过硬件抽象层设备可以找到对应的硬件抽象层模块。在Gralloc模块中，无论是定义fb设备还是gpu设备，都是用来处理图形缓冲区，以下是关于缓冲区的数据结构 定义：private_handle_t用来描述一块缓冲区，Android对缓冲区的定义提供了C和C++两种方式，C++：12345678910111213141516171819202122232425[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\gralloc_priv.h]struct private_handle_t : public native_handle &#123;#else struct private_handle_t &#123; native_handle_t nativeHandle;#endif enum &#123; PRIV_FLAGS_FRAMEBUFFER = 0x00000001, ...... &#125;; // file-descriptors int fd; int fd_metadata; // fd for the meta-data // ints int magic; int flags; unsigned int size; unsigned int offset; int bufferType; ...... int format; int width; int height; ...... 两种编译器下的private_handle_t定义都继承于native_handle，native_handle的定义如下： 123456789[-&gt;/system/core/include/cutils/native_handle.h]typedef struct native_handle &#123; int version; //设置为结构体native_handle_t的大小，用来标识结构体native_handle_t的版本 int numFds; //表示结构体native_handle_t所包含的文件描述符的个数，这些文件描述符保存在成员变量data所指向的一块缓冲区中。 int numInts; //表示结构体native_handle_t所包含的整数值的个数，这些整数保存在成员变量data所指向的一块缓冲区中。 int data[0]; //指向的一块缓冲区中 &#125; native_handle_t; typedef const native_handle_t* buffer_handle_t; 下面就分析Gralloc模块中定义了两种设备的打开过程。 （二）、Fb设备打开过程Fb设备打开过程是从SurfaceFlinger.init()函数通过HWComposer对象初始化过程中打开的 1234567891011121314151617181920[-&gt;\\frameworks\\native\\services\\surfaceflinger\\SurfaceFlinger_hwc1.cpp]void SurfaceFlinger::init() &#123; // initialize EGL for the default display mEGLDisplay = eglGetDisplay(EGL_DEFAULT_DISPLAY); eglInitialize(mEGLDisplay, NULL, NULL); // start the EventThread sp&lt;VSyncSource&gt; vsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, vsyncPhaseOffsetNs, true, \"app\"); mEventThread = new EventThread(vsyncSrc, *this); sp&lt;VSyncSource&gt; sfVsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, sfVsyncPhaseOffsetNs, true, \"sf\"); mSFEventThread = new EventThread(sfVsyncSrc, *this); mEventQueue.setEventThread(mSFEventThread); ...... // Initialize the H/W composer object. There may or may not be an // actual hardware composer underneath. mHwc = new HWComposer(this, *static_cast&lt;HWComposer::EventHandler *&gt;(this)); ...... 看看HWComposer构造函数 1234567891011121314151617181920212223HWComposer::HWComposer( const sp&lt;SurfaceFlinger&gt;&amp; flinger, EventHandler&amp; handler) : mFlinger(flinger), mFbDev(0), mHwc(0), mNumDisplays(1), mCBContext(new cb_context), mEventHandler(handler), mDebugForceFakeVSync(false)&#123; ...... // Note: some devices may insist that the FB HAL be opened before HWC. int fberr = loadFbHalModule(); loadHwcModule(); ......&#125;int HWComposer::loadFbHalModule()&#123; hw_module_t const* module; int err = hw_get_module(GRALLOC_HARDWARE_MODULE_ID, &amp;module); ...... return framebuffer_open(module, &amp;mFbDev);&#125; Android系统在硬件抽象层中提供了一个Gralloc模块，封装了对framebuffer的所有访问操作。Gralloc模块符合Android标准的HAL架构设计。Gralloc对应的hardware id为：GRALLOC_HARDWARE_MODULE_ID 123456[-&gt;\\hardware\\libhardware\\include\\hardware\\fb.h]static inline int framebuffer_open(const struct hw_module_t* module, struct framebuffer_device_t** device) &#123; return module-&gt;methods-&gt;open(module, GRALLOC_HARDWARE_FB0, (struct hw_device_t**)device);&#125; 用户空间的应用程序在使用帧缓冲区之前，首先要加载Gralloc模块，并且获得一个gpu0设备(gralloc_device, modulename:GRALLOC_HARDWARE_GPU0)和一个fb0设备(modulename:GRALLOC_HARDWARE_FB0)。 有了alloc设备之后，用户空间中的应用程序就可以申请分配一块图形缓冲区，并且将这块图形缓冲区映射到应用程序的地址空间来，以便可以向里面写入要绘制的画面的内容。最后，用户空间中的应用程序就通过fb0设备来将已经准备好了的图形缓冲区渲染到帧缓冲区中去，即将图形缓冲区的内容绘制到显示屏中去。相应地，当用户空间中的应用程序不再需要使用一块图形缓冲区的时候，就可以通过alloc设备来释放它，并且将它从地址空间中解除映射。 1234567891011121314151617181920212223[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\gralloc.cpp]// Open Gralloc deviceint gralloc_device_open(const hw_module_t* module, const char* name, hw_device_t** device)&#123; int status = -EINVAL; if (!strcmp(name, GRALLOC_HARDWARE_GPU0)) &#123; const private_module_t* m = reinterpret_cast&lt;const private_module_t*&gt;( module); gpu_context_t *dev; IAllocController* alloc_ctrl = IAllocController::getInstance(); dev = new gpu_context_t(m, alloc_ctrl); if(!dev) return status; *device = &amp;dev-&gt;common; status = 0; &#125; else &#123; //GRALLOC_HARDWARE_FB0, status = fb_device_open(module, name, device); &#125; return status;&#125; 2.1、Fb设备打开过程fb_device_open()看下fb_device_open()函数实现过程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\framebuffer.cpp]int fb_device_open(hw_module_t const* module, const char* name, hw_device_t** device)&#123; int status = -EINVAL; if (!strcmp(name, GRALLOC_HARDWARE_FB0)) &#123; alloc_device_t* gralloc_device; // 打开gralloc_device设备。GRALLOC_HARDWARE_GPU0 status = gralloc_open(module, &amp;gralloc_device); if (status &lt; 0) return status; //创建一个fb_context_t对象，用来描述fb设备上下文 fb_context_t *dev = (fb_context_t*)malloc(sizeof(*dev)); ...... memset(dev, 0, sizeof(*dev)); //初始化fb_context_t对象 /* initialize the procs */ dev-&gt;device.common.tag = HARDWARE_DEVICE_TAG; dev-&gt;device.common.version = 0; dev-&gt;device.common.module = const_cast&lt;hw_module_t*&gt;(module); //注册fb设备的操作函数 dev-&gt;device.common.close = fb_close; dev-&gt;device.setSwapInterval = fb_setSwapInterval; dev-&gt;device.post = fb_post; dev-&gt;device.setUpdateRect = 0; dev-&gt;device.compositionComplete = fb_compositionComplete; //将fb映射到当前进程地址空间 status = mapFrameBuffer((framebuffer_device_t*)dev); private_module_t* m = (private_module_t*)dev-&gt;device.common.module; if (status &gt;= 0) &#123; int stride = m-&gt;finfo.line_length / (m-&gt;info.bits_per_pixel &gt;&gt; 3); const_cast&lt;uint32_t&amp;&gt;(dev-&gt;device.flags) = 0; const_cast&lt;uint32_t&amp;&gt;(dev-&gt;device.width) = m-&gt;info.xres; const_cast&lt;uint32_t&amp;&gt;(dev-&gt;device.height) = m-&gt;info.yres; const_cast&lt;int&amp;&gt;(dev-&gt;device.stride) = stride; const_cast&lt;int&amp;&gt;(dev-&gt;device.format) = m-&gt;fbFormat; const_cast&lt;float&amp;&gt;(dev-&gt;device.xdpi) = m-&gt;xdpi; const_cast&lt;float&amp;&gt;(dev-&gt;device.ydpi) = m-&gt;ydpi; const_cast&lt;float&amp;&gt;(dev-&gt;device.fps) = m-&gt;fps; const_cast&lt;int&amp;&gt;(dev-&gt;device.minSwapInterval) = PRIV_MIN_SWAP_INTERVAL; const_cast&lt;int&amp;&gt;(dev-&gt;device.maxSwapInterval) = PRIV_MAX_SWAP_INTERVAL; const_cast&lt;int&amp;&gt;(dev-&gt;device.numFramebuffers) = m-&gt;numBuffers; dev-&gt;device.setUpdateRect = 0; *device = &amp;dev-&gt;device.common; &#125; // Close the gralloc module gralloc_close(gralloc_device); &#125; return status;&#125; 这个函数主要是用来创建一个fb_context_t结构体，并且对它的成员变量device进行初始化。结构体fb_context_t的成员变量device的类型为framebuffer_device_t，它是用来描述fb设备的。fb设备主要是用来渲染图形缓冲区的，这是通过调用它的成员函数post来实现的。函数fb_device_open所打开的fb设备的成员函数post被设置为Gralloc模块中的函数fb_post。函数mapFrameBuffer除了用来获得系统帧缓冲区的信息之外，还会将系统帧缓冲区映射到当前进程的地址空间来。line_length用来描述显示屏一行像素总共所占用的字节数，bits_per_pixel用来描述显示屏每一个像素所占用的位数，bits_per_pixel的值向右移3位，就可以得到显示屏每一个像素所占用的字节数。用显示屏像素总共所占用的字节数line_length除以每一个像素所占用的字节数就可以得到显示屏一行有多少个像素点，并保存在stride中。 2.2、Fb设备地址空间映射mapFrameBuffer()12345678910111213141516[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\framebuffer.cpp]static int mapFrameBuffer(framebuffer_device_t *dev)&#123; int err = -1; char property[PROPERTY_VALUE_MAX]; if((property_get(\"debug.gralloc.map_fb_memory\", property, NULL) &gt; 0) &amp;&amp; (!strncmp(property, \"1\", PROPERTY_VALUE_MAX ) || (!strncasecmp(property,\"true\", PROPERTY_VALUE_MAX )))) &#123; private_module_t* module = reinterpret_cast&lt;private_module_t*&gt;(dev-&gt;common.module); pthread_mutex_lock(&amp;module-&gt;lock); err = mapFrameBufferLocked(dev); pthread_mutex_unlock(&amp;module-&gt;lock); &#125; return err;&#125; 调用mapFrameBufferLocked函数执行映射过程，该函数在线程保护下完成。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\framebuffer.cpp]int mapFrameBufferLocked(framebuffer_device_t *dev)&#123; private_module_t* module = reinterpret_cast&lt;private_module_t*&gt;(dev-&gt;common.module); fb_context_t *ctx = reinterpret_cast&lt;fb_context_t*&gt;(dev); // already initialized... if (module-&gt;framebuffer) &#123; return 0; &#125; char const * const device_template[] = &#123; \"/dev/graphics/fb%u\", \"/dev/fb%u\", 0 &#125;; int fd = -1; int i=0; char name[64]; char property[PROPERTY_VALUE_MAX]; //检查是否存在设备文件/dev/graphics/fb0或者/dev/fb0。如果存在的话，那么就调用函数open来打开它，并且将得到的文件描述符保存在变量fd中 while ((fd==-1) &amp;&amp; device_template[i]) &#123; snprintf(name, 64, device_template[i], 0); fd = open(name, O_RDWR, 0); i++; &#125; ...... //通过IO控制命令FBIOGET_FSCREENINFO来获得系统帧缓冲区的固定信息，保存在fb_fix_screeninfo结构体finfo中 struct fb_fix_screeninfo finfo; if (ioctl(fd, FBIOGET_FSCREENINFO, &amp;finfo) == -1) &#123; close(fd); return -errno; &#125; //通过IO控制命令FBIOGET_VSCREENINFO来获得系统帧缓冲区的可变信息，保存在fb_var_screeninfo结构体info中 struct fb_var_screeninfo info; if (ioctl(fd, FBIOGET_VSCREENINFO, &amp;info) == -1) &#123; close(fd); return -errno; &#125; //初始化info info.reserved[0] = 0; info.reserved[1] = 0; info.reserved[2] = 0; info.xoffset = 0; info.yoffset = 0; info.activate = FB_ACTIVATE_NOW; /* Interpretation of offset for color fields: All offsets are from the * right, inside a \"pixel\" value, which is exactly 'bits_per_pixel' wide * (means: you can use the offset as right argument to &lt;&lt;). A pixel * afterwards is a bit stream and is written to video memory as that * unmodified. This implies big-endian byte order if bits_per_pixel is * greater than 8. */ if(info.bits_per_pixel == 32) &#123; /* * Explicitly request RGBA_8888 */ info.bits_per_pixel = 32; info.red.offset = 24; info.red.length = 8; info.green.offset = 16; info.green.length = 8; info.blue.offset = 8; info.blue.length = 8; info.transp.offset = 0; info.transp.length = 8; /* Note: the GL driver does not have a r=8 g=8 b=8 a=0 config, so if we * do not use the MDP for composition (i.e. hw composition == 0), ask * for RGBA instead of RGBX. */ if (property_get(\"debug.sf.hw\", property, NULL) &gt; 0 &amp;&amp; atoi(property) == 0) module-&gt;fbFormat = HAL_PIXEL_FORMAT_RGBX_8888; else if(property_get(\"debug.composition.type\", property, NULL) &gt; 0 &amp;&amp; (strncmp(property, \"mdp\", 3) == 0)) module-&gt;fbFormat = HAL_PIXEL_FORMAT_RGBX_8888; else module-&gt;fbFormat = HAL_PIXEL_FORMAT_RGBA_8888; &#125; else &#123; /* * Explicitly request 5/6/5 */ info.bits_per_pixel = 16; info.red.offset = 11; info.red.length = 5; info.green.offset = 5; info.green.length = 6; info.blue.offset = 0; info.blue.length = 5; info.transp.offset = 0; info.transp.length = 0; module-&gt;fbFormat = HAL_PIXEL_FORMAT_RGB_565; &#125; //adreno needs 4k aligned offsets. Max hole size is 4096-1 unsigned int size = roundUpToPageSize(info.yres * info.xres * (info.bits_per_pixel/8)); /* * Request NUM_BUFFERS screens (at least 2 for page flipping) */ int numberOfBuffers = (int)(finfo.smem_len/size); ALOGV(\"num supported framebuffers in kernel = %d\", numberOfBuffers); if (property_get(\"debug.gr.numframebuffers\", property, NULL) &gt; 0) &#123; int num = atoi(property); if ((num &gt;= NUM_FRAMEBUFFERS_MIN) &amp;&amp; (num &lt;= NUM_FRAMEBUFFERS_MAX)) &#123; numberOfBuffers = num; &#125; &#125; if (numberOfBuffers &gt; NUM_FRAMEBUFFERS_MAX) numberOfBuffers = NUM_FRAMEBUFFERS_MAX; ALOGV(\"We support %d buffers\", numberOfBuffers); //consider the included hole by 4k alignment uint32_t line_length = (info.xres * info.bits_per_pixel / 8); info.yres_virtual = (uint32_t) ((size * numberOfBuffers) / line_length); uint32_t flags = PAGE_FLIP; if (info.yres_virtual &lt; ((size * 2) / line_length) ) &#123; // we need at least 2 for page-flipping info.yres_virtual = (int)(size / line_length); flags &amp;= ~PAGE_FLIP; ...... &#125; if (ioctl(fd, FBIOGET_VSCREENINFO, &amp;info) == -1) &#123; close(fd); return -errno; &#125; if (int(info.width) &lt;= 0 || int(info.height) &lt;= 0) &#123; info.width = (uint32_t)(((float)(info.xres) * 25.4f)/160.0f + 0.5f); info.height = (uint32_t)(((float)(info.yres) * 25.4f)/160.0f + 0.5f); &#125; float xdpi = ((float)(info.xres) * 25.4f) / (float)info.width; float ydpi = ((float)(info.yres) * 25.4f) / (float)info.height; ...... //通过IO控制命令FBIOGET_VSCREENINFO来重新获得系统帧缓冲区的可变信息 if (ioctl(fd, FBIOGET_FSCREENINFO, &amp;finfo) == -1) &#123; close(fd); return -errno; &#125;...... module-&gt;flags = flags; module-&gt;info = info; module-&gt;finfo = finfo; module-&gt;xdpi = xdpi; module-&gt;ydpi = ydpi; module-&gt;fps = fps; module-&gt;swapInterval = 1; /* * map the framebuffer */ module-&gt;numBuffers = info.yres_virtual / info.yres; module-&gt;bufferMask = 0; //整个系统帧缓冲区的大小=虚拟分辨率的高度值info.yres_virtual * 每一行所占用的字节数finfo.line_length,并将整个系统帧缓冲区的大小对齐到页面边界 unsigned int fbSize = roundUpToPageSize(finfo.line_length * info.yres)* module-&gt;numBuffers; //系统帧缓冲区在当前进程的地址空间中的起始地址保存到private_handle_t的域base中 void* vaddr = mmap(0, fbSize, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0); ...... //store the framebuffer fd in the ctx ctx-&gt;fbFd = fd; ...... //创建一个private_handle_t，用来描述整个系统帧缓冲区的信息 // Create framebuffer handle using the ION fd module-&gt;framebuffer = new private_handle_t(fd, fbSize, private_handle_t::PRIV_FLAGS_USES_ION, BUFFER_TYPE_UI, module-&gt;fbFormat, info.xres, info.yres); //以读写共享方式将帧缓冲区映射到当前进程地址空间中 module-&gt;framebuffer-&gt;base = uint64_t(vaddr); memset(vaddr, 0, fbSize); //Enable vsync int enable = 1; ioctl(ctx-&gt;fbFd, MSMFB_OVERLAY_VSYNC_CTRL, &amp;enable); return 0;&#125; 2.3、GPU设备打开过程gralloc_open()12345678[-&gt;\\hardware\\libhardware\\include\\hardware\\gralloc.h]/** convenience API for opening and closing a supported device */static inline int gralloc_open(const struct hw_module_t* module, struct alloc_device_t** device) &#123; return module-&gt;methods-&gt;open(module, GRALLOC_HARDWARE_GPU0, (struct hw_device_t**)device);&#125; 最终会走到gralloc_device_open()函数12345678910111213141516171819202122232425[-&gt;\\hardware\\libhardware\\include\\hardware\\gralloc.cpp]// HAL module methodsstatic struct hw_module_methods_t gralloc_module_methods = &#123; .open = gralloc_device_open&#125;;// Open Gralloc deviceint gralloc_device_open(const hw_module_t* module, const char* name, hw_device_t** device)&#123; int status = -EINVAL; if (!strcmp(name, GRALLOC_HARDWARE_GPU0)) &#123; const private_module_t* m = reinterpret_cast&lt;const private_module_t*&gt;( module); gpu_context_t *dev; IAllocController* alloc_ctrl = IAllocController::getInstance(); dev = new gpu_context_t(m, alloc_ctrl); ...... *device = &amp;dev-&gt;common; status = 0; &#125; else &#123; status = fb_device_open(module, name, device); &#125; return status;&#125; 这个函数主要是用来创建一个gpu_context_t 结构体，并且对它的成员变量device进行初始化。gpu_context_t类继承了alloc_device_t，并实现了alloc_device_t中的alloc，free等方法。 1234567891011121314151617[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\gpu.cpp]gpu_context_t::gpu_context_t(const private_module_t* module, IAllocController* alloc_ctrl ) : mAllocCtrl(alloc_ctrl)&#123; // Zero out the alloc_device_t memset(static_cast&lt;alloc_device_t*&gt;(this), 0, sizeof(alloc_device_t)); // Initialize the procs common.tag = HARDWARE_DEVICE_TAG; common.version = 0; common.module = const_cast&lt;hw_module_t*&gt;(&amp;module-&gt;base.common); common.close = gralloc_close; alloc = gralloc_alloc; free = gralloc_free;&#125; 主要是完成alloc_device_t参数的初始化。其成员函数alloc，free被设置成gralloc_alloc &amp; gralloc_free。自此，alloc设备的打开过程就分析完成了。接下来，我们重点分析alloc_device_t中提供的几个关键函数。 （三）、 Gralloc分配和释放Buffer3.1、Gralloc分配buffer先来回忆一下SurfacFlinger图形缓冲区创建过程 12345GraphicBuffer::GraphicBuffer -&gt; initSize -&gt; GraphicBufferAllocator::alloc -&gt; alloc_device_t::alloc -&gt; gralloc_alloc 用户空间的应用程序用到的图形缓冲区是由Gralloc模块中的函数gralloc_alloc来分配的，这个函数实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\gpu.cpp]int gpu_context_t::gralloc_alloc(alloc_device_t* dev, int w, int h, int format, int usage, buffer_handle_t* pHandle, int* pStride)&#123; gpu_context_t* gpu = reinterpret_cast&lt;gpu_context_t*&gt;(dev); return gpu-&gt;alloc_impl(w, h, format, usage, pHandle, pStride, 0);&#125;int gpu_context_t::alloc_impl(int w, int h, int format, int usage, buffer_handle_t* pHandle, int* pStride, unsigned int bufferSize) &#123; ...... // 参数format用来描述要分配的图形缓冲区的颜色格式。这些格式定义在system/core/include/system/graphic.h中 if(format == HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED || format == HAL_PIXEL_FORMAT_YCbCr_420_888) &#123; if (usage &amp; GRALLOC_USAGE_PRIVATE_ALLOC_UBWC) grallocFormat = HAL_PIXEL_FORMAT_YCbCr_420_SP_VENUS_UBWC; else if(usage &amp; GRALLOC_USAGE_HW_VIDEO_ENCODER) grallocFormat = HAL_PIXEL_FORMAT_NV12_ENCODEABLE; //NV12 ...... &#125; // 设置buffertype，BUFFER_TYPE_UI: RGB formats &amp; HAL_PIXEL_FORMAT_R_8 &amp;HAL_PIXEL_FORMAT_RG_88。其他的都为BUFFER_TYPE_VIDEO getGrallocInformationFromFormat(grallocFormat, &amp;bufferType); // 根据formate &amp; w，h算出buffersize size = getBufferSizeAndDimensions(w, h, grallocFormat, usage, alignedw, alignedh); ...... size = (bufferSize &gt;= size)? bufferSize : size; int err = 0; if(useFbMem) &#123; err = gralloc_alloc_framebuffer(usage, pHandle); &#125; else &#123; err = gralloc_alloc_buffer(size, usage, pHandle, bufferType, grallocFormat, alignedw, alignedh); &#125; ...... *pStride = alignedw; return 0;&#125; 最后根据memory alloc出处，区别调用gralloc_alloc_framebuffer&amp; gralloc_alloc_buffer函数。 首先来看看 gralloc_alloc_framebuffer的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\gpu.cpp]int gpu_context_t::gralloc_alloc_framebuffer_locked(int usage, buffer_handle_t* pHandle)&#123; // 变量bufferMask用来描述系统帧缓冲区的使用情况 // 变量numBuffers用来描述系统帧缓冲区可以划分为多少个图形缓冲区来使用 // 变量bufferSize用来描述设备显示屏一屏内容所占用的内存的大小,同时高通的硬件要求4K对齐。 const unsigned int bufferMask = m-&gt;bufferMask; const uint32_t numBuffers = m-&gt;numBuffers; unsigned int bufferSize = m-&gt;finfo.line_length * m-&gt;info.yres; //adreno needs FB size to be page aligned bufferSize = roundUpToPageSize(bufferSize); // 假设此时系统帧缓冲区中尚有空闲的图形缓冲区的，接下来函数就会创建一个private_handle_t结构体hnd来描述这个即将要分配出去的图形缓冲区。注意，这个图形缓冲区的标志值等于PRIV_FLAGS_FRAMEBUFFER，即表示这是一块在系统帧缓冲区中分配的图形缓冲区。 uint64_t vaddr = uint64_t(m-&gt;framebuffer-&gt;base); // As GPU needs ION FD, the private handle is created // using ION fd and ION flags are set private_handle_t* hnd = new private_handle_t( dup(m-&gt;framebuffer-&gt;fd), bufferSize, private_handle_t::PRIV_FLAGS_USES_ION | private_handle_t::PRIV_FLAGS_FRAMEBUFFER, BUFFER_TYPE_UI, m-&gt;fbFormat, m-&gt;info.xres, m-&gt;info.yres); // 接下来的for循环从低位到高位检查变量bufferMask的值，并且找到第一个值等于0的位，这样就可以知道在系统帧缓冲区中，第几个图形缓冲区的是空闲的。注意，变量vadrr的值开始的时候指向系统帧缓冲区的基地址，在下面的for循环中，每循环一次它的值都会增加bufferSize。从这里就可以看出，每次从系统帧缓冲区中分配出去的图形缓冲区的大小都是刚好等于显示屏一屏内容大小的。 // find a free slot for (uint32_t i=0 ; i&lt;numBuffers ; i++) &#123; if ((bufferMask &amp; (1LU&lt;&lt;i)) == 0) &#123; m-&gt;bufferMask |= (uint32_t)(1LU&lt;&lt;i); break; &#125; vaddr += bufferSize; &#125; / 将分配的缓冲区的开始地址保存到变量base中，这样用户控件的应用程序可以直接将需要渲染的图形内容拷贝到这个地址上。这样，就相当于是直接将图形渲染到系统帧缓冲区中去。// offset表示分配到的图形缓冲区的起始地址正对于系统帧缓冲区基地址的偏移量。 hnd-&gt;base = vaddr; hnd-&gt;offset = (unsigned int)(vaddr - m-&gt;framebuffer-&gt;base); *pHandle = hnd; return 0;&#125;int gpu_context_t::gralloc_alloc_framebuffer(int usage, buffer_handle_t* pHandle)&#123; private_module_t* m = reinterpret_cast&lt;private_module_t*&gt;(common.module); pthread_mutex_lock(&amp;m-&gt;lock); int err = gralloc_alloc_framebuffer_locked(usage, pHandle); pthread_mutex_unlock(&amp;m-&gt;lock); return err;&#125; 上面分析了从framebuffer中分配图形缓冲区的过程。总结下这块buffer的来历。首先在fb设备open的时候，通过mmap从fb0中映射一块内存到用户空间，即一个内存池（module-&gt;framebuffer)，通过bufferMask来表示该池中内存的使用情况。而alloc做的事情，就是从这个内存池中找到一个空闲的区块，然后返回该区块的hanlder指针pHandle。 我们现在来看看从内存中分配图形缓冲区的情况。从Android 4.0开始，Android启动新的内存管理方式ION，以取代PMEM。PMEM需要一个连续的物理内存，同时需要在系统启动的时候，就完成分配。 1234567891011121314151617181920212223242526272829303132333435363738394041[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\gpu.cpp]int gpu_context_t::gralloc_alloc_buffer(unsigned int size, int usage, buffer_handle_t* pHandle, int bufferType, int format, int width, int height)&#123; int err = 0; int flags = 0; size = roundUpToPageSize(size); // 首先分配一个data区域 alloc_data data; ...... // 追查代码可以知道mallocCtrl指向IonController对象，关键代码可以参考hardware/qrom/display/msm8974/libgralloc/alloc_controller.cpp。具体怎么从ion中分配buffer data.size = size; data.pHandle = (uintptr_t) pHandle; err = mAllocCtrl-&gt;allocate(data, usage); if (!err) &#123; /* allocate memory for enhancement data */ alloc_data eData; ... int eDataErr = mAllocCtrl-&gt;allocate(eData, eDataUsage); ...... flags |= data.allocType; uint64_t eBaseAddr = (uint64_t)(eData.base) + eData.offset; private_handle_t *hnd = new private_handle_t(data.fd, size, flags, bufferType, format, width, height, eData.fd, eData.offset, eBaseAddr); hnd-&gt;offset = data.offset; hnd-&gt;base = (uint64_t)(data.base) + data.offset; hnd-&gt;gpuaddr = 0; ColorSpace_t colorSpace = ITU_R_601; setMetaData(hnd, UPDATE_COLOR_SPACE, (void*) &amp;colorSpace); *pHandle = hnd; &#125; ALOGE_IF(err, \"gralloc failed err=%s\", strerror(-err)); return err;&#125; 3.2、Gralloc释放buffer释放buffer本质是调用gralloc_free函数，该函数又调用了free_impl函数。在处理free buffer的时候，也是按照两种情况来分别处理的。如果之前这个buffer是从framebuffer分配的话，就只要把bufferMask中设置成0即可。而对应从内存中申请的，则是调用allocCtrl（ion）中的free_buffer来完成释放。 123456789101112131415161718192021222324252627[-&gt;\\hardware\\qcom\\display\\msm8996\\libgralloc\\framebuffer.cpp]int gpu_context_t::free_impl(private_handle_t const* hnd) &#123; private_module_t* m = reinterpret_cast&lt;private_module_t*&gt;(common.module); if (hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_FRAMEBUFFER) &#123; const unsigned int bufferSize = m-&gt;finfo.line_length * m-&gt;info.yres; unsigned int index = (unsigned int) ((hnd-&gt;base - m-&gt;framebuffer-&gt;base) / bufferSize); m-&gt;bufferMask &amp;= (uint32_t)~(1LU&lt;&lt;index); &#125; else &#123; terminateBuffer(&amp;m-&gt;base, const_cast&lt;private_handle_t*&gt;(hnd)); IMemAlloc* memalloc = mAllocCtrl-&gt;getAllocator(hnd-&gt;flags); int err = memalloc-&gt;free_buffer((void*)hnd-&gt;base, hnd-&gt;size, hnd-&gt;offset, hnd-&gt;fd); if(err) return err; // free the metadata space unsigned int size = ROUND_UP_PAGESIZE(sizeof(MetaData_t)); err = memalloc-&gt;free_buffer((void*)hnd-&gt;base_metadata, size, hnd-&gt;offset_metadata, hnd-&gt;fd_metadata); if (err) return err; &#125; delete hnd; return 0;&#125; （四）、图形缓冲区映射过程 图形缓冲区可以从系统帧缓冲区分配也可以从内存中分配，分配一个图形缓冲区后还需要将该图形缓冲区映射到分配该buffer的进程地址空间来，在Android系统中，图形缓冲区的管理由SurfaceFlinger服务来负责。在系统帧缓冲区中分配的图形缓冲区是在SurfaceFlinger服务中使用，而在内存中分配的图形缓冲区既可以在SurfaceFlinger服务中使用，也可以在其它的应用程序中使用。当其它的应用程序需要使用图形缓冲区的时候，它们就会请求SurfaceFlinger服务为它们分配并将SurfaceFlinger服务返回来的图形缓冲区映射到应用程序进程地址空间。在从内存中分配buffer时，已经将分配的buffer映射到了SurfaceFlinger服务进程地址空间，如果该buffer是应用程序请求SurfaceFlinger服务为它们分配的，那么还需要将SurfaceFlinger服务返回来的图形缓冲区映射到应用程序进程地址空间。 一个对象要在进程间传输必须继承于Flattenable类，并且实现flatten和unflatten方法，flatten方法用于序列化该对象，unflatten方法用于反序列化对象。 GraphicBuffer类从模板类Flattenable派生，这个派生类可以通过Parcel传递，通常派生类需要重载flatten和unflatten方法，用于对象的序列化和反序列化。 1）将一个对象写入到Parcel中，需要使用flatten函数序列化该对象，我们先来看下flatten函数： 1234567891011121314151617181920212223242526272829303132333435363738[-&gt;\\frameworks\\native\\libs\\ui\\GraphicBuffer.cpp]status_t GraphicBuffer::flatten(void*&amp; buffer, size_t&amp; size, int*&amp; fds, size_t&amp; count) const &#123; size_t sizeNeeded = GraphicBuffer::getFlattenedSize(); if (size &lt; sizeNeeded) return NO_MEMORY; size_t fdCountNeeded = GraphicBuffer::getFdCount(); if (count &lt; fdCountNeeded) return NO_MEMORY; int32_t* buf = static_cast&lt;int32_t*&gt;(buffer); buf[0] = 'GBFR'; buf[1] = width; buf[2] = height; buf[3] = stride; buf[4] = format; buf[5] = usage; buf[6] = static_cast&lt;int32_t&gt;(mId &gt;&gt; 32); buf[7] = static_cast&lt;int32_t&gt;(mId &amp; 0xFFFFFFFFull); buf[8] = 0; buf[9] = 0; if (handle) &#123; buf[8] = handle-&gt;numFds; buf[9] = handle-&gt;numInts; native_handle_t const* const h = handle; //把handle中的data复制到fds中 memcpy(fds, h-&gt;data, h-&gt;numFds*sizeof(int)); memcpy(&amp;buf[10], h-&gt;data + h-&gt;numFds, h-&gt;numInts*sizeof(int)); &#125; buffer = reinterpret_cast&lt;void*&gt;(static_cast&lt;int*&gt;(buffer) + sizeNeeded); size -= sizeNeeded; if (handle) &#123; fds += handle-&gt;numFds; count -= handle-&gt;numFds; &#125; return NO_ERROR;&#125; 这个handle类型为native_handle_t ，且typedef成了buffer_handle_t，我们贴一下它的定义： 12345678[-&gt;/system/core/include/cutils/native_handle.h]typedef struct native_handle &#123; int version; //设置为结构体native_handle_t的大小，用来标识结构体native_handle_t的版本 int numFds; //表示结构体native_handle_t所包含的文件描述符的个数，这些文件描述符保存在成员变量data所指向的一块缓冲区中。 int numInts; //表示结构体native_handle_t所包含的整数值的个数，这些整数保存在成员变量data所指向的一块缓冲区中。 int data[0]; //指向的一块缓冲区中 &#125; native_handle_t; 所以我们回到flatten函数中，fds参数用来传递文件句柄，函数把handle中的表示指向图形缓冲区文件描述符句柄复制到fds中，因此这些句柄就能通过binder传递到目标进程中去。 2）在应用程序读取来自服务进程的GraphicBuffer对象时，也就是result = reply.read(*p)，会调用GraphicBuffer类的unflatten函数进行反序列化过程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[-&gt;\\frameworks\\native\\libs\\ui\\GraphicBuffer.cpp]status_t GraphicBuffer::unflatten( void const*&amp; buffer, size_t&amp; size, int const*&amp; fds, size_t&amp; count) &#123; if (size &lt; 8*sizeof(int)) return NO_MEMORY; int const* buf = static_cast&lt;int const*&gt;(buffer); if (buf[0] != 'GBFR') return BAD_TYPE; const size_t numFds = buf[8]; const size_t numInts = buf[9]; const size_t sizeNeeded = (10 + numInts) * sizeof(int); if (size &lt; sizeNeeded) return NO_MEMORY; size_t fdCountNeeded = 0; if (count &lt; fdCountNeeded) return NO_MEMORY; if (handle) &#123; // free previous handle if any free_handle(); &#125; if (numFds || numInts) &#123; width = buf[1]; height = buf[2]; stride = buf[3]; format = buf[4]; usage = buf[5]; //创建一个native_handle对象 native_handle* h = native_handle_create(numFds, numInts); //将fds复制到native_handle对象的data中，和flatten操作相反 memcpy(h-&gt;data, fds, numFds*sizeof(int)); memcpy(h-&gt;data + numFds, &amp;buf[10], numInts*sizeof(int)); handle = h; &#125; else &#123; width = height = stride = format = usage = 0; handle = NULL; &#125; mId = static_cast&lt;uint64_t&gt;(buf[6]) &lt;&lt; 32; mId |= static_cast&lt;uint32_t&gt;(buf[7]); mOwner = ownHandle; if (handle != 0) &#123; //使用GraphicBufferMapper将服务端创建的图形缓冲区映射到当前进程地址空间 status_t err = mBufferMapper.registerBuffer(handle); if (err != NO_ERROR) &#123; width = height = stride = format = usage = 0; handle = NULL; ...... return err; &#125; &#125; buffer = reinterpret_cast&lt;void const*&gt;(static_cast&lt;int const*&gt;(buffer) + sizeNeeded); size -= sizeNeeded; fds += numFds; count -= numFds; return NO_ERROR;&#125; 调用unflatten函数时，共享区的文件句柄已经准备好了，但是内存还没有进行映射，调用了mBufferMapper.registerBuffer函数来进行内存映射。 4.1、图形缓冲区的注册过程12345status_t GraphicBufferMapper::registerBuffer(const GraphicBuffer* buffer)&#123; gralloc1_error_t error = mDevice-&gt;retain(buffer); return error;&#125; 用了mDevice-&gt;retain(buffer)函数， 123456789101112131415161718192021[-&gt;\\frameworks\\native\\libs\\ui\\Gralloc1On0Adapter.cpp]gralloc1_error_t Gralloc1On0Adapter::retain( const std::shared_ptr&lt;Buffer&gt;&amp; buffer)&#123; std::lock_guard&lt;std::mutex&gt; lock(mBufferMutex); buffer-&gt;retain(); return GRALLOC1_ERROR_NONE;&#125;gralloc1_error_t Gralloc1On0Adapter::retain( const android::GraphicBuffer* graphicBuffer)&#123; ...... buffer_handle_t handle = graphicBuffer-&gt;getNativeBuffer()-&gt;handle; std::lock_guard&lt;std::mutex&gt; lock(mBufferMutex); ALOGV(\"Calling registerBuffer(%p)\", handle); int result = mModule-&gt;registerBuffer(mModule, handle); ......&#125; 经过一系列步骤的调用 123456789101112131415161718192021222324252627282930313233343536373839[-&gt;/hardware/qcom/display/msm8996/libgralloc/mapper.cpp]int gralloc_register_buffer(gralloc_module_t const* module, buffer_handle_t handle)&#123; ...... int err = gralloc_map(module, handle); ...... return err;&#125;static int gralloc_map(gralloc_module_t const* module, buffer_handle_t handle)&#123; ...... private_handle_t* hnd = (private_handle_t*)handle; unsigned int size = 0; int err = 0; IMemAlloc* memalloc = getAllocator(hnd-&gt;flags) ; void *mappedAddress = MAP_FAILED; hnd-&gt;base = 0; // Dont map framebuffer and secure buffers if (!(hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_FRAMEBUFFER) &amp;&amp; !(hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_SECURE_BUFFER)) &#123; size = hnd-&gt;size; err = memalloc-&gt;map_buffer(&amp;mappedAddress, size, hnd-&gt;offset, hnd-&gt;fd); ...... hnd-&gt;base = uint64_t(mappedAddress) + hnd-&gt;offset; &#125; else &#123; err = -EACCES; &#125; //Allow mapping of metadata for all buffers including secure ones, but not //of framebuffer int metadata_err = gralloc_map_metadata(handle); return err;&#125; 进一步调用 1234567891011121314151617[-&gt;/hardware/qcom/display/msm8996/libgralloc/ionalloc.cpp]int IonAlloc::map_buffer(void **pBase, unsigned int size, unsigned int offset, int fd)&#123; ATRACE_CALL(); int err = 0; void *base = 0; // It is a (quirky) requirement of ION to have opened the // ion fd in the process that is doing the mapping err = open_device(); ...... base = mmap(0, size, PROT_READ| PROT_WRITE, MAP_SHARED, fd, 0); *pBase = base; ....... return err;&#125; 这个函数就是调用了mmap来进行共享内存的映射。 4.2、图形缓冲区的释放过程释放过程调用流程类似，最后会调用unmap_buffer()释放图像缓冲区。 4.3、小结 （五）HWComposer模块前面分析HWComposer构造函数没有分析loadHwcModule()函数，loadHwcModule()函数用来加载HWC模块，我们继续查看： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[E-&gt;\\frameworks\\native\\services\\surfaceflinger\\DisplayHardware\\HWComposer_hwc1.cpp]HWComposer::HWComposer( const sp&lt;SurfaceFlinger&gt;&amp; flinger, EventHandler&amp; handler) : mFlinger(flinger), mFbDev(0), mHwc(0), mNumDisplays(1), mCBContext(new cb_context),//这里直接new了一个设备上下文对象 mEventHandler(handler), mDebugForceFakeVSync(false)&#123; ...... //装载HWComposer的硬件模块,这个函数中会将mHwc置为true loadHwcModule(); ...... //硬件vsync信号 if (mHwc) &#123; ALOGI(\"Using %s version %u.%u\", HWC_HARDWARE_COMPOSER, (hwcApiVersion(mHwc) &gt;&gt; 24) &amp; 0xff, (hwcApiVersion(mHwc) &gt;&gt; 16) &amp; 0xff); if (mHwc-&gt;registerProcs) &#123; //HWComposer设备上下文变量mCBContext赋值 mCBContext-&gt;hwc = this; //函数指针钩子函数hook_invalidate放入上下文 mCBContext-&gt;procs.invalidate = &amp;hook_invalidate; //vsync钩子函数放入上下文 mCBContext-&gt;procs.vsync = &amp;hook_vsync; if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) //hotplug狗子函数放入上下文 mCBContext-&gt;procs.hotplug = &amp;hook_hotplug; else mCBContext-&gt;procs.hotplug = NULL; memset(mCBContext-&gt;procs.zero, 0, sizeof(mCBContext-&gt;procs.zero)); //将钩子函数注册进硬件设备，硬件驱动回调这些钩子函数 mHwc-&gt;registerProcs(mHwc, &amp;mCBContext-&gt;procs); &#125; // don't need a vsync thread if we have a hardware composer //如果有硬件vsync信号， 则不需要软件vsync实现 needVSyncThread = false; ...... &#125;&#125;// Load and prepare the hardware composer module. Sets mHwc.void HWComposer::loadHwcModule()&#123; hw_module_t const* module; //同样是HAL层封装的函数，参数是HWC_HARDWARE_MODULE_ID，加载hwc模块 if (hw_get_module(HWC_HARDWARE_MODULE_ID, &amp;module) != 0) &#123; return; &#125; //打开hwc设备 int err = hwc_open_1(module, &amp;mHwc); ......&#125; 如果硬件设备打开成功，则将钩子函数hook_invalidate、hook_vsync和hook_hotplug注册进硬件设备，作为回调函数。这三个都是硬件产生事件信号，通知上层SurfaceFlinger的回调函数，用于处理这个信号。 因为我们本节是Vsync信号相关，所以我们只看看hook_vsync钩子函数。这里指定了vsync的回调函数是hook_vsync，如果硬件中产生了VSync信号，将通过这个函数来通知上层，看看它的代码： 1234567[E-&gt;\\frameworks\\native\\services\\surfaceflinger\\DisplayHardware\\HWComposer_hwc1.cpp]void HWComposer::hook_vsync(const struct hwc_procs* procs, int disp, int64_t timestamp) &#123; cb_context* ctx = reinterpret_cast&lt;cb_context*&gt;( const_cast&lt;hwc_procs_t*&gt;(procs)); ctx-&gt;hwc-&gt;vsync(disp, timestamp);&#125; hook_vsync钩子函数会调用vsync函数，我们继续看： 12345678910111213141516[E-&gt;\\frameworks\\native\\services\\surfaceflinger\\DisplayHardware\\HWComposer_hwc1.cpp]void HWComposer::vsync(int disp, int64_t timestamp) &#123; if (uint32_t(disp) &lt; HWC_NUM_PHYSICAL_DISPLAY_TYPES) &#123; &#123; Mutex::Autolock _l(mLock); mLastHwVSync[disp] = timestamp; &#125; char tag[16]; snprintf(tag, sizeof(tag), \"HW_VSYNC_%1u\", disp); ATRACE_INT(tag, ++mVSyncCounts[disp] &amp; 1); //这里调用EventHandler类型变量mEventHandler就是SurfaceFlinger， //所以调用了SurfaceFlinger的onVSyncReceived函数 mEventHandler.onVSyncReceived(disp, timestamp); &#125;&#125; mEventHandler对象类型为EventHandler，我们在SurfaceFlinger的init函数创建HWComposer类实例时候讲SurfaceFlinger强转为EventHandler作为构造函数的参数传入其中。再者SurfaceFlinger继承HWComposer::EventHandler，所以最终会调用SurfaceFlinger的onVSyncReceived函数，这就是硬件vsync信号的产生。 5.1、HWC设备打开过程1234567[-&gt;/hardware/libhardware/include/hardware/hwcomposer.h]/** convenience API for opening and closing a device */static inline int hwc_open_1(const struct hw_module_t* module, hwc_composer_device_1_t** device) &#123; return module-&gt;methods-&gt;open(module, HWC_HARDWARE_COMPOSER, (struct hw_device_t**)device);&#125; 具体实现/hardware/qcom/display/msm8996/sdm/libs/hwc/ or /hardware/qcom/display/msm8996/sdm/libs/hwc2/。 123456789101112131415161718192021222324252627282930313233343536373839404142434445[-&gt;/hardware/qcom/display/msm8996/sdm/libs/hwc/hwc_session.h] struct HWCModuleMethods : public hw_module_methods_t &#123; HWCModuleMethods() &#123; hw_module_methods_t::open = HWCSession::Open; &#125; &#125;;[-&gt;/hardware/qcom/display/msm8996/sdm/libs/hwc/hwc_session.cpp]int HWCSession::Open(const hw_module_t *module, const char *name, hw_device_t **device) &#123; ...... if (!strcmp(name, HWC_HARDWARE_COMPOSER)) &#123; HWCSession *hwc_session = new HWCSession(module); ...... int status = hwc_session-&gt;Init(); ...... hwc_composer_device_1_t *composer_device = hwc_session; *device = reinterpret_cast&lt;hw_device_t *&gt;(composer_device); &#125; return 0;&#125;HWCSession::HWCSession(const hw_module_t *module) &#123; // By default, drop any events. Calls will be routed to SurfaceFlinger after registerProcs. hwc_procs_default_.invalidate = Invalidate; hwc_procs_default_.vsync = VSync; hwc_procs_default_.hotplug = Hotplug; hwc_composer_device_1_t::common.tag = HARDWARE_DEVICE_TAG; hwc_composer_device_1_t::common.version = HWC_DEVICE_API_VERSION_1_5; hwc_composer_device_1_t::common.module = const_cast&lt;hw_module_t*&gt;(module); hwc_composer_device_1_t::common.close = Close; hwc_composer_device_1_t::prepare = Prepare; hwc_composer_device_1_t::set = Set; hwc_composer_device_1_t::eventControl = EventControl; hwc_composer_device_1_t::setPowerMode = SetPowerMode; hwc_composer_device_1_t::query = Query; hwc_composer_device_1_t::registerProcs = RegisterProcs; hwc_composer_device_1_t::dump = Dump; hwc_composer_device_1_t::getDisplayConfigs = GetDisplayConfigs; hwc_composer_device_1_t::getDisplayAttributes = GetDisplayAttributes; hwc_composer_device_1_t::getActiveConfig = GetActiveConfig; hwc_composer_device_1_t::setActiveConfig = SetActiveConfig; hwc_composer_device_1_t::setCursorPositionAsync = SetCursorPositionAsync;&#125; （六）、参考资料(特别感谢各位前辈的分析和图示)： Android研究 Gralloc &amp;&amp; HWComposer系列分析Android Display 系列分析Android display: framebuffer 映射关系Android display框架与数据流Android图形显示之硬件抽象层GrallocSurfaceFlinger中Buffer的创建与显示Android 图形系统之gralloc深入剖析Android系统 显示模块Android SurfaceFlinger 学习之路","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Display System（3）：Android Display System 系统分析 之  HardwareRenderer.draw()绘制流程分析","slug":"Android Display System（3）：Android Display System 系统分析之HardwareRenderer.draw绘制流程分析","date":"2018-07-31T16:00:00.000Z","updated":"2018-06-20T15:11:18.995Z","comments":true,"path":"2018/08/01/Android Display System（3）：Android Display System 系统分析之HardwareRenderer.draw绘制流程分析/","link":"","permalink":"http://zhoujinjian.cc/2018/08/01/Android Display System（3）：Android Display System 系统分析之HardwareRenderer.draw绘制流程分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】 【特别感谢 - Android应用程序UI硬件加速渲染技术简要介绍和学习计划】【特别感谢 - Android N中UI硬件渲染（hwui）的HWUI_NEW_OPS(基于Android 7.1)】【特别感谢 - Android DisplayList 构建过程】 Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) 🌀🌀：专注于Linux &amp;&amp; Android Multimedia（Camera、Video、Audio、Display）系统分析与研究 【Android Display System 系统分析系列】：【Android Display System（1）：Android 7.1.2 (Android N) Android Graphics 系统 分析】【Android Display System（2）：Android Display System 系统分析之Android EGL &amp;&amp; OpenGL】【Android Display System（3）：Android Display System 系统分析之HardwareRenderer.draw()绘制流程分析】【Android Display System（4）：Android Display System 系统分析之Gralloc &amp;&amp; HWComposer模块分析】【Android Display System（5）：Android Display System 系统分析之Display Driver Architecture】 \\frameworks\\base\\core\\java\\android\\view\\ ViewRootImpl.java RenderNode.java View.java DisplayListCanvas.java \\frameworks\\base\\core\\jni\\ android_view_DisplayListCanvas.cpp android_view_RenderNode.cpp android_view_ThreadedRenderer.cpp android_view_Surface.cpp android_server_AssetAtlasService.cpp \\frameworks\\base\\core\\jni\\android\\graphics\\ Graphics.cpp Bitmap.cpp \\frameworks\\base\\libs\\hwui\\ AssetAtlas.cpp BakedOpDispatcher.cpp BakedOpRenderer.cpp DeferredDisplayList.cpp DeferredLayerUpdater.cpp DisplayList.cpp DisplayListCanvas.cpp LayerBuilder.cpp LayerRenderer.cpp LayerUpdateQueue.cpp Patch.cpp RecordingCanvas.cpp RenderNode.cpp \\frameworks\\base\\libs\\hwui\\hwui\\ Canvas.cpp \\frameworks\\base\\libs\\hwui\\renderthread\\ EglManager.cpp CanvasContext.cpp DrawFrameTask.cpp RenderProxy.cpp RenderTask.cpp RenderThread.cpp UI作为用户体验的核心之一，始终是Android每次升级中的重点。从Androd 3.0(Honeycomb)开始，Android开始支持hwui（UI硬件加速）。到Android 4.0（ICS）时，硬件加速被默认开启。同时ICS还引入了DisplayList的概念（不是OpenGL里的那个），它相当于是从View的绘制命令到GL命令之间的“中间语言”。它记录了绘制该View所需的全部信息，之后只要重放（replay）即可完成内容的绘制。这样如果View没有改动或只部分改动，便可重用或修改DisplayList，从而避免调用了一些上层代码，提高了效率。Android 4.3（JB）中引入了DisplayList的defer操作，它主要用于对DisplayList中命令进行Batch（批次）和Merge（合并）。这样可以减少GL draw call和context切换以提高效率。之后，在Android 5.0（Lollipop）中又引入了RenderNode（渲染节点）的概念，它是对DisplayList及一些View显示属性的进一步封装。代码上，一个View对应一个RenderNode（Native层对应同名类），其中管理着对应的DisplayList和OffscreenBuffer（如果该View为硬件绘制层）。每个向WindowManagerService注册的窗口对应一个RootRenderNode，通过它可以找到View层次结构中所有View的DisplayList信息。在Java层的DisplayListCanvas用于生成DisplayList，其在native层的对应类为RecordingCanvas（在Android N前为DisplayListCanvas）。另外Android L中还引入了RenderThread（渲染线程）。所有的GL命令执行都放到这个线程上。渲染线程在RenderNode中存有渲染帧的所有信息，且还监听VSync信号，因此可以独立做一些属性动画。这样即便主线程block也可以保证动画流畅。引入渲染线程后ThreadedRenderer替代了Gl20Renderer，作为proxy用于主线程（UI线程）把渲染任务交给渲染线程。近期，在Android 7.0（Nougat）中又对hwui进行了小规模重构，引入了BakedOpRenderer, FrameBuilder, LayerBuilder, RecordingCanvas等类，用宏HWUI_NEW_OPS管理。下面简单介绍下这些新成员： ☯ RecordingCanvas: 之前Java层的DisplayListCanvas对应native层的DisplayListCanvas。引入RecordingCanvas后，其在native层的对应物就变成了RecordingCanvas。和DisplayListCanvas类似，画在RecordingCanvas上的内容都会被记录在RenderNode的DisplayList中。 ☯ BakedOpRenderer: 顾名思义，就是用于绘制batch/merge好的操作。用于替代之前的OpenGLRenderer。它是真正用GL绘制到on-screen surface上的。 ☯ BakedOpDispatcher: 提供一系列onXXX（如onBitmapOp）和onMergedXXX（如onMergedBitmapOps）静态函数供replay时调用。这些dispatch函数最后一般都会通过GlopBuilder来构造Glop然后通过BakedOpRenderer的renderGlop()函数来用OpenGL绘制。 ☯ LayerBuilder: 用于存储绘制某一层的操作和状态。替代了部分原DeferredDisplayList的工作。对于所有View通用，即如果View有render layer，它对应一个FBO；如果对于普通View，它对应的是SurfaceFlinger提供的surface。 其中的mBatches存储了当前层defer后（即batch/merge好）的绘制操作。 ☯ FrameBuilder: 管理某一帧的构建，用于处理，优化和存储从RenderNode和LayerUpdateQueue中来的渲染命令，同时它的replayBakedOps()方法还用于该帧的绘制命令重放。一帧中可能需要绘制多个层，每一层的上下文都会存在相应的LayerBuilder中。在FrameBuilder中通过mLayerBuilders和mLayerStack存储一个layer stack。它替代了原Snapshot类的一部分功能。 ☯ OffscreenBuffer: 用于替代Layer类，但是设计上更轻量，而且自带内存池（通过OffscreenBufferPool）。 ☯ LayerUpdateQueue：用于记录类型为硬件绘制层的RenderNode的更新操作。之后会通过FrameBuilder将该layer对应的RenderNode通过deferNodeOps()方法进行处理。 ☯ RecordedOp: 由RecordedCanvas将View中的绘制命令转化为RecordedOp。RecordedOp也是DisplayList中的基本元素，用于替代Android N之前的DisplayListOp。它有一坨各式各样的继承类代表各种各样的绘制操作。BakedOpState是RecordedOp和相应的状态的自包含封装（封装的过程称为bake）。 ☯ BatchBase: LayerBuilder中对DisplayList进行batch/merge处理后的结果以BatchBase形式保存在LayerBuilder的mBatches成员中。它有两个继承类分别为OpBatch和MergingOpBatch，分别用于不可合并和可合并操作。 概括下它们和相关类的关系图如下，接下来从DisplayList(RenderNode)的构建和绘制两个阶段分析下具体有哪些改动。 首先看看总体时序图：然后一步一步分析： （一）、Android硬件渲染环境初始化ViewRootImpl.enableHardwareAcceleration()在ViewRootImpl.java的setView里，会去enable硬件加速功能。如果当前创建的窗口支持硬件加速渲染，那么就会创建一个HardwareRenderer对象，这个HardwareRenderer对象以后将负责执行窗口硬件加速渲染的相关操作。123456789101112131415[-&gt;\\frameworks\\base\\core\\java\\android\\view\\ViewRootImpl.java]private void enableHardwareAcceleration(WindowManager.LayoutParams attrs) &#123; ...... if (hardwareAccelerated) &#123; ...... if (fakeHwAccelerated) &#123; ...... &#125; else if (!ThreadedRenderer.sRendererDisabled || (ThreadedRenderer.sSystemRendererDisabled &amp;&amp; forceHwAccelerated)) &#123; ...... mAttachInfo.mHardwareRenderer = ThreadedRenderer.create(mContext, translucent); ...... &#125; &#125; &#125; ThreadedRenderer类的静态成员函数create的实现如下所示： 1.1、ThreadedRenderer创建过程123456789101112131415161718192021[-&gt;frameworks\\base\\core\\java\\android\\view\\ThreadedRenderer.java] public static ThreadedRenderer create(Context context, boolean translucent) &#123; ThreadedRenderer renderer = null; if (DisplayListCanvas.isAvailable()) &#123; renderer = new ThreadedRenderer(context, translucent); &#125; return renderer; &#125; ThreadedRenderer(Context context, boolean translucent) &#123; ...... //1、nCreateRootRenderNode在Native层创建了一个Render Node long rootNodePtr = nCreateRootRenderNode(); mRootNode = RenderNode.adopt(rootNodePtr); ...... //2、nCreateProxy在Native层创建了一个Render Proxy对象 mNativeProxy = nCreateProxy(translucent, rootNodePtr); //3、初始化一个系统预加载资源的地图集,优化资源的内存使用 ProcessInitializer.sInstance.init(context, mNativeProxy); loadSystemProperties(); &#125; 1.1.1、创建RenderNodenCreateRootRenderNode在Native层创建了一个Render Node。 从这里就可以看出，窗口在Native层的Root Render Node实际上是一个RootRenderNode对象。12345678910111213141516[-&gt;\\frameworks\\base\\core\\jni\\android_view_ThreadedRenderer.cpp]class RootRenderNode : public RenderNode, ErrorHandler &#123;public: RootRenderNode(JNIEnv* env) : RenderNode() &#123; mLooper = Looper::getForThread(); env-&gt;GetJavaVM(&amp;mVm); &#125; ......&#125;static jlong android_view_ThreadedRenderer_createRootRenderNode(JNIEnv* env, jobject clazz) &#123; RootRenderNode* node = new RootRenderNode(env); node-&gt;incStrong(0); node-&gt;setName(\"RootRenderNode\"); return reinterpret_cast&lt;jlong&gt;(node);&#125; 可以看出会创建一个C++层的RootRenderNode对象，RootRenderNode继承自RenderNode，去看看RenderNode构造函数。 123456789[-&gt;\\frameworks\\base\\libs\\hwui\\RenderNode.cpp]RenderNode::RenderNode() : mDirtyPropertyFields(0) , mNeedsDisplayListSync(false) , mDisplayList(nullptr) , mStagingDisplayList(nullptr) , mAnimatorManager(*this) , mParentCount(0) &#123;&#125; 有了这个RootRenderNode对象之后，函数android_view_ThreadedRenderer_createProxy就创建了一个RenderProxy对象。 1.1.2、创建RenderProxy1234567[-&gt;\\frameworks\\base\\core\\jni\\android_view_ThreadedRenderer.cpp]static jlong android_view_ThreadedRenderer_createProxy(JNIEnv* env, jobject clazz, jboolean translucent, jlong rootRenderNodePtr) &#123; RootRenderNode* rootRenderNode = reinterpret_cast&lt;RootRenderNode*&gt;(rootRenderNodePtr); ContextFactoryImpl factory(rootRenderNode); return (jlong) new RenderProxy(translucent, rootRenderNode, &amp;factory);&#125; RenderProxy对象的创建过程如下所示 123456789101112 [-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\RenderProxy.cpp] RenderProxy::RenderProxy(bool translucent, RenderNode* rootRenderNode, IContextFactory* contextFactory) : mRenderThread(RenderThread::getInstance()) , mContext(nullptr) &#123; SETUP_TASK(createContext); args-&gt;translucent = translucent; args-&gt;rootRenderNode = rootRenderNode; args-&gt;thread = &amp;mRenderThread; args-&gt;contextFactory = contextFactory; mContext = (CanvasContext*) postAndWait(task); mDrawFrameTask.setContext(&amp;mRenderThread, mContext, rootRenderNode);&#125; RenderProxy类有三个重要的成员变量mRenderThread、mContext和mDrawFrameTask，mRenderThread描述的就是Render Thread，mContext描述的是一个画布上下文，mDrawFrameTask描述的是一个用来执行渲染任务的Task 1.1.2.1、RenderThread 实例化来看看RenderThread::getInstance()实例化过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\RenderThread.cpp]RenderThread&amp; RenderThread::getInstance() &#123; static RenderThread* sInstance = new RenderThread(); gHasRenderThreadInstance = true; return *sInstance;&#125;RenderThread::RenderThread() : Thread(true) , mNextWakeup(LLONG_MAX) , mDisplayEventReceiver(nullptr) , mVsyncRequested(false) , mFrameCallbackTaskPending(false) , mFrameCallbackTask(nullptr) , mRenderState(nullptr) , mEglManager(nullptr) &#123; Properties::load(); mFrameCallbackTask = new DispatchFrameCallbacks(this); mLooper = new Looper(false); run(\"RenderThread\");&#125;//RenderThread类的成员变量mFrameCallbackTask描述的Task是用来做什么的呢？原来就是用来显示动画的。当Java层注册一个动画//类型的Render Node到Render Thread时，一个类型为IFrameCallback的回调接口就会通过RenderThread类的成员函数//postFrameCallback注册到Render Thread的一个Pending Registration Frame Callbacks列表中class DispatchFrameCallbacks : public RenderTask &#123;private: RenderThread* mRenderThread;public: DispatchFrameCallbacks(RenderThread* rt) : mRenderThread(rt) &#123;&#125; virtual void run() override &#123; mRenderThread-&gt;dispatchFrameCallbacks(); &#125;&#125;;void RenderThread::dispatchFrameCallbacks() &#123; ...... if (callbacks.size()) &#123; ...... requestVsync(); for (std::set&lt;IFrameCallback*&gt;::iterator it = callbacks.begin(); it != callbacks.end(); it++) &#123; (*it)-&gt;doFrame(); &#125; &#125;&#125; 1、mFrameCallbackTask指向一个DispatchFrameCallbacks对象，用来描述一个帧绘制任务2、mLooper指向一个Looper对象，消息驱动模型3、RenderThread类是从Thread类继承下来的，当我们调用它的成员函数run的时候，就会创建一个新的线程。这个新的线程的入口点函数为RenderThread类的成员函数threadLoop，它的实现如下所示： 1234567891011121314151617181920212223242526272829303132[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\RenderThread.cpp]bool RenderThread::threadLoop() &#123; setpriority(PRIO_PROCESS, 0, PRIORITY_DISPLAY); initThreadLocals(); int timeoutMillis = -1; for (;;) &#123; int result = mLooper-&gt;pollOnce(timeoutMillis); nsecs_t nextWakeup; // Process our queue, if we have anything while (RenderTask* task = nextTask(&amp;nextWakeup)) &#123; task-&gt;run(); // task may have deleted itself, do not reference it again &#125; ...... if (mPendingRegistrationFrameCallbacks.size() &amp;&amp; !mFrameCallbackTaskPending) &#123; drainDisplayEventQueue(); mFrameCallbacks.insert( mPendingRegistrationFrameCallbacks.begin(), mPendingRegistrationFrameCallbacks.end()); mPendingRegistrationFrameCallbacks.clear(); requestVsync(); &#125; if (!mFrameCallbackTaskPending &amp;&amp; !mVsyncRequested &amp;&amp; mFrameCallbacks.size()) &#123; requestVsync(); &#125; &#125; return false;&#125; 这里我们就可以看到Render Thread的运行模型：1、空闲的时候，Render Thread就睡眠在成员变量mLooper指向的一个Looper对象的成员函数pollOnce中。2、当其它线程需要调度Render Thread，就会向它的任务队列增加一个任务，然后唤醒Render Thread进行处理。Render Thread通过成员函数nextTask获得需要处理的任务，并且调用它的成员函数run进行处理。RenderThread类的成员函数nextTask的实现如下所示： 1234567891011121314151617[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\RenderThread.cpp]RenderTask* RenderThread::nextTask(nsecs_t* nextWakeup) &#123; AutoMutex _lock(mLock); RenderTask* next = mQueue.peek(); if (!next) &#123; mNextWakeup = LLONG_MAX; &#125; else &#123; ...... if (next-&gt;mRunAt &lt;= 0 || next-&gt;mRunAt &lt;= systemTime(SYSTEM_TIME_MONOTONIC)) &#123; next = mQueue.next(); &#125; else &#123; next = nullptr; &#125; &#125; ...... return next;&#125; 注意，如果没有下一个任务可以执行，那么RenderThread类的成员函数nextTask通过参数nextWakeup返回的值为LLONG_MAX，表示Render Thread接下来无限期进入睡眠状态，直到被其它线程唤醒为止。 RenderThread类提供了queue、queueAndWait、queueAtFront和queueAt四个成员函数向Task Queue增加一个Task，它们的实现如下所示： 123456789101112131415161718192021222324252627282930[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\RenderThread.cpp]void RenderThread::queue(RenderTask* task) &#123; AutoMutex _lock(mLock); mQueue.queue(task); if (mNextWakeup &amp;&amp; task-&gt;mRunAt &lt; mNextWakeup) &#123; mNextWakeup = 0; mLooper-&gt;wake(); &#125;&#125;void RenderThread::queueAndWait(RenderTask* task) &#123; Mutex mutex; Condition condition; SignalingRenderTask syncTask(task, &amp;mutex, &amp;condition); AutoMutex _lock(mutex); queue(&amp;syncTask); condition.wait(mutex);&#125;void RenderThread::queueAtFront(RenderTask* task) &#123; AutoMutex _lock(mLock); mQueue.queueAtFront(task); mLooper-&gt;wake();&#125;void RenderThread::queueAt(RenderTask* task, nsecs_t runAtNs) &#123; task-&gt;mRunAt = runAtNs; queue(task);&#125; 再来看Render Thread在进入无限循环之前调用的initThreadLocals()函数 1234567891011[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\RenderThread.cpp]void RenderThread::initThreadLocals() &#123; sp&lt;IBinder&gt; dtoken(SurfaceComposerClient::getBuiltInDisplay( ISurfaceComposer::eDisplayIdMain)); status_t status = SurfaceComposerClient::getDisplayInfo(dtoken, &amp;mDisplayInfo); ...... initializeDisplayEventReceiver(); mEglManager = new EglManager(*this); mRenderState = new RenderState(*this); mJankTracker = new JankTracker(mDisplayInfo);&#125; 1、initializeDisplayEventReceiver创建和初始化一个DisplayEventReceiver对象，用来接收Vsync信号。产生Vsync信号时，SurfaceFlinger服务（Vsync信号由SurfaceFlinger服务进行管理和分发）会通过上述文件描述符号唤醒Render Thread。这时候Render Thread就会调用RenderThread类的静态成员函数displayEventReceiverCallback()-&gt;drainDisplayEventQueue()-&gt;2、创建一个EglManager对象，之后会调用EglManager::initialize() Open GL ES环境初始化3、创建一个RenderState对象（记录Render Thread当前的一些渲染状态） 1.1.2.2、CanvasContext(画布上下文)的初始化过程了解了Render Thread的创建过程之后，回到RenderProxy类的构造函数中，接下来我们继续分析它的成员变量mContext的初始化过程，也就是画布上下文的初始化过程。这是通过向Render Thread发送一个createContext命令来完成的。为了方便描述，我们将相关的代码列出来，如下所示： 12345678910111213141516171819202122[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\RenderProxy.cpp]#define ARGS(method) method ## Args #define CREATE_BRIDGE4(name, a1, a2, a3, a4) CREATE_BRIDGE(name, a1,a2,a3,a4,,,,) #define CREATE_BRIDGE(name, a1, a2, a3, a4, a5, a6, a7, a8) \\ typedef struct &#123; \\ a1; a2; a3; a4; a5; a6; a7; a8; \\ &#125; ARGS(name); \\ static void* Bridge_ ## name(ARGS(name)* args) #define SETUP_TASK(method) \\ ....... MethodInvokeRenderTask* task = new MethodInvokeRenderTask((RunnableMethod) Bridge_ ## method); \\ ARGS(method) *args = (ARGS(method) *) task-&gt;payload() CREATE_BRIDGE4(createContext, RenderThread* thread, bool translucent, RenderNode* rootRenderNode, IContextFactory* contextFactory) &#123; return new CanvasContext(*args-&gt;thread, args-&gt;translucent, args-&gt;rootRenderNode, args-&gt;contextFactory); &#125; 我们首先看宏SETUP_TASK，它需要一个函数作为参数。这个函数通过CREATE_BRIDGEX来声明，其中X是一个数字，数字的大小就等于函数需要的参数的个数。例如，通过CREATE_BRIDGE4声明的函数有4个参数。在上面的代码段中，我们通过CREATE_BRIDGE4宏声明了一个createContext函数。 宏SETUP_TASK的作用创建一个类型MethodInvokeRenderTask的Task。这个Task关联有一个由CREATE_BRIDGEX宏声明的函数。例如，SETUP_TASK(createContext)创建的MethodInvokeRenderTask关联的函数是由CREATE_BRIDGE4声明的函数createContext。这个Task最终会通过RenderProxy类的成员函数postAndWait添加到Render Thread的Task Queue中，如下所示： 12345678910[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\RenderProxy.cpp]void* RenderProxy::postAndWait(MethodInvokeRenderTask* task) &#123; void* retval; task-&gt;setReturnPtr(&amp;retval); SignalingRenderTask syncTask(task, &amp;mSyncMutex, &amp;mSyncCondition); AutoMutex _lock(mSyncMutex); mRenderThread.queue(&amp;syncTask); mSyncCondition.wait(mSyncMutex); return retval;&#125; RenderProxy对象的创建过程就分析完成了，从中我们也看到Render Thread的创建过程和运行模型，以及Render Proxy与Render Thread的交互模型，总结来说： 1、RenderProxy内部有一个成员变量mRenderThread，它指向的是一个RenderThread对象，通过它可以向Render Thread线程发送命令。 2、 RenderProxy内部有一个成员变量mContext，它指向的是一个CanvasContext对象，Render Thread的渲染工作就是通过它来完成的。 3、RenderProxy内部有一个成员变量mDrawFrameTask，它指向的是一个DrawFrameTask对象，Main Thread通过它向Render Thread线程发送渲染下一帧的命令。 （二）、Open GL ES环境初始化 (绑定窗口到Render Thread中)Activity窗口的绘制流程是在ViewRoot(Impl)类的成员函数performTraversals发起的。在绘制之前，首先要获得一个Surface。这个Surface描述的就是一个窗口。因此，一旦获得了对应的Surface，就需要将它绑定到Render Thread中，如下所示： 12345678910[-&gt;/frameworks/base/core/java/android/view/ViewRootImpl.java]public final class ViewRootImpl implements ViewParent, View.AttachInfo.Callbacks, HardwareRenderer.HardwareDrawCallbacks &#123; ...... private void performTraversals() &#123; ...... hwInitialized = mAttachInfo.mHardwareRenderer.initialize( mSurface); &#125;&#125; mHardwareRenderer对象，它的成员函数initialize的实现如下所示： 12345678[-&gt;\\frameworks\\base\\core\\java\\android\\view\\ThreadedRenderer.java] boolean initialize(Surface surface) throws OutOfResourcesException &#123; boolean status = !mInitialized; mInitialized = true; updateEnabledState(surface); nInitialize(mNativeProxy, surface); return status; &#125; nInitialize是一个JNI函数，由Native层的函数android_view_ThreadedRenderer_initialize实现 1234567[-&gt;\\frameworks\\base\\core\\jni\\android_view_ThreadedRenderer.cpp]static void android_view_ThreadedRenderer_initialize(JNIEnv* env, jobject clazz, jlong proxyPtr, jobject jsurface) &#123; RenderProxy* proxy = reinterpret_cast&lt;RenderProxy*&gt;(proxyPtr); sp&lt;Surface&gt; surface = android_view_Surface_getSurface(env, jsurface); proxy-&gt;initialize(surface);&#125; Java层的Surface在Native层对应的是一个ANativeWindow。我们可以通过函数android_view_Surface_getNativeWindow来获得一个Java层的Surface在Native层对应的ANativeWindowRenderProxy-&gt;initialize()函数实现 1234567891011CREATE_BRIDGE2(initialize, CanvasContext* context, Surface* surface) &#123; args-&gt;context-&gt;initialize(args-&gt;surface); return nullptr;&#125;void RenderProxy::initialize(const sp&lt;Surface&gt;&amp; surface) &#123; SETUP_TASK(initialize); args-&gt;context = mContext; args-&gt;surface = surface.get(); post(task);&#125; 当这个Task在Render Thread中执行时，由宏CREATE_BRIDGE2声明的函数initialize就会被执行。 在由宏CREATE_BRIDGE2声明的函数initialize中，参数context指向的是RenderProxy类的成员变量mContext指向的一个CanvasContext对象，而参数window指向的ANativeWindow就是要绑定到Render Thread的ANativeWindow。 由宏CREATE_BRIDGE2声明的函数initialize通过调用参数context指向的CanvasContext对象的成员函数initialize来绑定参数window指向的ANativeWindow，如下所示： 12345678910111213141516171819202122232425[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\CanvasContext.cpp]void CanvasContext::initialize(Surface* surface) &#123; setSurface(surface);#if !HWUI_NEW_OPS if (mCanvas) return; mCanvas = new OpenGLRenderer(mRenderThread.renderState()); mCanvas-&gt;initProperties();#endif&#125;void CanvasContext::setSurface(Surface* surface) &#123; mNativeSurface = surface; ...... if (surface) &#123; mEglSurface = mEglManager.createSurface(surface); &#125; if (mEglSurface != EGL_NO_SURFACE) &#123; ...... mHaveNewSurface = true; ..... &#125; else &#123; mRenderThread.removeFrameCallback(this); &#125;&#125; 每一个Open GL渲染上下文都需要关联有一个EGL Surface。这个EGL Surface描述的是一个绘图表面，它封装的实际上是一个ANativeWindow。有了这个EGL Surface之后，我们在执行Open GL命令的时候，才能确定这些命令是作用在哪个窗口上。 12345678910111213141516171819[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\EglManager.cpp]EGLSurface EglManager::createSurface(EGLNativeWindowType window) &#123; initialize(); EGLSurface surface = eglCreateWindowSurface(mEglDisplay, mEglConfig, window, nullptr); ...... &#125; return surface;&#125;void EglManager::initialize() &#123; ...... loadConfig();// createContext();// createPBufferSurface();// makeCurrent(mPBufferSurface);// DeviceInfo::initialize(); mRenderThread.renderState().onGLContextCreated(); initAtlas();&#125; 使用EGL的绘图的一般步骤：1、获取 EGL Display 对象：eglGetDisplay()2、初始化与 EGLDisplay 之间的连接：eglInitialize()3、获取 EGLConfig 对象：eglChooseConfig()4、创建 EGLContext 实例：eglCreateContext()5、创建 EGLSurface 实例：eglCreateWindowSurface()6、连接 EGLContext 和 EGLSurface：eglMakeCurrent()7、使用 OpenGL ES API 绘制图形：gl_*()8、切换 front buffer 和 back buffer 送显：eglSwapBuffer()9、断开并释放与 EGLSurface 关联的 EGLContext 对象：eglRelease()10、删除 EGLSurface 对象11、删除 EGLContext 对象12、终止与 EGLDisplay 之间的连接 Android EGL &amp;&amp; OpenGL分析请参考【Android Display System（2）：Android Display System 系统分析 之 Android EGL &amp;&amp; OpenGL】 至此，将当前窗口绑定到Render Thread的过程就分析完成了，整个Android应用程序UI硬件加速渲染环境的初始化过程也分析完成了。 （三）、RenderNode构建我们知道，通常情况下，App要完成一帧渲染，是通过ViewRootImpl的performTraversals()函数来实现。而它又可分为measure, layout, draw三个阶段。上面这些改动主要影响的是最后这步，因此我们就主要focus在draw这个阶段的流程。首先看DisplayList是怎么录制的。在ViewRootImpl::performDraw()中会调用draw()函数。当判断需要进行绘制时（比如有脏区域，或在动画中时），又如果硬件加速可用（通过ThreadedRenderer的isEnabled()），会进行下面的重绘动作。接下来根据是否有相关请求（如resize时）或offset是否有变化来判断是否要调用ThreadedRenderer的invalidRoot()来标记更新RootRenderNode。 扯个题外话。和Android M相比，N中UI子系统中加入了不少对用户进行窗口resize的处理，主要应该是为了Android N新增加的多窗口分屏模式。比如当用户拖拽分屏窗口边缘时，onWindowDragResizeStart()被调用。它其中会创建BackdropFrameRenderer。BackdropFrameRenderer本身运行单独的线程，它负责在resize窗口而窗口绘制来不及的情况下填充背景。它会通过addRenderNode()加入专用的RenderNode。同时，Android N中将DecorView从PhoneWindow中分离成一个单独的文件，并实现新加的WindowCallbacks接口。它主要用于当用户变化窗口大小时ViewRootImpl对DecorView的回调。因为ViewRootImpl和WindowManagerService通信，它会被通知到窗口变化，然后回调到DecorView中。而DecorView中的相应回调会和BackupdropFrameRenderer交互。如updateContentDrawBounds()中最后会调用到了BackupdropFrmeRenderer的onContentDrawn()函数，其返回值代表在下面的内容绘制后是否需要再发起一次绘制。如果需要，之后会调用requestDrawWindow()。 回到ViewRootImpl::performDraw()函数，接下来，最重要的就是通过ThreadedRenderer的draw()来进行绘制。在这个draw()函数中，比较重要的一步是通过updateRootDisplayList()函数来更新根结点的DisplayList。 12345678910111213141516171819202122232425262728293031323334[-&gt;\\frameworks\\base\\core\\java\\android\\view\\ThreadedRenderer.java] private void updateViewTreeDisplayList(View view) &#123; view.mPrivateFlags |= View.PFLAG_DRAWN; view.mRecreateDisplayList = (view.mPrivateFlags &amp; View.PFLAG_INVALIDATED) == View.PFLAG_INVALIDATED; view.mPrivateFlags &amp;= ~View.PFLAG_INVALIDATED; view.updateDisplayListIfDirty(); view.mRecreateDisplayList = false; &#125; private void updateRootDisplayList(View view, HardwareDrawCallbacks callbacks) &#123; Trace.traceBegin(Trace.TRACE_TAG_VIEW, \"Record View#draw()\"); updateViewTreeDisplayList(view); if (mRootNodeNeedsUpdate || !mRootNode.isValid()) &#123; DisplayListCanvas canvas = mRootNode.start(mSurfaceWidth, mSurfaceHeight); try &#123; final int saveCount = canvas.save(); canvas.translate(mInsetLeft, mInsetTop); callbacks.onHardwarePreDraw(canvas); canvas.insertReorderBarrier(); canvas.drawRenderNode(view.updateDisplayListIfDirty()); canvas.insertInorderBarrier(); callbacks.onHardwarePostDraw(canvas); canvas.restoreToCount(saveCount); mRootNodeNeedsUpdate = false; &#125; finally &#123; mRootNode.end(canvas); &#125; &#125; Trace.traceEnd(Trace.TRACE_TAG_VIEW); &#125;...... 函数updateRootDisplayList()中的updateViewTreeDisplayList()会调到DecorView的updateDisplayListIfDirty()函数。这个函数主要功能是更新DecorView对应的RenderNode中的DisplayList。它返回的RenderNode会通过RecordingCanvas::drawRenderNode()函数将之作为RenderNodeOp加入到RootRenderNode的DisplayList中。函数updateDisplayListIfDirty()中首先判断当前View是否需要更新。如果不需要就调用dispatchGetDisplayList()让子View更新，然后直接返回。否则就是当前View的DisplayList需要更新。这里我们假设是第一次绘制，更新DisplayList的流程首先通过RenderNode的start()来获得一个用于记录绘制操作的Canvas，即DisplayListCanvas（在Android M中Java层由GLES20RecordingCanvas改为DisplayListCanvas，native层中的DisplayListRenderer改为DisplayListCanvas，Android N中native层中的DisplayListCanvas改为RecordingCanvas）。 接下去就是比较关键的步骤了。这里就要分几种情况了，一个View可以为三种类型（LAYER_TYPE_NONE, LAYER_TYPE_SOFTWARE, LAYER_TYPE_HARDWARE）中的一种。LAYER_TYPE_NONE为默认值，代表没有layer。LAYER_TYPE_SOFTWARE代表该View有软件层，以bitmap为back，内容用软件渲染。LAYER_TYPE_HARDWARE和LAYER_TYPE_SOFTWARE类似，区别在于其有硬件层，以FBO（Framebuffer object）为back，内容使用硬件渲染。如果硬件加速没有打开，它的行为和LAYER_TYPE_SOFTWARE是一样的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[-&gt;\\frameworks\\base\\core\\java\\android\\view\\ThreadedRenderer.java] public RenderNode updateDisplayListIfDirty() &#123; final RenderNode renderNode = mRenderNode; ....... if ((mPrivateFlags &amp; PFLAG_DRAWING_CACHE_VALID) == 0 || !renderNode.isValid() || (mRecreateDisplayList)) &#123; // Don't need to recreate the display list, just need to tell our // children to restore/recreate theirs if (renderNode.isValid() &amp;&amp; !mRecreateDisplayList) &#123; mPrivateFlags |= PFLAG_DRAWN | PFLAG_DRAWING_CACHE_VALID; mPrivateFlags &amp;= ~PFLAG_DIRTY_MASK; dispatchGetDisplayList(); return renderNode; // no work needed &#125; // If we got here, we're recreating it. Mark it as such to ensure that // we copy in child display lists into ours in drawChild() mRecreateDisplayList = true; int width = mRight - mLeft; int height = mBottom - mTop; int layerType = getLayerType(); final DisplayListCanvas canvas = renderNode.start(width, height); canvas.setHighContrastText(mAttachInfo.mHighContrastText); try &#123; if (layerType == LAYER_TYPE_SOFTWARE) &#123; buildDrawingCache(true); Bitmap cache = getDrawingCache(true); if (cache != null) &#123; canvas.drawBitmap(cache, 0, 0, mLayerPaint); &#125; &#125; else &#123; computeScroll(); canvas.translate(-mScrollX, -mScrollY); mPrivateFlags |= PFLAG_DRAWN | PFLAG_DRAWING_CACHE_VALID; mPrivateFlags &amp;= ~PFLAG_DIRTY_MASK; // Fast path for layouts with no backgrounds if ((mPrivateFlags &amp; PFLAG_SKIP_DRAW) == PFLAG_SKIP_DRAW) &#123; dispatchDraw(canvas); if (mOverlay != null &amp;&amp; !mOverlay.isEmpty()) &#123; mOverlay.getOverlayView().draw(canvas); &#125; &#125; else &#123; draw(canvas); &#125; &#125; &#125; finally &#123; renderNode.end(canvas); setDisplayListProperties(renderNode); &#125; &#125; else &#123; mPrivateFlags |= PFLAG_DRAWN | PFLAG_DRAWING_CACHE_VALID; mPrivateFlags &amp;= ~PFLAG_DIRTY_MASK; &#125; return renderNode; &#125; 如果当前View是软件渲染层（类型为LAYER_TYPE_SOFTWARE）的话，则调用buildDrawingCache()获得Bitmap后调用drawBitmap()将该Bitmap记录到DisplayListCanvas中。现在Android中都默认硬件渲染了，为什么还要考虑软件渲染层呢?一方面有些平台不支持硬件渲染，或app不启用硬件加速，另一方面有些UI控件不支持硬件渲染 。在复杂的View（及子View）在动画过程中，可以被绘制成纹理，这样只需要画一次。显然，在View经常更新的情况下并不适用。因为这样每次都需要重新用软件渲染，如果硬件渲染打开时还要上传成硬件纹理（上传纹理是个比较慢的操作）。类似的，硬件渲染层（LAYER_TYPE_HARDWARE）也是适用于类似的复杂View结构进行属性动画的场景，但它与LAYER_TYPE_SOFTWARE的层的区别为它对应FBO，可以直接硬件渲染生成纹理。因此渲染的过程中不需要先生成Bitmap，从而省去了上传成硬件纹理的这一步操作。 如果当前View对应LAYER_TYPE_NONE或者LAYER_TYPE_HARDWARE，下面会考查是否为没有背景的Layout。这种情况下当前View没什么好画的，会走快速路径。即通过dispatchDraw()直接让子View重绘。否则就调draw()来绘制当前View及其子View。注意View中的draw()有两个重载同名函数。一个参数的版本用于直接调用。三个参数的版本用于ViewGroup中drawChild()时调用。这里调的是一个参数的版本。这个draw()函数中会按下面的顺序进行绘制（DisplayList的更新）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@CallSuperpublic void draw(Canvas canvas) &#123; final int privateFlags = mPrivateFlags; final boolean dirtyOpaque = (privateFlags &amp; PFLAG_DIRTY_MASK) == PFLAG_DIRTY_OPAQUE &amp;&amp; (mAttachInfo == null || !mAttachInfo.mIgnoreDirtyState); mPrivateFlags = (privateFlags &amp; ~PFLAG_DIRTY_MASK) | PFLAG_DRAWN; /* * Draw traversal performs several drawing steps which must be executed * in the appropriate order: * * 1. Draw the background * 2. If necessary, save the canvas' layers to prepare for fading * 3. Draw view's content * 4. Draw children * 5. If necessary, draw the fading edges and restore layers * 6. Draw decorations (scrollbars for instance) */ // Step 1, draw the background, if needed ...... drawBackground(canvas); // skip step 2 &amp; 5 if possible (common case) final int viewFlags = mViewFlags; boolean horizontalEdges = (viewFlags &amp; FADING_EDGE_HORIZONTAL) != 0; boolean verticalEdges = (viewFlags &amp; FADING_EDGE_VERTICAL) != 0; // Step 3, draw the content if (!dirtyOpaque) onDraw(canvas); // Step 4, draw the children dispatchDraw(canvas); // Step 5, draw the fade effect and restore layers final Paint p = scrollabilityCache.paint; final Matrix matrix = scrollabilityCache.matrix; final Shader fade = scrollabilityCache.shader; if (drawTop) &#123; matrix.setScale(1, fadeHeight * topFadeStrength); matrix.postTranslate(left, top); fade.setLocalMatrix(matrix); p.setShader(fade); canvas.drawRect(left, top, right, top + length, p); &#125; ...... canvas.restoreToCount(saveCount); // Overlay is part of the content and draws beneath Foreground if (mOverlay != null &amp;&amp; !mOverlay.isEmpty()) &#123; mOverlay.getOverlayView().dispatchDraw(canvas); &#125; // Step 6, draw decorations (foreground, scrollbars) onDrawForeground(canvas);&#125; 这些是通过drawBackground(), onDraw(), dispatchDraw()和onDrawForeground()等函数实现。这些函数本质上就是将相应内容绘制到提供的DisplayListCanvas上。由于View是以树形层次结构组织的，draw()中会通过dispatchDraw()来更新子View的DisplayList。dispatchDraw()为对每个子View调用drawChild()。然后调用子View的draw()函数（这次就是上面说的draw()的三个参数的版本了）。这个版本的draw()函数里会更新其View的DisplayList，然后调用DisplayListCanvas的drawRenderNode()将该子view对应的RenderNode记录到其父view的DisplayList中去。这样便根据View的树型结构生成了DisplayList的树型结构。 其中onDraw()用于绘制当前View的自定义UI，它是每个View需要自定义的成员函数。比较典型地，在View的绘制函数中会调用canvas的drawXXX函数。比如canvas.drawLine()-&gt;drawLines(android_graphics_Canvas.cpp)，它会通过JNI最后调到RecordingCanvas.cpp中的RecordingCanvas::drawLines()： 1234567891011[-&gt;\\frameworks\\base\\libs\\hwui\\RecordingCanvas.cpp]void RecordingCanvas::drawLines(const float* points, int floatCount, const SkPaint&amp; paint) &#123; if (CC_UNLIKELY(floatCount &lt; 4 || PaintUtils::paintWillNotDraw(paint))) return; floatCount &amp;= ~0x3; // round down to nearest four addOp(alloc().create_trivial&lt;LinesOp&gt;( calcBoundsOfPoints(points, floatCount), *mState.currentSnapshot()-&gt;transform, getRecordedClip(), refPaint(&amp;paint), refBuffer&lt;float&gt;(points, floatCount), floatCount));&#125; RecordingCanvas中绝大多数的drawXXX系函数都是类似于这样，通过addOp()将一个RecordedOp的继承类存到其成员mDisplayList中。RecordedOp家庭成员很多，有不少继承类，每个对应一种操作。操作的种类可以参照下这个表： 12345678910111213141516171819202122232425262728293031323334[-&gt;\\frameworks\\base\\libs\\hwui\\RecordedOp.h]#define MAP_OPS_BASED_ON_TYPE(PRE_RENDER_OP_FN, RENDER_ONLY_OP_FN, UNMERGEABLE_OP_FN, MERGEABLE_OP_FN) \\ PRE_RENDER_OP_FN(RenderNodeOp) \\ PRE_RENDER_OP_FN(CirclePropsOp) \\ PRE_RENDER_OP_FN(RoundRectPropsOp) \\ PRE_RENDER_OP_FN(BeginLayerOp) \\ PRE_RENDER_OP_FN(EndLayerOp) \\ PRE_RENDER_OP_FN(BeginUnclippedLayerOp) \\ PRE_RENDER_OP_FN(EndUnclippedLayerOp) \\ PRE_RENDER_OP_FN(VectorDrawableOp) \\ \\ RENDER_ONLY_OP_FN(ShadowOp) \\ RENDER_ONLY_OP_FN(LayerOp) \\ RENDER_ONLY_OP_FN(CopyToLayerOp) \\ RENDER_ONLY_OP_FN(CopyFromLayerOp) \\ \\ UNMERGEABLE_OP_FN(ArcOp) \\ UNMERGEABLE_OP_FN(BitmapMeshOp) \\ UNMERGEABLE_OP_FN(BitmapRectOp) \\ UNMERGEABLE_OP_FN(ColorOp) \\ UNMERGEABLE_OP_FN(FunctorOp) \\ UNMERGEABLE_OP_FN(LinesOp) \\ UNMERGEABLE_OP_FN(OvalOp) \\ UNMERGEABLE_OP_FN(PathOp) \\ UNMERGEABLE_OP_FN(PointsOp) \\ UNMERGEABLE_OP_FN(RectOp) \\ UNMERGEABLE_OP_FN(RoundRectOp) \\ UNMERGEABLE_OP_FN(SimpleRectsOp) \\ UNMERGEABLE_OP_FN(TextOnPathOp) \\ UNMERGEABLE_OP_FN(TextureLayerOp) \\ \\ MERGEABLE_OP_FN(BitmapOp) \\ MERGEABLE_OP_FN(PatchOp) \\ MERGEABLE_OP_FN(TextOp) 各个View的DisplayList更新好后，回到udpateRootDisplayList()。如果发现RootRenderNode也需要更新，则先通过Java层的RenderNode::start()获得DisplayListCanvas，在这个Canvas上的动作都会被记录到DisplayList中，直到调用RenderNode.end()。然后为了防止对上下文状态的影响，用Canvas::save()和Canvas::restoreToCount()来生成临时的画布状态。再接下来就是通过drawRenderNode()将DecorView的RenderNode以RenderNodeOp的形式记录到RootRenderNode。 DisplayList构建实例：Android DisplayList 构建过程activity_main.xml 12345678910111213141516171819链接：https://www.jianshu.com/p/7bf306c09c7e&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;LinearLayout xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot; android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;match_parent&quot; android:orientation=&quot;vertical&quot;&gt; &lt;TextView android:id=&quot;@+id/sample_text&quot; android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;60dp&quot; android:gravity=&quot;center&quot; android:textSize=&quot;20sp&quot; android:text=&quot;Hello World!&quot; /&gt; &lt;cc.bobby.debugapp.MyView android:layout_width=&quot;match_parent&quot; android:layout_height=&quot;100dp&quot; /&gt;&lt;/LinearLayout&gt; udpateRootDisplayList() 父View与子View的DisplayList （四）、RenderNode绘制在ThreadedRenderer的draw()函数中构建完DisplayList后，接下来需要准备渲染了。首先通过JNI调用nSyncAndDrawFrame()调用到native层的android_view_ThreadedRenderer_syncAndDrawFrame()。其中将参数中的FrameInfo数组传到RenderProxy的mFrameInfo成员中。它是Android M开始加入用来细化hwui性能统计的。同时调用RenderProxy的syncAndDrawFrame()函数，并将创建的TreeObserver作为参数。函数syncAndDrawFrame()中即调用DrawFrameTask（这是RenderThread的TaskQueue中的特殊Task实例）的drawFrame()函数。继而通过postAndWait()往RenderThread的TaskQueue里插入自身（即DrawFrameTask）来申请新一帧的渲染。在RenderThread的queue()函数中会按Task的运行时间将之插入到适当的位置。接着postAndWait()函数中会block UI线程等待渲染线程将之unblock。渲染线程在N中的改动不大，这里就不花太多文字介绍了，需要的时候把它当作跨线程调用即可。 另一边，渲染线程处理这个DrawFrameTask时会调用到其run()函数： 1234567891011121314151617181920212223242526272829303132[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\DrawFrameTask.cpp]void DrawFrameTask::run() &#123; ATRACE_NAME(\"DrawFrame\"); bool canUnblockUiThread; bool canDrawThisFrame; &#123; TreeInfo info(TreeInfo::MODE_FULL, *mContext); info.observer = mObserver; canUnblockUiThread = syncFrameState(info); canDrawThisFrame = info.out.canDrawThisFrame; &#125; // Grab a copy of everything we need CanvasContext* context = mContext; // From this point on anything in \"this\" is *UNSAFE TO ACCESS* if (canUnblockUiThread) &#123; unblockUiThread(); &#125; if (CC_LIKELY(canDrawThisFrame)) &#123; context-&gt;draw(); &#125; else &#123; // wait on fences so tasks don't overlap next frame context-&gt;waitOnFences(); &#125; if (!canUnblockUiThread) &#123; unblockUiThread(); &#125;&#125; 其中首先通过DrawFrameTask::syncFrameState()函数将主线程的渲染信息（如DisplayList，Property和Bitmap等）同步到渲染线程。 12345678syncFrameState() -&gt; eglMakeCurrent -&gt; eglMakeCurrent(OEM EGL) -&gt; egl_window_surface_v2_t::connect() -&gt; Surface::hook_dequeueBuffer() -&gt; Surface::dequeueBuffer() -&gt; BpGraphicBufferProducer::dequeueBuffer() -&gt; BpGraphicBufferProducer::requestBuffer() 这个函数中首先会处理DrawFrameTask中的mLayers。它是DeferredLayerUpdater的vector，顾名思义，就是延迟处理的layer更新任务。这主要用于TextureView。TextureView是比较特殊的类。它通常用于显示内容流，生产者端可以是另一个进程。中间通过BufferQueue进行buffer的传输和交换。当有新的buffer来到（或者有属性变化，如visibility等）是，会通过回调设置标志位(mUpdateLayer)并通过invalidate()调度下一次重绘。当下一次draw()被调用时，先通过applyUpdate()-&gt;updateSurfaceTexture()-&gt;ThreadedRenderer::pushLayerUpdate()，再调到渲染线程中的 DrawFrameTask::pushLayerUpdate()，将本次更新记录在DrawFrameTask的mLayers中。这样，在后面调用DrawFrameTask::syncFrameState()是会依次调用mLayers中的apply()进行真正的更新。这里调用它的apply()函数就会取新可用buffer（通过doUpdateTexImage()函数），并将相关纹理信息更新到mLayer。在syncFrameState()函数中，接下来，通过CanvasContext的prepareTree()继而调用RenderNode的prepareTree()同步渲染信息。最后会输出TreeInfo结构，其中的prepareTextures代表纹理上传是否成功。如果为false，说明texture cache用完了。这样为了防止渲染线程在渲染过程中使用的资源和主线程竞争，在渲染线程绘制当前帧时就不能让主线程继续往下跑了，也就不能做到真正并行。在sync完数据后，DrawFrameTask::run()最后会调用CanvasContext::draw()来进行接下来的渲染。这部分的大体流程如下： 接下来瞄下CanvasContext::draw()里做了什么。先要小小准备下EGL环境，比如通过EglManager的beginFrame()函数， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061beginFrame() -&gt; eglMakeCurrent -&gt; eglMakeCurrent(OEM EGL) -&gt; Surface::hook_dequeueBuffer() -&gt; Surface::dequeueBuffer() -&gt; BpGraphicBufferProducer::dequeueBuffer() -&gt; BpGraphicBufferProducer::requestBuffer()[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]Frame EglManager::beginFrame(EGLSurface surface) &#123; ..... makeCurrent(surface); ....&#125;bool EglManager::makeCurrent(EGLSurface surface, EGLint* errOut) &#123; if (!eglMakeCurrent(mEglDisplay, surface, surface, mEglContext)) &#123; .... &#125; .....&#125;EGLBoolean eglMakeCurrent( EGLDisplay dpy, EGLSurface draw, EGLSurface read, EGLContext ctx)&#123; if (ctx == EGL_NO_CONTEXT) &#123; // if we're detaching, we need the current context current_ctx = (EGLContext)getGlThreadSpecific(); &#125; else &#123; egl_surface_t* d = (egl_surface_t*)draw; &#125; if (d) &#123; if (d-&gt;connect() == EGL_FALSE) &#123; return EGL_FALSE; &#125; &#125; return setError(EGL_BAD_ACCESS, EGL_FALSE);&#125;EGLBoolean egl_window_surface_v2_t::connect()&#123; // dequeue a buffer int fenceFd = -1; if (nativeWindow-&gt;dequeueBuffer(nativeWindow, &amp;buffer, &amp;fenceFd) != NO_ERROR) &#123; return setError(EGL_BAD_ALLOC, EGL_FALSE); &#125; // wait for the buffer sp&lt;Fence&gt; fence(new Fence(fenceFd)); if (fence-&gt;wait(Fence::TIMEOUT_NEVER) != NO_ERROR) &#123; nativeWindow-&gt;cancelBuffer(nativeWindow, buffer, fenceFd); return setError(EGL_BAD_ALLOC, EGL_FALSE); &#125; return EGL_TRUE;&#125; 继而用eglMakeCurrent()将渲染context切换到相应的surface。然后EglManager的damageFrame()设定当前帧的脏区域（如果gfx平台支持局部更新的话）。接下来就是绘制的主体部分了。这也是N中改动比较大的部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[-&gt;\\frameworks\\base\\libs\\hwui\\renderthread\\CanvasContext.cpp]void CanvasContext::draw() &#123; SkRect dirty; mDamageAccumulator.finish(&amp;dirty); mCurrentFrameInfo-&gt;markIssueDrawCommandsStart(); Frame frame = mEglManager.beginFrame(mEglSurface); ...... SkRect screenDirty(dirty); ...... mEglManager.damageFrame(frame, dirty);#if HWUI_NEW_OPS auto&amp; caches = Caches::getInstance(); FrameBuilder frameBuilder(dirty, frame.width(), frame.height(), mLightGeometry, caches); frameBuilder.deferLayers(mLayerUpdateQueue); mLayerUpdateQueue.clear(); frameBuilder.deferRenderNodeScene(mRenderNodes, mContentDrawBounds); BakedOpRenderer renderer(caches, mRenderThread.renderState(), mOpaque, mLightInfo); frameBuilder.replayBakedOps&lt;BakedOpDispatcher&gt;(renderer); profiler().draw(&amp;renderer); bool drew = renderer.didDraw(); // post frame cleanup caches.clearGarbage(); caches.pathCache.trim(); caches.tessellationCache.trim(); ...... mCanvas-&gt;prepareDirty(frame.width(), frame.height(), dirty.fLeft, dirty.fTop, dirty.fRight, dirty.fBottom, mOpaque); Rect outBounds; // It there are multiple render nodes, they are laid out as follows: // #0 - backdrop (content + caption) // #1 - content (positioned at (0,0) and clipped to - its bounds mContentDrawBounds) // #2 - additional overlay nodes ...... // Draw all render nodes. Note that for (const sp&lt;RenderNode&gt;&amp; node : mRenderNodes) &#123; if (layer == 0) &#123; // Backdrop. ....... // Check if we have to draw something on the left side ... if (targetBounds.left &lt; contentBounds.left) &#123; mCanvas-&gt;save(SaveFlags::Clip); if (mCanvas-&gt;clipRect(targetBounds.left, targetBounds.top, contentBounds.left, targetBounds.bottom, SkRegion::kIntersect_Op)) &#123; mCanvas-&gt;drawRenderNode(node.get(), outBounds); &#125; // Reduce the target area by the area we have just painted. targetBounds.left = std::min(contentBounds.left, targetBounds.right); mCanvas-&gt;restore(); &#125; // ... or on the right side ... // ... or at the top ... // ... or at the bottom. &#125; else if (layer == 1) &#123; // Content // It gets cropped against the bounds of the backdrop to stay inside. mCanvas-&gt;save(SaveFlags::MatrixClip); ...... mCanvas-&gt;translate(dx, dy); if (mCanvas-&gt;clipRect(left, top, left + width, top + height, SkRegion::kIntersect_Op)) &#123; mCanvas-&gt;drawRenderNode(node.get(), outBounds); &#125; mCanvas-&gt;restore(); &#125; else &#123; // draw the rest on top at will! mCanvas-&gt;drawRenderNode(node.get(), outBounds); &#125; layer++; &#125; profiler().draw(mCanvas); bool drew = mCanvas-&gt;finish(); ...... 先得到Caches的实例。它是一个单例类，包含了各种绘制资源的cache。然后创建FrameBuilder。该类用于当前帧的构建。FrameBuilder的构造函数中又会创建对应fbo0的LayerBuilder。fbo0即对应通过SurfaceFlinger申请来的on-screen surface，然后将之放入layer stack（通过mLayerBuilders和mLayerStack两个成员维护）。同时还会在initializeSaveStack()函数中创建和初始化Snapshot。就像名字一样，它保存了渲染surface的当前状态的一个“快照”。每个Snapshot有一个指向前继的Snapshot，从而形成一个”栈”。每次调用save()和restore()就相当于压栈和弹栈。 接下来deferLayers()函数处理LayerUpdateQueue中的元素。之前在渲染线程每画一帧前同步信息时调用RenderNode::prepareTree()会遍历DisplayList的树形结构，对于子节点递归调用prepareTreeImpl()，如果是render layer，在RenderNode::pushLayerUpdate()中会将该layer的更新操作记录到LayerUpdateQueue中。至于哪些节点是render layer。主要是根据之前提到的view类型（LAYER_TYPE_NONE/SOFTWARE/HARDWARE）。但会有一个优化，如果一个普通view满足promotedToLayer()定义的条件，它会被当做render layer处理。 123456789101112131415161718192021222324252627282930313233[-&gt;\\frameworks\\base\\libs\\hwui\\FrameBuilder.cpp]void FrameBuilder::deferLayers(const LayerUpdateQueue&amp; layers) &#123; // Render all layers to be updated, in order. Defer in reverse order, so that they'll be // updated in the order they're passed in (mLayerBuilders are issued to Renderer in reverse) for (int i = layers.entries().size() - 1; i &gt;= 0; i--) &#123; RenderNode* layerNode = layers.entries()[i].renderNode; // only schedule repaint if node still on layer - possible it may have been // removed during a dropped frame, but layers may still remain scheduled so // as not to lose info on what portion is damaged OffscreenBuffer* layer = layerNode-&gt;getLayer(); if (CC_LIKELY(layer)) &#123; ATRACE_FORMAT(\"Optimize HW Layer DisplayList %s %ux%u\", layerNode-&gt;getName(), layerNode-&gt;getWidth(), layerNode-&gt;getHeight()); Rect layerDamage = layers.entries()[i].damage; // TODO: ensure layer damage can't be larger than layer layerDamage.doIntersect(0, 0, layer-&gt;viewportWidth, layer-&gt;viewportHeight); layerNode-&gt;computeOrdering(); // map current light center into RenderNode's coordinate space Vector3 lightCenter = mCanvasState.currentSnapshot()-&gt;getRelativeLightCenter(); layer-&gt;inverseTransformInWindow.mapPoint3d(lightCenter); saveForLayer(layerNode-&gt;getWidth(), layerNode-&gt;getHeight(), 0, 0, layerDamage, lightCenter, nullptr, layerNode); if (layerNode-&gt;getDisplayList()) &#123; deferNodeOps(*layerNode); &#125; restoreForLayer(); &#125; &#125;&#125; 回到deferLayers()函数。这里就是把LayerUpdateQueue里的元素按逆序拿出来，依次调用saveForLayer()，deferNodeOps()和restoreForLayer()。saveForLayer()为该render laye创建Snapshot和LayerBuilder并放进mLayerStack和mLayerBuilders。而restoreForLayer()则是它的逆操作。Layer stack和canvas state是栈的结构。saveForLayer() 和restoreForLayer()就相当于一个push stack，一个pop stack。这里核心的deferNodeOps()函数处理该layer对应的DisplayList，将它们按以下类型以batch的形式组织存放在LayerBuilder的mBatches成员中。其中同一类型中能合并的操作还会进行合并（目前只支持Bitmap, Text和Patch三种类型的操作合并）。Batch的类型有以下几种： 12345678910111213141516171819[-&gt;\\frameworks\\base\\libs\\hwui\\LayerBuilder.h]namespace OpBatchType &#123; enum &#123; Bitmap, MergedPatch, AlphaVertices, Vertices, AlphaMaskTexture, Text, ColorText, Shadow, TextureLayer, Functor, CopyToLayer, CopyFromLayer, Count // must be last &#125;;&#125; 下面看下deferNodeOps()函数里是怎么处理RenderNode的。 123456789101112131415161718192021222324252627282930313233[-&gt;\\frameworks\\base\\libs\\hwui\\FrameBuilder.cpp]/** * Used to define a list of lambdas referencing private FrameBuilder::onXX::defer() methods. * * This allows opIds embedded in the RecordedOps to be used for dispatching to these lambdas. * E.g. a BitmapOp op then would be dispatched to FrameBuilder::onBitmapOp(const BitmapOp&amp;) */#define OP_RECEIVER(Type) \\ [](FrameBuilder&amp; frameBuilder, const RecordedOp&amp; op) &#123; frameBuilder.defer##Type(static_cast&lt;const Type&amp;&gt;(op)); &#125;,void FrameBuilder::deferNodeOps(const RenderNode&amp; renderNode) &#123; typedef void (*OpDispatcher) (FrameBuilder&amp; frameBuilder, const RecordedOp&amp; op); static OpDispatcher receivers[] = BUILD_DEFERRABLE_OP_LUT(OP_RECEIVER); // can't be null, since DL=null node rejection happens before deferNodePropsAndOps const DisplayList&amp; displayList = *(renderNode.getDisplayList()); for (auto&amp; chunk : displayList.getChunks()) &#123; FatVector&lt;ZRenderNodeOpPair, 16&gt; zTranslatedNodes; buildZSortedChildList(&amp;zTranslatedNodes, displayList, chunk); defer3dChildren(chunk.reorderClip, ChildrenSelectMode::Negative, zTranslatedNodes); for (size_t opIndex = chunk.beginOpIndex; opIndex &lt; chunk.endOpIndex; opIndex++) &#123; const RecordedOp* op = displayList.getOps()[opIndex]; receivers[op-&gt;opId](*this, *op); if (CC_UNLIKELY(!renderNode.mProjectedNodes.empty() &amp;&amp; displayList.projectionReceiveIndex &gt;= 0 &amp;&amp; static_cast&lt;int&gt;(opIndex) == displayList.projectionReceiveIndex)) &#123; deferProjectedChildren(renderNode); &#125; &#125; defer3dChildren(chunk.reorderClip, ChildrenSelectMode::Positive, zTranslatedNodes); &#125;&#125; DisplayList以chunk为单位组合RecordedOp。这些RecordedOp的opId代表它们的类型。根据这个类型调用receivers这个查找表（通过BUILD_DEFERABLE_OP_LUT构造）中的函数。它会调用FrameBuilder中相应的deferXXX函数（比如deferArcOp, deferBitmapOp, deferRenderNodeOp等）。这些deferXXX系函数一般会将RecordedOp用BakedOpState封装一下，然后会调用LayerBuilder的deferUnmergeableOp()和deferMergeableOp()函数将BakedOpState组织进mBatches成员。同时还有两个查找表mBatchLookup和mMergingBatchLookup分别用于不能合并的batch（OpBatch）和能合并的batch（MergingOpBatch）。它们分别用于查找特定类型的最近一个OpBatch或者MergingOpBatch。 先看下deferUnmergeableOp()函数。它会将BakedOpState按batch类型放进mBatches中。mBatches是指向BatchBase对象的vector，每个处理好的BakedOpState都会按类型放进来。如果还未有该类型的batch则创建OpBatch，并把它插入到mBatches的末尾。同时插入mBatchLookup这个查找表（batchId到最近一个该类型的OpBatch对象的映射）。这样之后处理同类型的BakedOpState时候，就会先搜索这个查找表。假如找到了，则进一步在mBatches数组中找到相应的OpBatch并通过它的batchOp()将该BakedOpState加入。 123456789101112131415161718192021[-&gt;\\frameworks\\base\\libs\\hwui\\LayerBuilder.cpp]void LayerBuilder::deferUnmergeableOp(LinearAllocator&amp; allocator, BakedOpState* op, batchid_t batchId) &#123; onDeferOp(allocator, op); OpBatch* targetBatch = mBatchLookup[batchId]; size_t insertBatchIndex = mBatches.size(); if (targetBatch) &#123; locateInsertIndex(batchId, op-&gt;computedState.clippedBounds, (BatchBase**)(&amp;targetBatch), &amp;insertBatchIndex); &#125; if (targetBatch) &#123; targetBatch-&gt;batchOp(op); &#125; else &#123; // new non-merging batch targetBatch = allocator.create&lt;OpBatch&gt;(batchId, op); mBatchLookup[batchId] = targetBatch; mBatches.insert(mBatches.begin() + insertBatchIndex, targetBatch); &#125;&#125; 接下来看用于合并操作的deferMergeableOp()函数。它也是类似的，当没有可以合并的MergingOpBatch时会创建新的，并且插入到mBatches。因为可能存在情况这个batchId在mBatches中有但是mMergingBatchLookup中没找到（说明还没有可合并的MergingOpBatch对象）或者通过MergingOpBatch::canMergeWidth()判断不满足合并条件。这时候就要插入到mBatches中该类型所在位置。如果很顺利的情况下，前面已经有MergingOpBatch在mMergingBatchLookup中而且又满足合并条件，就通过MergingOpBatch::mergeOp()将该BakedOpState和已有的进行合并。1234567891011121314151617181920212223242526272829[-&gt;\\frameworks\\base\\libs\\hwui\\LayerBuilder.cpp]void LayerBuilder::deferMergeableOp(LinearAllocator&amp; allocator, BakedOpState* op, batchid_t batchId, mergeid_t mergeId) &#123; onDeferOp(allocator, op); MergingOpBatch* targetBatch = nullptr; // Try to merge with any existing batch with same mergeId auto getResult = mMergingBatchLookup[batchId].find(mergeId); if (getResult != mMergingBatchLookup[batchId].end()) &#123; targetBatch = getResult-&gt;second; if (!targetBatch-&gt;canMergeWith(op)) &#123; targetBatch = nullptr; &#125; &#125; size_t insertBatchIndex = mBatches.size(); locateInsertIndex(batchId, op-&gt;computedState.clippedBounds, (BatchBase**)(&amp;targetBatch), &amp;insertBatchIndex); if (targetBatch) &#123; targetBatch-&gt;mergeOp(op); &#125; else &#123; // new merging batch targetBatch = allocator.create&lt;MergingOpBatch&gt;(batchId, op); mMergingBatchLookup[batchId].insert(std::make_pair(mergeId, targetBatch)); mBatches.insert(mBatches.begin() + insertBatchIndex, targetBatch); &#125;&#125; 回到CanvasContext::draw()函数，处理好layer后，下面得就是通过FrameBuilder::deferRenderNodeScene()函数处理FrameBuilder成员mRenderNodes中的RenderNode，其中包含了RootRenderNode（也可能有其它的RenderNode，比如backdrop和overlay nodes）。对于每个RenderNode，如果需要绘制则调用FrameBuilder的deferRenderNode()函数： 12345678[-&gt;\\frameworks\\base\\libs\\hwui\\FrameBuilder.cpp]void FrameBuilder::deferRenderNode(RenderNode&amp; renderNode) &#123; renderNode.computeOrdering(); mCanvasState.save(SaveFlags::MatrixClip); deferNodePropsAndOps(renderNode); mCanvasState.restore();&#125; 这里和前面类似，会为之创建独立的Snapshot（Canvas渲染状态），deferNodePropsAndOps()根据RenderNode中的RenderProperties通过CanvasState设置一堆状态。如果该RenderNode对应是一个render layer，则将它封装为LayerOp（绘制offscreen buffer）并通过deferUnmergeableOp()加入batch。如果该RenderNode对应RenderProperties有半透明效果且不是render layer，则可以将该RenderNode绘制到一个临时的layer（称为save layer）。这是通过BeginLayerOp和EndLayerOp来记录的。正常情况下，还是通过deferNodeOps()来将RenderNode进行batch/merge。这个函数前面已有说明。 再次回到CanvasContext::draw()函数，下面终于要真得进行渲染了。首先创建BakedOpRenderer，然后调用FrameBuilder::replayBakedOps()函数并将BakedOpRenderer作为参数传进去。注意这是个模板函数，这里模板参数为BakedOpDispatcher。在replayBakedOps()函数中会构造两个用于处理BakedOpState的函数查找表。它们将BakedOpState按操作类型分发到BakedOpDispatcher的相应静态处理函数（onXXX或者onMergedXXX，分别用于非合并和合并的操作） 。 12345678910111213141516171819202122232425262728293031[-&gt;\\frameworks\\base\\libs\\hwui\\FrameBuilder.h]template &lt;typename StaticDispatcher, typename Renderer&gt; void replayBakedOps(Renderer&amp; renderer) &#123; std::vector&lt;OffscreenBuffer*&gt; temporaryLayers; finishDefer(); /** * Defines a LUT of lambdas which allow a recorded BakedOpState to use state-&gt;op-&gt;opId to * dispatch the op via a method on a static dispatcher when the op is replayed. * * For example a BitmapOp would resolve, via the lambda lookup, to calling: * * StaticDispatcher::onBitmapOp(Renderer&amp; renderer, const BitmapOp&amp; op, const BakedOpState&amp; state); */ #define X(Type) \\ [](void* renderer, const BakedOpState&amp; state) &#123; \\ StaticDispatcher::on##Type(*(static_cast&lt;Renderer*&gt;(renderer)), \\ static_cast&lt;const Type&amp;&gt;(*(state.op)), state); \\ &#125;, static BakedOpReceiver unmergedReceivers[] = BUILD_RENDERABLE_OP_LUT(X); #undef X /** * Defines a LUT of lambdas which allow merged arrays of BakedOpState* to be passed to a * static dispatcher when the group of merged ops is replayed. */ #define X(Type) \\ [](void* renderer, const MergedBakedOpList&amp; opList) &#123; \\ StaticDispatcher::onMerged##Type##s(*(static_cast&lt;Renderer*&gt;(renderer)), opList); \\ &#125;, static MergedOpReceiver mergedReceivers[] = BUILD_MERGEABLE_OP_LUT(X); #undef X 如前面如述，之前已经在FrameBuilder中构造了LayerBuilder的stack。接下来，这儿就是按push时的逆序（z-order高到底）对其中的BakedOpState进行replay，因为下面的layer可能会依赖的上面layer的渲染结果。比如要把上面layer画在FBO上的东西当成纹理画到下一层layer上。对于layer（persistent或者temporary的），先在BakedOpRenderer::startRepaintLayer()中初始化相关GL环境，比如创建FBO，绑定layer对应OffscreenBuffer中的纹理，设置viewport，清color buffer等等。对应地，BakedOpRenderer::endLayer()中最相应的销毁和清理工作。中间调用LayerBuilder::replayBakedOpsImpl()函数做真正的replay动作。对于fbo0（即on-screen surface），也是类似的，只是把startRepaintLayer()和endLayer()换成BakedOpRenderer::startFrame()和BakedOpRenderer::endFrame()。它们的功能也是初始化和销毁GL环境。 1234567891011121314151617181920212223242526272829303132333435363738394041[-&gt;\\frameworks\\base\\libs\\hwui\\FrameBuilder.h]template &lt;typename StaticDispatcher, typename Renderer&gt; void replayBakedOps(Renderer&amp; renderer) &#123; ..... // Relay through layers in reverse order, since layers // later in the list will be drawn by earlier ones for (int i = mLayerBuilders.size() - 1; i &gt;= 1; i--) &#123; GL_CHECKPOINT(MODERATE); LayerBuilder&amp; layer = *(mLayerBuilders[i]); if (layer.renderNode) &#123; // cached HW layer - can&apos;t skip layer if empty renderer.startRepaintLayer(layer.offscreenBuffer, layer.repaintRect); GL_CHECKPOINT(MODERATE); layer.replayBakedOpsImpl((void*)&amp;renderer, unmergedReceivers, mergedReceivers); GL_CHECKPOINT(MODERATE); renderer.endLayer(); &#125; else if (!layer.empty()) &#123; // save layer - skip entire layer if empty (in which case, LayerOp has null layer). layer.offscreenBuffer = renderer.startTemporaryLayer(layer.width, layer.height); temporaryLayers.push_back(layer.offscreenBuffer); GL_CHECKPOINT(MODERATE); layer.replayBakedOpsImpl((void*)&amp;renderer, unmergedReceivers, mergedReceivers); GL_CHECKPOINT(MODERATE); renderer.endLayer(); &#125; &#125; GL_CHECKPOINT(MODERATE); if (CC_LIKELY(mDrawFbo0)) &#123; const LayerBuilder&amp; fbo0 = *(mLayerBuilders[0]); renderer.startFrame(fbo0.width, fbo0.height, fbo0.repaintRect); GL_CHECKPOINT(MODERATE); fbo0.replayBakedOpsImpl((void*)&amp;renderer, unmergedReceivers, mergedReceivers); GL_CHECKPOINT(MODERATE); renderer.endFrame(fbo0.repaintRect); &#125; for (auto&amp; temporaryLayer : temporaryLayers) &#123; renderer.recycleTemporaryLayer(temporaryLayer); &#125; &#125; 在replayBakedOpsImpl()函数中，会根据操作的类型调用前面生成的unmergedReceivers和mergedReceivers两个函数分发表中的对应处理函数。它们实质指向BakedOpDispatcher中的静态函数。这些函数onXXXOp()和onMergedXXXOps()函数大同小异，基本都是通过GlopBuilder将BakedOpState和相关的信息封装成Glop对象，然后调用BakedOpRenderer::renderGlop()，接着通过DefaultGlopReceiver()调用BakedOpRenderer::renderGlopImpl()函数，最后在RenderState::render()中通过GL命令将Glop渲染出来。大功告成。 结语合并之后，DeferredDisplayList Vector mBatches 包含全部整合后的绘制命令，之后渲染即可，需要注意的是这里的合并并不是多个变一个，只是做了一个集合，主要是方便使用各资源纹理等，比如绘制文字的时候，需要根据文字的纹理进行渲染，而这个时候就需要查询文字的纹理坐标系，合并到一起方便统一处理，一次渲染，减少资源加载的浪费，当然对于理解硬件加速的整体流程，这个合并操作可以完全无视，甚至可以直观认为，构建完之后，就可以直接渲染，它的主要特点是在另一个Render线程使用OpenGL进行绘制，这个是它最重要的特点。而mBatches中所有的DrawOp都会通过OpenGL被绘制到GraphicBuffer中，最后通过swapBuffers函数调用queueBuffer()通知SurfaceFlinger合成。 12345678[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]EGLBoolean egl_window_surface_v2_t::swapBuffers()&#123;......nativeWindow-&gt;queueBuffer(nativeWindow, buffer); nativeWindow-&gt;dequeueBuffer(nativeWindow, &amp;buffer);......&#125; 总的来说，可以看到一个View上的东西要绘制出来，要经过多步的转化。 这样做有几个好处：第一、对绘制操作进行batch/merge可以减少GL的draw call，从而减少渲染状态切换，提高了性能。第二、因为将View层次结构要绘制的东西转化为DisplayList这种“中间语言”的形式，当需要绘制时才转化为GL命令。因此在View中内容没有更改或只有部分属性更改时只要修改中间表示（即RenderNode和RenderProperties）即可，从而避免很多重复劳动。第三、由于DisplayList中包含了要绘制的所有信息，一些属性动画可以由渲染线程全权处理，无需主线程介入，主线程卡住也不会让界面卡住。另一方面，也可以看到一些潜力可挖。比如当前可以合并的操作类型有限。另外主线程和渲染线程间的很多调用还是同步的，并行度或许可以进一步提高。另外Vulkan的引入也可以帮助进一步榨干GPU的能力。例如： 绘制的批次按文本、图片资源、几何图形等进行分类，分批绘制的效果如下图所示: （五）、参考资料(特别感谢各位前辈的分析和图示)：Android N中UI硬件渲染（hwui）的HWUI_NEW_OPS(基于Android 7.1) Android应用程序UI硬件加速渲染技术简要介绍和学习计划【特别感谢 - Android N中UI硬件渲染（hwui）的HWUI_NEW_OPS(基于Android 7.1)】Android DisplayList 构建过程Android5.0中 hwui 中 RenderThread 工作流程","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Display System（2）：Android Display System 系统分析之Android EGL && OpenGL","slug":"Android Display System（2）：Android Display System 系统分析之Android EGL && OpenGL","date":"2018-07-19T16:00:00.000Z","updated":"2018-06-20T15:11:14.394Z","comments":true,"path":"2018/07/20/Android Display System（2）：Android Display System 系统分析之Android EGL && OpenGL/","link":"","permalink":"http://zhoujinjian.cc/2018/07/20/Android Display System（2）：Android Display System 系统分析之Android EGL && OpenGL/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】 【特别感谢 - Android Graphics and Android EGL】【特别感谢 - Android 4.4 (KitKat) Design Pattern-Graphics Subsystem】【特别感谢 - Android 4.4 (KitKat) in virtualization VSync signal】【特别感谢 - Android显示系统设计框架介绍】【特别感谢 - 图解Android - Android GUI 系统 (2) - 窗口管理 (View, Canvas, Window Manager)】 Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) 🌀🌀：专注于Linux &amp;&amp; Android Multimedia（Camera、Video、Audio、Display）系统分析与研究 【Android Display System 系统分析系列】：【Android Display System（1）：Android 7.1.2 (Android N) Android Graphics 系统 分析】【Android Display System（2）：Android Display System 系统分析之Android EGL &amp;&amp; OpenGL】【Android Display System（3）：Android Display System 系统分析之HardwareRenderer.draw()绘制流程分析】【Android Display System（4）：Android Display System 系统分析之Gralloc &amp;&amp; HWComposer模块分析】【Android Display System（5）：Android Display System 系统分析之Display Driver Architecture】 Android EGL、GLES_CM、GLES2： \\frameworks\\native\\opengl\\libs\\EGL egl.cpp egl_display.cpp eglApi.cpp Loader.cpp \\frameworks\\native\\opengl\\libs\\GLES_CM gl.cpp gl_api.in glext_api.in \\frameworks\\native\\opengl\\libs\\GLES2 gl2.cpp gl2_api.in gl2ext_api.in OpenGL Native &amp;&amp; JNI ： \\frameworks\\base\\core\\jni\\ android_opengl_GLES10.cpp android_opengl_GLES10Ext.cpp android_opengl_GLES11.cpp android_opengl_GLES11Ext.cpp android_opengl_GLES20.cpp android_opengl_GLES30.cpp android_opengl_GLES31.cpp android_opengl_GLES31Ext.cpp android_opengl_GLES32.cpp com_google_android_gles_jni_EGLImpl.cpp com_google_android_gles_jni_GLImpl.cpp Opengl Java： \\frameworks\\base\\opengl\\java\\android\\opengl GLES10.java GLES10Ext.java GLLogWrapper.java GLSurfaceView.java EGLDisplay.java EGLConfig.java EGLContext.java EGLSurface.java \\frameworks\\base\\opengl\\java\\javax\\microedition\\khronos\\opengles GL10.java GL11.java \\frameworks\\base\\opengl\\java\\com\\google\\android\\gles_jni GLImpl.java EGLImpl.java EGLConfigImpl.java EGLContextImpl.java EGLDisplayImpl.java 总体架构： 【Android Display System（1）- Android Graphics 系统 分析】 ####（一）、 Android EGL 应用实例 1.1、Android Graphics 测试程序首先看一下Android测试程序： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207参考\\frameworks\\native\\services\\surfaceflinger\\tests\\Transaction_test.cpp拷贝同目录下.mk文件push到手机运行即可看到效果。#include &lt;gtest/gtest.h&gt;#include &lt;android/native_window.h&gt;#include &lt;binder/IMemory.h&gt;#include &lt;gui/ISurfaceComposer.h&gt;#include &lt;gui/Surface.h&gt;#include &lt;gui/SurfaceComposerClient.h&gt;#include &lt;private/gui/ComposerService.h&gt;#include &lt;private/gui/LayerState.h&gt;#include &lt;utils/String8.h&gt;#include &lt;ui/DisplayInfo.h&gt;#include &lt;math.h&gt;namespace android &#123;// Fill an RGBA_8888 formatted surface with a single color.static void fillSurfaceRGBA8(const sp&lt;SurfaceControl&gt;&amp; sc, uint8_t r, uint8_t g, uint8_t b) &#123; ANativeWindow_Buffer outBuffer; sp&lt;Surface&gt; s = sc-&gt;getSurface(); ASSERT_TRUE(s != NULL); ASSERT_EQ(NO_ERROR, s-&gt;lock(&amp;outBuffer, NULL)); uint8_t* img = reinterpret_cast&lt;uint8_t*&gt;(outBuffer.bits); for (int y = 0; y &lt; outBuffer.height; y++) &#123; for (int x = 0; x &lt; outBuffer.width; x++) &#123; uint8_t* pixel = img + (4 * (y*outBuffer.stride + x)); pixel[0] = r; pixel[1] = g; pixel[2] = b; pixel[3] = 255; &#125; &#125; ASSERT_EQ(NO_ERROR, s-&gt;unlockAndPost());&#125;class LayerTest : public ::testing::Test &#123;protected: virtual void SetUp() &#123; mComposerClient = new SurfaceComposerClient; ASSERT_EQ(NO_ERROR, mComposerClient-&gt;initCheck()); sp&lt;IBinder&gt; display(SurfaceComposerClient::getBuiltInDisplay( ISurfaceComposer::eDisplayIdMain)); DisplayInfo info; SurfaceComposerClient::getDisplayInfo(display, &amp;info); ssize_t displayWidth = info.w; ssize_t displayHeight = info.h; // Background surface black mBGSurfaceControl = mComposerClient-&gt;createSurface( String8(\"BG Test Surface\"), displayWidth, displayHeight-720, PIXEL_FORMAT_RGBA_8888, 0); ASSERT_TRUE(mBGSurfaceControl != NULL); ASSERT_TRUE(mBGSurfaceControl-&gt;isValid()); fillSurfaceRGBA8(mBGSurfaceControl, 0, 0, 0); // Foreground surface red mFGSurfaceControl = mComposerClient-&gt;createSurface( String8(\"FG Test Surface\"), 360, 360, PIXEL_FORMAT_RGBA_8888, 0); ASSERT_TRUE(mFGSurfaceControl != NULL); ASSERT_TRUE(mFGSurfaceControl-&gt;isValid()); fillSurfaceRGBA8(mFGSurfaceControl, 255, 0, 0); // Foreground surface blue mFGSurfaceControlBlue = mComposerClient-&gt;createSurface( String8(\"FG Test Surface\"), 360, 360, PIXEL_FORMAT_RGBA_8888, 0); ASSERT_TRUE(mFGSurfaceControlBlue != NULL); ASSERT_TRUE(mFGSurfaceControlBlue-&gt;isValid()); fillSurfaceRGBA8(mFGSurfaceControl, 0, 255, 0); // Foreground surface green mFGSurfaceControlGreen = mComposerClient-&gt;createSurface( String8(\"FG Test Surface\"), 360, 360, PIXEL_FORMAT_RGBA_8888, 0); ASSERT_TRUE(mFGSurfaceControlGreen != NULL); ASSERT_TRUE(mFGSurfaceControlGreen-&gt;isValid()); fillSurfaceRGBA8(mFGSurfaceControl, 0, 0, 255); // Synchronization surface mSyncSurfaceControl = mComposerClient-&gt;createSurface( String8(\"Sync Test Surface\"), 1, 1, PIXEL_FORMAT_RGBA_8888, 0); ASSERT_TRUE(mSyncSurfaceControl != NULL); ASSERT_TRUE(mSyncSurfaceControl-&gt;isValid()); fillSurfaceRGBA8(mSyncSurfaceControl, 31, 31, 31); //SurfaceComposerClient::openGlobalTransaction() SurfaceComposerClient::openGlobalTransaction(); mComposerClient-&gt;setDisplayLayerStack(display, 0); //black ASSERT_EQ(NO_ERROR, mBGSurfaceControl-&gt;setLayer(INT_MAX-4)); ASSERT_EQ(NO_ERROR, mBGSurfaceControl-&gt;show()); //red ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;setLayer(INT_MAX-3)); ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;setPosition(0, 0)); ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;show()); //blue ASSERT_EQ(NO_ERROR, mFGSurfaceControlBlue-&gt;setLayer(INT_MAX-2)); ASSERT_EQ(NO_ERROR, mFGSurfaceControlBlue-&gt;setPosition(360, 360)); ASSERT_EQ(NO_ERROR, mFGSurfaceControlBlue-&gt;show()); //green ASSERT_EQ(NO_ERROR, mFGSurfaceControlGreen-&gt;setLayer(INT_MAX-1)); ASSERT_EQ(NO_ERROR, mFGSurfaceControlGreen-&gt;setPosition(720, 720)); ASSERT_EQ(NO_ERROR, mFGSurfaceControlGreen-&gt;show()); ASSERT_EQ(NO_ERROR, mSyncSurfaceControl-&gt;setLayer(INT_MAX-1)); ASSERT_EQ(NO_ERROR, mSyncSurfaceControl-&gt;setPosition(displayWidth-2, displayHeight-2)); ASSERT_EQ(NO_ERROR, mSyncSurfaceControl-&gt;show()); SurfaceComposerClient::closeGlobalTransaction(true); waitForPostedBuffers(); &#125; virtual void TearDown() &#123; mComposerClient-&gt;dispose(); mBGSurfaceControl = 0; mFGSurfaceControl = 0; mSyncSurfaceControl = 0; mComposerClient = 0; &#125; void waitForPostedBuffers() &#123; // Since the sync surface is in synchronous mode (i.e. double buffered) // posting three buffers to it should ensure that at least two // SurfaceFlinger::handlePageFlip calls have been made, which should // guaranteed that a buffer posted to another Surface has been retired. fillSurfaceRGBA8(mFGSurfaceControl, 255, 0, 0); fillSurfaceRGBA8(mFGSurfaceControlBlue, 0, 255, 0); fillSurfaceRGBA8(mFGSurfaceControlGreen, 0, 0, 255); fillSurfaceRGBA8(mSyncSurfaceControl, 0, 0, 0); &#125; sp&lt;SurfaceComposerClient&gt; mComposerClient; sp&lt;SurfaceControl&gt; mBGSurfaceControl; sp&lt;SurfaceControl&gt; mFGSurfaceControl;//red sp&lt;SurfaceControl&gt; mFGSurfaceControlBlue; sp&lt;SurfaceControl&gt; mFGSurfaceControlGreen; // This surface is used to ensure that the buffers posted to // mFGSurfaceControl have been picked up by SurfaceFlinger. sp&lt;SurfaceControl&gt; mSyncSurfaceControl;&#125;;TEST_F(LayerTest, LayerWorks) &#123; SurfaceComposerClient::openGlobalTransaction(); ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;setPosition(0, 0)); SurfaceComposerClient::closeGlobalTransaction(true); for(;;)&#123; sp&lt;IBinder&gt; display(SurfaceComposerClient::getBuiltInDisplay( ISurfaceComposer::eDisplayIdMain)); DisplayInfo info; SurfaceComposerClient::getDisplayInfo(display, &amp;info); ssize_t displayWidth = info.w; ssize_t displayHeight = info.h; SurfaceComposerClient::openGlobalTransaction(); mComposerClient-&gt;setDisplayLayerStack(display, 0); ASSERT_EQ(NO_ERROR, mBGSurfaceControl-&gt;setLayer(INT_MAX-4)); ASSERT_EQ(NO_ERROR, mBGSurfaceControl-&gt;show()); //red ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;setLayer(INT_MAX-3)); ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;setPosition(0, 0)); ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;show()); //blue ASSERT_EQ(NO_ERROR, mFGSurfaceControlBlue-&gt;setLayer(INT_MAX-2)); ASSERT_EQ(NO_ERROR, mFGSurfaceControlBlue-&gt;setPosition(360, 360)); ASSERT_EQ(NO_ERROR, mFGSurfaceControlBlue-&gt;show()); //green ASSERT_EQ(NO_ERROR, mFGSurfaceControlGreen-&gt;setLayer(INT_MAX-1)); ASSERT_EQ(NO_ERROR, mFGSurfaceControlGreen-&gt;setPosition(720, 720)); ASSERT_EQ(NO_ERROR, mFGSurfaceControlGreen-&gt;show()); //Sync ASSERT_EQ(NO_ERROR, mSyncSurfaceControl-&gt;setLayer(INT_MAX-1)); ASSERT_EQ(NO_ERROR, mSyncSurfaceControl-&gt;setPosition(displayWidth-2, displayHeight-2)); ASSERT_EQ(NO_ERROR, mSyncSurfaceControl-&gt;show()); SurfaceComposerClient::closeGlobalTransaction(true); &#125;&#125; 实现效果（保持运行，可以看到界面最顶层会绘制黑色背景和红绿蓝三个色块）： 可以看到比较关键的代码： 1234567891011121314151617181920212223//创建SurfaceComposerClientmComposerClient = new SurfaceComposerClient;ASSERT_EQ(NO_ERROR, mComposerClient-&gt;initCheck());//获取display信息sp&lt;IBinder&gt; display(SurfaceComposerClient::getBuiltInDisplay( ISurfaceComposer::eDisplayIdMain));DisplayInfo info;SurfaceComposerClient::getDisplayInfo(display, &amp;info);ssize_t displayWidth = info.w;ssize_t displayHeight = info.h;// Background surface black//请求SurfaceFlinger创建SurfacemBGSurfaceControl = mComposerClient-&gt;createSurface( String8(\"BG Test Surface\"), displayWidth, displayHeight-720, PIXEL_FORMAT_RGBA_8888, 0);//填充SurfacefillSurfaceRGBA8(mBGSurfaceControl, 0, 0, 0);//设置Layer层级ASSERT_EQ(NO_ERROR, mBGSurfaceControl-&gt;setLayer(INT_MAX-4));//SurfaceControl-&gt;show()显示surfaceASSERT_EQ(NO_ERROR, mBGSurfaceControl-&gt;show()); 这部分的分析请参考【Android Display System（1）- Android Graphics 系统 分析】我们这里主要为了引出Android底层如何利用EGL绘图。 1.2、Android BootAnimation 开机动画(EGL在Android中应用) 关键代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[-&gt;\\frameworks\\base\\cmds\\bootanimation\\BootAnimation.cpp]status_t BootAnimation::readyToRun() &#123; mAssets.addDefaultAssets(); sp&lt;IBinder&gt; dtoken(SurfaceComposerClient::getBuiltInDisplay( ISurfaceComposer::eDisplayIdMain)); DisplayInfo dinfo; status_t status = SurfaceComposerClient::getDisplayInfo(dtoken, &amp;dinfo); // create the native surface sp&lt;SurfaceControl&gt; control = session()-&gt;createSurface(String8(\"BootAnimation\"), dinfo.w, dinfo.h, PIXEL_FORMAT_RGB_565); SurfaceComposerClient::openGlobalTransaction(); control-&gt;setLayer(0x40000000); SurfaceComposerClient::closeGlobalTransaction(); sp&lt;Surface&gt; s = control-&gt;getSurface(); // initialize opengl and egl ...... // (1)、获得 EGLDisplay 对象。 EGLDisplay display = eglGetDisplay(EGL_DEFAULT_DISPLAY); // (2)、初始化 EGLDisplay 对象 eglInitialize(display, 0, 0); // (3)、选择 EGLConfig eglChooseConfig(display, attribs, &amp;config, 1, &amp;numConfigs); // (4)、创建 Windows Surface surface = eglCreateWindowSurface(display, config, s.get(), NULL); // (5)、创建 EGL context context = eglCreateContext(display, config, NULL, NULL); eglQuerySurface(display, surface, EGL_WIDTH, &amp;w); eglQuerySurface(display, surface, EGL_HEIGHT, &amp;h); // (6)、启用前面创建的 EGL context if (eglMakeCurrent(display, surface, surface, context) == EGL_FALSE) return NO_INIT; &#125;bool BootAnimation::playAnimation(const Animation&amp; animation)&#123; ...... // (7)、OpenGL ES API 绘制图形：gl_*() glDrawTexiOES(xc, mHeight - (yc + frame.trimHeight), 0, frame.trimWidth, frame.trimHeight); ...... // (8)、SwapBuffers显示 eglSwapBuffers(mDisplay, mSurface); ...... &#125; 使用EGL一般会经历以上几个步骤。 1.2、Understanding Android Graphics Internals要深入了解Android Graphics机制，需要了解熟悉以下知识（包括但不限于）。 ####（二）、OpenGL ES 2.0 知识串讲在了解EGL之前，先来看看前人总结的知识OPENGL ES 2.0 知识串讲： 2.1、写在前面的话2.1.1、电脑是做什么用的?电脑又被称为计算机,那么最重要的工作就是计算。看过三体的同学都知道, 电脑中有无数纳米级别的计算单元,通过 0 和 1 的转换,完成加减乘除的操作。 2.1.2、是什么使得电脑工作?驱动,驱使着硬件完成工作。 2.1.3、谁来写驱动?制造电脑的公司自己来写驱动,因为他们对自己的底层硬件架构最熟悉。 2.1.4、谁会使用驱动?所有的软件工程师都会直接或者间接的使用到驱动。 那么问题来了,如果说不同的电脑公司,制造出来不同的硬件,使用不同的 驱动,提供出来不同的接口供软件工程师进行使用,那么软件工程师就要崩溃了。 所以,一定是需要一个标准,来统一一下。 2.1.5、那么在哪里进行统一?硬件没有办法统一,每个电脑公司为了优化自己电脑性能和功耗,制造出来 不同的硬件架构,这是需要无数的心血完成的,如果统一了,那么就不需要那么 多电脑公司了。 所以只能统一驱动的接口。 电脑组件大致分为:CPU、GPU、内存、总线等。而 OpenGL 就是 GPU 驱动 的一套标准接口(OpenGL ES 为嵌入式设备 GPU 驱动的标准接口,比如手机, OpenGL ES 全称:OpenGL for Embedded Systems)。 所以综上所述,我使用了 5 个问题,引出了 OpenGL 的用处:就是将复杂的、 各种各样的 GPU 硬件包装起来,各个电脑公司编写自家的驱动,然后提供出来 一套统一的接口,供上层软件工程师调用。这样,世界就和平了。 2.1.6、谁这么牛,定义了 OpenGL 这套标准?Khronos。每当我打这几个字母的时候,都会抱有一种敬畏的心理,因为它 不是一家公司,它是一个组织,它是由众多大公司联合组建而来,比如 Apple、 Intel、AMD、Google、ARM、Qualcomm、Nvidia 等等等等。各个大公司投入了大 量的人力、资金等创建了这个组织。对电脑 GPU 定义了统一的接口 OpenGL,对 手机 GPU 定义了统一的接口 OpenGL ES(我也非常有幸,在 Intel 工作期间,跟 Intel 驻 Khronos 的 3D 负责人共事了一段时间,每周一次的跨洋电话,都会让我受益匪浅) 这个组织除了定义了 OpenGL 接口之外,还定义了很多其他接口。目前针对 GPU 又提出了另外一套更底层的接口 Vulkan,这是一套比 OpenGL 更底层的接口, 使用其可以更容易优化,不过目前硬件厂商的驱动还有待开发,可能普及 Vulkan 还需要很多年。就好比 OpenGL ES 已经发展到了 3.1,而市面上的手机很多还是 只能支持 OpenGL ES 2.0 一样。所以新的科技从提出,到实现,到量产,到使用, 到普及,是一段很长的路。 所以,我们现在学习 OpenGL ES 2.0 是适时的,且是非常必要的(不懂 2.0, 想直接学习更难的 3.0、3.1、Vulkan,很难)。 事先预告一下,OpenGL ES 2.0 会分十三个课程,结束之后,我会立即奉上 OpenGL ES 3.0 在 OpenGL ES 2.0 基础上的改变。 2.1.7、OpenGL 和我们游戏（Android ）开发者有什么关系?电脑/手机屏幕上显示的东西,要么是 2D 的,要么是 3D 的,那么如果是 3D 的,不管是 App 也好,游戏也好,简单的图片界面也好,底层都是通过 GPU、 通过 OpenGL(ES)绘制出来的。 开发 App 的时候,是通过创建控件的方式,而控件已经对底层进行了一层封装,所以 App 开发者很少会接触到 OpenGL(ES)。 游戏的开发是通过游戏引擎,而游戏引擎的最底层,是直接调用了 OpenGL(ES),直接对 GPU 进行控制。 所以说游戏引擎工程师必须懂 OpenGL(ES),而游戏开发者,想要更好的对游戏进行更好的理解和优化,也建议学一些 OpenGL(ES)。 2.1.8、DirectX 是什么?最后一个问题。我们发现 Khronos 组织的成员中,我没有提到大名鼎鼎的微 软,因为微软不在组织中,而它提出了自己的 GPU 驱动标准,DirectX。 所以目前手机,不管是 iOS 还是 Android,都是支持 OpenGL ES。电脑,Windows 系统支持 DirectX 和 OpenGL,Linux/Mac(Unix)系统支持 OpenGL。 2.2、OpenGL ES 的两个小伙伴虽然,我们教程的标题是 OpenGL ES,但是我们的内容将不仅限于 OpenGL ES。 OpenGL ES 是负责 GPU 工作的,目的是通过 GPU 计算,得到一张图片,这张图 片在内存中其实就是一块 buffer,存储有每个点的颜色信息等。而这张图片最终是要显示到屏幕上,所以还需要具体的窗口系统来操作,OpenGL ES 并没有相关的函数。所以,OpenGL ES 有一个好搭档 EGL。 EGL,全称:embedded Graphic Interface,是 OpenGL ES 和底层 Native 平台 视窗系统之间的接口。所以大概流程是这样的:首先,通过 EGL 获取到手机屏幕 的 handle,获取到手机支持的配置(RGBA8888/RGB565 之类,表示每个像素中包 含的颜色等信息的存储空间是多少位),然后根据这个配置创建一块包含默认 buffer 的 surface(buffer 的大小是根据屏幕分辨率乘以每个像素信息所占大小计 算而得)和用于存放 OpenGL ES 状态集的 context,并将它们 enable 起来。然后, 通过 OpenGL ES 操作 GPU 进行计算,将计算的结果保存在 surface 的 buffer 中。 最后,使用 EGL,将绘制的图片显示到手机屏幕上。 而在 OpenGL ES 操作 GPU 计算的时候,还需要介绍 OpenGL ES 的另外一个好搭档 GLSL。 GLSL,全称:OpenGL Shading Language,是 OpenGL ES 中使用到的着色器的 语言,用这个语言可以编写小程序运行在 GPU 上。 在这里需要先提到 CPU 和 GPU 的区别,它们的功能都是用于计算,也都是由很多核组成,区别在于 CPU 的核比较少,但是单个核的计算能力比较强,而 GPU 的核很多,但是每个核的计算能力都不算特别强。目前 GPU 的主要工作是用于生成图片(现在也有通过 GPU 进行高性能运算_并行运算,但是在这里不属于讨论的范围),原因就是图片是由很多像素组成,每个像素都包含有颜色、深度等信息,而为了得到这些信息数据,针对每个像素点的计算,是可以通过统一的算法来完成。GPU 就擅长处理针对这种大规模数据,使用同一个算法进行计算。而这个算法,就是使用 GLSL 写成 Shader,供 GPU 运算使用。 在图形学的视角中,所有的图片都是由三角形构成的。所以通过 OpenGL ES 绘制图片的时候,我们需要通过 OpenGL ES API 创建用于在 GPU 上运行的 shader, 然后将通过 CPU 获取到的图片顶点信息,传入 GPU 中的 Shader 中。在 Vertex Shader 中通过矩阵变换,将顶点坐标从模型坐标系转换到世界坐标系,再到观察坐标系,到裁剪坐标系,最后投影到屏幕坐标系中,计算出在屏幕上各个顶点的坐标。然后,通过光栅化,以插值的方法得到所有像素点的信息,并在 Fragment shader 中计算出所有像素点的颜色。最后,通过 OpenGL ES 的 API 设定的状态,将得到的像素信息进行 depth/stencil test、blend,得到最终的图片。 2.3、屏幕图片的本质和产生过程当我们买一个手机的时候,我们会非常关注这个手机的分辨率。分辨率代表着像素的多少,比如我们熟知的 iphone6 的分辨率为 1334×750,而 iphone6 plus 的分辨率是1920×1080。 手机屏幕上的图片,是由一个一个的像素组成,那么可以计算出来,一个屏幕上的图片,是由上百万个像素点组成。而每个像素点都有自己的颜色,每种颜色都是由 RGB 三原色组成。三原色按照不同的比例混合,组成了手机所能显示出来的颜色。 每个像素的颜色信息都保存在 buffer 中,这块 buffer 可以分给 RGB 每个通 道各 8bit 进行信息保存,也可以分给 RGB 每个通道不同的空间进行信息保存, 比如由于人眼对绿色最敏感,那么可以分配给 G 通道 6 位,R 和 B 通道各 5 位。这些都是常见的手机配置。假如使用 RGB888 的手机配置,也就是每种颜色的取值从 0 到 255,0 最小,255 最大。那么红绿蓝都为 0 的时候,这个像素点的颜色就是黑色,红绿蓝都为 255 的时候,这个像素点的颜色就是白色。当红为 255, 绿蓝都为 0 的时候,这个像素点的颜色就是红色。当红绿为 255,蓝为 0 的时候, 这个像素点的颜色就是黄色。当然不是只取 0 或者 255,可以取 0-255 中间的值, 100,200,任意在 0 和 255 中间的值都没有问题。那么我们可以算一下,按照红绿蓝不同比例进行搭配,每个像素点,可以显示的颜色有 255255255=16581375 种,这个数字是非常恐怖,所以我们的手机可以显示出来各种各样的颜色。 这里在延伸的科普一下,我们看到手机可以显示那么多种颜色了,但是是不是说我们的手机在颜色上就已经发展到极致了呢?其实是远远没有的,在这个手机配置下,三原色中每一种的取值可以从 0 到 255,而在现实生活中,它们的取 值可以从 0 到 1 亿,而我们人类的眼睛所能看到的范围是,从 0 到 10 万。所以手机硬件还存在很大的提升空间。而在手机硬件提升之前,我们也可以通过 HDR 等技术尽量的在手机中多显示一些颜色。所以,讲到这里,我们知道了,手机屏幕上显示的图片,是由这上百万个像素点,以及这上百万个像素点对应的颜色组成的。 用程序员的角度来看,就是手机屏幕对应着一块 buffer,这块 buffer 对应上百万个像素点,每个像素点需要一定的空间来存储其颜色。如果使用更加形象的例子来比喻,手机屏幕对应的 buffer 就好像一块巨大的棋盘,棋盘上有上百万个格子,每个格子都有自己的颜色,那么从远处整体的看这个棋盘,就是我们看手机的时候显示的样子。这就是手机屏幕上图片的本质。 通过我们对 EGL、GLSL、OpenGL ES 的理解,借助一张图片,从专业的角度来解释一下手机屏幕上的图片是如何生成的。 首先,通过 EGL 获取手机屏幕,进而获取到手机屏幕对应的这个棋盘,同时, 在手机的 GPU 中根据手机的配置信息,生成另外一个的棋盘和一个本子,本子是用于记录这个棋盘初始颜色等信息。 然后,OpenGL ES 就好像程序员的画笔,程序员需要知道自己想画什么东西,比如想画一个苹果,那么就需要通过为数不多的基本几何图元(如点、直线、三 角形)来创建所需要的模型。比如用几个三角形和点和线来近似的组成这个苹果 (图形学的根本就是点、线和三角形,所有的图形,都可以由这些基本图形组成, 比如正方形或者长方形,就可以由两个三角形组成,圆形可以由无数个三角形组成,只是三角形的数量越多,圆形看上去越圆润)。 根据这些几何图元,建立数学描述,比如每个三角形或者线的顶点坐标位置、每个顶点的颜色。得到这些信息之后,可以先通过 OpenGL ES 将 EGL 生成的棋盘 (buffer)进行颜色初始化,一般会被初始化为黑色。然后将刚才我们获取到的顶点坐标位置,通过矩阵变化的方式,进行模型变换、观察变换、投影变换,最后映射到屏幕上,得到屏幕上的坐标。这个步骤可以在 CPU 中完成,也就是在 OpenGL ES 把坐标信息传给 Shader 之前,在 CPU 中通过矩阵相乘等方式进行更新,或者是直接把坐标信息通过 OpenGL ES 传给 Shader,同时也把矩阵信息传给 Shader,通过 Shader 在 GPU 端进行坐标更新,更新的算法通过 GLSL 写在 Shader 中。这个进行坐标更新的 Shader 被称为 vertex shader,简称 VS,是 OpenGL ES2.0, 也是 GLSL130 版本对应的最重要两个 shader 之一,作用是完成顶点操作阶段中的所有操作。经过矩阵变换后的像素坐标信息,为屏幕坐标系中的坐标信息。在 VS 中,最重要的输入为顶点坐标、矩阵(还可以传入顶点的颜色、法线、纹理 坐标等信息),而最重要的运算结果,就是这个将要显示在屏幕上的坐标信息。 VS 会针对传入的所有顶点进行运算,比如在 OpenGL ES 中只想绘制一个三角形 和一条线,这两个图元不共享顶点,那么在 VS 中,也就传入了 5 个顶点信息, 根据矩阵变换,这 5 个顶点的坐标转换成了屏幕上的顶点坐标信息,从图上显示, 也就是从左上角的图一,更新成了中上图的图二。 再然后,当图二生成之后,我们知道了图元在屏幕上的顶点位置,而顶点的颜色在 VS 中没有发生变化,所以图元的顶点颜色我们也是知道的。下面就是根据 OpenGL ES 中设置的状态,表明哪些点连成线,哪些点组成三角形,进行图元装配,也就是我们在右上角的图三中看到的样子。这个样子在 GPU 中不会显示, 那几条线也是虚拟的线,是不会显示在棋盘 buffer 中的,而 GPU 做的是光珊化,这一步是发生在从 VS 出来,进入另外一个Shader (Pixel shader,也称 fragment shader)之前,在 GPU 中进行的。作用是把线上,或者三角形内部所有的像素点找到,并根据插值或者其他方式计算出其颜色等信息(如果不通过插值,可以使用其他的方法,这些在 OpenGL ES 和 GLSL 中都可以进行设置)。也就生成了下面一行的图四和图五。 我们大概可以看到在图 4 和图 5 种出现了大量的顶点,大概数一下估计有 40 个点左右,这些点全部都会进入 PS 进行操作,在 PS 中可以对这些点的颜色进行操作,比如可以只显示这些点的红色通道,其他的绿蓝通道的值设置为 0, 比如之前某个点的 RGB 为 200,100,100。在 PS 中可以将其通过计算,更新为 200,0,0。这样做的结果就是所显示的图片均为红色,只是深浅不同。这也就好像戴上了一层红色的滤镜,其他颜色均为滤掉了。所以用 PS 来做滤镜是非常方便的。再比如,假如一盏红色的灯照到了苹果上,那么显示出来的颜色就是在苹果原本的颜色基础上,红色值进行一定的增值。 所以,总结一下,经过 VS 和 PS 之后,程序员想要画的东西,就已经被画出来了。想要绘制的东西,也就是左下角图五的样子。然后再根据 OpenGL ES 的设置,对新绘制出来的东西进行 Depth/Stencil Test,剔除掉被遮挡的部分,将剩余部分与原图片进行 Blend,生成新的图片。 最后,通过 EGL,把这个生成的棋盘 buffer 和手机屏幕上对应的棋盘 buffer 进行调换,让手机屏幕显示这个新生成的棋盘,旧的那个棋盘再去绘制新的图片信息。周而复始,不停的把棋盘进行切换,也就像过去看连环画一样,动画就是由一幅幅的图片组成,当每秒切换的图片数量超过 30 张的时候,我们的手机也就看到了动态的效果。这就是屏幕上图片的产生过程。 在这里再进行一下延伸,这个例子中,VS 计算了 5 个顶点的数据,PS 计算 了大概 40 个顶点的数据,而我们刚才说过,手机中存在上百万个像素点,这上百万个像素点都可以是顶点,那么这个计算量是非常大的。而这也是为什么要将 shader 运算放在 GPU 中的原因,因为 GPU 擅长进行这种运算。 我们知道 CPU 现在一般都是双核或者 4 核,多的也就是 8 核或者 16 核,但是 GPU 动辄就是 72 核,多的还有上千核,这么多核的目的就是进行并行运算, 虽然单个的 GPU 核不如 CPU 核,但是单个的 GPU 核足够进行加减乘除运算,所以大量的 GPU 核用在图形学像素点运算上,是非常有效的。而 CPU 虽然单个很强大,而且也可以通过多级流水来提高吞吐率,但是终究还是不如 GPU 的多核来得快。但是在通过 GPU 进行多核运算的时候,需要注意的是:如果 shader 中存放判断语句,就会对 GPU 造成比较大的负荷,不同 GPU 的实现方式不同,多数 GPU 会对判断语句的两种情况都进行运算,然后根据判断结果取其中一个。 我们通过这个例子再次清楚了 OpenGL ES 绘制的整个流程,而这个例子也是最简单的一个例子,其中有很多 OpenGL ES 的其他操作没有被涉及到。比如,我们绘制物体的颜色大多是从纹理中采样出来,那么设计到通过 OpenGL ES 对纹理 进行操作。而 OpenGL ES 的这些功能,我们会在下面一点一点进行学习。 2.4.2、OpenGL 流水线（pipeline） EGL 是用于与手机设备打交道,比如获取绘制 buffer,将绘制 buffer 展现到手机屏幕中。那么抛开 EGL 不说,OpenGL ES 与 GLSL 的主要功能,就是往这块 buffer 上绘制图片。 所以,我们可以把OpenGL ES和GLSL的流程单独拿出来进行归纳总结,而这幅流程图就是著名的 OpenGL ES2.0 pipeline。 首先,最左边的 API 指的就是 OpenGL ES 的 API,OpenGL ES 其实是一个图形学库,由 109 个 API 组成,只要明白了这 109 个 API 的意义和用途,就掌握了OpenGL ES 2.0。 然后,我们通过 API 先设定了顶点的信息,顶点的坐标、索引、颜色等信息,将这些信息传入 VS。 在 VS 中进行运算,得到最终的顶点坐标。再把算出来的顶点坐标进行图元装配,构建成虚拟的线和三角形。再进行光珊化(在光珊化的时候,把顶点连接起来形成直线,或者填充多边形的时候,需要考虑直线和多边形的直线宽度、点的大小、渐变算法以及是否使用支持抗锯齿处理的覆盖算法。最终的每个像素点,都具有各自的颜色和深度值)。 将光珊化的结果传入 PS,进行最终的颜色计算。 然后,这所谓最终的结果在被实际存储到绘制 buffer 之前,还需要进行一系列的操作。这些操作可能会修改甚至丢弃这些像素点。 这些操作主要为 alpha test、Depth/Stencil test、Blend、Dither。 Alpha Test 采用一种很霸道极端的机制,只要一个像素的 alpha 不满足条件, 那么它就会被 fragment shader 舍弃,被舍弃的 fragments 不会对后面的各种 Tests 产生影响;否则,就会按正常方式继续下面的检验。Alpha Test 产生的效果也很极端,要么完全透明,即看不到,要么完全不透明。 Depth/stencil test 比较容易理解。由于我们绘制的是 3D 图形,那么坐标为 XYZ,而 Z 一般就是深度值,OpenGL ES 可以对深度测试进行设定,比如设定深度值大的被抛弃,那么假如绘制 buffer 上某个像素点的深度值为 0,而 PS 输出的 像素点的深度值为 1,那么 PS 输出的像素点就被抛弃了。而 stencil 测试更加简单,其又被称为蒙版测试,比如可以通过 OpenGL ES 设定不同 stencil 值的配抛弃, 那么假如绘制 buffer 上某个像素点的 stencil 值为 0,而 PS 输出的像素点的 stencil 值为 1,那么 PS 输出的像素点就被抛弃了。 既然说到了 Depth/stencil,那么就在这里说一下绘制 buffer 到底有多大,存 储了多少信息。按照我们刚才的说法,手机可以支持一百万个像素,那么生成的 绘制 buffer 就需要存储这一百万个像素所包含的信息,而每个像素包含的信息, 与手机配置有关,假如手机支持 Depth/stencil。那么通过 EGL 获取的绘制 buffer 中,每个像素点就包含了 RGBA 的颜色值,depth 值和 stencil 值,其中 RGBA 每个分量一般占据 8 位,也就是 8bit,也就是 1byte,而 depth 大多数占 24 位,stencil 占 8 位。所以每个像素占 64bit,也就是 8byte。那么 iphone6 plus 的绘制 buffer 的尺寸为 1920×1080×8=16588800byte=16200KB=15.8MB。 下面还有 blend,通过 OpenGL ES 可以设置 blend 混合模式。由于绘制 buffer 中原本每个像素点已经有颜色了,那么 PS 输出的颜色与绘制 buffer 中的颜色如何混合,生成新的颜色存储在绘制 buffer 中,就是通过 blend 来进行设定。 最后的 dither,dither 是一种图像处理技术,是故意造成的噪音,用以随机化量化误差,阻止大幅度拉升图像时,导致的像 banding(色带)这样的问题。也 是通过OpenGL ES 可以开启或者关闭。 经过了这一系列的运算和测试,也就得到了最终的像素点信息,将其存储到绘制 buffer 上之后,OpenGL ES 的 pipeline 也就结束了。 整个pipeline中，纵向按照流水线作业，横线按照独立作业，多级并行、提高渲染性能 ####（三）、 Android EGL Overview： OpenGL ES 和 EGL 介绍 3.1.0、OpenGL ESOpenGL ES（OpenGL for Embedded Systems）是 OpenGL 三维图形API的子集，针对手机、PDA和游戏主机等嵌入式设备而设计，各显卡制造商和系统制造商来实现这组 API。 3.1.1、OpenGL 基本概念OpenGL 的结构可以从逻辑上划分为下面 3 个部分： ☯ 图元（Primitives）☯ 缓冲区（Buffers）☯ 光栅化（Rasterize） 图元（Primitives）在 OpenGL 的世界里，我们只能画点、线、三角形这三种基本图形，而其它复杂的图形都可以通过三角形来组成。所以这里的图元指的就是这三种基础图形： ☯ 点：点存在于三维空间，坐标用（x,y,z）表示。☯ 线：由两个三维空间中的点组成。☯ 三角形：由三个三维空间的点组成。缓冲区（Buffers）OpenGL 中主要有 3 种 Buffer： 帧缓冲区（Frame Buffers） 帧缓冲区：这个是存储OpenGL 最终渲染输出结果的地方，它是一个包含多个图像的集合，例如颜色图像、深度图像、模板图像等。 渲染缓冲区（Render Buffers） 渲染缓冲区：渲染缓冲区就是一个图像，它是 Frame Buffer 的一个子集。 缓冲区对象（Buffer Objects） 缓冲区对象就是程序员输入到 OpenGL 的数据，分为结构类和索引类的。前者被称为“数组缓冲区对象”或“顶点缓冲区对象”（“Array Buffer Object”或“Vertex Buff er Object”），即用来描述模型的数组，如顶点数组、纹理数组等； 后者被称为“索引缓冲区对象”（“Index Buffer Object”），是对上述数组的索引。 光栅化（Rasterize）在介绍光栅化之前，首先来补充 OpenGL 中的两个非常重要的概念： Vertex Vertex 就是图形中顶点，一系列的顶点就围成了一个图形。Fragment Fragment 是三维空间的点、线、三角形这些基本图元映射到二维平面上的映射区域，通常一个 Fragment 对应于屏幕上的一个像素，但高分辨率的屏幕可能会用多个像素点映射到一个 Fragment，以减少 GPU 的工作。而光栅化是把点、线、三角形映射到屏幕上的像素点的过程。 着色器程序（Shader）Shader 用来描述如何绘制（渲染），GLSL 是 OpenGL 的编程语言，全称 OpenGL Shader Language，它的语法类似于 C 语言。OpenGL 渲染需要两种 Shader：Vertex Shader 和 Fragment Shader。 Vertex Shader Vertex Shader 对于3D模型网格的每个顶点执行一次，主要是确定该顶点的最终位置。Fragment Shader Fragment Shader对光栅化之后2D图像中的每个像素处理一次。3D物体的表面最终显示成什么样将由它决定，例如为模型的可见表面添加纹理，处理光照、阴影的影响等等。 3.2、EGL Overview What is the Direction?SW : Standard API (Java, NDK Stable API)HW : OpenGLES, OpenSLES, OpenMAXEGL™ is an interface between Khronos rendering APIs such as OpenGL ES or OpenVG and the underlying native platform window system 3.2.1、什么是 EGL？EGL 是 OpenGL ES 渲染 API 和本地窗口系统(native platform window system)之间的一个中间接口层，它主要由系统制造商实现。 EGL提供如下机制：与设备的原生窗口系统通信查询绘图表面的可用类型和配置创建绘图表面在OpenGL ES 和其他图形渲染API之间同步渲染管理纹理贴图等渲染资源为了让OpenGL ES能够绘制在当前设备上，我们需要EGL作为OpenGL ES与设备的桥梁。 3.2.2、使用 EGL 绘图的基本步骤 ☯ Display(EGLDisplay) 是对实际显示设备的抽象。☯ Surface（EGLSurface）是对用来存储图像的内存区域☯ FrameBuffer 的抽象，包括 Color Buffer， Stencil Buffer ，Depth Buffer。Context (EGLContext) 存储 OpenGL ES绘图的一些状态信息。使用EGL的绘图的一般步骤： 1、获取 EGL Display 对象：eglGetDisplay()2、初始化与 EGLDisplay 之间的连接：eglInitialize()3、获取 EGLConfig 对象：eglChooseConfig()4、创建 EGLContext 实例：eglCreateContext()5、创建 EGLSurface 实例：eglCreateWindowSurface()6、连接 EGLContext 和 EGLSurface：eglMakeCurrent()7、使用 OpenGL ES API 绘制图形：gl_*()8、切换 front buffer 和 back buffer 送显：eglSwapBuffer()9、断开并释放与 EGLSurface 关联的 EGLContext 对象：eglRelease()10、删除 EGLSurface 对象11、删除 EGLContext 对象12、终止与 EGLDisplay 之间的连接 3.3、EGLSurface and ANativeWindow 关系OpenGL ES 定义了一个渲染图形的 API，但没有定义窗口系统。为了让 GLES 能够适合各种平台，GLES 将与知道如何通过操作系统创建和访问窗口的库结合使用。用于 Android 的库称为 EGL。如果要绘制纹理多边形，应使用 GLES 调用；如果要在屏幕上进行渲染，应使用 EGL 调用。 在使用 GLES 进行任何操作之前，需要创建一个 GL 上下文。在 EGL 中，这意味着要创建一个 EGLContext 和一个 EGLSurface。GLES 操作适用于当前上下文，该上下文通过线程局部存储访问，而不是作为参数进行传递。这意味着您必须注意渲染代码在哪个线程上执行，以及该线程上的当前上下文。 3.3.1、EGLSurface EGLSurface 可以是由 EGL 分配的离屏缓冲区（称为“pbuffer”），或由操作系统分配的窗口。EGL 窗口 Surface 通过 eglCreateWindowSurface() 调用被创建。该调用将“窗口对象”作为参数，在 Android 上，该对象可以是 SurfaceView、SurfaceTexture、SurfaceHolder 或 Surface，所有这些对象下面都有一个 BufferQueue。当您进行此调用时，EGL 将创建一个新的 EGLSurface 对象，并将其连接到窗口对象的 BufferQueue 的生产方接口。此后，渲染到该 EGLSurface 会导致一个缓冲区离开队列、进行渲染，然后排队等待消耗方使用。（术语“窗口”表示预期用途，但请注意，输出内容不一定会显示在显示屏上。） EGL 不提供锁定/解锁调用，而是由您发出绘制命令，然后调用 eglSwapBuffers() 来提交当前帧。方法名称来自传统的前后缓冲区交换，但实际实现可能会有很大的不同。 一个 Surface 一次只能与一个 EGLSurface 关联（您只能将一个生产方连接到一个 BufferQueue），但是如果您销毁该 EGLSurface，它将与该 BufferQueue 断开连接，并允许其他内容连接到该 BufferQueue。 通过更改“当前”EGLSurface，指定线程可在多个 EGLSurface 之间进行切换。一个 EGLSurface 一次只能在一个线程上处于当前状态。 关于 EGLSurface 最常见的一个错误理解就是假设它只是 Surface 的另一方面（如 SurfaceHolder）。它是一个相关但独立的概念。您可以在没有 Surface 作为支持的 EGLSurface 上绘制，也可以在没有 EGL 的情况下使用 Surface。EGLSurface 仅为 GLES 提供一个绘制的地方。 3.3.2、ANativeWindow公开的 Surface 类以 Java 编程语言实现。C/C++ 中的同等项是 ANATIONWindow 类，由 Android NDK 半公开。您可以使用 ANativeWindow_fromSurface() 调用从 Surface 获取 ANativeWindow。就像它的 Java 语言同等项一样，您可以对 ANativeWindow 进行锁定、在软件中进行渲染，以及解锁并发布。 要从原生代码创建 EGL 窗口 Surface，可将 EGLNativeWindowType 的实例传递到 eglCreateWindowSurface()。EGLNativeWindowType 是 ANativeWindow 的同义词，您可以自由地在它们之间转换。 基本的“原生窗口”类型只是封装 BufferQueue 的生产方，这一点并不足为奇。 3.3.3、egl_surface_t 关系图 1234567891011121314151617181920212223242526272829303132[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]static EGLSurface createWindowSurface(EGLDisplay dpy, EGLConfig config, NativeWindowType window, const EGLint* /*attrib_list*/)&#123; ...... EGLint surfaceType; if (!(surfaceType &amp; EGL_WINDOW_BIT)) return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); if (static_cast&lt;ANativeWindow*&gt;(window)-&gt;common.magic != ANDROID_NATIVE_WINDOW_MAGIC) &#123; return setError(EGL_BAD_NATIVE_WINDOW, EGL_NO_SURFACE); &#125; EGLint configID; if (getConfigAttrib(dpy, config, EGL_CONFIG_ID, &amp;configID) == EGL_FALSE) return EGL_FALSE; int32_t depthFormat; int32_t pixelFormat; if (getConfigFormatInfo(configID, pixelFormat, depthFormat) != NO_ERROR) &#123; return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); &#125; ...... egl_surface_t* surface; surface = new egl_window_surface_v2_t(dpy, config, depthFormat, static_cast&lt;ANativeWindow*&gt;(window)); ...... return surface;&#125; 3.3.4、EGLContext and Thread Local Storage3.3.4.1、EGLContext 3.3.4.2、Thread Local Storage 3.3.5、EGLImplementation : HWCompser and SurfaceFlinger3.3.5.1、HWCompser 3.3.5.2、SurfaceFlinger ####（四）、Android EGL：OpenGL ES 库和 EGL 库加载过程在详细分析 EGL 绘图基本步骤 前，先来看看OpenGL ES 库和 EGL 库加载过程。 4.1、OpenGL ES 和 OpenGL ES 库的区别OpenGL ES ： 它本身只是一个协议规范，定义了一套可以供上层应用程序进行调用的 API，它抽象了 GPU 的功能，使应用开发者不必关心底层的 GPU 类型和具体实现。OpenGL ES 库：OpenGL ES 库就是上面 OpenGL ES 中定义的 API 的具体实现。由于每个显卡制造厂商的 GPU 硬件结构不同，从而导致各个厂商的OpenGL ES 库也各不相同，所以 Android 系统中的 OpenGL ES 库通常是由硬件厂商提供的，通常存放在 Android 系统中的 /system/lib64/（/system/lib/） 。OpenGL ES Wrapper 库：OpenGL ES Wrapper 库是一个对 OpenGL ES API 进行封装的一个包裹库，它向上为应用程序提供了标准的 OpenGL ES API，向下可以和不同厂商实现的 OpenGL ES 库进行绑定，将 OpenGL ES API 和对应的实现函数一一绑定在一起。并且，OpenGL ES 库的实现分为：软件模拟实现硬件加速实现现在，因为我们 Android 手机中的 Soc 片上芯片中都集成了 GPU 模块，所以这里使用的就是硬件加速实现的 OpenGL ES 库。但是，像 Android Emulator 中的 Android 系统，如果不支持将 OpenGL ES API 指令重定向到主机系统的 GPU 加速执行的话，它所采用的 OpenGL ES 库就是软件模拟实现的。 补充：如前面小节【OpenGL ES 和 EGL 介绍】中介绍的，EGL 也是一套 API，它的实现也需要系统厂商来提供。系统厂商通常会将这两套 API 的实现封装在一个共享链接库中，但是根据最新的标准，OpenGL ES API 实现的共享链接库和 EGL API 实现的共享链接库是独立分开的，例如 Nexus 9 平板设备中 OpenGL ES 和 EGL API 实现库就是独立分开的。 4.2、Android 中 OpenGL ES 软件层次栈按照分层理念的设计，Android 中的 OpenGL ES 实现也是层次设计的，形成一个软件层次栈。最上面的是 Java 层，接着下面是 JNI 层，再调用下面的 wrapper 层，wrapper 层下面则是 OpenGL ES API 的具体软件实或者硬件实现了。整个 OpenGL 软件层次栈的调用关系如下所示： 4.3、OpenGL ES/EGL Wrapper 库前面我们已经介绍过 OpenGL ES/EGL Wrapper 库是一个将 OpenGL ES API 和 OpenGL ES API 具体实现绑定在一起的库，它对应的源码路径是：/frameworks/native/opengl/libs/，其中: 123libGLESv1_CM.so：OpenGL ES 1.x API 的 Wrapper 库libGLESv2.so：OpenGL ES 2.0 的 Wrapper 库libGLESv3.so：OpenGL ES 3.0 的 Wrapper 库 其中因为 OpenGL ES 3.0 API 是兼容 OpenGL ES 2.0 API 的，所以 libGLESv2.so 库本质上和 libGLESv3.so 库是一样的。 4.3.1、OpenGL ES/EGL 实现库如果Android系统平台支持 OpenGL ES 硬件加速渲染，那么 OpenGL ES/EGL 实现库由系统厂商以.so的共享链接库的形式提供，例如，高通的实现：system\\vendor\\lib\\egl 123libEGL_adreno.so libGLESv1_CM_adreno.solibGLESv2_adreno.so 如果Android系统平台不支持 OpenGL ES 硬件加速渲染，那么它就会默认启用软件模拟渲染，这时 OpenGL ES/EGL 实现库就是由 AOSP 提供，链接库的存在的路径为： /system/lib64/egl/libGLES_android.so。而 libGLES_android.so 库在 Android 7.1 系统对应的实现源码路径为：/frameworks/native/opengl/libagl/ 。 4.3.2、Android 7.1 中加载 OpenGL ES 库的过程Android 中图形渲染所采用的方式（硬件 or 软件）是在系统启动之后动态确定的，而确定渲染方式的这个源码文件就是 /frameworks/native/opengl/libs/EGL/Loader.cpp 。 4.3.2.1、 Android 7.1 OpenGL ES 库和 EGL 库加载说明How Android finds OpenGL libraries, and the death of egl.cfg 这篇文章中提到了非常关键的一点，就是从 Android Kitkat 4.4 之后，Android 中加载 OpenGL ES/EGL 库的方法发生了变化了（但是整个加载过程都是由 /frameworks/native/opengl/libs/EGL/Loader.cpp 程序所决定的，也就是说 Loader.cpp 文件发生了变化）。 在 Android 4.4 之前，加载 OpenGL ES 库是由 /system/lib/egl/egl.cfg 文件所决定的，通过读取这个配置文件来确定是加载 OpenGL ES 软件模拟实现的库，还是OpenGL ES 硬件加速实现的库。 但是，在Android 4.4 之后，Android 不再通过读取 egl.cfg 配置文件的方式来加载 OpenGL ES 库，新的加载 OpenGL ES 库的规则，如下所示： 从 /system/lib/egl 或者 /system/vendor/lib/egl/ 目录下加载 libGLES.so 库文件或者 libEGL_vendor.so，libGLESv1_CM_vendor.so，libGLESv2vendor.so 库文件。为了向下兼容旧的库的命名方式，同样也会加载 /system/lib/egl 或者 /vendor/lib/egl/ 目录下的 libGLES.so 或者 libEGL_.so，libGLESv1CM.so，libGLESv2_.so 库文件。 4.3.2.2、硬件加速渲染 or 软件模拟渲染？前面我们提到 OpenGL ES 库的实现方式有两种，一种是硬件加速实现，一种是软件模拟实现，那么系统是怎么确定加载那一种 OpenGL ES 库的呢？ Android 7.1 源码中负责加载 OpenGL ES/EGL 库部分的代码位于：/frameworks/native/opengl/libs/EGL/Loader.cpp 文件中，这个文件中代码的主要入口函数是 Loader::open() 函数，而决定加载硬件加速渲染库还是软件模拟渲染库主要涉及到下面两个函数： 12setEmulatorGlesValue()checkGlesEmulationStatus() 下面就来简要的分析一下 Android 系统是如何选择加载硬件加速渲染库还是软件模拟渲染库： 首先，Loader::open() 入口函数会调用 setEmulatorGlesValue() 从 property 属性系统中获取一些属性值来判断当前 Android 系统是否在 Emulator 环境中运行，并根据读取出来的信息来重新设置新的属性键值对，setEmulatorGlesValue() 函数的代码如下所示： 123456789101112131415161718192021222324252627282930313233[-&gt;/frameworks/native/opengl/libs/EGL/Loader.cpp]static void setEmulatorGlesValue(void) &#123; char prop[PROPERTY_VALUE_MAX]; property_get(\"ro.kernel.qemu\", prop, \"0\"); //读取 ro.kernel.qemu 属性值，判断Android系统是否运行在 qemu 中 if (atoi(prop) != 1) return; property_get(\"ro.kernel.qemu.gles\", prop, \"0\"); //读取 ro.kernel.qemu.gles 属性值，判断 qemu 中 OpenGL ES 库的实现方式 if (atoi(prop) == 1) &#123; ALOGD(\"Emulator has host GPU support, qemu.gles is set to 1.\"); property_set(\"qemu.gles\", \"1\"); return; &#125; // for now, checking the following // directory is good enough for emulator system images const char* vendor_lib_path = #if defined(__LP64__) \"/vendor/lib64/egl\"; #else \"/vendor/lib/egl\"; #endif const bool has_vendor_lib = (access(vendor_lib_path, R_OK) == 0); //如果存在 vendor_lib_path 这个路径，那么就说明厂商提供了 OpenGL ES库自己的软件模拟渲染库，而不是 Android 系统自己编译得到的软件模拟渲染库 if (has_vendor_lib) &#123; ALOGD(\"Emulator has vendor provided software renderer, qemu.gles is set to 2.\"); property_set(\"qemu.gles\", \"2\"); &#125; else &#123; ALOGD(\"Emulator without GPU support detected. \" \"Fallback to legacy software renderer, qemu.gles is set to 0.\"); property_set(\"qemu.gles\", \"0\"); //最后，默认采取的是方案就是调用传统的Android系统自己编译得到软件模拟渲染库 &#125; &#125; 在 load_system_driver() 函数中，内部类 MatchFile 类中会调用 checkGlesEmulationStatus() 函数来检查 Android 系统是否运行在模拟器中，以及在模拟器中是否启用了主机硬件加速的功能，然后根据 checkGlesEmulationStatus() 函数的返回状态值来确定要加载共享链接库的文件绝对路径。load_system_driver() 和 checkGlesEmulationStatus() 函数代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[-&gt;/frameworks/native/opengl/libs/EGL/Loader.cpp]static void* load_system_driver(const char* kind) &#123; ATRACE_CALL(); class MatchFile &#123; public: //这个函数作用是返回需要加载打开的 OpenGL ES 和 EGL API 实现库文件的绝对路径 static String8 find(const char* kind) &#123; String8 result; int emulationStatus = checkGlesEmulationStatus(); //检查 Android 系统是否运行在模拟器中，以及在模拟器中是否启用了主机硬件加速的功能 switch (emulationStatus) &#123; case 0: //Android 运行在模拟器中，使用系统软件模拟实现的 OpenGL ES API 库 libGLES_android.so #if defined(__LP64__) result.setTo(\"/system/lib64/egl/libGLES_android.so\"); #else result.setTo(\"/system/lib/egl/libGLES_android.so\"); #endif return result; case 1: // Android 运行在模拟器中，通过主机系统中实现 OpenGL ES 加速渲染，通过 libGLES_emulation.so 库将 OpenGL ES API 指令重定向到 host 中执行 // Use host-side OpenGL through the \"emulation\" library #if defined(__LP64__) result.appendFormat(\"/system/lib64/egl/lib%s_emulation.so\", kind); #else result.appendFormat(\"/system/lib/egl/lib%s_emulation.so\", kind); #endif return result; default: // Not in emulator, or use other guest-side implementation break; &#125; // 如果不是上面两种情况，就根据库的命名规则去找到厂商实现库文件的绝对路径 String8 pattern; pattern.appendFormat(\"lib%s\", kind); const char* const searchPaths[] = &#123; #if defined(__LP64__) \"/vendor/lib64/egl\", \"/system/lib64/egl\" #else \"/vendor/lib/egl\", \"/system/lib/egl\" #endif &#125;; ...... &#125; &#125; 总结一下上面代码的功能就是，首先判断 Android 是否在 qemu 虚拟机中运行，如果不是，那么就直接去加载厂商存放库的路径中去加载 OpenGL ES 实现库（不管是硬件加速实现的，还是软件模拟实现的）；如果是在 qemu 中运行，那么就要根据返回的 emulationStatus 值 来确定是加软件模拟实现的 OpenGL ES API 库 libGLES_android.so，还是加载 libGLES_emulation.so库将 OpenGL ES 指令重定向到 Host 系统中去执行。 4.3.3、OpenGL ES/EGL 库加载和解析过程正如前面分析，在进行 OpenGL 编程时，最先开始需要获取 Display，这将调用 eglgGetDisplay() 函数被调用。在 eglGetDisplay() 里则会调用 egl_init_drivers() 初始化驱动：装载各个库进行解析，将 OpenGL ES/EGL API 函数接口和具体的实现绑定在一起，并将结果保存在 egl_connection_t 类型的全局变量 gEGLImpl 的结构体的成员变量中。 下面以 SurfaceFlinger 进程init()为例进行分析，整个 OpenGL ES/EGL 库的加载和解析流程如下所示： 这里通过调用 EGL 库的 eglGetDisplay() 获得 Display。 123456789101112[-&gt;\\frameworks\\native\\opengl\\libs\\EGL\\eglApi.cpp]EGLDisplay eglGetDisplay(EGLNativeDisplayType display)&#123; ...... if (egl_init_drivers() == EGL_FALSE) &#123; return setError(EGL_BAD_PARAMETER, EGL_NO_DISPLAY); &#125; EGLDisplay dpy = egl_display_t::getFromNativeDisplay(display); return dpy;&#125; 函数EGLBoolean egl_init_drivers()就是负责OpenGL库的加载。12345678[-&gt;\\frameworks\\native\\opengl\\libs\\EGL\\egl.cpp]EGLBoolean egl_init_drivers() &#123; EGLBoolean res; pthread_mutex_lock(&amp;sInitDriverMutex); res = egl_init_drivers_locked(); pthread_mutex_unlock(&amp;sInitDriverMutex); return res;&#125; 为保证多线程访问的安全性，使用线程锁来放完另一个接口函数egl_init_drivers_locked()123456789101112131415161718192021222324252627282930313233343536[-&gt;\\frameworks\\native\\opengl\\libs\\EGL\\egl.cpp]//在该文件起始位置定义的全局变量egl_connection_t gEGLImpl; // 描述EGL实现内容的结构体对象gl_hooks_t gHooks[2]; // gl_hooks_t 是包含 OpenGL ES API 函数声明对应的函数指针结构体gl_hooks_t gHooksNoContext;pthread_key_t gGLWrapperKey = -1;static EGLBoolean egl_init_drivers_locked() &#123; if (sEarlyInitState) &#123; // initialized by static ctor. should be set here. return EGL_FALSE; &#125; // 得到 Loader 对象单例 // get our driver loader Loader&amp; loader(Loader::getInstance()); // gEGLImple 是一个全局变量，数据类型为 egl_connection_t 结构体类型 // dynamically load our EGL implementation egl_connection_t* cnx = &amp;gEGLImpl; // cnx-&gt;dso 本质上是一个 (void *)类型的指针，它指向的对象是 EGL 共享库打开之后的句柄 if (cnx-&gt;dso == 0) &#123; // &gt;= 将cnx中的 hooks 数组中指向OpenGL ES API 函数指针结构体指的数组成员，用 gHooks 中的成员的地址去初始化 //也就是说 gEGLImpl 中 hook 数组指向 gHooks 数组，最终指向同一个 OpenGL ES API 函数指针的实现 cnx-&gt;hooks[egl_connection_t::GLESv1_INDEX] = &amp;gHooks[egl_connection_t::GLESv1_INDEX]; cnx-&gt;hooks[egl_connection_t::GLESv2_INDEX] = &amp;gHooks[egl_connection_t::GLESv2_INDEX]; // &gt;= 最后通过loader对象的open函数开始加载 OpenGL ES 和 EGL wrapper 库 cnx-&gt;dso = loader.open(cnx); &#125; return cnx-&gt;dso ? EGL_TRUE : EGL_FALSE;&#125; 在这个函数中，有一个非常关键的 egl_connection_t 指针指向一个全局变量 gEGLImpl，当第一次初始化加载 OpenGL ES 实现库和 EGL 实现库时，还需要将 gEGLImpl 中的 hooks 数组中的两个指针指向一个全局的 gl_hooks_t 数组 gHooks（这就是两个指针钩子，最终初始化完成后将分别勾住 OpenGL ES 1.0 和 OpenGL ES 2.0 的实现库），接着调用 Loader 类的实例的 open() 函数完成从 OpenGL ES 实现库中完成符号解析工作。 Loader::open() 函数的代码如下所示： 1234567891011121314151617181920212223242526[/frameworks/native/opengl/libs/EGL/Loader.cpp]// &gt;= Loader 类对象构造完成后，就在 /EGL/egl.cpp 文件中的 egl_init_drivers_locked() 中被调用void* Loader::open(egl_connection_t* cnx)&#123; void* dso; driver_t* hnd = 0; setEmulatorGlesValue(); dso = load_driver(\"GLES\", cnx, EGL | GLESv1_CM | GLESv2); if (dso) &#123; hnd = new driver_t(dso); &#125; else &#123; // Always load EGL first dso = load_driver(\"EGL\", cnx, EGL); if (dso) &#123; hnd = new driver_t(dso); hnd-&gt;set( load_driver(\"GLESv1_CM\", cnx, GLESv1_CM), GLESv1_CM ); hnd-&gt;set( load_driver(\"GLESv2\", cnx, GLESv2), GLESv2 ); &#125; &#125; ...... cnx-&gt;libEgl = load_wrapper(EGL_WRAPPER_DIR \"/libEGL.so\"); cnx-&gt;libGles2 = load_wrapper(EGL_WRAPPER_DIR \"/libGLESv2.so\"); cnx-&gt;libGles1 = load_wrapper(EGL_WRAPPER_DIR \"/libGLESv1_CM.so\"); ...... return (void*)hnd;&#125; open() 函数主要负责 OpenGL ES 库加载前的准备工作，具体的加载细节，则是通过调用 load_driver() 去完成的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[/frameworks/native/opengl/libs/EGL/Loader.cpp]oid *Loader::load_driver(const char* kind, egl_connection_t* cnx, uint32_t mask)&#123; void* dso = nullptr; if (mGetDriverNamespace) &#123; android_namespace_t* ns = mGetDriverNamespace(); if (ns) &#123; dso = load_updated_driver(kind, ns); //加载 OpenGL ES 实现库，放回打开的共享链接库的句柄 &#125; &#125; if (!dso) &#123; dso = load_system_driver(kind); ...... &#125; // 解析 EGL 库，并将wrapper 库 libEGL.so 中的函数 API 指针和具体的实现绑定在一起 if (mask &amp; EGL) &#123; getProcAddress = (getProcAddressType)dlsym(dso, \"eglGetProcAddress\"); ...... egl_t* egl = &amp;cnx-&gt;egl; //将 egl 指针指向描述当前系统支持 OpenGL ES和 EGL 全局变量的 gEGLImpl __eglMustCastToProperFunctionPointerType* curr = (__eglMustCastToProperFunctionPointerType*)egl; char const * const * api = egl_names; //egl_names 是定义在 egl.cpp 文件中的一个数组，数组中的元素是 EGL API 函数指针 while (*api) &#123; char const * name = *api; __eglMustCastToProperFunctionPointerType f = (__eglMustCastToProperFunctionPointerType)dlsym(dso, name); if (f == NULL) &#123; // couldn't find the entry-point, use eglGetProcAddress() f = getProcAddress(name); if (f == NULL) &#123; f = (__eglMustCastToProperFunctionPointerType)0; &#125; &#125; *curr++ = f; //这一步就是最关键的将共享链接库中的 EGL API 的实现和上层调用的 API 函数指针绑定在一起 api++; //指向下一个需要绑定的 api 函数 &#125; &#125; // 解析 OpenGL ES 库中的 OpenGL ES 1.x API 符号 if (mask &amp; GLESv1_CM) &#123; // 调用 init_api 实现 OpenGL API 和对应实现函数的绑定 init_api(dso, gl_names, // gl_names 是定义在 egl.cpp 文件中的一个数组，数组中的元素是 OpenGL ES API 函数指针 (__eglMustCastToProperFunctionPointerType*) &amp;cnx-&gt;hooks[egl_connection_t::GLESv1_INDEX]-&gt;gl, //gl成员变量是一个结构体变量，结构体中的是 OpenGL ES API 函数指针 getProcAddress); &#125; // 解析 OpenGL ES 库中的 OpenGL ES 2.0 API 符号 if (mask &amp; GLESv2) &#123; init_api(dso, gl_names, (__eglMustCastToProperFunctionPointerType*) &amp;cnx-&gt;hooks[egl_connection_t::GLESv2_INDEX]-&gt;gl, getProcAddress); &#125; return dso;&#125; Loader::load_driver() 它主要实现了两个功能： 通过 load_system_driver() 函数查找 OpenGL ES/EGL 实现库，并在指定的存放路径中找到共享链接库文件并打开它。调用 init_api()解析打开的 OpenGL ES/EGL 共享链接库，将 OpenGL ES/EGL API 函数指针和共享链接库中实现的对应的函数符号绑定在一起，这样调用 OpenGL ES/EGL API 就会调用到具体实现的OpenGL ES/EGL 共享链接库中对应函数。 4.4、小结Android OpenGL ES 图形库结构Android 的 OpenGL ES 图形系统涉及多个库，根据设备类型的不同，这些库有着不同的结构。 对于模拟器，没有开启 OpenGL ES 的 GPU 硬件模拟的情况，Android OpenGL ES 图形库结构如下： 当为模拟器开启了 OpenGL ES 的 GPU 硬件模拟，实际的 EGL 和 OpenGL ES 实现库会采用由 android-7.1.1_r22/device/generic/goldfish-opengl 下的源码编译出来的几个库文件，即 libGLESv2_emulation.so、libGLESv1_CM_emulation.so 和 libEGL_emulation.so。此时，OpenGL ES 图形库结构如下： 对于真实的物理 Android 设备，OpenGL ES 图形库结构如下，例如高通实现（libEGL_adreno.solibGLESv1_CM_adreno.so libGLESv2_adreno.so [\\system\\vendor\\lib64\\egl]）： ####（五）、OpenGL ES：EGL接口解析与理解 由前面的分析知道EGL的绘图的一般步骤如下，接下来分析主要的1-8个小步骤： 使用EGL的绘图的一般步骤：1、获取 EGL Display 对象：eglGetDisplay()2、初始化与 EGLDisplay 之间的连接：eglInitialize()3、获取 EGLConfig 对象：eglChooseConfig()4、创建 EGLContext 实例：eglCreateContext()5、创建 EGLSurface 实例：eglCreateWindowSurface()6、连接 EGLContext 和 EGLSurface：eglMakeCurrent()7、使用 OpenGL ES API 绘制图形：gl_*()8、切换 front buffer 和 back buffer 送显：eglSwapBuffer()9、断开并释放与 EGLSurface 关联的 EGLContext 对象：eglRelease()10、删除 EGLSurface 对象11、删除 EGLContext 对象12、终止与 EGLDisplay 之间的连接 标准 EGL 数据类型如下所示： EGLBoolean ——EGL_TRUE =1, EGL_FALSE=0EGLint ——int 数据类型EGLDisplay ——系统显示 ID 或句柄，可以理解为一个前端的显示窗口EGLConfig ——Surface的EGL配置，可以理解为绘制目标framebuffer的配置属性EGLSurface ——系统窗口或 frame buffer 句柄 ，可以理解为一个后端的渲染目标窗口。EGLContext ——OpenGL ES 图形上下文，它代表了OpenGL状态机；如果没有它，OpenGL指令就没有执行的环境。 下面几个类型比较复杂，通过例子可以更深入的理解。这里要说明的是这几个类型在不同平台其实现是不同的，EGL只提供抽象标准。 NativeDisplayType——Native 系统显示类型，标识你所开发设备的物理屏幕NativeWindowType ——Native 系统窗口缓存类型，标识系统窗口NativePixmapType ——Native 系统 frame buffer，可以作为 Framebuffer 的系统图像（内存）数据类型，该类型只用于离屏渲染. 5.1、eglGetDisplay()EGLDisplay 是一个关联系统物理屏幕的通用数据类型，表示显示设备句柄，也可以认为是一个前端显示窗。为了使用系统的显示设备， EGL 提供了 EGLDisplay 数据类型，以及一组操作设备显示的 API 。下面的函数原型用于获取 Native Display ：12345678910111213141516[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]EGLDisplay eglGetDisplay(NativeDisplayType display)&#123;...... if (display == EGL_DEFAULT_DISPLAY) &#123; EGLDisplay dpy = (EGLDisplay)1; egl_display_t&amp; d = egl_display_t::get_display(dpy); d.type = display; return dpy; &#125; return EGL_NO_DISPLAY;&#125;egl_display_t&amp; egl_display_t::get_display(EGLDisplay dpy) &#123; return gDisplays[uintptr_t(dpy)-1U];&#125; 其 中 display 参数是 native 系统的窗口显示 ID 值。如果你只是想得到一个系统默认的 Display ，你可以使用 EGL_DEFAULT_DISPLAY 参数。如果系统中没有一个可用的 native display ID 与给定的 display 参数匹配，函数将返回 EGL_NO_DISPLAY ，而没有任何 Error 状态被设置。 5.2、eglInitialize()每个 EGLDisplay 在使用前都需要初始化。初始化 EGLDisplay 的同时，你可以得到系统中 EGL 的实现版本号。了解当前的版本号在向后兼容性方面是非常有价值的。在移动设备上，通过动态查询 EGL 版本号，你可以为新旧版本的 EGL 附加额外的特性或运行环境。基于平台配置，软件开发可用清楚知道哪些 API 可用访问，这将会为你的代码提供最大限度的可移植性。12345678910111213141516171819[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]EGLBoolean eglInitialize(EGLDisplay dpy, EGLint *major, EGLint *minor)&#123; if (egl_display_t::is_valid(dpy) == EGL_FALSE) return setError(EGL_BAD_DISPLAY, EGL_FALSE); EGLBoolean res = EGL_TRUE; egl_display_t&amp; d = egl_display_t::get_display(dpy); if (d.initialized.fetch_add(1, std::memory_order_acquire) == 0) &#123; ...... &#125; if (res == EGL_TRUE) &#123; if (major != NULL) *major = VERSION_MAJOR; if (minor != NULL) *minor = VERSION_MINOR; &#125; return res;&#125; 其中 dpy 应该是一个有效的 EGLDisplay 。函数返回时， major 和 minor 将被赋予当前 EGL 版本号。比如 EGL1.0 ， major 返回 1 ， minor 则返回 0 。给 major 和 minor 传 NULL 是有效的，如果你不关心版本号。eglQueryString() 函数是另外一个获取版本信息和其他信息的途径。通过 eglQueryString() 获取版本信息需要解析版本字符串，所以通过传递一个指针给 eglInitializ() 函数比较容易获得这个信息。注意在调用 eglQueryString() 必须先使用 eglInitialize() 初始化 EGLDisplay ，否则将得到 EGL_NOT_INITIALIZED 错误信息。 5.3、eglChooseConfig()基 于 EGL 的属性，可以得到一个和需求接近的Config，但也可以选择自己需要的Config，只要平台支持。不是所有的Config都是有效的，也就是不是所有Config都会支持。 eglChooseConfig() 函数将适配一个所期望的配置，并且尽可能接近一个有效的系统配置。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]EGLBoolean eglChooseConfig( EGLDisplay dpy, const EGLint *attrib_list, EGLConfig *configs, EGLint config_size, EGLint *num_config)&#123; if (egl_display_t::is_valid(dpy) == EGL_FALSE) return setError(EGL_BAD_DISPLAY, EGL_FALSE); if (ggl_unlikely(num_config==0)) &#123; return setError(EGL_BAD_PARAMETER, EGL_FALSE); &#125; if (ggl_unlikely(attrib_list==0)) &#123; /* * A NULL attrib_list should be treated as though it was an empty * one (terminated with EGL_NONE) as defined in * section 3.4.1 \"Querying Configurations\" in the EGL specification. */ static const EGLint dummy = EGL_NONE; attrib_list = &amp;dummy; &#125; int numAttributes = 0; int numConfigs = NELEM(gConfigs); uint32_t possibleMatch = (1&lt;&lt;numConfigs)-1; while(possibleMatch &amp;&amp; *attrib_list != EGL_NONE) &#123; numAttributes++; EGLint attr = *attrib_list++; EGLint val = *attrib_list++; for (int i=0 ; possibleMatch &amp;&amp; i&lt;numConfigs ; i++) &#123; if (!(possibleMatch &amp; (1&lt;&lt;i))) continue; if (isAttributeMatching(i, attr, val) == 0) &#123; possibleMatch &amp;= ~(1&lt;&lt;i); &#125; &#125; &#125; // now, handle the attributes which have a useful default value for (size_t j=0 ; possibleMatch &amp;&amp; j&lt;NELEM(config_defaults) ; j++) &#123; // see if this attribute was specified, if not, apply its // default value if (binarySearch&lt;config_pair_t&gt;( (config_pair_t const*)attrib_list, 0, numAttributes-1, config_defaults[j].key) &lt; 0) &#123; for (int i=0 ; possibleMatch &amp;&amp; i&lt;numConfigs ; i++) &#123; if (!(possibleMatch &amp; (1&lt;&lt;i))) continue; if (isAttributeMatching(i, config_defaults[j].key, config_defaults[j].value) == 0) &#123; possibleMatch &amp;= ~(1&lt;&lt;i); &#125; &#125; &#125; &#125; // return the configurations found int n=0; if (possibleMatch) &#123; if (configs) &#123; for (int i=0 ; config_size &amp;&amp; i&lt;numConfigs ; i++) &#123; if (possibleMatch &amp; (1&lt;&lt;i)) &#123; *configs++ = (EGLConfig)(uintptr_t)i; config_size--; n++; &#125; &#125; &#125; else &#123; for (int i=0 ; i&lt;numConfigs ; i++) &#123; if (possibleMatch &amp; (1&lt;&lt;i)) &#123; n++; &#125; &#125; &#125; &#125; *num_config = n; return EGL_TRUE;&#125; 参数 attrib_list 指定了选择配置时需要参照的属性。参数 configs 将返回一个按照 attrib_list 排序的平台有效的所有 EGL framebuffer 配置列表。参数 config_size 指定了可以返回到 configs 的总配置个数。参数 num_config 返回了实际匹配的配置总数。 5.4、eglCreateContext()OpenGL ES的pipeline从程序的角度看就是一个状态机，有当前的颜色、纹理坐标、变换矩阵、绚染模式等一大堆状态，这些状态作用于OpenGL API程序提交的顶点坐标等图元从而形成帧缓冲内的像素。在OpenGL的编程接口中，Context就代表这个状态机，OpenGL API程序的主要工作就是向Context提供图元、设置状态，偶尔也从Context里获取一些信息。123456789101112131415161718[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]EGLContext eglCreateContext(EGLDisplay dpy, EGLConfig config, EGLContext /*share_list*/, const EGLint* /*attrib_list*/)&#123; if (egl_display_t::is_valid(dpy) == EGL_FALSE) return setError(EGL_BAD_DISPLAY, EGL_NO_SURFACE); ogles_context_t* gl = ogles_init(sizeof(egl_context_t)); if (!gl) return setError(EGL_BAD_ALLOC, EGL_NO_CONTEXT); egl_context_t* c = static_cast&lt;egl_context_t*&gt;(gl-&gt;rasterizer.base); c-&gt;flags = egl_context_t::NEVER_CURRENT; c-&gt;dpy = dpy; c-&gt;config = config; c-&gt;read = 0; c-&gt;draw = 0; return (EGLContext)gl;&#125; 5.5、eglCreateWindowSurface()Surface实际上就是一个FrameBuffer，也就是渲染目的地， 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]EGLSurface eglCreateWindowSurface( EGLDisplay dpy, EGLConfig config, NativeWindowType window, const EGLint *attrib_list)&#123; return createWindowSurface(dpy, config, window, attrib_list);&#125;static EGLSurface createWindowSurface(EGLDisplay dpy, EGLConfig config, NativeWindowType window, const EGLint* /*attrib_list*/)&#123; if (egl_display_t::is_valid(dpy) == EGL_FALSE) return setError(EGL_BAD_DISPLAY, EGL_NO_SURFACE); if (window == 0) return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); EGLint surfaceType; if (getConfigAttrib(dpy, config, EGL_SURFACE_TYPE, &amp;surfaceType) == EGL_FALSE) return EGL_FALSE; if (!(surfaceType &amp; EGL_WINDOW_BIT)) return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); if (static_cast&lt;ANativeWindow*&gt;(window)-&gt;common.magic != ANDROID_NATIVE_WINDOW_MAGIC) &#123; return setError(EGL_BAD_NATIVE_WINDOW, EGL_NO_SURFACE); &#125; EGLint configID; if (getConfigAttrib(dpy, config, EGL_CONFIG_ID, &amp;configID) == EGL_FALSE) return EGL_FALSE; int32_t depthFormat; int32_t pixelFormat; if (getConfigFormatInfo(configID, pixelFormat, depthFormat) != NO_ERROR) &#123; return setError(EGL_BAD_MATCH, EGL_NO_SURFACE); &#125; ...... egl_surface_t* surface; surface = new egl_window_surface_v2_t(dpy, config, depthFormat, static_cast&lt;ANativeWindow*&gt;(window)); if (!surface-&gt;initCheck()) &#123; delete surface; surface = 0; &#125; return surface;&#125; 来创建一个可实际显示的Surface。 系统通常还支持另外两种Surface：PixmapSurface和PBufferSurface，这两种都不是可显示的Surface，PixmapSurface是保存在系统内存中的位图，PBuffer则是保存在显存中的帧。 对于这两种surface，Android系统中，支持PBufferSurface。 5.6、eglMakeCurrent()该接口将申请到的display，draw（surface）和 context进行了绑定。也就是说，在context下的OpenGLAPI指令将draw（surface）作为其渲染最终目的地。而display作为draw（surface）的前端显示。调用后，当前线程使用的EGLContex为context。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]EGLBoolean eglMakeCurrent( EGLDisplay dpy, EGLSurface draw, EGLSurface read, EGLContext ctx)&#123; if (egl_display_t::is_valid(dpy) == EGL_FALSE) return setError(EGL_BAD_DISPLAY, EGL_FALSE); if (draw) &#123; egl_surface_t* s = (egl_surface_t*)draw; if (!s-&gt;isValid()) return setError(EGL_BAD_SURFACE, EGL_FALSE); if (s-&gt;dpy != dpy) return setError(EGL_BAD_DISPLAY, EGL_FALSE); // TODO: check that draw is compatible with the context &#125; if (read &amp;&amp; read!=draw) &#123; egl_surface_t* s = (egl_surface_t*)read; if (!s-&gt;isValid()) return setError(EGL_BAD_SURFACE, EGL_FALSE); if (s-&gt;dpy != dpy) return setError(EGL_BAD_DISPLAY, EGL_FALSE); // TODO: check that read is compatible with the context &#125; EGLContext current_ctx = EGL_NO_CONTEXT; if ((read == EGL_NO_SURFACE &amp;&amp; draw == EGL_NO_SURFACE) &amp;&amp; (ctx != EGL_NO_CONTEXT)) return setError(EGL_BAD_MATCH, EGL_FALSE); if ((read != EGL_NO_SURFACE || draw != EGL_NO_SURFACE) &amp;&amp; (ctx == EGL_NO_CONTEXT)) return setError(EGL_BAD_MATCH, EGL_FALSE); if (ctx == EGL_NO_CONTEXT) &#123; // if we're detaching, we need the current context current_ctx = (EGLContext)getGlThreadSpecific(); &#125; else &#123; egl_context_t* c = egl_context_t::context(ctx); egl_surface_t* d = (egl_surface_t*)draw; egl_surface_t* r = (egl_surface_t*)read; if ((d &amp;&amp; d-&gt;ctx &amp;&amp; d-&gt;ctx != ctx) || (r &amp;&amp; r-&gt;ctx &amp;&amp; r-&gt;ctx != ctx)) &#123; // one of the surface is bound to a context in another thread return setError(EGL_BAD_ACCESS, EGL_FALSE); &#125; &#125; ogles_context_t* gl = (ogles_context_t*)ctx; if (makeCurrent(gl) == 0) &#123; if (ctx) &#123; egl_context_t* c = egl_context_t::context(ctx); egl_surface_t* d = (egl_surface_t*)draw; egl_surface_t* r = (egl_surface_t*)read; if (c-&gt;draw) &#123; egl_surface_t* s = reinterpret_cast&lt;egl_surface_t*&gt;(c-&gt;draw); s-&gt;disconnect(); s-&gt;ctx = EGL_NO_CONTEXT; if (s-&gt;zombie) delete s; &#125; if (c-&gt;read) &#123; // FIXME: unlock/disconnect the read surface too &#125; c-&gt;draw = draw; c-&gt;read = read; if (c-&gt;flags &amp; egl_context_t::NEVER_CURRENT) &#123; c-&gt;flags &amp;= ~egl_context_t::NEVER_CURRENT; GLint w = 0; GLint h = 0; if (draw) &#123; w = d-&gt;getWidth(); h = d-&gt;getHeight(); &#125; ogles_surfaceport(gl, 0, 0); ogles_viewport(gl, 0, 0, w, h); ogles_scissor(gl, 0, 0, w, h); &#125; if (d) &#123; if (d-&gt;connect() == EGL_FALSE) &#123; return EGL_FALSE; &#125; d-&gt;ctx = ctx; d-&gt;bindDrawSurface(gl); &#125; if (r) &#123; // FIXME: lock/connect the read surface too r-&gt;ctx = ctx; r-&gt;bindReadSurface(gl); &#125; &#125; else &#123; // if surfaces were bound to the context bound to this thread // mark then as unbound. if (current_ctx) &#123; egl_context_t* c = egl_context_t::context(current_ctx); egl_surface_t* d = (egl_surface_t*)c-&gt;draw; egl_surface_t* r = (egl_surface_t*)c-&gt;read; if (d) &#123; c-&gt;draw = 0; d-&gt;disconnect(); d-&gt;ctx = EGL_NO_CONTEXT; if (d-&gt;zombie) delete d; &#125; if (r) &#123; c-&gt;read = 0; r-&gt;ctx = EGL_NO_CONTEXT; // FIXME: unlock/disconnect the read surface too &#125; &#125; &#125; return EGL_TRUE; &#125; return setError(EGL_BAD_ACCESS, EGL_FALSE);&#125; 5.7、绘制gl_*()应用程序通过OpenGL API进行绘制，一帧完成之后，调用eglSwapBuffers(EGLDisplay dpy, EGLContext ctx)来显示。 5.8、eglSwapBuffers接口实现说明Android平台： 为了实现eglSwapBuffers， eglSurface其实代表了一个从NativeWindow 申请到的一个Buffer（Dequeue操作）。当调用eglSwapBuffers时，对于一般应用窗口而言，NativeWindow将该Surface的Buffer 提交回去给SurfaceFlinger（Queue操作)， 12345678[-&gt;\\frameworks\\native\\opengl\\libagl\\egl.cpp]EGLBoolean egl_window_surface_v2_t::swapBuffers()&#123;......nativeWindow-&gt;queueBuffer(nativeWindow, buffer); nativeWindow-&gt;dequeueBuffer(nativeWindow, &amp;buffer);......&#125; 然后又重新从NativeWindow中重新Dequeue出来一个新的Buffer给eglSurface。而eglDisplay并不代表实际的意义。我们只是从接口上感觉是，surface和display进行了交换。（注：现在是Triple Buffer） 总结：从前面关于Android EGL、OpenGL ES的分析知道，现在我们可以通过SurfaceFlinger申请一块Surface（Buffer），然后可以利用OpenGL ES接口在Native 层绘制相关的图片、文字；那么疑问来了，Android上层绚丽多彩的App界面是如何绘制而成的呢、App层如何通过底层的OpenGL ES接口来完成绘制呢？？？ （六）、参考资料(特别感谢各位前辈的分析和图示)：Khronos GroupAndroid图形架构 官方文档OPENGL ES 2.0 知识串讲OpenGL ES EGL Spec&amp;APIsUnderstaing-Android-EglAndroid 系统图形栈(1) &amp;&amp;(2)： OpenGL ES 和 EGLAndroid L 的开机动画流程 - CSDN博客","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Camera System（2）：Camera System(Camera 系统)startPreview、takePicture、Recorder流程分析","slug":"Android Camera System（2）：Camera System[Camera 系统]startPreview、takePicture、Recorder流程分析","date":"2018-07-09T16:00:00.000Z","updated":"2018-05-25T12:12:49.620Z","comments":true,"path":"2018/07/10/Android Camera System（2）：Camera System[Camera 系统]startPreview、takePicture、Recorder流程分析/","link":"","permalink":"http://zhoujinjian.cc/2018/07/10/Android Camera System（2）：Camera System[Camera 系统]startPreview、takePicture、Recorder流程分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】【特别感谢 - Android Camera fw学习-Armwind】【特别感谢 - Android Camera API2分析-Gzzaigcnforever】【特别感谢 - Android Camera 流程学习记录 Android 7.12-QQ_16775897】【特别感谢 - 专栏：古冥的android6.0下的Camera API2.0的源码分析之旅】Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) 🌀🌀：专注于Linux &amp;&amp; Android Multimedia（Camera、Video、Audio、Display）系统分析与研究 ☯ Application：☯ /packages/apps/Camera2/src/com/android/camera/ ☯ Framework：☯ /frameworks/base/core/java/android/hardware/Camera.java ☯ JNI:☯ /frameworks/base/core/jni/android_hardware_Camera.cpp ☯ Native:☯ Client：frameworks/av/camera/CameraBase.cppframeworks/av/camera/Camera.cppframeworks/av/camera/ICamera.cppframeworks/av/camera/aidl/android/hardware/ICamera.aidlframeworks/av/camera/aidl/android/hardware/ICameraClient.aidl☯ Server：frameworks/av/camera/cameraserver/main_cameraserver.cppframeworks/av/services/camera/libcameraservice/CameraService.cppframeworks/av/services/camera/libcameraservice/api1/CameraClient.cppframeworks/av/camera/aidl/android/hardware/ICameraService.aidl ☯ HAL：☯ /frameworks/av/services/camera/libcameraservice/device3/☯ /hardware/qcom/camera/QCamera2(高通HAL)☯ /vendor/qcom/proprietary/mm-camera(高通mm-camera)☯ /vendor/qcom/proprietary/mm-still(高通JPEG) ☯ Kernel：☯ /kernel/drivers/media/platform/msm/camera_v2(高通V4L2)☯ /kernel/arch/arm/boot/dts/(高通dts) （一）、Camera System startPreview流程分析1.1、Camera2 startPreview的应用层(Java)流程分析preview流程都是从startPreview开始的，所以来看startPreview方法的代码：1234567[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]Overridepublic void startPreview(Surface previewSurface, CaptureReadyCallback listener) &#123; mPreviewSurface = previewSurface; //根据Surface以及CaptureReadyCallback回调来建立preview环境 setupAsync(mPreviewSurface, listener);&#125; 这其中有一个比较重要的回调CaptureReadyCallback，先分析setupAsync方法：12345678910[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]private void setupAsync(final Surface previewSurface, final CaptureReadyCallback listener) &#123; mCameraHandler.post(new Runnable() &#123; @Override public void run() &#123; //建立preview环境 setup(previewSurface, listener); &#125; &#125;);&#125; 这里通过CameraHandler来post一个Runnable对象，它只会调用Runnable的run方法，它仍然属于UI线程，并没有创建新的线程。所以，继续分析setup方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]private void setup(Surface previewSurface, final CaptureReadyCallback listener) &#123; try &#123; if (mCaptureSession != null) &#123; mCaptureSession.abortCaptures(); mCaptureSession = null; &#125; List&lt;Surface&gt; outputSurfaces = new ArrayList&lt;Surface&gt;(2); outputSurfaces.add(previewSurface); outputSurfaces.add(mCaptureImageReader.getSurface()); //创建CaptureSession会话来与Camera Device发送Preview请求 mDevice.createCaptureSession(outputSurfaces, new CameraCaptureSession.StateCallback() &#123; @Override public void onConfigureFailed(CameraCaptureSession session) &#123; //如果配置失败，则回调CaptureReadyCallback的onSetupFailed方法 listener.onSetupFailed(); &#125; @Override public void onConfigured(CameraCaptureSession session) &#123; mCaptureSession = session; mAFRegions = ZERO_WEIGHT_3A_REGION; mAERegions = ZERO_WEIGHT_3A_REGION; mZoomValue = 1f; mCropRegion = cropRegionForZoom(mZoomValue); //调用repeatingPreview来启动preview boolean success = repeatingPreview(null); if (success) &#123; //若启动成功，则回调CaptureReadyCallback的onReadyForCapture，表示准备拍照成功 listener.onReadyForCapture(); &#125; else &#123; //若启动失败，则回调CaptureReadyCallback的onSetupFailed，表示preview建立失败 listener.onSetupFailed(); &#125; &#125; @Override public void onClosed(CameraCaptureSession session) &#123; super.onClosed(session); &#125; &#125;, mCameraHandler); &#125; catch (CameraAccessException ex) &#123; Log.e(TAG, \"Could not set up capture session\", ex); listener.onSetupFailed(); &#125;&#125; 首先，调用Device的createCaptureSession方法来创建一个会话，并定义了会话的状态回调CameraCaptureSession.StateCallback()，其中，当会话创建成功，则会回调onConfigured()方法,在其中，首先调用repeatingPreview来启动preview，然后处理preview的结果并调用先前定义的CaptureReadyCallback来通知用户进行Capture操作。先分析repeatingPreview方法： 1234567891011121314151617181920[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]private boolean repeatingPreview(Object tag) &#123; try &#123; //通过CameraDevice对象创建一个CaptureRequest的preview请求 CaptureRequest.Builder builder = mDevice.createCaptureRequest( CameraDevice.TEMPLATE_PREVIEW); //添加预览的目标Surface builder.addTarget(mPreviewSurface); //设置预览模式 builder.set(CaptureRequest.CONTROL_MODE, CameraMetadata.CONTROL_MODE_AUTO); addBaselineCaptureKeysToRequest(builder); //利用会话发送请求，mCaptureCallback为 mCaptureSession.setRepeatingRequest(builder.build(), mCaptureCallback,mCameraHandler); Log.v(TAG, String.format(\"Sent repeating Preview request, zoom = %.2f\", mZoomValue)); return true; &#125; catch (CameraAccessException ex) &#123; Log.e(TAG, \"Could not access camera setting up preview.\", ex); return false; &#125;&#125; 首先调用CameraDeviceImpl的createCaptureRequest方法创建类型为TEMPLATE_PREVIEW 的CaptureRequest，然后调用CameraCaptureSessionImpl的setRepeatingRequest方法将此请求发送出去： 1234567891011121314151617[-&gt;/frameworks/base/core/java/android/hardware/camera2/impl/CameraCaptureSessionImpl.java]Overridepublic synchronized int setRepeatingRequest(CaptureRequest request, CaptureCallback callback, Handler handler) throws CameraAccessException &#123; if (request == null) &#123; throw new IllegalArgumentException(&quot;request must not be null&quot;); &#125; else if (request.isReprocess()) &#123; throw new IllegalArgumentException(&quot;repeating reprocess requests are not supported&quot;); &#125; checkNotClosed(); handler = checkHandler(handler, callback); ... //将此请求添加到待处理的序列里 return addPendingSequence(mDeviceImpl.setRepeatingRequest(request,createCaptureCallbackProxy( handler, callback), mDeviceHandler));&#125; 至此应用层的preview的请求流程分析结束，继续分析其结果处理，如果preview开启成功，则会回调CaptureReadyCallback的onReadyForCapture方法，现在分析CaptureReadyCallback回调： 12345678910111213141516171819202122232425262728293031323334353637383940[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]new CaptureReadyCallback() &#123; @Override public void onSetupFailed() &#123; mCameraOpenCloseLock.release(); Log.e(TAG, \"Could not set up preview.\"); mMainThread.execute(new Runnable() &#123; @Override public void run() &#123; if (mCamera == null) &#123; Log.d(TAG, \"Camera closed, aborting.\"); return; &#125; mCamera.close(); mCamera = null; &#125; &#125;); &#125; @Override public void onReadyForCapture() &#123; mCameraOpenCloseLock.release(); mMainThread.execute(new Runnable() &#123; @Override public void run() &#123; Log.d(TAG, \"Ready for capture.\"); if (mCamera == null) &#123; Log.d(TAG, \"Camera closed, aborting.\"); return; &#125; // onPreviewStarted(); onReadyStateChanged(true); mCamera.setReadyStateChangedListener(CaptureModule.this); mUI.initializeZoom(mCamera.getMaxZoom()); mCamera.setFocusStateListener(CaptureModule.this); &#125; &#125;); &#125;&#125; 根据前面的分析，预览成功后会回调onReadyForCapture方法，它主要是通知主线程的状态改变，并设置Camera的ReadyStateChangedListener的监听，其回调方法如下： 12345678[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]Overridepublic void onReadyStateChanged(boolean readyForCapture) &#123; if (readyForCapture) &#123; mAppController.getCameraAppUI().enableModeOptions(); &#125; mAppController.setShutterEnabled(readyForCapture);&#125; 如代码所示，当其状态变成准备好拍照，则将会调用CameraActivity的setShutterEnabled方法，即使能快门按键，此时也就是说预览成功结束，可以按快门进行拍照了，所以，到这里，应用层的preview的流程基本分析完毕，下图是应用层的关键调用的流程时序图： 1.2、Camera2 startPreview的Native层流程分析分析Preview的Native的代码真是费了九牛二虎之力，若有分析不正确之处，请各位大神指正，在第一小节的后段最后会调用CameraDeviceImpl的setRepeatingRequest方法来提交请求，而在android6.0源码分析之Camera API2.0简介中，分析了Camera2框架Java IPC通信使用了CameraDeviceUser来进行通信，所以看Native层的ICameraDeviceUser的onTransact方法来处理请求的提交： 12345678910111213141516171819202122232425262728293031[-&gt;/frameworks/av/camera/aidl/android/hardware/camera2/ICameraDeviceUser.aidl]status_t BnCameraDeviceUser::onTransact(uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; switch(code) &#123; … //请求提交 case SUBMIT_REQUEST: &#123; CHECK_INTERFACE(ICameraDeviceUser, data, reply); // arg0 = request sp&lt;CaptureRequest&gt; request; if (data.readInt32() != 0) &#123; request = new CaptureRequest(); request-&gt;readFromParcel(const_cast&lt;Parcel*&gt;(&amp;data)); &#125; // arg1 = streaming (bool) bool repeating = data.readInt32(); // return code: requestId (int32) reply-&gt;writeNoException(); int64_t lastFrameNumber = -1; //将实现BnCameraDeviceUser的对下岗的submitRequest方法代码写入Binder reply-&gt;writeInt32(submitRequest(request, repeating, &amp;lastFrameNumber)); reply-&gt;writeInt32(1); reply-&gt;writeInt64(lastFrameNumber); return NO_ERROR; &#125; break; ...&#125; CameraDeviceClientBase继承了BnCameraDeviceUser类，所以CameraDeviceClientBase相当于IPC Binder中的client，所以会调用其submitRequest方法，此处，至于IPC Binder通信原理不做分析，其参照其它资料： 1234567[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\api2\\CameraDeviceClient.cpp]status_t CameraDeviceClient::submitRequest(sp&lt;CaptureRequest&gt; request,bool streaming, /*out*/int64_t* lastFrameNumber) &#123; List&lt;sp&lt;CaptureRequest&gt; &gt; requestList; requestList.push_back(request); return submitRequestList(requestList, streaming, lastFrameNumber);&#125; 简单的调用，继续分析submitRequestList： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\api2\\CameraDeviceClient.cpp]status_t CameraDeviceClient::submitRequestList(List&lt;sp&lt;CaptureRequest&gt; &gt; requests,bool streaming, int64_t* lastFrameNumber) &#123; ... //Metadata链表 List&lt;const CameraMetadata&gt; metadataRequestList; ... for (List&lt;sp&lt;CaptureRequest&gt; &gt;::iterator it = requests.begin(); it != requests.end(); ++it) &#123; sp&lt;CaptureRequest&gt; request = *it; ... //初始化Metadata数据 CameraMetadata metadata(request-&gt;mMetadata); ... //设置Stream的容量 Vector&lt;int32_t&gt; outputStreamIds; outputStreamIds.setCapacity(request-&gt;mSurfaceList.size()); //循环初始化Surface for (size_t i = 0; i &lt; request-&gt;mSurfaceList.size(); ++i) &#123; sp&lt;Surface&gt; surface = request-&gt;mSurfaceList[i]; if (surface == 0) continue; sp&lt;IGraphicBufferProducer&gt; gbp = surface-&gt;getIGraphicBufferProducer(); int idx = mStreamMap.indexOfKey(IInterface::asBinder(gbp)); ... int streamId = mStreamMap.valueAt(idx); outputStreamIds.push_back(streamId); &#125; //更新数据 metadata.update(ANDROID_REQUEST_OUTPUT_STREAMS, &amp;outputStreamIds[0], outputStreamIds.size()); if (request-&gt;mIsReprocess) &#123; metadata.update(ANDROID_REQUEST_INPUT_STREAMS, &amp;mInputStream.id, 1); &#125; metadata.update(ANDROID_REQUEST_ID, &amp;requestId, /*size*/1); loopCounter++; // loopCounter starts from 1 //压栈 metadataRequestList.push_back(metadata); &#125; mRequestIdCounter++; if (streaming) &#123; //预览会走此条通道 res = mDevice-&gt;setStreamingRequestList(metadataRequestList, lastFrameNumber); if (res != OK) &#123; ... &#125; else &#123; mStreamingRequestList.push_back(requestId); &#125; &#125; else &#123; //Capture等走此条通道 res = mDevice-&gt;captureList(metadataRequestList, lastFrameNumber); if (res != OK) &#123; ... &#125; &#125; if (res == OK) &#123; return requestId; &#125; return res;&#125; setStreamingRequestList和captureList方法都调用了submitRequestsHelper方法，只是他们的repeating参数一个ture,一个为false，而本节分析的preview调用的是setStreamingRequestList方法，并且API2.0下Device的实现为Camera3Device，所以看它的submitRequestsHelper实现： 123456789101112131415161718[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\device3\\Camera3Device.cpp]status_t Camera3Device::submitRequestsHelper(const List&lt;const CameraMetadata&gt; &amp;requests, bool repeating,/*out*/int64_t *lastFrameNumber) &#123; ... RequestList requestList; //在这里面会进行CaptureRequest的创建，并调用configureStreamLocked进行stream的配置，主要是设置了一个回调captureResultCb，即后面要分析的重要的回调 res = convertMetadataListToRequestListLocked(requests, /*out*/&amp;requestList); ... if (repeating) &#123; //眼熟不，这个方法名和应用层中CameraDevice的setRepeatingRequests一样 res = mRequestThread-&gt;setRepeatingRequests(requestList, lastFrameNumber); &#125; else &#123; //不需重复，即repeating为false时，调用此方法来讲请求提交 res = mRequestThread-&gt;queueRequestList(requestList, lastFrameNumber); &#125; ... return res;&#125; 从代码可知，在Camera3Device里创建了要给RequestThread线程，调用它的setRepeatingRequests或者queueRequestList方法来将应用层发送过来的Request提交，继续看setRepeatingRequests方法： 1234567891011121314151617[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\device3\\Camera3Device.cpp]status_t Camera3Device::RequestThread::setRepeatingRequests(const RequestList &amp;requests, /*out*/int64_t *lastFrameNumber) &#123; Mutex::Autolock l(mRequestLock); if (lastFrameNumber != NULL) &#123; *lastFrameNumber = mRepeatingLastFrameNumber; &#125; mRepeatingRequests.clear(); //将其插入mRepeatingRequest链表 mRepeatingRequests.insert(mRepeatingRequests.begin(), requests.begin(), requests.end()); unpauseForNewRequests(); mRepeatingLastFrameNumber = NO_IN_FLIGHT_REPEATING_FRAMES; return OK;&#125; 至此，Native层的preview过程基本分析结束，下面的工作将会交给Camera HAL层来处理，先给出Native层的调用时序图： 1.3、Camera2 startPreview的HAL层流程分析本节将不再对Camera的HAL层的初始化以及相关配置进行分析，只对preview等相关流程中的frame metadata的处理流程进行分析，具体的CameraHAL分析请参考前一篇分析，在第二小节的submitRequestsHelper方法中调用convertMetadataListToRequestListLocked的时候会进行CaptureRequest的创建，并调用configureStreamLocked进行stream的配置，主要是设置了一个回调captureResultCb，所以Native层在request提交后，会回调此captureResultCb方法，首先分析captureResultCb： 123456789101112131415161718192021[-&gt;/hardware/qcom/camera/QCamera2/HAL3/QCamera3HWI.cpp]void QCamera3HardwareInterface::captureResultCb(mm_camera_super_buf_t *metadata_buf, camera3_stream_buffer_t *buffer, uint32_t frame_number)&#123; if (metadata_buf) &#123; if (mBatchSize) &#123; //批处理模式，但代码也是循环调用handleMetadataWithLock方法 handleBatchMetadata(metadata_buf, true /* free_and_bufdone_meta_buf */); &#125; else &#123; /* mBatchSize = 0 */ pthread_mutex_lock(&amp;mMutex); //处理元数据 handleMetadataWithLock(metadata_buf, true /* free_and_bufdone_meta_buf */); pthread_mutex_unlock(&amp;mMutex); &#125; &#125; else &#123; pthread_mutex_lock(&amp;mMutex); handleBufferWithLock(buffer, frame_number); pthread_mutex_unlock(&amp;mMutex); &#125; return;&#125; 一种是通过循环来进行元数据的批处理，另一种是直接进行元数据的处理，但是批处理最终也是循环调用handleMetadataWithLock来处理： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[-&gt;/hardware/qcom/camera/QCamera2/HAL3/QCamera3HWI.cpp]void QCamera3HardwareInterface::handleMetadataWithLock(mm_camera_super_buf_t *metadata_buf, bool free_and_bufdone_meta_buf)&#123; ... //Partial result on process_capture_result for timestamp if (urgent_frame_number_valid) &#123; ... for (List&lt;PendingRequestInfo&gt;::iterator i =mPendingRequestsList.begin(); i != mPendingRequestsList.end(); i++) &#123; ... if (i-&gt;frame_number == urgent_frame_number &amp;&amp;i-&gt;bUrgentReceived == 0) &#123; camera3_capture_result_t result; memset(&amp;result, 0, sizeof(camera3_capture_result_t)); i-&gt;partial_result_cnt++; i-&gt;bUrgentReceived = 1; //提取3A数据 result.result =translateCbUrgentMetadataToResultMetadata(metadata); ... //对Capture Result进行处理 mCallbackOps-&gt;process_capture_result(mCallbackOps, &amp;result); //释放camera_metadata_t free_camera_metadata((camera_metadata_t *)result.result); break; &#125; &#125; &#125; ... for (List&lt;PendingRequestInfo&gt;::iterator i = mPendingRequestsList.begin(); i != mPendingRequestsList.end() &amp;&amp; i-&gt;frame_number &lt;= frame_number;) &#123; camera3_capture_result_t result; memset(&amp;result, 0, sizeof(camera3_capture_result_t)); ... if (i-&gt;frame_number &lt; frame_number) &#123; //清空数据结构 camera3_notify_msg_t notify_msg; memset(&amp;notify_msg, 0, sizeof(camera3_notify_msg_t)); //定义消息类型 notify_msg.type = CAMERA3_MSG_SHUTTER; notify_msg.message.shutter.frame_number = i-&gt;frame_number; notify_msg.message.shutter.timestamp = (uint64_t)capture_time (urgent_frame_number - i-&gt;frame_number) * NSEC_PER_33MSEC; //调用回调通知应用层发生CAMERA3_MSG_SHUTTER消息 mCallbackOps-&gt;notify(mCallbackOps, &amp;notify_msg); ... CameraMetadata dummyMetadata; //更新元数据 dummyMetadata.update(ANDROID_SENSOR_TIMESTAMP, &amp;i-&gt;timestamp, 1); dummyMetadata.update(ANDROID_REQUEST_ID, &amp;(i-&gt;request_id), 1); //得到元数据释放结果 result.result = dummyMetadata.release(); &#125; else &#123; camera3_notify_msg_t notify_msg; memset(&amp;notify_msg, 0, sizeof(camera3_notify_msg_t)); // Send shutter notify to frameworks notify_msg.type = CAMERA3_MSG_SHUTTER; ... //从HAL中获得Metadata result.result = translateFromHalMetadata(metadata, i-&gt;timestamp, i-&gt;request_id, i-&gt;jpegMetadata, i-&gt;pipeline_depth, i-&gt;capture_intent); saveExifParams(metadata); if (i-&gt;blob_request) &#123; ... if (enabled &amp;&amp; metadata-&gt;is_tuning_params_valid) &#123; //将Metadata复制到文件 dumpMetadataToFile(metadata-&gt;tuning_params, mMetaFrameCount, enabled, \"Snapshot\",frame_number); &#125; mPictureChannel-&gt;queueReprocMetadata(metadata_buf); &#125; else &#123; // Return metadata buffer if (free_and_bufdone_meta_buf) &#123; mMetadataChannel-&gt;bufDone(metadata_buf); free(metadata_buf); &#125; &#125; &#125; ... &#125;&#125; 其中，首先会调用回调的process_capture_result方法来对Capture Result进行处理，然后会调用回调的notify方法来发送一个CAMERA3_MSG_SHUTTER消息，而process_capture_result所对应的实现其实就是Camera3Device的processCaptureResult方法，先分析processCaptureResult： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\device3\\Camera3Device.cpp]void Camera3Device::processCaptureResult(const camera3_capture_result *result) &#123; ... //对于HAL3.2+,如果HAL不支持partial，当metadata被包含在result中时，它必须将partial_result设置为1 ... &#123; Mutex::Autolock l(mInFlightLock); ssize_t idx = mInFlightMap.indexOfKey(frameNumber); ... InFlightRequest &amp;request = mInFlightMap.editValueAt(idx); if (result-&gt;partial_result != 0) request.resultExtras.partialResultCount = result-&gt;partial_result; // 检查结果是否只有partial metadata if (mUsePartialResult &amp;&amp; result-&gt;result != NULL) &#123; if (mDeviceVersion &gt;= CAMERA_DEVICE_API_VERSION_3_2) &#123;//HAL版本高于3.2 if (result-&gt;partial_result &gt; mNumPartialResults || result-&gt;partial_result &lt; 1) &#123; //Log显示错误 return; &#125; isPartialResult = (result-&gt;partial_result &lt; mNumPartialResults); if (isPartialResult) &#123; //将结果加入到请求的结果集中 request.partialResult.collectedResult.append(result-&gt;result); &#125; &#125; else &#123;//低于3.2 ... &#125; if (isPartialResult) &#123; // Fire off a 3A-only result if possible if (!request.partialResult.haveSent3A) &#123; request.partialResult.haveSent3A =processPartial3AResult(frameNumber, request.partialResult.collectedResult,request.resultExtras); &#125; &#125; &#125; ... if (result-&gt;result != NULL &amp;&amp; !isPartialResult) &#123; if (shutterTimestamp == 0) &#123; request.pendingMetadata = result-&gt;result; request.partialResult.collectedResult = collectedPartialResult; &#125; else &#123; CameraMetadata metadata; metadata = result-&gt;result; //发送Capture Result sendCaptureResult(metadata, request.resultExtras, collectedPartialResult, frameNumber, hasInputBufferInRequest,request.aeTriggerCancelOverride); &#125; &#125; //结果处理好了，将请求移除 removeInFlightRequestIfReadyLocked(idx); &#125; // scope for mInFlightLock ...&#125; 由代码可知，它会处理局部的或者全部的metadata数据，最后如果result不为空，且得到的是请求处理的全部数据，则会调用sendCaptureResult方法来将请求结果发送出去： 12345678910111213141516171819202122232425262728293031323334353637[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\device3\\Camera3Device.cpp]void Camera3Device::sendCaptureResult(CameraMetadata &amp;pendingMetadata,CaptureResultExtras &amp;resultExtras,CameraMetadata &amp;collectedPartialResult,uint32_t frameNumber,bool reprocess, const AeTriggerCancelOverride_t &amp;aeTriggerCancelOverride) &#123; if (pendingMetadata.isEmpty())//如果数据为空，直接返回 return; ... CaptureResult captureResult; captureResult.mResultExtras = resultExtras; captureResult.mMetadata = pendingMetadata; //更新metadata if (captureResult.mMetadata.update(ANDROID_REQUEST_FRAME_COUNT(int32_t*)&amp;frameNumber, 1) != OK) &#123; SET_ERR(\"Failed to set frame# in metadata (%d)\",frameNumber); return; &#125; else &#123; ... &#125; // Append any previous partials to form a complete result if (mUsePartialResult &amp;&amp; !collectedPartialResult.isEmpty()) &#123; captureResult.mMetadata.append(collectedPartialResult); &#125; //排序 captureResult.mMetadata.sort(); // Check that there's a timestamp in the result metadata camera_metadata_entry entry = captureResult.mMetadata.find(ANDROID_SENSOR_TIMESTAMP); ... overrideResultForPrecaptureCancel(&amp;captureResult.mMetadata, aeTriggerCancelOverride); // 有效的结果，将其插入Buffer List&lt;CaptureResult&gt;::iterator queuedResult =mResultQueue.insert(mResultQueue.end(), CaptureResult(captureResult)); ... mResultSignal.signal();&#125; 最后，它将Capture Result插入了结果队列，并释放了结果的信号量，所以到这里，Capture Result处理成功，下面分析前面的notify发送CAMERA3_MSG_SHUTTER消息： 1234567891011121314151617181920212223[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\device3\\Camera3Device.cpp]void Camera3Device::notify(const camera3_notify_msg *msg) &#123; NotificationListener *listener; &#123; Mutex::Autolock l(mOutputLock); listener = mListener; &#125; ... switch (msg-&gt;type) &#123; case CAMERA3_MSG_ERROR: &#123; notifyError(msg-&gt;message.error, listener); break; &#125; case CAMERA3_MSG_SHUTTER: &#123; notifyShutter(msg-&gt;message.shutter, listener); break; &#125; default: SET_ERR(\"Unknown notify message from HAL: %d\", msg-&gt;type); &#125;&#125; 它调用了notifyShutter方法：123456789101112131415161718192021222324252627282930[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\device3\\Camera3Device.cpp]void Camera3Device::notifyShutter(const camera3_shutter_msg_t &amp;msg, NotificationListener *listener) &#123; ... // Set timestamp for the request in the in-flight tracking // and get the request ID to send upstream &#123; Mutex::Autolock l(mInFlightLock); idx = mInFlightMap.indexOfKey(msg.frame_number); if (idx &gt;= 0) &#123; InFlightRequest &amp;r = mInFlightMap.editValueAt(idx); // Call listener, if any if (listener != NULL) &#123; //调用监听的notifyShutter法国法 listener-&gt;notifyShutter(r.resultExtras, msg.timestamp); &#125; ... //将待处理的result发送到Buffer sendCaptureResult(r.pendingMetadata, r.resultExtras, r.partialResult.collectedResult, msg.frame_number, r.hasInputBuffer, r.aeTriggerCancelOverride); returnOutputBuffers(r.pendingOutputBuffers.array(), r.pendingOutputBuffers.size(), r.shutterTimestamp); r.pendingOutputBuffers.clear(); removeInFlightRequestIfReadyLocked(idx); &#125; &#125; ...&#125; 首先它会通知listener preview成功，最后会调用sendCaptureResult将结果加入到结果队列。它会调用listener的notifyShutter方法，此处的listener其实是CameraDeviceClient类，所以会调用CameraDeviceClient类的notifyShutter方法： 123456789[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\api2\\CameraDeviceClient.cpp]void CameraDeviceClient::notifyShutter(const CaptureResultExtras&amp; resultExtras,nsecs_t timestamp) &#123; // Thread safe. Don't bother locking. sp&lt;ICameraDeviceCallbacks&gt; remoteCb = getRemoteCallback(); if (remoteCb != 0) &#123; //调用应用层的回调(CaptureCallback的onCaptureStarted方法) remoteCb-&gt;onCaptureStarted(resultExtras, timestamp); &#125;&#125; 此处的ICameraDeviceCallbacks对应的是Java层的CameraDeviceImpl.java中的内部类CameraDeviceCallbacks，所以会调用它的onCaptureStarted方法：123456789101112131415161718192021222324[-&gt;\\frameworks\\base\\core\\java\\android\\hardware\\camera2\\impl\\CameraDeviceImpl.java]@Overridepublic void onCaptureStarted(final CaptureResultExtras resultExtras, final long timestamp) &#123; int requestId = resultExtras.getRequestId(); final long frameNumber = resultExtras.getFrameNumber(); final CaptureCallbackHolder holder; synchronized(mInterfaceLock) &#123; if (mRemoteDevice == null) return; // Camera already closed // Get the callback for this frame ID, if there is one holder = CameraDeviceImpl.this.mCaptureCallbackMap.get(requestId); ... // Dispatch capture start notice holder.getHandler().post(new Runnable() &#123; @Override public void run() &#123; if (!CameraDeviceImpl.this.isClosed()) &#123; holder.getCallback().onCaptureStarted(CameraDeviceImpl.this,holder.getRequest( resultExtras.getSubsequenceId()),timestamp, frameNumber); &#125; &#125; &#125;); &#125;&#125; 它会调用OneCameraImpl.java中的mCaptureCallback的onCaptureStarted方法：12345678910111213[-&gt;\\frameworks\\base\\core\\java\\android\\hardware\\camera2\\impl\\CameraDeviceImpl.java]//Common listener for preview frame metadata. private final CameraCaptureSession.CaptureCallback mCaptureCallback = new CameraCaptureSession.CaptureCallback() &#123; @Override public void onCaptureStarted(CameraCaptureSession session,CaptureRequest request, long timestamp,long frameNumber) &#123; if (request.getTag() == RequestTag.CAPTURE&amp;&amp; mLastPictureCallback != null) &#123; mLastPictureCallback.onQuickExpose(); &#125; &#125; …&#125; camera工作时，存在了５中流处理线程和一个专门向hal发送请求的request线程。线程之间通过信号来同步，稍不注意就搞不明白代码是如何运行的了。其中很容易让我们忽视的就是在流发送之前的parent-&gt;registerInFlight()该操作将当前的请求保存到一个数组(可以理解成)中。这个数组对象在后续回帧操作中，会将相应帧的shutter,时间戳信息填充到对应的request中，紧接着就把对应帧的信息返回给app。好了先到这吧，下一篇分析Camera recording流程。 注意：Capture,preview以及autoFocus都是使用的这个回调，而Capture调用的时候，其RequestTag为CAPTURE，而autoFocus的时候为TAP_TO_FOCUS,而preview请求时没有对RequestTag进行设置，所以回调到onCaptureStarted方法时，不需要进行处理，但是到此时，preview已经启动成功，可以进行预览了，其数据都在buffer里。所以到此时，preview的流程全部分析结束，下面给出HAL层上的流程时序图 （二）、Camera System takePicture流程分析与TakePicture息息相关的主要有4个线程CaptureSequencer,JpegProcessor,Camera3Device::RequestThread,FrameProcessorBase如下面的代码可以发现，在Camera2client对象初始化后，已经有３个线程已经run起来了，还有有一个RequestThread线程会在Camera3Device初始化时创建的。他们工作非常密切，如下大概画了一个他们的工作机制，４个线程都是通过Conditon条件变量来同步的。 前面分析preview的时候，当预览成功后，会使能ShutterButton，即可以进行拍照，定位到ShutterButton的监听事件为onShutterButtonClick方法： 12345678910111213141516171819202122[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]@Overridepublic void onShutterButtonClick() &#123; //Camera未打开 if (mCamera == null) &#123; return; &#125; int countDownDuration = mSettingsManager.getInteger(SettingsManager .SCOPE_GLOBAL,Keys.KEY_COUNTDOWN_DURATION); if (countDownDuration &gt; 0) &#123; // 开始倒计时 mAppController.getCameraAppUI().transitionToCancel(); mAppController.getCameraAppUI().hideModeOptions(); mUI.setCountdownFinishedListener(this); mUI.startCountdown(countDownDuration); // Will take picture later via listener callback. &#125; else &#123; //即刻拍照 takePictureNow(); &#125;&#125; 首先，读取Camera的配置，判断配置是否需要延时拍照，此处分析不需延时的情况，即调用takePictureNow方法： 123456789101112131415161718192021[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]private void takePictureNow() &#123; if (mCamera == null) &#123; Log.i(TAG, \"Not taking picture since Camera is closed.\"); return; &#125; //创建Capture会话并开启会话 CaptureSession session = createAndStartCaptureSession(); //获取Camera的方向 int orientation = mAppController.getOrientationManager() .getDeviceOrientation().getDegrees(); //初始化图片参数 PhotoCaptureParameters params = new PhotoCaptureParameters( session.getTitle(), orientation, session.getLocation(), mContext.getExternalCacheDir(), this, mPictureSaverCallback, mHeadingSensor.getCurrentHeading(), mZoomValue, 0); //装配Session decorateSessionAtCaptureTime(session); //拍照 mCamera.takePicture(params, session);&#125; 它首先调用createAndStartCaptureSession来创建一个CaptureSession并且启动会话,这里并且会进行初始参数的设置，譬如设置CaptureModule(此处实参为this)为图片处理的回调(后面再分析)： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]private CaptureSession createAndStartCaptureSession() &#123; //获取会话时间 long sessionTime = getSessionTime(); //当前位置 Location location = mLocationManager.getCurrentLocation(); //设置picture name String title = CameraUtil.instance().createJpegName(sessionTime); //创建会话 CaptureSession session = getServices().getCaptureSessionManager() .createNewSession(title, sessionTime, location); //开启会话 session.startEmpty(new CaptureStats(mHdrPlusEnabled),new Size( (int) mPreviewArea.width(), (int) mPreviewArea.height())); return session;&#125;``` 首先，获取会话的相关参数，包括会话时间，拍照的照片名字以及位置信息等，然后调用Session管理来创建CaptureSession，最后将此CaptureSession启动。到这里，会话就创建并启动了，所以接着分析上面的拍照流程，它会调用OneCameraImpl的takePicture方法来进行拍照：``` java[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]@Overridepublic void takePicture(final PhotoCaptureParameters params, final CaptureSession session) &#123; ... // 除非拍照已经返回，否则就广播一个未准备好状态的广播，即等待本次拍照结束 broadcastReadyState(false); //创建一个线程 mTakePictureRunnable = new Runnable() &#123; @Override public void run() &#123; //拍照 takePictureNow(params, session); &#125; &#125;; //设置回调，此回调后面将分析，它其实就是CaptureModule,它实现了PictureCallback mLastPictureCallback = params.callback; mTakePictureStartMillis = SystemClock.uptimeMillis(); //如果需要自动聚焦 if (mLastResultAFState == AutoFocusState.ACTIVE_SCAN) &#123; mTakePictureWhenLensIsStopped = true; &#125; else &#123; //拍照 takePictureNow(params, session); &#125;&#125; 在拍照里，首先广播一个未准备好的状态广播，然后进行拍照的回调设置，并且判断是否有自动聚焦，如果是则将mTakePictureWhenLensIsStopped 设为ture，即即刻拍照被停止了，否则则调用OneCameraImpl的takePictureNow方法来发起拍照请求： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]public void takePictureNow(PhotoCaptureParameters params, CaptureSession session) &#123; long dt = SystemClock.uptimeMillis() - mTakePictureStartMillis; try &#123; // 构造JPEG图片拍照的请求 CaptureRequest.Builder builder = mDevice.createCaptureRequest( CameraDevice.TEMPLATE_STILL_CAPTURE); builder.setTag(RequestTag.CAPTURE); addBaselineCaptureKeysToRequest(builder); // Enable lens-shading correction for even better DNGs. if (sCaptureImageFormat == ImageFormat.RAW_SENSOR) &#123; builder.set(CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE, CaptureRequest.STATISTICS_LENS_SHADING_MAP_MODE_ON); &#125; else if (sCaptureImageFormat == ImageFormat.JPEG) &#123; builder.set(CaptureRequest.JPEG_QUALITY, JPEG_QUALITY); .getJpegRotation(params.orientation, mCharacteristics)); &#125; //用于preview的控件 builder.addTarget(mPreviewSurface); //用于图片显示的控件 builder.addTarget(mCaptureImageReader.getSurface()); CaptureRequest request = builder.build(); if (DEBUG_WRITE_CAPTURE_DATA) &#123; final String debugDataDir = makeDebugDir(params.debugDataFolder, \"normal_capture_debug\"); Log.i(TAG, \"Writing capture data to: \" + debugDataDir); CaptureDataSerializer.toFile(\"Normal Capture\", request, new File(debugDataDir,\"capture.txt\")); &#125; //拍照，mCaptureCallback为回调 mCaptureSession.capture(request, mCaptureCallback, mCameraHandler); &#125; catch (CameraAccessException e) &#123; Log.e(TAG, \"Could not access camera for still image capture.\"); broadcastReadyState(true); params.callback.onPictureTakingFailed(); return; &#125; synchronized (mCaptureQueue) &#123; mCaptureQueue.add(new InFlightCapture(params, session)); &#125;&#125; 与preview类似，都是通过CaptureRequest来与Camera进行通信的，通过session的capture来进行拍照，并设置拍照的回调函数为mCaptureCallback： 12345678[-&gt;/frameworks/base/core/java/android/hardware/camera2/impl/CameraCaptureSessionImpl.java]@Overridepublic synchronized int capture(CaptureRequest request,CaptureCallback callback,Handler handler)throws CameraAccessException&#123; ... handler = checkHandler(handler,callback); return addPendingSequence(mDeviceImpl.capture(request,createCaptureCallbackProxy( handler,callback),mDeviceHandler));&#125; 代码与preview中的类似，都是将请求加入到待处理的请求集，现在看CaptureCallback回调： 1234567891011121314151617181920212223242526272829303132333435[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]private final CameraCaptureSession.CaptureCallback mCaptureCallback = new CameraCaptureSession.CaptureCallback()&#123; @Override public void onCaptureStarted(CameraCaptureSession session,CaptureRequest request,long timestamp,long frameNumber)&#123; //与preview类似 if(request.getTag() == RequestTag.CAPTURE&amp;&amp;mLastPictureCallback!=null)&#123; mLastPictureCallback.onQuickExpose(); &#125; &#125; ... @Override public void onCaptureCompleted(CameraCaptureSession session,CaptureRequest request ,TotalCaptureResult result)&#123; autofocusStateChangeDispatcher(result); if(result.get(CaptureResult.CONTROL_AF_STATE) == null)&#123; //检查自动聚焦的状态 AutoFocusHelper.checkControlAfState(result); &#125; ... if(request.getTag() == RequestTag.CAPTURE)&#123; synchronized(mCaptureQueue)&#123; if(mCaptureQueue.getFirst().setCaptureResult(result).isCaptureComplete())&#123; capture = mCaptureQueue.removeFirst(); &#125; &#125; if(capture != null)&#123; //拍照结束 OneCameraImpl.this.onCaptureCompleted(capture); &#125; &#125; super.onCaptureCompleted(session,request,result); &#125; ...&#125; 这是Native层在处理请求时，会调用相应的回调，如capture开始时，会回调onCaptureStarted,具体的在preview中有过分析，当拍照结束时，会回调onCaptureCompleted方法，其中会根据CaptureResult来检查自动聚焦的状态，并通过TAG判断其是Capture动作时，再来看它是否是队列中的第一个请求，如果是，则将请求移除，因为请求已经处理成功，最后再调用OneCameraImpl的onCaptureCompleted方法来进行处理： 12345678910111213141516[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]private void onCaptureCompleted(InFlightCapture capture)&#123; if(isCaptureImageFormat == ImageFormat.RAW_SENSOR)&#123; ... File dngFile = new File(RAW_DIRECTORY,capture.session.getTitle()+\".dng\"); writeDngBytesAndClose(capture.image,capture.totalCaptureResult,mCharacteristics,dngFile); &#125;else&#123; //解析result中的图片数据 byte[] imageBytes = acquireJpegBytesAndClose(capture.image); //保存Jpeg图片 saveJpegPicture(imageBytes,capture.parameters,capture.session,capture.totalCaptureResult); &#125; broadcastReadyState(true); //调用回调 capture.parameters.callback.onPictureTaken(capture.session);&#125; 如代码所示，首先，对result中的图片数据进行了解析，然后调用saveJpegPicture方法将解析得到的图片数据进行保存，最后再调用里面的回调(即CaptureModule，前面在初始化Parameters时说明了，它实现了PictureCallbak接口)的onPictureTaken方法，所以，接下来先分析saveJpegPicture方法： 1234567891011121314151617[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/OneCameraImpl.java]private void saveJpegPicture(byte[] jpegData,final PhotoCaptureParameters captureParams,CaptureSession session,CaptureResult result)&#123; ... ListenableFuture&lt;Optional&lt;Uri&gt;&gt; futureUri = session.saveAndFinish(jpegData,width, height,rotation,exif); Futures.addCallback(futureUri,new FutureCallback&lt;Optional&lt;Uri&gt;&gt;()&#123; @Override public void onSuccess(Optional&lt;Uri&gt; uriOptional)&#123; captureParams.callback.onPictureSaved(mOptional.orNull()); &#125; @Override public void onFailure(Throwable throwable)&#123; captureParams.callback.onPictureSaved(null); &#125; &#125;);&#125; 它最后会回调onPictureSaved方法来对图片进行保存，所以需要分析CaptureModule的onPictureSaved方法： 12345[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]@Overridepublic void onPictureSaved(Uri uri)&#123; mAppController.notifyNewMedia(uri);&#125; mAppController的实现为CameraActivity，所以分析notifyNewMedia方法： 123456789101112131415161718192021222324252627[-&gt;/packages/apps/Camera2/src/com/android/camera/CameraActivity.java]@Overridepublic void notifyNewMedia(Uri uri)&#123; ... if(FilmstripItemUtils.isMimeTypeVideo(mimeType))&#123; //如果拍摄的是video sendBroadcast(new Intent(CameraUtil.ACTION_NEW_VIDEO,uri)); newData = mVideoItemFactory.queryContentUri(uri); ... &#125;else if(FilmstripItemUtils.isMimeTypeImage(mimeType))&#123; //如果是拍摄图片 CameraUtil.broadcastNewPicture(mAppContext,uri); newData = mPhotoItemFactory.queryCotentUri(uri); ... &#125;else&#123; return; &#125; new AsyncTask&lt;FilmstripItem,Void,FilmstripItem&gt;()&#123; @Override protected FilmstripItem doInBackground(FilmstripItem... Params)&#123; FilmstripItem data = params[0]; MetadataLoader.loadMetadata(getAndroidContet(),data); return data; &#125; ... &#125;&#125; 由代码可知，这里有两种数据的处理，一种是video，另一种是image。而我们这里分析的是capture图片数据，所以首先会根据在回调函数传入的参数Uri和PhotoItemFactory来查询到相应的拍照数据，然后再开启一个异步的Task来对此数据进行处理，即通过MetadataLoader的loadMetadata来加载数据，并返回。至此，capture的流程就基本分析结束了，下面将给出capture流程的整个过程中的时序图： （三）、Camera System takepicture(ZSL)流程分析（本小节基于 Android 5.1 API11源码）ZSL(zear shutter lag)即零延时，就是在拍照时不停预览就可以拍照.由于有较好的用户体验度，该feature是现在大部分手机都拥有的功能。面不再贴出大量代码来描述过程，直接上图。下图是画了2个小时整理出来的Android5.1 Zsl的基本流程，可以看到与ZSL密切相关的有5个线程frameprocessor、captureSequencer、ZslProcessor3、JpegProcessor、Camera3Device:requestThread。其实还有一个主线程用于更新参数。针对Android5.1看代码所得，ZSL过程中大概分成下面7个流程. 更正：图中左上角的FrameProcessor线程起来后会在waitForNextFrame中执行mResultSignal.waitRelative()，图中没有更改过来。 3.0、注册帧监听对象3.0.1、captureSequence线程注册帧监听对象3.0.1.1、注册时机当上层发出ZSL拍照请求时，底层就会触发拍照捕获状态机，改状态机的基本流程图在上篇笔记中已经整理出来过，这里就不多说了。由于camera2Client与其它处理线程对象基本符合金字塔形的架构，可以看到这里是通过camera2Client的对象将帧可用监听对象注册到FrameProcess对象中的List mRangeListeners;对象中。 1234567891011121314[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/CaptureSequencer.cpp]CaptureSequencer::CaptureState CaptureSequencer::manageZslStart( sp&lt;Camera2Client&gt; &amp;client) &#123; ALOGV(\"%s\", __FUNCTION__); status_t res; sp&lt;ZslProcessorInterface&gt; processor = mZslProcessor.promote(); // We don't want to get partial results for ZSL capture. client-&gt;registerFrameListener(mCaptureId, mCaptureId + 1, this, /*sendPartials*/false); // TODO: Actually select the right thing here. res = processor-&gt;pushToReprocess(mCaptureId); //.......&#125; 特别注意：可以看到在注册帧监听对象时，传入的两个参数是mCaptureId, mCaptureId + 1,为什么会是这样呢,因为这个就是标记我们想抓的是哪一帧,当拍照buffer从hal上来之后,Camera3Device就会回调帧可用监听对象，然后得到拍照帧的时间戳，紧接着根据时间戳从ZSL RingBuffer中找到最理想的inputBuffer，然后下发给hal进行Jpeg编解码。对比下面ZSL线程的CaptureId,应该就理解了. 3.0.1.2、捕获时机123456789101112[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/CaptureSequencer.cpp]void CaptureSequencer::onResultAvailable(const CaptureResult &amp;result) &#123; ATRACE_CALL(); ALOGV(\"%s: New result available.\", __FUNCTION__); Mutex::Autolock l(mInputMutex); mNewFrameId = result.mResultExtras.requestId; mNewFrame = result.mMetadata; if (!mNewFrameReceived) &#123; mNewFrameReceived = true; .signal(); &#125;&#125; 上面即是拍照状态机注册的回调函数，其中当ZSL拍照帧上来之后，机会激活正在等待中的CaptureSequencer线程，以进行后续的操作。 3.0.2、ZslProcess3线程注册帧监听对象3.0.2.1、注册时机12345678910111213141516171819202122[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/ZslProcessor.cpp]status_t ZslProcessor::updateStream(const Parameters &amp;params) &#123; if (mZslStreamId == NO_STREAM) &#123; // Create stream for HAL production // TODO: Sort out better way to select resolution for ZSL // Note that format specified internally in Camera3ZslStream res = device-&gt;createZslStream( params.fastInfo.arrayWidth, params.fastInfo.arrayHeight, mBufferQueueDepth, &amp;mZslStreamId, &amp;mZslStream); // Only add the camera3 buffer listener when the stream is created. mZslStream-&gt;addBufferListener(this);//这里是在BufferQueue注册的callback，暂时不用关心。 &#125; client-&gt;registerFrameListener(Camera2Client::kPreviewRequestIdStart, Camera2Client::kPreviewRequestIdEnd, this, /*sendPartials*/false); return OK;&#125; 上面的即为更新zsl流时调用的函数,可以看到其中使用registerFrameListener注册了RingBuffer可用监听对象，这里我们要特别注意的是下面2个宏。这个是专门为预览预留的requestId，考虑这样也会有录像和拍照的requestId,每次更新参数后，这个requestId会有+1操作，没有参数更新，则不会+1，这个可以在各自的Debug手机上发现。 123[-&gt;/frameworks/av/services/camera/libcameraservice/api1/Camera2Client.h]static const int32_t kPreviewRequestIdStart = 10000000;static const int32_t kPreviewRequestIdEnd = 20000000; 3.0.2.2、捕获时机123456789101112131415161718[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/ZslProcessor.cpp]void ZslProcessor::onResultAvailable(const CaptureResult &amp;result) &#123; ATRACE_CALL(); ALOGV(\"%s:\", __FUNCTION__); Mutex::Autolock l(mInputMutex); camera_metadata_ro_entry_t entry; entry = result.mMetadata.find(ANDROID_SENSOR_TIMESTAMP); nsecs_t timestamp = entry.data.i64[0]; entry = result.mMetadata.find(ANDROID_REQUEST_FRAME_COUNT); int32_t frameNumber = entry.data.i32[0]; // Corresponding buffer has been cleared. No need to push into mFrameList if (timestamp &lt;= mLatestClearedBufferTimestamp) return; mFrameList.editItemAt(mFrameListHead) = result.mMetadata; mFrameListHead = (mFrameListHead + 1) % mFrameListDepth;&#125; 去掉错误检查代码，上面由于CaptureID是下面2个，也就是ZSL的所有预览Buffer可用之后都会回调这个方法,当队列满之后，新buffer会覆盖旧buffer位置。上面可以看到mFrameList中会保存每一帧的metadata数据，mFrameListHead用来标识下一次存放数据的位置。 3.1、查找ZSL拍照最合适的buffer一开始我以为是是根据想要抓取那帧的captureId来找到zsl拍照buffer的，但是现在看来就是找时间戳最近的那个buffer来进行jpeg编解码(而且google工程师在源码中注释也是这样说的). 123456789101112131415161718192021222324252627282930313233343536373839[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/ZslProcessor.cpp]status_t ZslProcessor::pushToReprocess(int32_t requestId) &#123; ALOGV(\"%s: Send in reprocess request with id %d\", __FUNCTION__, requestId); Mutex::Autolock l(mInputMutex); status_t res; sp&lt;Camera2Client&gt; client = mClient.promote(); //下面是就是在mFrameList查找时间戳最近的帧。 size_t metadataIdx; nsecs_t candidateTimestamp = getCandidateTimestampLocked(&amp;metadataIdx); //根据上一次查找的时间戳，从ZSL BufferQueue中查找时间最接近的Buffer，并将 //buffer保存到mInputBufferQueue队列中。 res = mZslStream-&gt;enqueueInputBufferByTimestamp(candidateTimestamp, /*actualTimestamp*/NULL); //----------------- &#123;//获取zsl 编解码的metadataId，稍后会传入给hal编解码。 CameraMetadata request = mFrameList[metadataIdx]; // Verify that the frame is reasonable for reprocessing camera_metadata_entry_t entry; entry = request.find(ANDROID_CONTROL_AE_STATE); if (entry.data.u8[0] != ANDROID_CONTROL_AE_STATE_CONVERGED &amp;&amp; entry.data.u8[0] != ANDROID_CONTROL_AE_STATE_LOCKED) &#123; ALOGV(\"%s: ZSL queue frame AE state is %d, need full capture\", __FUNCTION__, entry.data.u8[0]); return NOT_ENOUGH_DATA; &#125; //这中间会更新输入stream的流ID、更新捕获意图为静态拍照、判断这一帧是否AE稳定、 //获取jpegStreamID并更新到metadata中、更新请求ID，最后根据更新后的request metadata //更新jpeg metadata。最后一步启动Camera3Device抓取图片。 // Update post-processing settings res = updateRequestWithDefaultStillRequest(request); mLatestCapturedRequest = request; res = client-&gt;getCameraDevice()-&gt;capture(request); mState = LOCKED; &#125; return OK;&#125; 还记得在启动状态机器时，注册的帧监听对象吧。这里参数requestId就是我们想要抓拍的图片的请求ID,目前发现该请求ID后面会更新到metadata中。这里只要知道该函数功能就可以了。 ☯ 1、从mFrameList中查找时间戳最小的metadata。☯ 2、根据从第一步获取到时间戳，从ZSL BufferQueue选择时间最接近Buffer.☯3、将Buffer放到mInputBufferQueue中，更新jpeg编解码metadata，启动Capture功能。 3.2、设置zsl input buffer和 jpeg out buffer 其实这一步之前已经讨论过，inputBuffer是ZslProcess3线程查找到最合适的用于jpeg编解码的buffer。outputBuffer为JpegProcessor线程更新的buffer用于存放hal编解码之后的jpeg图片。其中准备jpeg OutBuffer的操作就是在下面操作的。可以看到将outputStream的ID，保存到metadata中了。这样就会在Camera3Device中根据这项metadata来添加outputBuffer到hal。 1234567status_t ZslProcessor::pushToReprocess(int32_t requestId) &#123; // TODO: Shouldn't we also update the latest preview frame? int32_t outputStreams[1] = &#123; client-&gt;getCaptureStreamId() &#125;; res = request.update(ANDROID_REQUEST_OUTPUT_STREAMS, outputStreams, 1); &#125; 3.3、归还jpeg Buffer干了什么. 当framework将ZSL inputBuffer和jpeg outputBuffer,传给hal后，hal就会启动STLL_CAPTURE流程，将inputBuffer中的图像数据，进行一系列的后处理流程。当后处理完成后，hal则会将临时Buffer拷贝到outPutBuffer中(注意：这里要记得做flush操作，即刷新Buffer,要不然图片有可能会出现绿条). 因为JpegBuffer也是从BufferQueue Dequeue出来的buffer,而且在创建BufferQueue时，也注册了帧监听对象(即：onFrameAvailable()回调).这样的话当帧可用(即：进行了enqueue操作），就会回调onFrameAvailable()方法，这样当hal归还jpegBuffer时就是要进行enqueue()操作。在onFrameAvailable()方法中，会激活jpegproces线程，进行后续的处理，最后激活captureSequeue拍照状态机线程。 3.4、保存ZSLBuffer. 这里由于ZSL Buffer一直会从hal上来，所以当zslBuffer上来后，就会激活FrameProcesor线程保存这一ZslBuffer，目前FrameWork那边默认是4个buffer，这样的话当队列满之后，就会覆盖之前最老的buffer,如此反复操作。 3.5、获取拍照jpeg Buffer 当hal上来jpeg帧后，就会激活jpegProcess线程,并从BufferQueue中拿到jpegbuffer，下面可以发现进行lockNextBuffer,unlockBuffer操作。 12345678910[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/JpegProcessor.cpp]status_t JpegProcessor::processNewCapture() &#123; res = mCaptureConsumer-&gt;lockNextBuffer(&amp;imgBuffer); mCaptureConsumer-&gt;unlockBuffer(imgBuffer); sp&lt;CaptureSequencer&gt; sequencer = mSequencer.promote(); //...... if (sequencer != 0) &#123; sequencer-&gt;onCaptureAvailable(imgBuffer.timestamp, captureBuffer); &#125;&#125; 上面可以发现最后回调了captureSequencer线程的onCaptureAvailable()回调方法。该回调方法主要作用就是将时间戳和jpeg buffer的传送到CaptureSequencer线程中,然后激活CaptureSequencer线程。最后将Buffer CallBack到应用层。 12345678910111213[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/CaptureSequencer.cpp]void CaptureSequencer::onCaptureAvailable(nsecs_t timestamp, sp&lt;MemoryBase&gt; captureBuffer) &#123; ATRACE_CALL(); ALOGV(\"%s\", __FUNCTION__); Mutex::Autolock l(mInputMutex); mCaptureTimestamp = timestamp; mCaptureBuffer = captureBuffer; if (!mNewCaptureReceived) &#123; mNewCaptureReceived = true; mNewCaptureSignal.signal(); &#125;&#125; 3.6、拍照帧可用回调当拍照帧回到Framework后，就会回调CaptureSequencer的onResultAvailable()接口，用于设置captureSequencer状态机的标志位和条件激活,如下代码所示。条件变量和标志位的使用可以在状态机方法manageStandardCaptureWait()看到使用。 123456789101112[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/CaptureSequencer.cpp]void CaptureSequencer::onResultAvailable(const CaptureResult &amp;result) &#123; ATRACE_CALL(); ALOGV(\"%s: New result available.\", __FUNCTION__); Mutex::Autolock l(mInputMutex); mNewFrameId = result.mResultExtras.requestId; mNewFrame = result.mMetadata; if (!mNewFrameReceived) &#123; mNewFrameReceived = true; mNewFrameSignal.signal(); &#125;&#125; 3.7、jpeg buffer回调到app该callback是应用注册过来的一个代理对象，下面就是通过binder进程间调用将jpeg Buffer传送到APP端，注意这里的msgTyep = CAMERA_MSG_COMPRESSED_IMAGE,就是告诉上层这是一个压缩的图像数据。 1234567891011121314151617[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/CaptureSequencer.cpp]CaptureSequencer::CaptureState CaptureSequencer::manageDone(sp&lt;Camera2Client&gt; &amp;client) &#123; status_t res = OK; ATRACE_CALL(); mCaptureId++; ...... Camera2Client::SharedCameraCallbacks::Lock l(client-&gt;mSharedCameraCallbacks); ALOGV(\"%s: Sending still image to client\", __FUNCTION__); if (l.mRemoteCallback != 0) &#123; l.mRemoteCallback-&gt;dataCallback(CAMERA_MSG_COMPRESSED_IMAGE, mCaptureBuffer, NULL); &#125; else &#123; ALOGV(\"%s: No client!\", __FUNCTION__); &#125; ......&#125; （四）、Camera System Recorder流程分析camera Video.虽然标题是recording流程分析，但这里很多和preview是相似的(包含更新，创建Stream,创建Request)，这里主要分析MediaRecorder对象创建、video帧监听对象注册、帧可用事件以及一系列callback流程分析。 4.1、认识video(mediaRecorder)状态机 Used to record audio and video. The recording control is based on asimple state machine (see below).状态机请看上面源码中给的流程图。A common case of using MediaRecorder to record audio works as follows:1.MediaRecorder recorder = new MediaRecorder();2.recorder.setAudioSource(MediaRecorder.AudioSource.MIC);3.recorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);4.recorder.setAudioEncoder(MediaRecorder.AudioEncoder.AMR_NB);5.recorder.setOutputFile(PATH_NAME);6.recorder.prepare();7.recorder.start(); // Recording is now started8….9.recorder.stop();10.recorder.reset(); // You can reuse the object by going back to setAudioSource() steprecorder.release(); // Now the object cannot be reused Applications may want to register for informational and errorevents in order to be informed of some internal update and possibleruntime errors during recording. Registration for such events isdone by setting the appropriate listeners (via calls(to {@link #setOnInfoListener(OnInfoListener)}setOnInfoListener and/or{@link #setOnErrorListener(OnErrorListener)}setOnErrorListener).In order to receive the respective callback associated with these listeners,applications are required to create MediaRecorder objects on threads with aLooper running (the main UI thread by default already has a Looper running). 上面是googole工程师加的注释，最权威的资料。大概意思就是说“使用mediaRecorder记录音视频，需要一个简单的状态机来控制”。上面的1,2,3…就是在操作时需要准守的步骤。算了吧，翻译水平有限，重点还是放到camera这边吧。 4.2、Camera app如何启动录像1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556//源码路径:pdk/apps/TestingCamera/src/com/android/testingcamera/TestingCamera.java private void startRecording() &#123; log(\"Starting recording\"); logIndent(1); log(\"Configuring MediaRecoder\"); //这里会检查是否打开了录像功能。这里我们省略了，直接不如正题//上面首先创建了一个MediaRecorder的java对象(注意这里同camera.java类似，java对象中肯定包含了一个mediaRecorder jni本地对象，继续往下看) mRecorder = new MediaRecorder(); //下面就是设置一些callback. mRecorder.setOnErrorListener(mRecordingErrorListener); mRecorder.setOnInfoListener(mRecordingInfoListener); if (!mRecordHandoffCheckBox.isChecked()) &#123; //将当前camera java对象设置给了mediaRecorder java对象。 //这里setCamera是jni接口，后面我们贴代码在分析。 mRecorder.setCamera(mCamera); &#125; //将preview surface java对象设置给mediaRecorder java对象，后面贴代码 //详细说明。 mRecorder.setPreviewDisplay(mPreviewHolder.getSurface()); //下面２个是设置音频和视频的资源。 mRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER); mRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA); mRecorder.setProfile(mCamcorderProfiles.get(mCamcorderProfile)); //从app控件选择录像帧大小，并设置给mediaRecorder Camera.Size videoRecordSize = mVideoRecordSizes.get(mVideoRecordSize); if (videoRecordSize.width &gt; 0 &amp;&amp; videoRecordSize.height &gt; 0) &#123; mRecorder.setVideoSize(videoRecordSize.width, videoRecordSize.height); &#125; //从app控件选择录像帧率，并设置给mediaRecorder. if (mVideoFrameRates.get(mVideoFrameRate) &gt; 0) &#123; mRecorder.setVideoFrameRate(mVideoFrameRates.get(mVideoFrameRate)); &#125; File outputFile = getOutputMediaFile(MEDIA_TYPE_VIDEO); log(\"File name:\" + outputFile.toString()); mRecorder.setOutputFile(outputFile.toString()); boolean ready = false; log(\"Preparing MediaRecorder\"); try &#123; //准备一下，请看下面google给的使用mediaRecorder标准流程 mRecorder.prepare(); ready = true; &#125; catch (Exception e) &#123;//------异常处理省略 &#125; if (ready) &#123; try &#123; log(\"Starting MediaRecorder\"); mRecorder.start();//启动录像 mState = CAMERA_RECORD; log(\"Recording active\"); mRecordingFile = outputFile; &#125; catch (Exception e) &#123;//-----异常处理省略 &#125;//------------ &#125; 可以看到应用启动录像功能是是符合状态机流程的。在应用开发中，也要这样来做。 ☯ 1.创建mediaRecorderjava对象，mRecorder = new MediaRecorder();☯ 2.设置camera java对象到mediaRecorder中，mRecorder.setCamera(mCamera);☯ 3.将preview surface对象设置给mediaRecorder,mRecorder.setPreviewDisplay(mPreviewHolder.getSurface());☯ 4.设置音频源，mRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER);☯ 5.设置视频源，mRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);☯ 6.设置录像帧大小和帧率，以及setOutputFile☯ 8.准备工作，mRecorder.prepare();☯ 9.启动mdiaRecorder,mRecorder.start(); 4.3、与MediaPlayerService相关的类接口之间的关系简介4.3.1、mediaRecorder何时与MediaPlayerService发送关系 12345678910111213/frameworks/base/media/java/android/media/MediaRecorder.javaMediaRecorder::MediaRecorder() : mSurfaceMediaSource(NULL)&#123; ALOGV(\"constructor\"); const sp&lt;IMediaPlayerService&gt;&amp; service(getMediaPlayerService()); if (service != NULL) &#123; mMediaRecorder = service-&gt;createMediaRecorder(); &#125; if (mMediaRecorder != NULL) &#123; mCurrentState = MEDIA_RECORDER_IDLE; &#125; doCleanUp();&#125; 在jni中创建mediaRecorder对象时，其实在构造函数中偷偷的链接了mediaPlayerService，这也是Android习惯用的方法。获取到MediaPlayerService代理对象后，通过匿名binder获取mediaRecorder代理对象。 4.3.2、mediaPlayerService类和接口之间关系 接口类型 接口说明 virtual sp createMediaRecorder() = 0; 创建mediaRecorder录视频服务对象的接口 virtual sp create(const sp&amp; client, int audioSessionId = 0) = 0; 创建mediaPlayer播放音乐服务对象的接口，播放音乐都是通过mediaPlayer对象播放的 virtual status_t decode() = 0; 音频解码器 4.3.3、MediaRecorder类和接口之间关系 mediaRecorder功能就是来录像的。其中MediaRecorder类中，包含了BpMediaRecorder代理对象引用。MediaRecorderClient本地对象驻留在mediaPlayService中。它的接口比较多，这里就列出我们今天关注的几个接口。其它接口查看源码吧详细介绍可以参考源码：frameworks/av/include/media/IMediaRecorder.h| 接口类型 | 接口说明|| :——– |: ——–|| virtual status_t setCamera(const sp&amp; camera,const sp&amp; proxy) = 0; | 这个接口也是非常需要我们关注的，这里获取到了启动录像操作的本地对象(BnCameraRecordingProxy），并通过匿名binder通信方式，第二个参数就是本地对象.然后在startRecording时将帧监听对象注册到camera本地对象中了|| virtual status_t setPreviewSurface(const sp&amp; surface) = 0; | 将preview预览surface对象设置给medaiRecorder，因为mediaRecorder也有一个camera本地client,所以这个surface对象最终还是会设置到cameraService用于显示。而录像的帧会在CameraService本地创建一个bufferQueue，具体下面会详细说明|| virtual status_t setListener(const sp&amp; listener) = 0; | 这里一看就是设置监听对象，监听对象是jni中的JNIMediaRecorderListener对象，该对象可以回调MediaRecorder.java类中的postEventFromNative方法，将时间送到java层。其实MediaRecorder实现了BnMediaRecorderClient接口，即实现notify接口，那么这里其实将本地对象传到MediaRecorder本地的客户端对象中（本地对象拿到的就是代理对象了），参考代码片段1|| virtual status_t start() = 0; | 启动录像功能，函数追究下去和Camera关系不大了，这里就不细说了| 4.3.3.1、代码片段11234源码路径：frameworks/base/media/jni/android_media_MediaRecorder.cpp// create new listener and give it to MediaRecordersp&lt;JNIMediaRecorderListener&gt; listener = new JNIMediaRecorderListener(env, thiz, weak_this);mr-&gt;setListener(listener); mediaRecorder jni接口回调java方法，通知上层native事件。 4.3.3.2、代码片段2123456789101112131415161718192021222324252627282930313233343536373839404142434445[-&gt;\\frameworks\\base\\media\\jni\\android_media_MediaRecorder.cpp]static void android_media_MediaRecorder_setCamera(JNIEnv* env, jobject thiz, jobject camera)&#123;// we should not pass a null camera to get_native_camera() call.//这里检查camera是不是空的，显然不是空的。 //这个地方需要好好研究一下，其中camera是java层的camera对象(即camera.java) //这里由java对象获取到camera应用端本地对象。 sp&lt;Camera&gt; c = get_native_camera(env, camera, NULL); if (c == NULL) &#123; // get_native_camera will throw an exception in this case return; &#125; //获取mediaRecorder本地对象 sp&lt;MediaRecorder&gt; mr = getMediaRecorder(env, thiz); //下面要特别注意，这里为什么传入的不是Camera对象而是c-&gt;remote()，当时琢磨 //着，camera.cpp也没实现什么代理类的接口啊，不过后来在cameraBase类中发现 //重载了remote()方法，该方法返回ICamera代理对象，呵呵。这样的话就会在 //mediaRecorder中创建一个新的ICamera代理对象。并在mediaPlayerService中 //创建了一个本地的Camera对象。 //c-&gt;getRecordingProxy():获取camera本地对象实现的Recording本地对象。这里 //调用setCamera设置到mediaRecorder本地对象中了(见代码片段３) process_media_recorder_call(env, mr-&gt;setCamera(c-&gt;remote(), c-&gt;getRecordingProxy()), \"java/lang/RuntimeException\", \"setCamera failed.\");&#125;//camera端sp&lt;ICameraRecordingProxy&gt; Camera::getRecordingProxy() &#123; ALOGV(\"getProxy\"); return new RecordingProxy(this);&#125;//看看下面RecordingProxy实现了BnCameraRecordingProxy接口，//是个本地对象，水落石出了。class RecordingProxy : public BnCameraRecordingProxy &#123; public: RecordingProxy(const sp&lt;Camera&gt;&amp; camera); // ICameraRecordingProxy interface virtual status_t startRecording(const sp&lt;ICameraRecordingProxyListener&gt;&amp; listener); virtual void stopRecording(); virtual void releaseRecordingFrame(const sp&lt;IMemory&gt;&amp; mem); private: //这里的是mCamera已经不再是之前preview启动时对应的那个本地Camera对象 //这是mediaRecorder重新创建的camera本地对象。 sp&lt;Camera&gt; mCamera; &#125;; 4.3.3.3、代码片段3-setCamera本地实现123456789101112131415161718192021222324252627282930[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\MediaRecorderClient.cpp]status_t MediaRecorderClient::setCamera(const sp&lt;ICamera&gt;&amp; camera, const sp&lt;ICameraRecordingProxy&gt;&amp; proxy)&#123; ALOGV(\"setCamera\"); Mutex::Autolock lock(mLock); if (mRecorder == NULL) &#123; ALOGE(\"recorder is not initialized\"); return NO_INIT; &#125; return mRecorder-&gt;setCamera(camera, proxy);&#125;//构造函数中可以看到创建了一个StagefrightRecorder对象，后续的其它操作//都是通过mRecorder对象实现的MediaRecorderClient::MediaRecorderClient(const sp&lt;MediaPlayerService&gt;&amp; service, pid_t pid)&#123; ALOGV(\"Client constructor\"); mPid = pid; mRecorder = new StagefrightRecorder; mMediaPlayerService = service;&#125;//StagefrightRecorder::setCamera实现struct StagefrightRecorder : public MediaRecorderBase &#123;&#125;status_t StagefrightRecorder::setCamera(const sp&lt;ICamera&gt; &amp;camera, const sp&lt;ICameraRecordingProxy&gt; &amp;proxy) &#123;//省去一些错误检查代码 mCamera = camera; mCameraProxy = proxy; return OK;&#125; 最终ICamera,ICameraRecordingProxy代理对象都存放到StagefrightRecorder对应的成员变量中，看来猪脚就在这个类中。 4.3.3.4、代码片段412345678910111213141516171819202122232425262728[-&gt;/frameworks/av/media/libstagefright/CameraSource.cpp]status_t CameraSource::isCameraAvailable( const sp&lt;ICamera&gt;&amp; camera, const sp&lt;ICameraRecordingProxy&gt;&amp; proxy, int32_t cameraId, const String16&amp; clientName, uid_t clientUid) &#123; if (camera == 0) &#123; mCamera = Camera::connect(cameraId, clientName, clientUid); if (mCamera == 0) return -EBUSY; mCameraFlags &amp;= ~FLAGS_HOT_CAMERA; &#125; else &#123; // We get the proxy from Camera, not ICamera. We need to get the proxy // to the remote Camera owned by the application. Here mCamera is a // local Camera object created by us. We cannot use the proxy from // mCamera here. //根据ICamera代理对象重新创建Camera本地对象 mCamera = Camera::create(camera); if (mCamera == 0) return -EBUSY; mCameraRecordingProxy = proxy; //目前还不清楚是什么标记，权且理解成支持热插拔标记 mCameraFlags |= FLAGS_HOT_CAMERA; //代理对象绑定死亡通知对象 mDeathNotifier = new DeathNotifier(); // isBinderAlive needs linkToDeath to work. mCameraRecordingProxy-&gt;asBinder()-&gt;linkToDeath(mDeathNotifier); &#125; mCamera-&gt;lock(); return OK;&#125; 由上面的类图之间的关系的，就知道mediaRecorder间接包含了cameaSource对象，这里为了简单直接要害代码。 ☯ 1.在创建CameraSource对象时，会去检查一下Camera对象是否可用，可用的话就会根据传进来的代理对象重新创建Camera本地对象（注意这个时候Camera代理对象在mediaRecorder中）☯ 2.然后保存RecordingProxy代理对象到mCameraRecordingProxy成员中，然后绑定死亡通知对象到RecordingProxy代理对象。 4.3.3.5、代码片段512345678910111213141516171819202122232425[-&gt;/frameworks/av/media/libstagefright/CameraSource.cpp]status_t CameraSource::startCameraRecording() &#123; ALOGV(\"startCameraRecording\"); // Reset the identity to the current thread because media server owns the // camera and recording is started by the applications. The applications // will connect to the camera in ICameraRecordingProxy::startRecording. int64_t token = IPCThreadState::self()-&gt;clearCallingIdentity(); status_t err; if (mNumInputBuffers &gt; 0) &#123; err = mCamera-&gt;sendCommand( CAMERA_CMD_SET_VIDEO_BUFFER_COUNT, mNumInputBuffers, 0); &#125; err = OK; if (mCameraFlags &amp; FLAGS_HOT_CAMERA) &#123;//前面已经置位FLAGS_HOT_CAMERA，成立 mCamera-&gt;unlock(); mCamera.clear(); //通过recording代理对象，直接启动camera本地端的recording if ((err = mCameraRecordingProxy-&gt;startRecording( new ProxyListener(this))) != OK) &#123; &#125; &#125; else &#123; &#125; IPCThreadState::self()-&gt;restoreCallingIdentity(token); return err;&#125; 上面代码需要我们注意的是在启动startRecording()时，创建的监听对象new ProxyListener(this),该监听对象会传到Camera本地对象中。当帧可用时，用来通知mediaRecorder有帧可以使用了，赶紧编码吧。 4.3.3.6、代码片段6-mediaRecorder注册帧可用监听对象1234567891011121314151617[-&gt;/frameworks/av/include/media/stagefright/CameraSource.h]class ProxyListener: public BnCameraRecordingProxyListener &#123; public: ProxyListener(const sp&lt;CameraSource&gt;&amp; source); virtual void dataCallbackTimestamp(int64_t timestampUs, int32_t msgType, const sp&lt;IMemory&gt; &amp;data); private: sp&lt;CameraSource&gt; mSource; &#125;;//frameworks/av/camera/Camera.cppstatus_t Camera::RecordingProxy::startRecording(const sp&lt;ICameraRecordingProxyListener&gt;&amp; listener)&#123; ALOGV(\"RecordingProxy::startRecording\"); mCamera-&gt;setRecordingProxyListener(listener); mCamera-&gt;reconnect(); return mCamera-&gt;startRecording();&#125; 注册帧监听对象就是在启动Recording时注册，主要有下面几步： ☯ 1.使用setRecordingProxyListener接口，将监听对象设置给mRecordingProxyListener 成员。☯ 2.重新和cameraService握手(preview停止时就会断开链接，在切换瞬间就断开了)☯ 3.使用ICamera代理对象启动录像。 4.4、阶段小结到这里Camera如何使用medaiRecorder录像的基本流程已经清楚了，这里我画了一个流程图，大概包含下面9个流程。 ☯ 过程1：上层点击了录像功能，或者录像preview模式下，会创建一个mediaRecorDer Java层对象。☯ 过程2:java层mediaRecorder对象调用native_jni native_setup方法，创建一个native的mediaRecorder对象。创建的过程中连接mediaPlayerService,并通过匿名binder通信方式获取到一个mediaRecorderClient代理对象，并保存到mediaRecorder对象的成员变量mMediaRecorder中。☯ 过程3:ava层的Camera对象传给mediaRecorder native层时，可以通过本地方法获取到Camera本地对象和ICamera代理对象。这里是获取ICamera代理对象和RecordingProxy本地对象☯ 过程4:将ICamera代理对象和RecordingProxy本地对象传给在MedaiService本地端的MediaRecorderClient对象，这时ICamera是重新创建的ICamer代理对象，以及获取到RecordingProxy代理对象。☯ 过程5：根据过程４获取到的新的ICamera代理对象和RecordingProxy代理对象，创建新的本地Camera对象Camera2，以及注册录像帧监听对象到Camera2中。☯ 过程6：启动StartRecording☯ 过程7:当录像帧可用时，通知驻留在MedaiRecorderClient中的Camera2本地对象收帧，于此同时Camera2又是通过注册的帧监听对象告知MediaClientClient对象。MediaClientClient对象拿到帧后进行录像编码。☯ 过程8,过程９：通过回调函数，将一些消息发送给应用端。 4.5、Camera video创建BufferQueue.12345678910111213141516171819202122232425262728293031323334353637383940[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/StreamingProcessor.cpp]status_t StreamingProcessor::updateRecordingStream(const Parameters &amp;params) &#123; ATRACE_CALL(); status_t res; Mutex::Autolock m(mMutex); sp&lt;CameraDeviceBase&gt; device = mDevice.promote(); //---------------- bool newConsumer = false; if (mRecordingConsumer == 0) &#123; ALOGV(\"%s: Camera %d: Creating recording consumer with %zu + 1 \" \"consumer-side buffers\", __FUNCTION__, mId, mRecordingHeapCount); // Create CPU buffer queue endpoint. We need one more buffer here so that we can // always acquire and free a buffer when the heap is full; otherwise the consumer // will have buffers in flight we'll never clear out. sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferConsumer&gt; consumer; //创建bufferQueue，同时获取到生产者和消费者对象。 BufferQueue::createBufferQueue(&amp;producer, &amp;consumer); //注意下面设置buffer的用处是GRALLOC_USAGE_HW_VIDEO_ENCODER，这个会在 //mediaRecorder中使用到。 mRecordingConsumer = new BufferItemConsumer(consumer, GRALLOC_USAGE_HW_VIDEO_ENCODER, mRecordingHeapCount + 1); mRecordingConsumer-&gt;setFrameAvailableListener(this); mRecordingConsumer-&gt;setName(String8(\"Camera2-RecordingConsumer\")); mRecordingWindow = new Surface(producer); newConsumer = true; // Allocate memory later, since we don't know buffer size until receipt &#125;//更新部分代码，就不贴出来了－－－－//注意下面video 录像buffer的像素格式是CAMERA2_HAL_PIXEL_FORMAT_OPAQUE if (mRecordingStreamId == NO_STREAM) &#123; mRecordingFrameCount = 0; res = device-&gt;createStream(mRecordingWindow, params.videoWidth, params.videoHeight, CAMERA2_HAL_PIXEL_FORMAT_OPAQUE, &amp;mRecordingStreamId); &#125; return OK;&#125; 主要处理下面几件事情。 ☯ 1.由于录像不需要显示，这里创建CameraService BufferQueue本地对象，这个时候获取到的生产者和消费者都是本地的，只有BufferQueue保存的有IGraphicBufferAlloc代理对象mAllocator，专门用来分配buffer。☯ 2.由于StremingProcess.cpp中实现了FrameAvailableListener监听接口方法onFrameAvailable()。这里会通过setFrameAvailableListener方法注册到BufferQueue中。☯ 3.根据生产者对象创建surface对象，并传给Camera3Device申请录像buffer.☯ 4.如果参数有偏差或者之前已经创建过video Stream.这里会删除或者更新videoStream.如果压根没有创建VideoStream,直接创建VideoStream并根据参数更新流信息。 4.6、何时录像帧可用4.6.1、onFrameAvailable()12345678910[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/StreamingProcessor.cpp]void StreamingProcessor::onFrameAvailable(const BufferItem&amp; /*item*/) &#123; ATRACE_CALL(); Mutex::Autolock l(mMutex); if (!mRecordingFrameAvailable) &#123; mRecordingFrameAvailable = true; mRecordingFrameAvailableSignal.signal(); &#125;&#125; 当video buffer进行enqueue操作后,该函数会被调用。函数中可用发现，激活了StreamingProcessor主线程。 4.6.2、StreamingProcessor线程loop12345678910111213141516171819[-&gt;/frameworks/av/services/camera/libcameraservice/api1/client2/StreamingProcessor.cpp]bool StreamingProcessor::threadLoop() &#123; status_t res; &#123; Mutex::Autolock l(mMutex); while (!mRecordingFrameAvailable) &#123; //之前是在这里挂起的,现在有帧可用就会从这里唤醒。 res = mRecordingFrameAvailableSignal.waitRelative( mMutex, kWaitDuration); if (res == TIMED_OUT) return true; &#125; mRecordingFrameAvailable = false; &#125; do &#123; res = processRecordingFrame();//进一步处理。 &#125; while (res == OK); return true;&#125; 到这里发现，原来StreamingProcessor主线程只为录像服务，previewStream只是使用了它的几个方法而已。 4.6.3、帧可用消息发送给Camera本地对象123456789101112131415161718192021222324[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\api1\\client2\\StreamingProcessor.cpp]status_t StreamingProcessor::processRecordingFrame() &#123; ATRACE_CALL(); status_t res; sp&lt;Camera2Heap&gt; recordingHeap; size_t heapIdx = 0; nsecs_t timestamp; sp&lt;Camera2Client&gt; client = mClient.promote(); BufferItemConsumer::BufferItem imgBuffer; //取出buffer消费，就是拿给mediaRecorder编码 res = mRecordingConsumer-&gt;acquireBuffer(&amp;imgBuffer, 0); //---------------------------- // Call outside locked parameters to allow re-entrancy from notification Camera2Client::SharedCameraCallbacks::Lock l(client-&gt;mSharedCameraCallbacks); if (l.mRemoteCallback != 0) &#123; //调用Callback通知Camea本地对象。 l.mRemoteCallback-&gt;dataCallbackTimestamp(timestamp, CAMERA_MSG_VIDEO_FRAME, recordingHeap-&gt;mBuffers[heapIdx]); &#125; else &#123; ALOGW(\"%s: Camera %d: Remote callback gone\", __FUNCTION__, mId); &#125; return OK; 之前我们已经知道Camera运行时存在类型为ICameraClient的两个对象,其中一个代理对象保存在CameraService中，本地对象保存的Camera本地对象中。这里代理对象通知本地对象取帧了。注意这里消息发送的是“CAMERA_MSG_VIDEO_FRAME”。 4.6.4、Camera本地对象转发消息给mediaRecorder.12345678910111213141516171819[-&gt;/frameworks/av/camera/Camera.cpp]void Camera::dataCallbackTimestamp(nsecs_t timestamp, int32_t msgType, const sp&lt;IMemory&gt;&amp; dataPtr)&#123; // If recording proxy listener is registered, forward the frame and return. // The other listener (mListener) is ignored because the receiver needs to // call releaseRecordingFrame. sp&lt;ICameraRecordingProxyListener&gt; proxylistener; &#123; //这里mRecordingProxyListener就是mediaRecorder注册过来的监听代理对象 Mutex::Autolock _l(mLock); proxylistener = mRecordingProxyListener; &#125; if (proxylistener != NULL) &#123; //这里就把buffer送到了mediaRecorder中进行编码 proxylistener-&gt;dataCallbackTimestamp(timestamp, msgType, dataPtr); return; &#125; //---------省略代码&#125; 到这里Camera本地对象就会调用mediaRecorder注册来的帧监听对象。前面我们已经做了那么长的铺垫，我想应该可以理解了。好了,mediaRecorder有饭吃了。 4.7、总结1.一开始我自以为preview和Video使用同一个camera本地对象，看了代码发现，原来是不同的对象。2.预览的BufferQueue是在CameraService中创建的，和surfaceFlinger没有关系，只是保留了IGraphicBufferAlloc代理对象mAllocator，用于分配buffer.3.之匿名binder没有理解透彻，以为只有传递本地对象才能使用writeStrongBinder()接口保存binder对象，同时在使用端使用readStrongBinder()就可以获取到代理对象了。其实也可以传递代理对象，只不过代码会走另外一套逻辑，在kernel中重新创建一个binder_ref索引对象返回给另一端。如下mediaRecorder设置camera时就是传递的ICamera代理对象。 123456789101112[-&gt;/frameworks/av/media/libmedia/IMediaRecorder.cpp] status_t setCamera(const sp&lt;ICamera&gt;&amp; camera, const sp&lt;ICameraRecordingProxy&gt;&amp; proxy) &#123; ALOGV(\"setCamera(%p,%p)\", camera.get(), proxy.get()); Parcel data, reply; data.writeInterfaceToken(IMediaRecorder::getInterfaceDescriptor()); //camera-&gt;asBinder()是ICamera代理对象 data.writeStrongBinder(camera-&gt;asBinder()); data.writeStrongBinder(proxy-&gt;asBinder()); remote()-&gt;transact(SET_CAMERA, data, &amp;reply); return reply.readInt32(); &#125; （五）、参考资料(特别感谢各位前辈的分析和图示)：Android Camera官方文档Android 5.0 Camera系统源码分析-CSDN博客Android Camera 流程学习记录 - StoneDemo - 博客园Android Camera 系统架构源码分析 - CSDN博客Camera安卓源码-高通mm_camera架构剖析 - CSDN博客5.2 应用程序和驱动程序中buffer的传输流程 - CSDN博客Camera2 数据流从framework到Hal源码分析 - 简书mm-camera层frame数据流源码分析 - 简书v4l2_capture.c分析—probe函数分析 - CSDN博客@@Android Camera fw学习 - CSDN博客@@Android Camera API2分析 - CSDN博客@@Android Camera 流程学习记录 7.12- CSDN博客@@专栏：古冥的android6.0下的Camera API2.0的源码分析之旅 - CSDN博客linux3.3 v4l2视频采集驱动框架(vfe, camera i2c driver，v4l2_subdev等之间的联系) - CSDN博客Android Camera从Camera HAL1到Camera HAL3的过渡（已更新到Android6.0 HAL3.3） - CSDN博客我心依旧之Android Camera模块FW/HAL3探学序 - CSDN博客Android Camera fw学习(四)-recording流程分析 - CSDN博客android camera动态库加载过程 - CSDN博客Android Camera API2.0下全新的Camera FW/HAL架构简述 - CSDN博客","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Camera System（1）：Camera System(Camera 系统)框架、Open()过程分析","slug":"Android Camera System（1）：Camera System[Camera 系统]框架、Open过程分析","date":"2018-06-29T16:00:00.000Z","updated":"2018-05-25T12:12:49.774Z","comments":true,"path":"2018/06/30/Android Camera System（1）：Camera System[Camera 系统]框架、Open过程分析/","link":"","permalink":"http://zhoujinjian.cc/2018/06/30/Android Camera System（1）：Camera System[Camera 系统]框架、Open过程分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】【特别感谢 - Android Camera fw学习-Armwind】【特别感谢 - Android Camera API2分析-Gzzaigcnforever】【特别感谢 - Android Camera 流程学习记录 Android 7.12-QQ_16775897】【特别感谢 - 专栏：古冥的android6.0下的Camera API2.0的源码分析之旅】Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) 🌀🌀：专注于Linux &amp;&amp; Android Multimedia（Camera、Video、Audio、Display）系统分析与研究 ☯ Application：☯ /packages/apps/Camera2/src/com/android/camera/ ☯ Framework：☯ /frameworks/base/core/java/android/hardware/Camera.java ☯ JNI:☯ /frameworks/base/core/jni/android_hardware_Camera.cpp ☯ Native:☯ Client：frameworks/av/camera/CameraBase.cppframeworks/av/camera/Camera.cppframeworks/av/camera/ICamera.cppframeworks/av/camera/aidl/android/hardware/ICamera.aidlframeworks/av/camera/aidl/android/hardware/ICameraClient.aidl☯ Server：frameworks/av/camera/cameraserver/main_cameraserver.cppframeworks/av/services/camera/libcameraservice/CameraService.cppframeworks/av/services/camera/libcameraservice/api1/CameraClient.cppframeworks/av/camera/aidl/android/hardware/ICameraService.aidl ☯ HAL：☯ /frameworks/av/services/camera/libcameraservice/device3/☯ /hardware/qcom/camera/QCamera2(高通HAL)☯ /vendor/qcom/proprietary/mm-camera(高通mm-camera)☯ /vendor/qcom/proprietary/mm-still(高通JPEG) ☯ Kernel：☯ /kernel/drivers/media/platform/msm/camera_v2(高通V4L2)☯ /kernel/arch/arm/boot/dts/(高通dts) （一）、Android Camera System Architecture（Camera系统框架）1.1、Android Camera System总体框架（Qualcomm平台）1.1.1、首先看看Android 官方Camera总体架构： ☯应用框架应用代码位于应用框架级别，它利用 android.hardware.Camera API 来与相机硬件进行互动。在内部，此代码会调用相应的 JNI 粘合类，以访问与该相机互动的原生代码。☯JNI与 android.hardware.Camera 关联的 JNI 代码位于 frameworks/base/core/jni/android_hardware_Camera.cpp 中。此代码会调用较低级别的原生代码以获取对物理相机的访问权限，并返回用于在框架级别创建 android.hardware.Camera 对象的数据。☯原生框架在 frameworks/av/camera/Camera.cpp 中定义的原生框架可提供相当于 android.hardware.Camera 类的原生类。此类会调用 IPC binder 代理，以获取对相机服务的访问权限。☯Binder IPC 代理IPC binder 代理用于促进跨越进程边界的通信。调用相机服务的 frameworks/av/camera 目录中有 3 个相机 binder 类。ICameraService 是相机服务的接口，ICamera 是已打开的特定相机设备的接口，ICameraClient 是返回应用框架的设备接口。☯相机服务位于 frameworks/av/services/camera/libcameraservice/CameraService.cpp 下的相机服务是与 HAL 进行互动的实际代码。☯HAL硬件抽象层定义了由相机服务调用且您必须实现以确保相机硬件正常运行的标准接口。☯内核驱动程序相机的驱动程序可与实际相机硬件以及您的 HAL 实现进行互动。相机和驱动程序必须支持 YV12 和 NV21 图片格式，以便在显示和视频录制时支持预览相机图片。 1.1.2、Qualcomm平台Camera 架构Qualcomm平台Camera 架构主要区别在于HAL层和Kernel层的变化，总体架构图如下： 1.1.2.1、Qualcomm平台Camera总体架构 1.1.2.2、Qualcomm平台Camera的HAL、mm-camera 1.1.2.3、Qualcomm平台Camera的Kernel 1.2、Android Camera API 2.0 全新的HAL 子系统Android 7.1.2现在使用的是Camera API 2.0 和 Camera Device 3以及 HAL3。 1.2.1、请求应用框架针对捕获的结果向相机子系统发出请求。一个请求对应一组结果。请求包含有关捕获和处理这些结果的所有配置信息。其中包括分辨率和像素格式；手动传感器、镜头和闪光灯控件；3A 操作模式；RAW 到 YUV 处理控件；以及统计信息的生成。这样一来，便可更好地控制结果的输出和处理。一次可发起多个请求，而且提交的请求不会出现阻塞的情况。请求始终按照接收的顺序进行处理。 1.2.2、HAL 和相机子系统相机子系统包括相机管道中组件的实现，例如 3A 算法和处理控件。相机 HAL 为您提供了实现您版本的这些组件所需的接口。为了保持多个设备制造商和图像信号处理器（ISP，也称为相机传感器）供应商之间的跨平台兼容性，相机管道模型是虚拟的，且不直接对应任何真正的 ISP。不过，它与真正的处理管道足够相似，因此您可以有效地将其映射到硬件。此外，它足够抽象，可支持多种不同的算法和操作顺序，而不会影响质量、效率或跨设备兼容性。相机管道还支持应用框架开启自动对焦等功能的触发器。它还会将通知发送回应用框架，以通知应用自动对焦锁定或错误等事件。 RAW Bayer 输出在 ISP 内部不经过任何处理。统计信息根据原始传感器数据生成。将原始传感器数据转换为 YUV 的各种处理块按任意顺序排列。当显示多个刻度和剪裁单元时，所有的缩放器单元共享输出区域控件（数字缩放）。不过，每个单元都可能具有不同的输出分辨率和像素格式。 1.2.3、HAL 操作摘要☯ 捕获的异步请求来自于框架。☯ HAL 设备必须按顺序处理请求。对于每个请求，均产生输出结果元数据以及一个或多个输出图片缓冲区。☯ 请求和结果以及后续请求引用的流遵守先进先出规则。☯ 指定请求的所有输出的时间戳必须完全相同，以便框架可以根据需要将它们匹配在一起。☯ 所有捕获配置和状态（不包括 3A 例程）都包含在请求和结果中。 1.2.4、启动和预期操作顺序1、框架调用 camera_module_t-&gt;common.open()，而这会返回一个 hardware_device_t 结构。2、框架检查 hardware_device_t-&gt;version 字段，并为该版本的相机硬件设备实例化相应的处理程序。如果版本是 CAMERA_DEVICE_API_VERSION_3_0，则该设备会转型为 camera3_device_t。3、框架调用 camera3_device_t-&gt;ops-&gt;initialize() 并显示框架回调函数指针。在调用 ops 结构中的任何其他函数之前，这只会在 open() 之后调用一次。4、框架调用 camera3_device_t-&gt;ops-&gt;configure_streams() 并显示到 HAL 设备的输入/输出流列表。5、框架为 configure_streams 中列出的至少一个输出流分配 gralloc 缓冲区并调用 camera3_device_t-&gt;ops-&gt;register_stream_buffers()。相同的流仅注册一次。6、框架通过调用 camera3_device_t-&gt;ops-&gt;construct_default_request_settings() 来为某些使用情形请求默认设置。这可能会在第 3 步之后的任何时间发生。7、框架通过基于其中一组默认设置的设置以及至少一个框架之前注册的输出流来构建第一个捕获请求并将其发送到 HAL。它通过 camera3_device_t-&gt;ops-&gt;process_capture_request() 发送到 HAL。HAL 必须阻止此调用返回，直到准备好发送下一个请求。8、框架继续提交请求，并且可能会为尚未注册的流调用 register_stream_buffers()，并调用 construct_default_request_settings 来为其他使用情形获取默认设置缓冲区。9、当请求捕获开始（传感器开始曝光以进行捕获）时，HAL 会调用 camera3_callback_ops_t-&gt;notify() 并显示 SHUTTER 事件，包括帧号和开始曝光的时间戳。此通知调用必须在第一次调用该帧号的 process_capture_result() 之前进行。10、在某个管道延迟后，HAL 开始使用 camera3_callback_ops_t-&gt;process_capture_result() 将完成的捕获返回到框架。这些捕获按照与提交请求相同的顺序返回。一次可发起多个请求，具体取决于相机 HAL 设备的管道深度。11、一段时间后，框架可能会停止提交新的请求、等待现有捕获完成（所有缓冲区都已填充，所有结果都已返回），然后再次调用 configure_streams()。这会重置相机硬件和管道，以获得一组新的输入/输出流。可重复使用先前配置中的部分流；如果这些流的缓冲区已经过 HAL 注册，则不会再次注册。如果至少还有一个已注册的输出流，则框架从第 7 步继续（否则，需要先完成第 5 步）。12、或者，框架可能会调用 camera3_device_t-&gt;common-&gt;close() 以结束相机会话。当框架中没有其他处于活动状态的调用时，它可能随时会被调用；尽管在所有发起的捕获完成（所有结果都已返回，所有缓冲区都已填充）之前，调用可能会阻塞。在 close 调用返回后，不允许再从 HAL 对 camera3_callback_ops_t 函数进行更多调用。一旦进行 close() 调用，该框架可能不会调用任何其他 HAL 设备函数。13、在发生错误或其他异步事件时，HAL 必须调用 camera3_callback_ops_t-&gt;notify() 并返回相应的错误/事件消息。从严重的设备范围错误通知返回后，HAL 应表现为在其上调用了 close()。但是，HAL 必须在调用 notify() 之前取消或完成所有待处理的捕获，以便在调用 notify() 并返回严重错误时，框架不会收到来自设备的更多回调。在严重的错误消息返回 notify() 方法后，close() 之外的方法应该返回 -ENODEV 或 NULL。 1.3、Android Graphics 学习－生产者、消费者、BufferQueue介绍 Graphics 系统详细分析请参考：【Android 7.1.2 (Android N) Android Graphics 系统分析】(http://zhoujinjian.cc/2018/02/01/Android-7-1-2-Android-N-Android-Graphics-%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90/) 1.4、Camera类之间的关系和作用1.4.1、Camera类关系总体概览 ☯ 1、ICameraClient: 这主要是一些消息发送的接口，包括帧可用通知，回调一些信息给client等消息。不过这里要注意的是，BnCameraClient对象其实是在client这端，不在CameraService端。☯ 2、ICamera:camera的一些标准操作接口，比如startpreview，takepicuture,autofocus,所有的操作动作都是用的这一套接口。☯ 3、ICameraService: 链接Camera服务，Camera device,获取Camera数量，Camera硬件信息，视厂角，镜头等信息。 1.4.2、ICameraClient 1.4.3、ICamera 1.4.4、ICameraService （二）、Android CameraService开机初始化分析首先看下总体时序图： 2.1、CameraService 初始化过程Android启动的时候会收集系统的.rc文件，启动对应的Native Service： 1234567[-&gt;\\frameworks\\av\\camera\\cameraserver\\cameraserver.rc]service cameraserver /system/bin/cameraserver class main user cameraserver group audio camera input drmrpc ioprio rt 4 writepid /dev/cpuset/camera-daemon/tasks /dev/stune/top-app/tasks 1234567891011[-&gt;\\frameworks\\av\\camera\\cameraserver\\main_cameraserver.cpp]int main(int argc __unused, char** argv __unused)&#123; signal(SIGPIPE, SIG_IGN); sp&lt;ProcessState&gt; proc(ProcessState::self()); sp&lt;IServiceManager&gt; sm = defaultServiceManager(); ALOGI(\"ServiceManager: %p\", sm.get()); CameraService::instantiate(); ProcessState::self()-&gt;startThreadPool(); IPCThreadState::self()-&gt;joinThreadPool();&#125; CameraService继承自BinderService，instantiate也是在BinderService中定义的，此方法就是调用publish方法，所以来看publish方法： 123456[-&gt;\\frameworks\\native\\include\\binder\\BinderService.h]static status_t publish(bool allowIsolated = false) &#123; sp&lt;IServiceManager&gt; sm(defaultServiceManager()); //将服务添加到ServiceManager return sm-&gt;addService(String16(SERVICE::getServiceName()),new SERVICE(), allowIsolated);&#125; 这里，将会把CameraService服务加入到ServiceManager进行管理。 CameraService的构造时，会调用CameraService的onFirstRef方法： 2.1.1、CameraService::onFirstRef()12345678910111213141516171819202122232425262728void CameraService::onFirstRef()&#123; ALOGI(\"CameraService process starting\"); BnCameraService::onFirstRef(); // Update battery life tracking if service is restarting BatteryNotifier&amp; notifier(BatteryNotifier::getInstance()); notifier.noteResetCamera(); notifier.noteResetFlashlight(); camera_module_t *rawModule; int err = hw_get_module(CAMERA_HARDWARE_MODULE_ID, (const hw_module_t **)&amp;rawModule); ...... mModule = new CameraModule(rawModule); err = mModule-&gt;init(); ...... mFlashlight = new CameraFlashlight(*mModule, *this); status_t res = mFlashlight-&gt;findFlashUnits(); ...... if (mModule-&gt;getModuleApiVersion() &gt;= CAMERA_MODULE_API_VERSION_2_1) &#123; mModule-&gt;setCallbacks(this); &#125; CameraService::pingCameraServiceProxy();&#125; 首先会通过HAL框架的hw_get_module来创建CameraModule对象，然后会对其进行相应的初始化，并会进行一些参数的设置，如camera的数量，闪光灯的初始化，以及回调函数的设置等，到这里，Camera2 HAL的模块就初始化结束了。 2.1.2、Camera 动态库加载过程在源码中不知大家有没有注意到第二个参数是hw_module_t module,这里是指针的指针，而我们刚才传的是camera_module_t指针。大家可以看到camera_module_t 结构第一个域就是hw_module_t 所以这里就不难理解了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102源码路径：hardware/libhardware/hardware.c/** Base path of the hal modules */#define HAL_LIBRARY_PATH1 \"/system/lib/hw\" #define HAL_LIBRARY_PATH2 \"/vendor/lib/hw\"int hw_get_module(const char *id, const struct hw_module_t **module)&#123; return hw_get_module_by_class(id, NULL, module); //这里的id就是camera模块的id，每一个hal module都有对应的id， //区分他们就通过这个id来区分了。 &#125;int hw_get_module_by_class(const char *class_id, const char *inst, const struct hw_module_t **module)&#123; int status; int i; const struct hw_module_t *hmi = NULL; char prop[PATH_MAX]; char path[PATH_MAX]; char name[PATH_MAX]; ...... /* Loop through the configuration variants looking for a module */ for (i=0 ; i&lt;HAL_VARIANT_KEYS_COUNT+1 ; i++) &#123; if (i &lt; HAL_VARIANT_KEYS_COUNT) &#123; if (property_get(variant_keys[i], prop, NULL) == 0) &#123; //关键字数组，上面有宏代码。 continue; &#125; snprintf(path, sizeof(path), \"%s/%s.%s.so\", HAL_LIBRARY_PATH2, name, prop); if (access(path, R_OK) == 0) break; snprintf(path, sizeof(path), \"%s/%s.%s.so\", //拼接完整的camera库。 HAL_LIBRARY_PATH1, name, prop); if (access(path, R_OK) == 0) break; &#125; else &#123; snprintf(path, sizeof(path), \"%s/%s.default.so\", HAL_LIBRARY_PATH2, name); if (access(path, R_OK) == 0) break; snprintf(path, sizeof(path), \"%s/%s.default.so\", HAL_LIBRARY_PATH1, name); if (access(path, R_OK) == 0) break; &#125; &#125; status = -ENOENT; if (i &lt; HAL_VARIANT_KEYS_COUNT+1) &#123; status = load(class_id, path, module); //如果上面都进行完毕，走到这里，说明已经找到库了，这里就去加载。 &#125; return status;&#125;//根据id来加载hal的modulestatic int load(const char *id, const char *path, const struct hw_module_t **pHmi)&#123; int status; void *handle; struct hw_module_t *hmi; ...... handle = dlopen(path, RTLD_NOW); //动态加载内存的api，这里的path=/system/lib/hw/camera.msm8996.so ...... /* Get the address of the struct hal_module_info. */ const char *sym = HAL_MODULE_INFO_SYM_AS_STR; //别的地方定义#define HAL_MODULE_INFO_SYM_AS_STR \"HMI\" hmi = (struct hw_module_t *)dlsym(handle, sym); //我们动态链接的是\"HMI\"这个符号。 ...... *pHmi = hmi; //最后将这个指针，赋给我们之前定义的 struct camera_module变量。这里模块就加载进来了。 return status;&#125;//hal代码[-&gt;\\hardware\\qcom\\camera\\QCamera2\\QCamera2Hal.cpp]static hw_module_t camera_common = &#123; .tag = HARDWARE_MODULE_TAG, .module_api_version = CAMERA_MODULE_API_VERSION_2_4, .hal_api_version = HARDWARE_HAL_API_VERSION, .id = CAMERA_HARDWARE_MODULE_ID, .name = \"QCamera Module\", .author = \"Qualcomm Innovation Center Inc\", .methods = &amp;qcamera::QCamera2Factory::mModuleMethods, //它的方法数组里绑定了open接口 .dso = NULL, .reserved = &#123;0&#125;&#125;;camera_module_t HAL_MODULE_INFO_SYM = &#123; .common = camera_common, .get_number_of_cameras = qcamera::QCamera2Factory::get_number_of_cameras, .get_camera_info = qcamera::QCamera2Factory::get_camera_info, .set_callbacks = qcamera::QCamera2Factory::set_callbacks, .get_vendor_tag_ops = qcamera::QCamera3VendorTags::get_vendor_tag_ops, .open_legacy = qcamera::QCamera2Factory::open_legacy, .set_torch_mode = qcamera::QCamera2Factory::set_torch_mode, .init = NULL, .reserved = &#123;0&#125;&#125;;struct hw_module_methods_t QCamera2Factory::mModuleMethods = &#123; //open方法的绑定 open: QCamera2Factory::camera_device_open,&#125;; Camera HAL层的open入口其实就是camera_device_open方法： 2.1.3、图解camera_module和camera_device_t关系camer module在系统中转指camera模块，camera_device_t 转指某一个camera 设备。在流程上，native framwork 先加载在hal层定义的camer_module对象，然后通过camera_module的methods open方法填充camera_device_t 结构体，并最终获取到camera ops这一整个camera最重要的操作集合。下图中我们可以看到struct hw_module_t在camera_module最上面 而camera_device_t最开始保存的是struct hw_device_t. 由此我们平时在看代码时，要注意一些指针转换。 （三）、Android Camera Open过程3.1、Camera2 HAL层Open()过程分析高通的Camera，它在后台会有一个守护进程daemon，daemon是介于应用和驱动之间翻译ioctl的中间层(委托处理)。本节将以Camera中的open流程为例，来分析Camera HAL的工作过程，在应用对硬件发出open请求后，会通过Camera HAL来发起open请求，而Camera HAL的open入口在QCamera2Hal.cpp进行了定义，即前面分析的Camera HAL层的open入口其实就是camera_device_open方法： 123456[-&gt;\\hardware\\qcom\\camera\\QCamera2\\QCamera2Factory.cpp]int QCamera2Factory::camera_device_open(const struct hw_module_t *module, const char *id, struct hw_device_t **hw_device)&#123; ... return gQCamera2Factory-&gt;cameraDeviceOpen(atoi(id), hw_device);&#125; 它调用了cameraDeviceOpen方法，而其中的hw_device就是最后要返回给应用层的CameraDeviceImpl在Camera HAL层的对象，继续分析cameraDeviceOpen方法： 12345678910111213141516171819202122[-&gt;\\hardware\\qcom\\camera\\QCamera2\\QCamera2Factory.cpp]int QCamera2Factory::cameraDeviceOpen(int camera_id, struct hw_device_t **hw_device)&#123; ... //Camera2采用的Camera HAL版本为HAL3.0 if ( mHalDescriptors[camera_id].device_version == CAMERA_DEVICE_API_VERSION_3_0 ) &#123; //初始化QCamera3HardwareInterface对象，这里构造函数里将会进行configure_streams以及 //process_capture_result等的绑定 QCamera3HardwareInterface *hw = new QCamera3HardwareInterface( mHalDescriptors[camera_id].cameraId, mCallbacks); //通过QCamera3HardwareInterface来打开Camera rc = hw-&gt;openCamera(hw_device); ... &#125; else if (mHalDescriptors[camera_id].device_version == CAMERA_DEVICE_API_VERSION_1_0) &#123; //HAL API为2.0 QCamera2HardwareInterface *hw = new QCamera2HardwareInterface((uint32_t)camera_id); rc = hw-&gt;openCamera(hw_device); ... &#125; else &#123; ... &#125; return rc;&#125; 此方法有两个关键点：一个是QCamera3HardwareInterface对象的创建，它是用户空间与内核空间进行交互的接口；另一个是调用它的openCamera方法来打开Camera，下面将分别进行分析。 3.1.1、QCamera3HardwareInterface构造函数分析在它的构造函数里面有一个关键的初始化，即mCameraDevice.ops = &amp;mCameraOps，它会定义Device操作的接口： 12345678910111213141516[-&gt;\\frameworks\\base\\core\\java\\android\\hardware\\camera2\\impl\\CameraDeviceImpl.java]camera3_device_ops_t QCamera3HardwareInterface::mCameraOps = &#123; initialize: QCamera3HardwareInterface::initialize, //配置流数据的相关处理 configure_streams: QCamera3HardwareInterface::configure_streams, register_stream_buffers: NULL, construct_default_request_settings: QCamera3HardwareInterface::construct_default_request_settings, //处理结果的接口 process_capture_request: QCamera3HardwareInterface::process_capture_request, get_metadata_vendor_tag_ops: NULL, dump: QCamera3HardwareInterface::dump, flush: QCamera3HardwareInterface::flush, reserved: &#123;0&#125;,&#125;; 其中，会在configure_streams中配置好流的处理handle： 1234567891011[-&gt;\\frameworks\\base\\core\\java\\android\\hardware\\camera2\\impl\\CameraDeviceImpl.java]int QCamera3HardwareInterface::configure_streams(const struct camera3_device *device, camera3_stream_configuration_t *stream_list)&#123; //获得QCamera3HardwareInterface对象 QCamera3HardwareInterface *hw =reinterpret_cast&lt;QCamera3HardwareInterface *&gt;(device-&gt;priv); ... //调用它的configureStreams进行配置 int rc = hw-&gt;configureStreams(stream_list); .. return rc;&#125; 继续追踪configureStream方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165[-&gt;\\frameworks\\base\\core\\java\\android\\hardware\\camera2\\impl\\CameraDeviceImpl.java]int QCamera3HardwareInterface::configureStreams(camera3_stream_configuration_t *streamList)&#123; ... //初始化Camera版本 al_version = CAM_HAL_V3; ... //开始配置stream ... //初始化相关Channel为NULL if (mMetadataChannel) &#123; delete mMetadataChannel; mMetadataChannel = NULL; &#125; if (mSupportChannel) &#123; delete mSupportChannel; mSupportChannel = NULL; &#125; if (mAnalysisChannel) &#123; delete mAnalysisChannel; mAnalysisChannel = NULL; &#125; //创建Metadata Channel，并对其进行初始化 mMetadataChannel = new QCamera3MetadataChannel(mCameraHandle-&gt;camera_handle, mCameraHandle-&gt;ops, captureResultCb,&amp;gCamCapability[mCameraId]-&gt;padding_info, CAM_QCOM_FEATURE_NONE, this); ... //初始化 rc = mMetadataChannel-&gt;initialize(IS_TYPE_NONE); ... //如果h/w support可用，则创建分析stream的Channel if (gCamCapability[mCameraId]-&gt;hw_analysis_supported) &#123; mAnalysisChannel = new QCamera3SupportChannel(mCameraHandle-&gt;camera_handle, mCameraHandle-&gt;ops,&amp;gCamCapability[mCameraId]-&gt;padding_info, CAM_QCOM_FEATURE_PP_SUPERSET_HAL3,CAM_STREAM_TYPE_ANALYSIS, &amp;gCamCapability[mCameraId]-&gt;analysis_recommended_res,this); ... &#125; bool isRawStreamRequested = false; //清空stream配置信息 memset(&amp;mStreamConfigInfo, 0, sizeof(cam_stream_size_info_t)); //为requested stream分配相关的channel对象 for (size_t i = 0; i &lt; streamList-&gt;num_streams; i++) &#123; camera3_stream_t *newStream = streamList-&gt;streams[i]; uint32_t stream_usage = newStream-&gt;usage; mStreamConfigInfo.stream_sizes[mStreamConfigInfo.num_streams].width = (int32_t)newStream- &gt;width; mStreamConfigInfo.stream_sizes[mStreamConfigInfo.num_streams].height = (int32_t)newStream- &gt;height; if ((newStream-&gt;stream_type == CAMERA3_STREAM_BIDIRECTIONAL||newStream-&gt;usage &amp; GRALLOC_USAGE_HW_CAMERA_ZSL) &amp;&amp;newStream-&gt;format == HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED &amp;&amp; jpegStream)&#123; mStreamConfigInfo.type[mStreamConfigInfo.num_streams] = CAM_STREAM_TYPE_SNAPSHOT; mStreamConfigInfo.postprocess_mask[mStreamConfigInfo.num_streams] = CAM_QCOM_FEATURE_NONE; &#125; else if(newStream-&gt;stream_type == CAMERA3_STREAM_INPUT) &#123; &#125; else &#123; switch (newStream-&gt;format) &#123; //为非zsl streams查找他们的format ... &#125; &#125; if (newStream-&gt;priv == NULL) &#123; //为新的stream构造Channel switch (newStream-&gt;stream_type) &#123;//分类型构造 case CAMERA3_STREAM_INPUT: newStream-&gt;usage |= GRALLOC_USAGE_HW_CAMERA_READ; newStream-&gt;usage |= GRALLOC_USAGE_HW_CAMERA_WRITE;//WR for inplace algo's break; case CAMERA3_STREAM_BIDIRECTIONAL: ... break; case CAMERA3_STREAM_OUTPUT: ... break; default: break; &#125; //根据前面的得到的stream的参数类型以及format分别对各类型的channel进行构造 if (newStream-&gt;stream_type == CAMERA3_STREAM_OUTPUT || newStream-&gt;stream_type == CAMERA3_STREAM_BIDIRECTIONAL) &#123; QCamera3Channel *channel = NULL; switch (newStream-&gt;format) &#123; case HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED: /* use higher number of buffers for HFR mode */ ... //创建Regular Channel channel = new QCamera3RegularChannel(mCameraHandle-&gt;camera_handle, mCameraHandle-&gt;ops, captureResultCb,&amp;gCamCapability[mCameraId]- &gt;padding_info,this,newStream,(cam_stream_type_t)mStreamConfigInfo.type[ mStreamConfigInfo.num_streams],mStreamConfigInfo.postprocess_mask[ mStreamConfigInfo.num_streams],mMetadataChannel,numBuffers); ... newStream-&gt;max_buffers = channel-&gt;getNumBuffers(); newStream-&gt;priv = channel; break; case HAL_PIXEL_FORMAT_YCbCr_420_888: //创建YWV Channel ... break; case HAL_PIXEL_FORMAT_RAW_OPAQUE: case HAL_PIXEL_FORMAT_RAW16: case HAL_PIXEL_FORMAT_RAW10: //创建Raw Channel ... break; case HAL_PIXEL_FORMAT_BLOB: //创建QCamera3PicChannel ... break; default: break; &#125; &#125; else if (newStream-&gt;stream_type == CAMERA3_STREAM_INPUT) &#123; newStream-&gt;max_buffers = MAX_INFLIGHT_REPROCESS_REQUESTS; &#125; else &#123; &#125; for (List&lt;stream_info_t*&gt;::iterator it=mStreamInfo.begin();it != mStreamInfo.end(); it++) &#123; if ((*it)-&gt;stream == newStream) &#123; (*it)-&gt;channel = (QCamera3Channel*) newStream-&gt;priv; break; &#125; &#125; &#125; else &#123; &#125; if (newStream-&gt;stream_type != CAMERA3_STREAM_INPUT) mStreamConfigInfo.num_streams++; &#125; &#125; if (isZsl) &#123; if (mPictureChannel) &#123; mPictureChannel-&gt;overrideYuvSize(zslStream-&gt;width, zslStream-&gt;height); &#125; &#125; else if (mPictureChannel &amp;&amp; m_bIs4KVideo) &#123; mPictureChannel-&gt;overrideYuvSize(videoWidth, videoHeight); &#125; //RAW DUMP channel if (mEnableRawDump &amp;&amp; isRawStreamRequested == false)&#123; cam_dimension_t rawDumpSize; rawDumpSize = getMaxRawSize(mCameraId); mRawDumpChannel = new QCamera3RawDumpChannel(mCameraHandle-&gt;camera_handle, mCameraHandle-&gt;ops,rawDumpSize,&amp;gCamCapability[mCameraId]-&gt;padding_info, this, CAM_QCOM_FEATURE_NONE); ... &#125; //进行相关Channel的配置 ... /* Initialize mPendingRequestInfo and mPendnigBuffersMap */ for (List&lt;PendingRequestInfo&gt;::iterator i = mPendingRequestsList.begin(); i != mPendingRequestsList.end(); i++) &#123; clearInputBuffer(i-&gt;input_buffer); i = mPendingRequestsList.erase(i); &#125; mPendingFrameDropList.clear(); // Initialize/Reset the pending buffers list mPendingBuffersMap.num_buffers = 0; mPendingBuffersMap.mPendingBufferList.clear(); mPendingReprocessResultList.clear(); return rc;&#125; 此方法内容比较多，只抽取其中核心的代码进行说明，它首先会根据HAL的版本来对stream进行相应的配置初始化，然后再根据stream类型对stream_list的stream创建相应的Channel，主要有QCamera3MetadataChannel，QCamera3SupportChannel等，然后再进行相应的配置，其中QCamera3MetadataChannel在后面的处理capture request的时候会用到，这里就不做分析，而Camerametadata则是Java层和CameraService之间传递的元数据，见android6.0源码分析之Camera API2.0简介中的Camera2架构图，至此，QCamera3HardwareInterface构造结束，与本文相关的就是配置了mCameraDevice.ops。 3.1.2、openCamera()分析本节主要分析Module是如何打开Camera的，openCamera的代码如下： 123456789101112131415161718[-&gt;\\hardware\\qcom\\camera\\QCamera2\\HAL3\\QCamera3HWI.cpp]int QCamera3HardwareInterface::openCamera(struct hw_device_t **hw_device)&#123; int rc = 0; if (mCameraOpened) &#123;//如果Camera已经被打开，则此次打开的设备为NULL，并且打开结果为PERMISSION_DENIED *hw_device = NULL; return PERMISSION_DENIED; &#125; //调用openCamera方法来打开 rc = openCamera(); //打开结果处理 if (rc == 0) &#123; //获取打开成功的hw_device_t对象 *hw_device = &amp;mCameraDevice.common; &#125; else *hw_device = NULL; &#125; return rc;&#125; 它调用了openCamera()方法来打开Camera: 12345678910111213[-&gt;\\hardware\\qcom\\camera\\QCamera2\\HAL3\\QCamera3HWI.cpp]int QCamera3HardwareInterface::openCamera()&#123; ... //打开camera，获取mCameraHandle mCameraHandle = camera_open((uint8_t)mCameraId); ... mCameraOpened = true; //注册mm-camera-interface里的事件处理,其中camEctHandle为事件处理Handle rc = mCameraHandle-&gt;ops-&gt;register_event_notify(mCameraHandle-&gt;camera_handle,camEvtHandle ,(void *)this); return NO_ERROR;&#125; 它调用camera_open方法来打开Camera，并且向CameraHandle注册了Camera 时间处理的Handle–camEvtHandle，首先分析camera_open方法，这里就将进入高通的Camera的实现了，而Mm_camera_interface.c是高通提供的相关操作的接口，接下来分析高通Camera的camera_open方法： 1234567891011121314151617181920212223242526272829303132333435[-&gt;\\vendor\\qcom\\proprietary\\mm-camera\\apps\\appslib\\mm_camera_interface.c]mm_camera_vtbl_t * camera_open(uint8_t camera_idx)&#123; int32_t rc = 0; mm_camera_obj_t* cam_obj = NULL; /* opened already 如果已经打开*/ if(NULL != g_cam_ctrl.cam_obj[camera_idx]) &#123; /* Add reference */ g_cam_ctrl.cam_obj[camera_idx]-&gt;ref_count++; pthread_mutex_unlock(&amp;g_intf_lock); return &amp;g_cam_ctrl.cam_obj[camera_idx]-&gt;vtbl; &#125; cam_obj = (mm_camera_obj_t *)malloc(sizeof(mm_camera_obj_t)); ... /* initialize camera obj */ memset(cam_obj, 0, sizeof(mm_camera_obj_t)); cam_obj-&gt;ctrl_fd = -1; cam_obj-&gt;ds_fd = -1; cam_obj-&gt;ref_count++; cam_obj-&gt;my_hdl = mm_camera_util_generate_handler(camera_idx); cam_obj-&gt;vtbl.camera_handle = cam_obj-&gt;my_hdl; /* set handler */ //mm_camera_ops里绑定了相关的操作接口 cam_obj-&gt;vtbl.ops = &amp;mm_camera_ops; pthread_mutex_init(&amp;cam_obj-&gt;cam_lock, NULL); pthread_mutex_lock(&amp;cam_obj-&gt;cam_lock); pthread_mutex_unlock(&amp;g_intf_lock); //调用mm_camera_open方法来打开camera rc = mm_camera_open(cam_obj); pthread_mutex_lock(&amp;g_intf_lock); ... //结果处理，并返回 ...&#125; 由代码可知，这里将会初始化一个mm_camera_obj_t对象，其中，ds_fd为socket fd，而mm_camera_ops则绑定了相关的接口，最后调用mm_camera_open来打开Camera，首先来看看mm_camera_ops绑定了哪些方法： 1234567891011121314151617181920212223242526272829303132333435363738[-&gt;\\vendor\\qcom\\proprietary\\mm-camera\\apps\\appslib\\mm_camera_interface.c]static mm_camera_ops_t mm_camera_ops = &#123; .query_capability = mm_camera_intf_query_capability, //注册事件通知的方法 .register_event_notify = mm_camera_intf_register_event_notify, .close_camera = mm_camera_intf_close, .set_parms = mm_camera_intf_set_parms, .get_parms = mm_camera_intf_get_parms, .do_auto_focus = mm_camera_intf_do_auto_focus, .cancel_auto_focus = mm_camera_intf_cancel_auto_focus, .prepare_snapshot = mm_camera_intf_prepare_snapshot, .start_zsl_snapshot = mm_camera_intf_start_zsl_snapshot, .stop_zsl_snapshot = mm_camera_intf_stop_zsl_snapshot, .map_buf = mm_camera_intf_map_buf, .unmap_buf = mm_camera_intf_unmap_buf, .add_channel = mm_camera_intf_add_channel, .delete_channel = mm_camera_intf_del_channel, .get_bundle_info = mm_camera_intf_get_bundle_info, .add_stream = mm_camera_intf_add_stream, .link_stream = mm_camera_intf_link_stream, .delete_stream = mm_camera_intf_del_stream, //配置stream的方法 .config_stream = mm_camera_intf_config_stream, .qbuf = mm_camera_intf_qbuf, .get_queued_buf_count = mm_camera_intf_get_queued_buf_count, .map_stream_buf = mm_camera_intf_map_stream_buf, .unmap_stream_buf = mm_camera_intf_unmap_stream_buf, .set_stream_parms = mm_camera_intf_set_stream_parms, .get_stream_parms = mm_camera_intf_get_stream_parms, .start_channel = mm_camera_intf_start_channel, .stop_channel = mm_camera_intf_stop_channel, .request_super_buf = mm_camera_intf_request_super_buf, .cancel_super_buf_request = mm_camera_intf_cancel_super_buf_request, .flush_super_buf_queue = mm_camera_intf_flush_super_buf_queue, .configure_notify_mode = mm_camera_intf_configure_notify_mode, //处理capture的方法 .process_advanced_capture = mm_camera_intf_process_advanced_capture&#125;; 接着分析mm_camera_open方法： 12345678910111213141516171819202122232425262728293031323334353637[-&gt;/hardware/qcom/camera/QCamera2/stack/mm-camera-interface/src/mm_camera.c]int32_t mm_camera_open(mm_camera_obj_t *my_obj)&#123; ... do&#123; n_try--; //根据设备名字，打开相应的设备驱动fd my_obj-&gt;ctrl_fd = open(dev_name, O_RDWR | O_NONBLOCK); if((my_obj-&gt;ctrl_fd &gt;= 0) || (errno != EIO) || (n_try &lt;= 0 )) &#123; break; &#125; usleep(sleep_msec * 1000U); &#125;while (n_try &gt; 0); ... //打开domain socket n_try = MM_CAMERA_DEV_OPEN_TRIES; do &#123; n_try--; my_obj-&gt;ds_fd = mm_camera_socket_create(cam_idx, MM_CAMERA_SOCK_TYPE_UDP); usleep(sleep_msec * 1000U); &#125; while (n_try &gt; 0); ... //初始化锁 pthread_mutex_init(&amp;my_obj-&gt;msg_lock, NULL); pthread_mutex_init(&amp;my_obj-&gt;cb_lock, NULL); pthread_mutex_init(&amp;my_obj-&gt;evt_lock, NULL); pthread_cond_init(&amp;my_obj-&gt;evt_cond, NULL); //开启线程，它的线程体在mm_camera_dispatch_app_event方法中 mm_camera_cmd_thread_launch(&amp;my_obj-&gt;evt_thread, mm_camera_dispatch_app_event, (void *)my_obj); mm_camera_poll_thread_launch(&amp;my_obj-&gt;evt_poll_thread, MM_CAMERA_POLL_TYPE_EVT); mm_camera_evt_sub(my_obj, TRUE); return rc; ...&#125; 由代码可知，它会打开Camera的设备文件，然后开启dispatch_app_event线程，线程方法体mm_camera_dispatch_app_event方法代码如下： 1234567891011121314151617181920[-&gt;/hardware/qcom/camera/QCamera2/stack/mm-camera-interface/src/mm_camera.c]static void mm_camera_dispatch_app_event(mm_camera_cmdcb_t *cmd_cb,void* user_data)&#123; mm_camera_cmd_thread_name(\"mm_cam_event\"); int i; mm_camera_event_t *event = &amp;cmd_cb-&gt;u.evt; mm_camera_obj_t * my_obj = (mm_camera_obj_t *)user_data; if (NULL != my_obj) &#123; pthread_mutex_lock(&amp;my_obj-&gt;cb_lock); for(i = 0; i &lt; MM_CAMERA_EVT_ENTRY_MAX; i++) &#123; if(my_obj-&gt;evt.evt[i].evt_cb) &#123; //调用camEvtHandle方法 my_obj-&gt;evt.evt[i].evt_cb( my_obj-&gt;my_hdl, event, my_obj-&gt;evt.evt[i].user_data); &#125; &#125; pthread_mutex_unlock(&amp;my_obj-&gt;cb_lock); &#125;&#125; 最后会调用mm-camera-interface中注册好的事件处理evt_cb，它就是在前面注册好的camEvtHandle： 12345678910111213141516171819202122232425262728293031[-&gt;\\hardware\\qcom\\camera\\QCamera2\\HAL3\\QCamera3HWI.cpp]void QCamera3HardwareInterface::camEvtHandle(uint32_t /*camera_handle*/,mm_camera_event_t *evt, void *user_data)&#123; //获取QCamera3HardwareInterface接口指针 QCamera3HardwareInterface *obj = (QCamera3HardwareInterface *)user_data; if (obj &amp;&amp; evt) &#123; switch(evt-&gt;server_event_type) &#123; case CAM_EVENT_TYPE_DAEMON_DIED: camera3_notify_msg_t notify_msg; memset(&amp;notify_msg, 0, sizeof(camera3_notify_msg_t)); notify_msg.type = CAMERA3_MSG_ERROR; notify_msg.message.error.error_code = CAMERA3_MSG_ERROR_DEVICE; notify_msg.message.error.error_stream = NULL; notify_msg.message.error.frame_number = 0; obj-&gt;mCallbackOps-&gt;notify(obj-&gt;mCallbackOps, &amp;notify_msg); break; case CAM_EVENT_TYPE_DAEMON_PULL_REQ: pthread_mutex_lock(&amp;obj-&gt;mMutex); obj-&gt;mWokenUpByDaemon = true; //开启process_capture_request obj-&gt;unblockRequestIfNecessary(); pthread_mutex_unlock(&amp;obj-&gt;mMutex); break; default: break; &#125; &#125; else &#123; &#125;&#125; 由代码可知，它会调用QCamera3HardwareInterface的unblockRequestIfNecessary来发起结果处理请求： 1234567[-&gt;\\hardware\\qcom\\camera\\QCamera2\\HAL3\\QCamera3HWI.cpp]void QCamera3HardwareInterface::unblockRequestIfNecessary()&#123; // Unblock process_capture_request //开启process_capture_request pthread_cond_signal(&amp;mRequestCond);&#125; 在初始化QCamera3HardwareInterface对象的时候，就绑定了处理Metadata的回调captureResultCb方法：它主要是对数据源进行相应的处理，而具体的capture请求的结果处理还是由process_capture_request来进行处理的，而这里会调用方法unblockRequestIfNecessary来触发process_capture_request方法执行，而在Camera框架中，发起请求时会启动一个RequestThread线程，在它的threadLoop方法中，会不停的调用process_capture_request方法来进行请求的处理，而它最后会回调Camera3Device中的processCaptureResult方法来进行结果处理： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475[-&gt;/frameworks/av/services/camera/libcameraservice/device3/Camera3Device.cpp]void Camera3Device::processCaptureResult(const camera3_capture_result *result) &#123; ... &#123; ... if (mUsePartialResult &amp;&amp; result-&gt;result != NULL) &#123; if (mDeviceVersion &gt;= CAMERA_DEVICE_API_VERSION_3_2) &#123; ... if (isPartialResult) &#123; request.partialResult.collectedResult.append(result-&gt;result); &#125; &#125; else &#123; camera_metadata_ro_entry_t partialResultEntry; res = find_camera_metadata_ro_entry(result-&gt;result, ANDROID_QUIRKS_PARTIAL_RESULT, &amp;partialResultEntry); if (res != NAME_NOT_FOUND &amp;&amp;partialResultEntry.count &gt; 0 &amp;&amp; partialResultEntry.data.u8[0] ==ANDROID_QUIRKS_PARTIAL_RESULT_PARTIAL) &#123; isPartialResult = true; request.partialResult.collectedResult.append( result-&gt;result); request.partialResult.collectedResult.erase( ANDROID_QUIRKS_PARTIAL_RESULT); &#125; &#125; if (isPartialResult) &#123; // Fire off a 3A-only result if possible if (!request.partialResult.haveSent3A) &#123; //处理3A结果 request.partialResult.haveSent3A =processPartial3AResult(frameNumber, request.partialResult.collectedResult,request.resultExtras); &#125; &#125; &#125; ... //查找camera元数据入口 camera_metadata_ro_entry_t entry; res = find_camera_metadata_ro_entry(result-&gt;result, ANDROID_SENSOR_TIMESTAMP, &amp;entry); if (shutterTimestamp == 0) &#123; request.pendingOutputBuffers.appendArray(result-&gt;output_buffers, result-&gt;num_output_buffers); &#125; else &#123; 重要的分析//返回处理的outputbuffer returnOutputBuffers(result-&gt;output_buffers, result-&gt;num_output_buffers, shutterTimestamp); &#125; if (result-&gt;result != NULL &amp;&amp; !isPartialResult) &#123; if (shutterTimestamp == 0) &#123; request.pendingMetadata = result-&gt;result; request.partialResult.collectedResult = collectedPartialResult; &#125; else &#123; CameraMetadata metadata; metadata = result-&gt;result; //发送Capture结构，即调用通知回调 sendCaptureResult(metadata, request.resultExtras, collectedPartialResult, frameNumber, hasInputBufferInRequest, request.aeTriggerCancelOverride); &#125; &#125; removeInFlightRequestIfReadyLocked(idx); &#125; // scope for mInFlightLock if (result-&gt;input_buffer != NULL) &#123; if (hasInputBufferInRequest) &#123; Camera3Stream *stream = Camera3Stream::cast(result-&gt;input_buffer-&gt;stream); 重要的分析//返回处理的inputbuffer res = stream-&gt;returnInputBuffer(*(result-&gt;input_buffer)); &#125; else &#123;&#125; &#125;&#125; 分析returnOutputBuffers方法，inputbuffer的runturnInputBuffer方法流程类似： 12345678910[-&gt;/frameworks/av/services/camera/libcameraservice/device3/Camera3Device.cpp]void Camera3Device::returnOutputBuffers(const camera3_stream_buffer_t *outputBuffers, size_t numBuffers, nsecs_t timestamp) &#123; for (size_t i = 0; i &lt; numBuffers; i++) &#123; Camera3Stream *stream = Camera3Stream::cast(outputBuffers[i].stream); status_t res = stream-&gt;returnBuffer(outputBuffers[i], timestamp); ... &#125;&#125; 方法里调用了returnBuffer方法： 12345678910[-&gt;/frameworks/av/services/camera/libcameraservice/device3/Camera3Stream.cpp]status_t Camera3Stream::returnBuffer(const camera3_stream_buffer &amp;buffer,nsecs_t timestamp) &#123; //返回buffer status_t res = returnBufferLocked(buffer, timestamp); if (res == OK) &#123; fireBufferListenersLocked(buffer, /*acquired*/false, /*output*/true); mOutputBufferReturnedSignal.signal(); &#125; return res;&#125; 再继续看returnBufferLocked,它调用了returnAnyBufferLocked方法，而returnAnyBufferLocked方法又调用了returnBufferCheckedLocked方法，现在分析returnBufferCheckedLocked： 12345678910111213141516171819202122232425[-&gt;/frameworks/av/services/camera/libcameraservice/device3/Camera3OutputStream.cpp]status_t Camera3OutputStream::returnBufferCheckedLocked(const camera3_stream_buffer &amp;buffer, nsecs_t timestamp,bool output,/*out*/sp&lt;Fence&gt; *releaseFenceOut) &#123; ... // Fence management - always honor release fence from HAL sp&lt;Fence&gt; releaseFence = new Fence(buffer.release_fence); int anwReleaseFence = releaseFence-&gt;dup(); if (buffer.status == CAMERA3_BUFFER_STATUS_ERROR) &#123; // Cancel buffer res = currentConsumer-&gt;cancelBuffer(currentConsumer.get(), container_of(buffer.buffer, ANativeWindowBuffer, handle), anwReleaseFence); ... &#125; else &#123; ... res = currentConsumer-&gt;queueBuffer(currentConsumer.get(), container_of(buffer.buffer, ANativeWindowBuffer, handle), anwReleaseFence); ... &#125; ... return res;&#125; 由代码可知，如果Buffer没有出现状态错误，它会调用currentConsumer的queueBuffer方法，而具体的Consumer则是在应用层初始化Camera时进行绑定的，典型的Consumer有SurfaceTexture，ImageReader等，而在Native层中，它会调用BufferQueueProducer的queueBuffer方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[-&gt;\\frameworks\\native\\libs\\gui\\BufferQueueProducer.cpp]status_t BufferQueueProducer::queueBuffer(int slot, const QueueBufferInput &amp;input, QueueBufferOutput *output) &#123; ... //初始化Frame可用的监听器 sp&lt;IConsumerListener&gt; frameAvailableListener; sp&lt;IConsumerListener&gt; frameReplacedListener; int callbackTicket = 0; BufferItem item; &#123; // Autolock scope ... const sp&lt;GraphicBuffer&gt;&amp; graphicBuffer(mSlots[slot].mGraphicBuffer); Rect bufferRect(graphicBuffer-&gt;getWidth(), graphicBuffer-&gt;getHeight()); Rect croppedRect; crop.intersect(bufferRect, &amp;croppedRect); ... //如果队列为空 if (mCore-&gt;mQueue.empty()) &#123; mCore-&gt;mQueue.push_back(item); frameAvailableListener = mCore-&gt;mConsumerListener; &#125; else &#123; //否则，不为空，对Buffer进行处理，并获取FrameAvailableListener监听 BufferQueueCore::Fifo::iterator front(mCore-&gt;mQueue.begin()); if (front-&gt;mIsDroppable) &#123; if (mCore-&gt;stillTracking(front)) &#123; mSlots[front-&gt;mSlot].mBufferState = BufferSlot::FREE; mCore-&gt;mFreeBuffers.push_front(front-&gt;mSlot); &#125; *front = item; frameReplacedListener = mCore-&gt;mConsumerListener; &#125; else &#123; mCore-&gt;mQueue.push_back(item); frameAvailableListener = mCore-&gt;mConsumerListener; &#125; &#125; mCore-&gt;mBufferHasBeenQueued = true; mCore-&gt;mDequeueCondition.broadcast(); output-&gt;inflate(mCore-&gt;mDefaultWidth, mCore-&gt;mDefaultHeight,mCore-&gt;mTransformHint, static_cast&lt;uint32_t&gt;(mCore-&gt;mQueue.size())); // Take a ticket for the callback functions callbackTicket = mNextCallbackTicket++; mCore-&gt;validateConsistencyLocked(); &#125; // Autolock scope ... &#123; ... if (frameAvailableListener != NULL) &#123; //回调SurfaceTexture中定义好的监听IConsumerListener的onFrameAvailable方法来对数据进行处理 frameAvailableListener-&gt;onFrameAvailable(item); &#125; else if (frameReplacedListener != NULL) &#123; frameReplacedListener-&gt;onFrameReplaced(item); &#125; ++mCurrentCallbackTicket; mCallbackCondition.broadcast(); &#125; return NO_ERROR;&#125; 由代码可知，它最后会调用Consumer的回调FrameAvailableListener的onFrameAvailable方法，到这里，就比较清晰为什么我们在写Camera应用，为其初始化Surface时，我们需要重写FrameAvailableListener了，因为在此方法里面，会进行结果的处理，至此，Camera HAL的Open流程就分析结束了。下面给出流程的时序图： （四）、Camera API2.0 初始化流程分析4.1、Camera2 应用层（Java层）Open()过程分析Camera2的初始化流程与Camera1.0有所区别，本文将就Camera2的内置应用来分析Camera2.0的初始化过程。Camera2.0首先启动的是CameraActivity，而它继承自QuickActivity，在代码中你会发现没有重写OnCreate等生命周期方法，因为此处采用的是模板方法的设计模式，在QuickActivity中的onCreate方法调用的是onCreateTasks等方法，所以要看onCreate方法就只须看onCreateTasks方法即可： 12345678910111213141516171819202122232425[-&gt;/packages/apps/Camera2/src/com/android/camera/CameraActivity.java]Overridepublic void onCreateTasks(Bundle state) &#123; Profile profile = mProfiler.create(\"CameraActivity.onCreateTasks\") .start(); ... mOnCreateTime = System.currentTimeMillis(); mAppContext = getApplicationContext(); mMainHandler = new MainHandler(this, getMainLooper()); … try &#123; //初始化OneCameraOpener对象 ①mOneCameraOpener = OneCameraModule.provideOneCameraOpener( mFeatureConfig, mAppContext,mActiveCameraDeviceTracker, ResolutionUtil.getDisplayMetrics(this)); mOneCameraManager = OneCameraModule.provideOneCameraManager(); &#125; catch (OneCameraException e) &#123;...&#125; … //建立模块信息 ②ModulesInfo.setupModules(mAppContext, mModuleManager, mFeatureConfig); … //进行初始化 ③mCurrentModule.init(this, isSecureCamera(), isCaptureIntent()); …&#125; 如代码所示，重要的有以上三点，先看第一点： 1234567891011121314[-&gt;/packages/apps/Camera2/src/com/android/camera/one/OneCameraModule.java]public static OneCameraOpener provideOneCameraOpener(OneCameraFeatureConfig featureConfig, Context context, ActiveCameraDeviceTracker activeCameraDeviceTracker,DisplayMetrics displayMetrics) throws OneCameraException &#123; //创建OneCameraOpener对象 Optional&lt;OneCameraOpener&gt; manager = Camera2OneCameraOpenerImpl.create( featureConfig, context, activeCameraDeviceTracker, displayMetrics); if (!manager.isPresent()) &#123; manager = LegacyOneCameraOpenerImpl.create(); &#125; ... return manager.get();&#125; 它调用Camera2OneCameraOpenerImpl的create方法来获得一个OneCameraOpener对象，以供CameraActivity之后的操作使用，继续看create方法： 123456789101112131415[-&gt;/packages/apps/Camera2/src/com/android/camera/one/OneCameraModule.java]public static Optional&lt;OneCameraOpener&gt; create(OneCameraFeatureConfig featureConfig, Context context, ActiveCameraDeviceTracker activeCameraDeviceTracker, DisplayMetrics displayMetrics) &#123; ... CameraManager cameraManager; try &#123; cameraManager = AndroidServices.instance().provideCameraManager(); &#125; catch (IllegalStateException ex) &#123;...&#125; //新建一个Camera2OneCameraOpenerImpl对象 OneCameraOpener oneCameraOpener = new Camera2OneCameraOpenerImpl( featureConfig, context, cameraManager, activeCameraDeviceTracker, displayMetrics); return Optional.of(oneCameraOpener);&#125; 很明显，它首先获取一个cameraManger对象，然后根据这个cameraManager对象来新创建了一个Camera2OneCameraOpenerImpl对象，所以第一步主要是为了获取一个OneCameraOpener对象，它的实现为Camera2OneCameraOpenerImpl类。继续看第二步，ModulesInfo.setupModules: 1234567891011121314151617181920212223242526272829303132333435363738394041424344[-&gt;/packages/apps/Camera2/src/com/android/camera/module/ModulesInfo.java]public static void setupModules(Context context, ModuleManager moduleManager, OneCameraFeatureConfig config) &#123; Resources res = context.getResources(); int photoModuleId = context.getResources().getInteger( R.integer.camera_mode_photo); //注册Photo模块 registerPhotoModule(moduleManager, photoModuleId, SettingsScopeNamespaces.PHOTO,config.isUsingCaptureModule()); //计算你还Photo模块设置为默认的模块 moduleManager.setDefaultModuleIndex(photoModuleId); //注册Videa模块 registerVideoModule(moduleManager, res.getInteger( R.integer.camera_mode_video),SettingsScopeNamespaces.VIDEO); if (PhotoSphereHelper.hasLightCycleCapture(context)) &#123;//开启闪光 //注册广角镜头 registerWideAngleModule(moduleManager, res.getInteger( R.integer.camera_mode_panorama),SettingsScopeNamespaces .PANORAMA); //注册光球模块 registerPhotoSphereModule(moduleManager,res.getInteger( R.integer.camera_mode_photosphere), SettingsScopeNamespaces.PANORAMA); &#125; //若需重新聚焦 if (RefocusHelper.hasRefocusCapture(context)) &#123; //注册重聚焦模块 registerRefocusModule(moduleManager, res.getInteger( R.integer.camera_mode_refocus), SettingsScopeNamespaces.REFOCUS); &#125; //如果有色分离模块 if (GcamHelper.hasGcamAsSeparateModule(config)) &#123; //注册色分离模块 registerGcamModule(moduleManager, res.getInteger( R.integer.camera_mode_gcam),SettingsScopeNamespaces.PHOTO, config.getHdrPlusSupportLevel(OneCamera.Facing.BACK)); &#125; int imageCaptureIntentModuleId = res.getInteger( R.integer.camera_mode_capture_intent); registerCaptureIntentModule(moduleManager, imageCaptureIntentModuleId,SettingsScopeNamespaces.PHOTO, config.isUsingCaptureModule());&#125; 代码根据配置信息，进行一系列模块的注册，其中PhotoModule和VideoModule被注册，而其他的module则是根据配置来进行的，因为打开Camera应用，既可以拍照片也可以拍视频，此处，只分析PhoneModule的注册： 1234567891011121314151617181920212223242526272829303132[-&gt;/packages/apps/Camera2/src/com/android/camera/module/ModulesInfo.java]private static void registerPhotoModule(ModuleManager moduleManager, final int moduleId, final String namespace, final boolean enableCaptureModule) &#123; //向ModuleManager注册PhotoModule模块 moduleManager.registerModule(new ModuleManager.ModuleAgent() &#123; @Override public int getModuleId() &#123; return moduleId; &#125; @Override public boolean requestAppForCamera() &#123; return !enableCaptureModule; &#125; @Override public String getScopeNamespace() &#123; return namespace; &#125; @Override public ModuleController createModule(AppController app, Intent intent) &#123; Log.v(TAG, \"EnableCaptureModule = \" + enableCaptureModule); //创建ModuleController return enableCaptureModule ? new CaptureModule(app) : new PhotoModule(app); &#125; &#125;);&#125; 由代码可知，它最终是由ModuleManager来新建一个CaptureModule实例，而CaptureModule其实实现了ModuleController ，即创建了一个CaptureModule模式下的ModuleController对象，而真正的CaptureModule的具体实现为ModuleManagerImpl。至此，前两步已经获得了OneCameraOpener以及新建了ModuleController，并进行了注册，接下来分析第三步，mCurrentModule.init(this, isSecureCamera(), isCaptureIntent()): 12345678910111213141516171819202122232425262728[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]public void init(CameraActivity activity, boolean isSecureCamera, boolean isCaptureIntent) &#123; ... HandlerThread thread = new HandlerThread(\"CaptureModule.mCameraHandler\"); thread.start(); mCameraHandler = new Handler(thread.getLooper()); //获取第一步中创建的OneCameraOpener对象 mOneCameraOpener = mAppController.getCameraOpener(); try &#123; //获取前面创建的OneCameraManager对象 mOneCameraManager = OneCameraModule.provideOneCameraManager(); &#125; catch (OneCameraException e) &#123; Log.e(TAG, \"Unable to provide a OneCameraManager. \", e); &#125; `... //新建CaptureModule的UI mUI = new CaptureModuleUI(activity, mAppController. getModuleLayoutRoot(), mUIListener); //设置预览状态的监听 mAppController.setPreviewStatusListener(mPreviewStatusListener); synchronized (mSurfaceTextureLock) &#123; //获取SurfaceTexture mPreviewSurfaceTexture = mAppController.getCameraAppUI() .getSurfaceTexture(); &#125; &#125; 首先获取前面创建的OneCameraOpener对象以及OneCameraManager对象，然后再设置预览状态监听，这里主要分析预览状态的监听：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]private final PreviewStatusListener mPreviewStatusListener = new PreviewStatusListener() &#123; ... @Override public void onSurfaceTextureAvailable(SurfaceTexture surface, int width, int height) &#123; updatePreviewTransform(width, height, true); synchronized (mSurfaceTextureLock) &#123; mPreviewSurfaceTexture = surface; &#125; //打开Camera reopenCamera(); &#125; @Override public boolean onSurfaceTextureDestroyed(SurfaceTexture surface) &#123; Log.d(TAG, \"onSurfaceTextureDestroyed\"); synchronized (mSurfaceTextureLock) &#123; mPreviewSurfaceTexture = null; &#125; //关闭Camera closeCamera(); return true; &#125; @Override public void onSurfaceTextureSizeChanged(SurfaceTexture surface, int width, int height) &#123; //更新预览尺寸 updatePreviewBufferSize(); &#125; ... &#125;;``` 由代码可知，当SurfaceTexture的状态变成可用的时候，会调用reopenCamera()方法来打开Camera，所以继续分析reopenCamera()方法：``` java[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]private void reopenCamera() &#123; if (mPaused) &#123; return; &#125; AsyncTask.THREAD_POOL_EXECUTOR.execute(new Runnable() &#123; @Override public void run() &#123; closeCamera(); if(!mAppController.isPaused()) &#123; //开启Camera并开始预览 openCameraAndStartPreview(); &#125; &#125; &#125;);&#125;``` 它采用异步任务的方法，开启一个异步线程来进行启动操作，首先关闭打开的Camera，然后如果AppController不处于暂停状态，则打开Camera并启动Preview操作，所以继续分析openCameraAndStartPreview方法：``` java[-&gt;/packages/apps/Camera2/src/com/android/camera/CaptureModule.java]private void openCameraAndStartPreview() &#123; ... if (mOneCameraOpener == null) &#123; Log.e(TAG, \"no available OneCameraManager, showing error dialog\"); //释放CameraOpenCloseLock锁 mCameraOpenCloseLock.release(); mAppController.getFatalErrorHandler().onGenericCameraAccessFailure(); guard.stop(\"No OneCameraManager\"); return; &#125; // Derive objects necessary for camera creation. MainThread mainThread = MainThread.create(); //查找需要打开的CameraId CameraId cameraId = mOneCameraManager.findFirstCameraFacing( mCameraFacing); ... //打开Camera mOneCameraOpener.open(cameraId, captureSetting, mCameraHandler, mainThread, imageRotationCalculator, mBurstController, mSoundPlayer,new OpenCallback() &#123; @Override public void onFailure() &#123; //进行失败的处理 ... &#125; @Override public void onCameraClosed() &#123; ... &#125; @Override public void onCameraOpened(@Nonnull final OneCamera camera) &#123; Log.d(TAG, \"onCameraOpened: \" + camera); mCamera = camera; if (mAppController.isPaused()) &#123; onFailure(); return; &#125; ... mMainThread.execute(new Runnable() &#123; @Override public void run() &#123; //通知UI，Camera状态变化 mAppController.getCameraAppUI().onChangeCamera(); //使能拍照按钮 mAppController.getButtonManager().enableCameraButton(); &#125; &#125;); //至此，Camera打开成功，开始预览 camera.startPreview(new Surface(getPreviewSurfaceTexture()), new CaptureReadyCallback() &#123; @Override public void onSetupFailed() &#123; ... &#125; @Override public void onReadyForCapture() &#123; //释放锁 mCameraOpenCloseLock.release(); mMainThread.execute(new Runnable() &#123; @Override public void run() &#123; ... onPreviewStarted(); ... onReadyStateChanged(true); //设置CaptureModule为Capture准备的状态监听 mCamera.setReadyStateChangedListener( CaptureModule.this); mUI.initializeZoom(mCamera.getMaxZoom()); mCamera.setFocusStateListener( CaptureModule.this); &#125; &#125;); &#125; &#125;); &#125; &#125;, mAppController.getFatalErrorHandler()); guard.stop(\"mOneCameraOpener.open()\"); &#125;&#125;``` 首先，它主要会调用Camera2OneCameraOpenerImpl的open方法来打开Camera，并定义了开启的回调函数，对开启结束后的结果进行处理，如失败则释放mCameraOpenCloseLock，并暂停mAppController，如果打开成功，通知UI成功，并开启Camera的Preview，并且定义了Preview的各种回调操作，这里主要分析Open过程，所以继续分析：``` java[-&gt;/packages/apps/Camera2/src/com/android/camera/one/v2/Camera2OneCameraOpenerImpl.java]Overridepublic void open( ... mActiveCameraDeviceTracker.onCameraOpening(cameraKey); //打开Camera，此处调用框架层的CameraManager类的openCamera，进入frameworks层 mCameraManager.openCamera(cameraKey.getValue(), new CameraDevice.StateCallback() &#123; private boolean isFirstCallback = true; @Override ... @Override public void onOpened(CameraDevice device) &#123; //第一次调用此回调 if (isFirstCallback) &#123; isFirstCallback = false; try &#123; CameraCharacteristics characteristics = mCameraManager .getCameraCharacteristics(device.getId()); ... //创建OneCamera对象 OneCamera oneCamera = OneCameraCreator.create(device, characteristics, mFeatureConfig, captureSetting, mDisplayMetrics, mContext, mainThread, imageRotationCalculator, burstController, soundPlayer, fatalErrorHandler); if (oneCamera != null) &#123; //如果oneCamera不为空，则回调onCameraOpened，后面将做分析 openCallback.onCameraOpened(oneCamera); &#125; else &#123; ... openCallback.onFailure(); &#125; &#125; catch (CameraAccessException e) &#123; openCallback.onFailure(); &#125; catch (OneCameraAccessException e) &#123; Log.d(TAG, \"Could not create OneCamera\", e); openCallback.onFailure(); &#125; &#125; &#125; &#125;, handler); ...&#125; 至此，Camera的初始化流程中应用层的分析就差不多了，下一步将会调用CameraManager的openCamera方法来进入框架层，并进行Camera的初始化，下面将应用层的初始化时序图： 4.2、Camera2 框架层（JNI &amp; Native）Open()过程分析由上面的分析可知，将由应用层进入到框架层处理，将会调用CameraManager的openCamera方法，并且定义了CameraDevice的状态回调函数，具体的回调操作此处不做分析，继续跟踪openCamera()方法： 12345678//CameraManager.java(frameworks/base/core/java/android/hardware/camera2)@RequiresPermission(android.Manifest.permission.CAMERA)public void openCamera(@NonNull String cameraId,@NonNull final CameraDevice.StateCallback callback, @Nullable Handler handler) throws CameraAccessException &#123; ... openCameraDeviceUserAsync(cameraId, callback, handler);&#125; 由代码可知，此处与Camera1.0有明显不同，Camera1.0是通过一个异步的线程以及JNI来调用android_hardware_camera.java里面的native_setup方法来连接Camera，其使用的是C++的Binder来与CameraService进行通信的，而此处则不一样，它直接使用的是Java层的Binder来进行通信，先看openCameraDeviceUserAsync代码: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//CameraManager.java(frameworks/base/core/java/android/hardware/camera2)private CameraDevice openCameraDeviceUserAsync(String cameraId, CameraDevice.StateCallback callback, Handler handler) throws CameraAccessException &#123; CameraCharacteristics characteristics = getCameraCharacteristics( cameraId); CameraDevice device = null; try &#123; synchronized (mLock) &#123; ICameraDeviceUser cameraUser = null; //初始化一个CameraDevice对象 android.hardware.camera2.impl.CameraDeviceImpl deviceImpl = new android.hardware.camera2.impl.CameraDeviceImpl(cameraId, callback, handler, characteristics); BinderHolder holder = new BinderHolder(); //获取回调 ICameraDeviceCallbacks callbacks = deviceImpl.getCallbacks(); int id = Integer.parseInt(cameraId); try &#123; if (supportsCamera2ApiLocked(cameraId)) &#123; //通过Java层的Binder获取CameraService ICameraService cameraService = CameraManagerGlobal.get() .getCameraService(); ... //通过CameraService连接Camera设备 cameraService.connectDevice(callbacks, id, mContext .getOpPackageName(), USE_CALLING_UID, holder); //获取连接成功的CameraUser对象，它用来与CameraService通信 cameraUser = ICameraDeviceUser.Stub.asInterface( holder.getBinder()); &#125; else &#123; //使用遗留的API cameraUser = CameraDeviceUserShim.connectBinderShim( callbacks, id); &#125; &#125; catch (CameraRuntimeException e) &#123; ... &#125; catch (RemoteException e) &#123; ... //将其包装成DeviceImpl对象，供应用层使用 deviceImpl.setRemoteDevice(cameraUser); device = deviceImpl; &#125; &#125; catch (NumberFormatException e) &#123; ... &#125; catch (CameraRuntimeException e) &#123; throw e.asChecked(); &#125; return device;&#125; 此方法的目的是通过CameraService来连接并获取CameraDevice对象，该对象用来与Camera进行通信操作。代码首先通过Java层的Binder机制获取CameraService，然后调用其connectDevice方法来连接CaneraDevice，最后Camera返回的是CameraDeviceUser对象，而接着将其封装成Jav层CameraDevice对象，而之后所有与Camera的通信都通过CameraDevice的接口来进行。接下来分析一下Native层下的CameraDevice的初始化过程： 123456789101112131415[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\CameraService.cpp]//CameraService.cpp，其中device为输出对象status_t CameraService::connectDevice(const sp&lt;ICameraDeviceCallbacks&gt;&amp; cameraCb,int cameraId, const String16&amp; clientPackageName,int clientUid,/*out*/sp&lt;ICameraDeviceUser&gt;&amp; device) &#123; status_t ret = NO_ERROR; String8 id = String8::format(\"%d\", cameraId); sp&lt;CameraDeviceClient&gt; client = nullptr; ret = connectHelper&lt;ICameraDeviceCallbacks,CameraDeviceClient&gt;(cameraCb, id, CAMERA_HAL_API_VERSION_UNSPECIFIED, clientPackageName, clientUid, API_2, false, false, /*out*/client);//client为输出对象 ... device = client; return NO_ERROR;&#125; Native层的connectDevice方法就是调用了connectHelper方法，所以继续分析connectHelper： 123456789101112131415161718192021222324252627282930313233343536373839404142434445[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\CameraService.h]//CameraService.htemplate&lt;class CALLBACK, class CLIENT&gt;status_t CameraService::connectHelper(const sp&lt;CALLBACK&gt;&amp; cameraCb, const String8&amp; cameraId, int halVersion, const String16&amp; clientPackageName, int clientUid, apiLevel effectiveApiLevel, bool legacyMode, bool shimUpdateOnly, /*out*/sp&lt;CLIENT&gt;&amp; device) &#123; status_t ret = NO_ERROR; String8 clientName8(clientPackageName); int clientPid = getCallingPid(); ... sp&lt;CLIENT&gt; client = nullptr; &#123; ... //如果有必要，给FlashLight关闭设备的机会 mFlashlight-&gt;prepareDeviceOpen(cameraId); //获取CameraId int id = cameraIdToInt(cameraId); ... //获取Device的版本，此处为Device3 int deviceVersion = getDeviceVersion(id, /*out*/&amp;facing); sp&lt;BasicClient&gt; tmp = nullptr; //获取client对象 if((ret = makeClient(this, cameraCb, clientPackageName, cameraId, facing, clientPid, clientUid, getpid(), legacyMode, halVersion, deviceVersion, effectiveApiLevel, /*out*/&amp;tmp)) != NO_ERROR) &#123; return ret; &#125; client = static_cast&lt;CLIENT*&gt;(tmp.get()); //调用client的初始化函数来初始化模块 if ((ret = client-&gt;initialize(mModule)) != OK) &#123; ALOGE(\"%s: Could not initialize client from HAL module.\", __FUNCTION__); return ret; &#125; sp&lt;IBinder&gt; remoteCallback = client-&gt;getRemote(); if (remoteCallback != nullptr) &#123; remoteCallback-&gt;linkToDeath(this); &#125; &#125; // lock is destroyed, allow further connect calls //将client赋值给输出Device device = client; return NO_ERROR;&#125; CameraService根据Camera的相关参数来获取一个client，如makeClient方法，然后再调用client的initialize来进行初始化，首先看makeClient：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\CameraService.cpp]//CameraService.cpp，其中device为输出对象status_t CameraService::makeClient(const sp&lt;CameraService&gt;&amp; cameraService, const sp&lt;IInterface&gt;&amp; cameraCb, const String16&amp; packageName, const String8&amp; cameraId, int facing, int clientPid, uid_t clientUid, int servicePid, bool legacyMode, int halVersion, int deviceVersion, apiLevel effectiveApiLevel, /*out*/sp&lt;BasicClient&gt;* client) &#123; //将字符串的CameraId转换成整形 int id = cameraIdToInt(cameraId); ... if (halVersion &lt; 0 || halVersion == deviceVersion) &#123;//判断Camera HAL版本是否和Device的版本相同 switch(deviceVersion) &#123; case CAMERA_DEVICE_API_VERSION_1_0: if (effectiveApiLevel == API_1) &#123; // Camera1 API route sp&lt;ICameraClient&gt; tmp = static_cast&lt;ICameraClient*&gt;(cameraCb.get()); *client = new CameraClient(cameraService, tmp, packageName, id, facing, clientPid, clientUid, getpid(), legacyMode); &#125; else &#123; // Camera2 API route ALOGW(\"Camera using old HAL version: %d\", deviceVersion); return -EOPNOTSUPP; &#125; break; case CAMERA_DEVICE_API_VERSION_2_0: case CAMERA_DEVICE_API_VERSION_2_1: case CAMERA_DEVICE_API_VERSION_3_0: case CAMERA_DEVICE_API_VERSION_3_1: case CAMERA_DEVICE_API_VERSION_3_2: case CAMERA_DEVICE_API_VERSION_3_3: if (effectiveApiLevel == API_1) &#123; // Camera1 API route sp&lt;ICameraClient&gt; tmp = static_cast&lt;ICameraClient*&gt;(cameraCb.get()); *client = new Camera2Client(cameraService, tmp, packageName, id, facing, clientPid, clientUid, servicePid, legacyMode); &#125; else &#123; // Camera2 API route sp&lt;ICameraDeviceCallbacks&gt; tmp = static_cast&lt;ICameraDeviceCallbacks*&gt;(cameraCb.get()); *client = new CameraDeviceClient(cameraService, tmp, packageName, id, facing, clientPid, clientUid, servicePid); &#125; break; default: // Should not be reachable ALOGE(\"Unknown camera device HAL version: %d\", deviceVersion); return INVALID_OPERATION; &#125; &#125; else &#123; // A particular HAL version is requested by caller. Create CameraClient // based on the requested HAL version. if (deviceVersion &gt; CAMERA_DEVICE_API_VERSION_1_0 &amp;&amp; halVersion == CAMERA_DEVICE_API_VERSION_1_0) &#123; // Only support higher HAL version device opened as HAL1.0 device. sp&lt;ICameraClient&gt; tmp = static_cast&lt;ICameraClient*&gt;(cameraCb.get()); *client = new CameraClient(cameraService, tmp, packageName, id, facing, clientPid, clientUid, servicePid, legacyMode); &#125; else &#123; // Other combinations (e.g. HAL3.x open as HAL2.x) are not supported yet. ALOGE(\"Invalid camera HAL version %x: HAL %x device can only be\" \" opened as HAL %x device\", halVersion, deviceVersion, CAMERA_DEVICE_API_VERSION_1_0); return INVALID_OPERATION; &#125; &#125; return NO_ERROR;&#125; 其中就是创建一个Client对象，由于此处分析的是Camera API2.0，其HAL的版本是3.0+，而Device的版本则其Device的版本即为3.0+，所以会创建一个CameraDeviceClient对象，至此，makeClient已经创建了client对象，并返回了，接着看它的初始化： 123456789101112131415161718192021[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\api2\\CameraDeviceClient.cpp]status_t CameraDeviceClient::initialize(CameraModule *module)&#123; ATRACE_CALL(); status_t res; //调用Camera2ClientBase的初始化函数来初始化CameraModule模块 res = Camera2ClientBase::initialize(module); if (res != OK) &#123; return res; &#125; String8 threadName; //初始化FrameProcessor mFrameProcessor = new FrameProcessorBase(mDevice); threadName = String8::format(\"CDU-%d-FrameProc\", mCameraId); mFrameProcessor-&gt;run(threadName.string()); //并注册监听，监听的实现就在CameraDeviceClient类中 mFrameProcessor-&gt;registerListener(FRAME_PROCESSOR_LISTENER_MIN_ID, FRAME_PROCESSOR_LISTENER_MAX_ID, /*listener*/this,/*sendPartials*/true); return OK;&#125; 它会调用Camera2ClientBase的initialize方法来初始化，并且会初始化一个FrameProcessor来进行帧处理，主要是回调每一帧的ExtraResult到应用中，也就是3A相关的数据信息。而Camera1.0中各种Processor模块，即将数据打包处理后再返回到应用的模块都已经不存在，而Camera2.0中将由MediaRecorder、SurfaceView、ImageReader等来直接处理，总体来说效率更好。继续看initialize： 12345678910[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\common\\Camera2ClientBase.cpp]template &lt;typename TClientBase&gt;status_t Camera2ClientBase&lt;TClientBase&gt;::initialize(CameraModule *module) &#123; ... //调用Device的initialie方法 res = mDevice-&gt;initialize(module); ... res = mDevice-&gt;setNotifyCallback(this); return OK;&#125; 代码就是调用了Device的initialize方法，此处的Device是在Camera2ClientBase的构造函数中创建的： 123456789101112131415161718192021222324[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\common\\Camera2ClientBase.cpp]template &lt;typename TClientBase&gt;Camera2ClientBase&lt;TClientBase&gt;::Camera2ClientBase( const sp&lt;CameraService&gt;&amp; cameraService, const sp&lt;TCamCallbacks&gt;&amp; remoteCallback, const String16&amp; clientPackageName, int cameraId, int cameraFacing, int clientPid, uid_t clientUid, int servicePid): TClientBase(cameraService, remoteCallback, clientPackageName, cameraId, cameraFacing, clientPid, clientUid, servicePid), mSharedCameraCallbacks(remoteCallback), mDeviceVersion(cameraService-&gt;getDeviceVersion(cameraId)), mDeviceActive(false)&#123; ALOGI(\"Camera %d: Opened. Client: %s (PID %d, UID %d)\", cameraId, String8(clientPackageName).string(), clientPid, clientUid); mInitialClientPid = clientPid; mDevice = new Camera3Device(cameraId); LOG_ALWAYS_FATAL_IF(mDevice == 0, \"Device should never be NULL here.\");&#125; 目前Camera API是2.0，而Device的API已经是3.0+了，继续看Camera3Device的构造方法。 123456789101112131415161718192021[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\device3\\Camera3Device.cpp]Camera3Device::Camera3Device(int id): mId(id), mIsConstrainedHighSpeedConfiguration(false), mHal3Device(NULL), mStatus(STATUS_UNINITIALIZED), mStatusWaiters(0), mUsePartialResult(false), mNumPartialResults(1), mTimestampOffset(0), mNextResultFrameNumber(0), mNextReprocessResultFrameNumber(0), mNextShutterFrameNumber(0), mNextReprocessShutterFrameNumber(0), mListener(NULL)&#123; ATRACE_CALL(); camera3_callback_ops::notify = &amp;sNotify; camera3_callback_ops::process_capture_result = &amp;sProcessCaptureResult; ALOGV(\"%s: Created device for camera %d\", __FUNCTION__, id);&#125; 很显然，它将会创建一个Camera3Device对象，所以，Device的initialize就是调用了Camera3Device的initialize方法： 123456789101112131415161718192021222324252627282930313233343536373839[-&gt;\\frameworks\\av\\services\\camera\\libcameraservice\\device3\\Camera3Device.cpp]status_t Camera3Device::initialize(CameraModule *module)&#123; ... camera3_device_t *device; //打开Camera HAL层的Deivce res = module-&gt;open(deviceName.string(), reinterpret_cast&lt;hw_device_t**&gt;(&amp;device)); ... //交叉检查Device的版本 if (device-&gt;common.version &lt; CAMERA_DEVICE_API_VERSION_3_0) &#123; SET_ERR_L(\"Could not open camera: \" \"Camera device should be at least %x, reports %x instead\", CAMERA_DEVICE_API_VERSION_3_0, device-&gt;common.version); device-&gt;common.close(&amp;device-&gt;common); return BAD_VALUE; &#125; ... //调用回调函数来进行初始化，即调用打开Device的initialize方法来进行初始化 res = device-&gt;ops-&gt;initialize(device, this); ... //启动请求队列线程 mRequestThread = new RequestThread(this, mStatusTracker, device, aeLockAvailable); res = mRequestThread-&gt;run(String8::format(\"C3Dev-%d-ReqQueue\", mId).string()); if (res != OK) &#123; SET_ERR_L(\"Unable to start request queue thread: %s (%d)\", strerror(-res), res); device-&gt;common.close(&amp;device-&gt;common); mRequestThread.clear(); return res; &#125; ... //返回初始成功 return OK;&#125; 首先，会依赖HAL框架打开并获得相应的Device对象，具体的流程请参考android6.0源码分析之Camera2 HAL分析，然后再回调此对象的initialize方法进行初始化，最后再启动RequestThread等线程，并返回initialize成功。至此Camera API2.0下的初始化过程就分析结束了。框架层的初始化时序图如下： 4.2、总结Open()过程道路艰辛，主要为了后续Camera正常工作，添砖加瓦，铺路。下面我们列举一下，主要都准备了什么。1、Camera应用将一些Callback函数，注册到Camera.java中，以使在线程处理函数中可以调用到相应的回调函数。2、camera connect成功后，创建了BpCamera代理对象和BnCameraClient本地对象。3、在JNICameraContext实现CameraListener接口，并将接口注册到客户端camera本地对象中，并在BnCameraClient本地对象中回调这些接口。4、CameraService connect过程中，根据hal硬件版本，创建对应的CameraClient对象。在后续的初始化过程中，创建6大线程。最后以一个简单的工作流程图来结束博文 （五）、参考资料(特别感谢各位前辈的分析和图示)：Android Camera官方文档Android 5.0 Camera系统源码分析-CSDN博客Android Camera 流程学习记录 - StoneDemo - 博客园Android Camera 系统架构源码分析 - CSDN博客Camera安卓源码-高通mm_camera架构剖析 - CSDN博客5.2 应用程序和驱动程序中buffer的传输流程 - CSDN博客Camera2 数据流从framework到Hal源码分析 - 简书mm-camera层frame数据流源码分析 - 简书v4l2_capture.c分析—probe函数分析 - CSDN博客@@Android Camera fw学习 - CSDN博客@@Android Camera API2分析 - CSDN博客@@Android Camera 流程学习记录 7.12- CSDN博客@@专栏：古冥的android6.0下的Camera API2.0的源码分析之旅 - CSDN博客linux3.3 v4l2视频采集驱动框架(vfe, camera i2c driver，v4l2_subdev等之间的联系) - CSDN博客Android Camera从Camera HAL1到Camera HAL3的过渡（已更新到Android6.0 HAL3.3） - CSDN博客我心依旧之Android Camera模块FW/HAL3探学序 - CSDN博客Android Camera fw学习(四)-recording流程分析 - CSDN博客android camera动态库加载过程 - CSDN博客Android Camera API2.0下全新的Camera FW/HAL架构简述 - CSDN博客","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Video System（3）：音视频录制Recorder、编码Encoder、混合MediaMuxer源码分析","slug":"Android Video System（3）：音视频录制Recorder、编码Encoder、混合MediaMuxer源码分析","date":"2018-06-17T16:00:00.000Z","updated":"2018-07-03T12:51:51.638Z","comments":true,"path":"2018/06/18/Android Video System（3）：音视频录制Recorder、编码Encoder、混合MediaMuxer源码分析/","link":"","permalink":"http://zhoujinjian.cc/2018/06/18/Android Video System（3）：音视频录制Recorder、编码Encoder、混合MediaMuxer源码分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】【特别感谢 - Android NuPlayer播放框架】【特别感谢 - android ACodec MediaCodec NuPlayer flow】 Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) ☯ V4l2 框架代码☯ kernel/drivers/media/v4l2-core/（文件前缀为 videobuf2） ☯ MSM 视频驱动程序文件☯ kernel/drivers/media/platform/msm/vidc/ ☯ 设备树☯ /kernel/arch/arm/boot/dts/qcom（Venus 的寄存器基址，时钟频率） ☯ Stagefright、libmedia、libmediaplayerservice、mediaserver☯ /frameworks/av/media/ ☯ OMX☯ /hardware/qcom/media/mam8996/mm-video-v4l2/vidc/ ☯ OMX 核心☯ /hardware/qcom/media/mm-core ☯ 软件编解码器路径☯ /vendor/qcom/proprietary/mm-video/omx_vpp(?) → 解码器代码☯ /vendor/qcom/proprietary/mm-video/omx_vpp(?) → 编码器代码 首先看一下使用MediaRecorder 录制音频的Java实例： 1234567891011MediaRecorder recorder=newMediaRecorder();recorder.setAudioSource(MediaRecorder.AudioSource.MIC);recorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);recorder.setAudioEncoder(MediaRecorder.AudioEncoder.AMR_NB);recorder.setOutputFile(PATH_NAME);recorder.prepare();recorder.start(); // Recording is now started...recorder.stop(); //recorder.reset(); // You can reuse the object by going back to setAudioSource() steprecorder.release();// Now the object cannot be reused 之前在Audio System（2）：Linux ALSA音频系统分析 第（八）节画过tinyplay capture录音时序图，但当时没有仔细分析，今天来分析Audio录音如何从Java层一步步最终到达tinyalsa层的pcm_open()、pcm_read()函数的 12345678910111213141516171819202122232425[-&gt;E:\\android-7.1.2_r1\\external\\tinyalsa\\tinycap.c]unsigned int capture_sample(FILE *file, unsigned int card, unsigned int device, unsigned int channels, unsigned int rate, enum pcm_format format, unsigned int period_size, unsigned int period_count)&#123; struct pcm_config config; struct pcm *pcm; char *buffer; unsigned int size; unsigned int bytes_read = 0; memset(&amp;config, 0, sizeof(config)); config.channels = channels; config.rate = rate; config.period_size = period_size; config.period_count = period_count; config.format = format; config.start_threshold = 0; config.stop_threshold = 0; config.silence_threshold = 0; //打开录制节点 pcm = pcm_open(card, device, PCM_IN, &amp;config); ......&#125; 并读取从Kernel内核传过来的数据最终合成音频文件的过程1234567891011121314151617[-&gt;E:\\android-7.1.2_r1\\external\\tinyalsa\\tinycap.c]unsigned int capture_sample(FILE *file, unsigned int card, unsigned int device, unsigned int channels, unsigned int rate, enum pcm_format format, unsigned int period_size, unsigned int period_count)&#123; ...... //循环读取音频数据 size = pcm_frames_to_bytes(pcm, pcm_get_buffer_size(pcm)); buffer = malloc(size); while (capturing &amp;&amp; !pcm_read(pcm, buffer, size)) &#123; if (fwrite(buffer, 1, size, file) != size) &#123; fprintf(stderr,\"Error capturing sample\\n\"); break; &#125; bytes_read += size; &#125; 开始分析音频录制编码合成之旅，之后分析视频录制编码合成过程。 （一）、Audio Recorder 音频录制源码分析 （二）、Media Recorder 视频录制源码分析12345678910111213141516171819202122232425mCamera = getCameraInstance();mCamera .open()mCamera.startPreview() mMediaRecorder = new MediaRecorder(); mMediaRecorder.setCamera(mCamera); mMediaRecorder.setPreviewDisplay(android.view.SurfaceHolder); mMediaRecorder.setAudioSource(MediaRecorder.AudioSource.CAMCORDER); mMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA); mMediaRecorder.setVideoSize(videoRecordSize.width, videoRecordSize.height);mMediaRecorder.setProfile(CamcorderProfile.get(CamcorderProfile.QUALITY_HIGH)); mMediaRecorder.setOutputFile(outputFile.toString()); //MediaRecorder.OutputFormat.MPEG_4. mMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.AMR_NB) mMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.MPEG_4_SP) mMediaRecorder.prepare(); mMediaRecorder.start();mMediaRecorder.stop()mMediaRecorder.release()mCamera.stopPreview() （三）、音频Recorder编码Encoder1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374 Line 3036: 07-02 17:00:07.175 8430 8479 V ACodec : Now uninitialized Line 3674: 07-02 17:00:35.717 745 1949 V ACodec : Now uninitialized Line 3675: 07-02 17:00:35.718 745 8649 V ACodec : onAllocateComponent Line 3678: 07-02 17:00:35.721 731 921 I OMXMaster: makeComponentInstance(OMX.google.amrnb.encoder) in mediacodec process Line 3679: 07-02 17:00:35.742 745 8649 V ACodec : [OMX.google.amrnb.encoder] Now Loaded Line 3680: 07-02 17:00:35.742 745 8649 I MediaCodec: MediaCodec will operate in async mode Line 3680: 07-02 17:00:35.742 745 8649 I MediaCodec: MediaCodec will operate in async mode Line 3681: 07-02 17:00:35.742 745 8649 V MediaCodec: Found 0 pieces of codec specific data. Line 3682: 07-02 17:00:35.742 745 8649 V ACodec : onConfigureComponent Line 3684: 07-02 17:00:35.744 745 8649 I ACodec : codec does not support config priority (err -2147483648) Line 3686: 07-02 17:00:35.751 745 8649 I ACodec : codec does not support config priority (err -2147483648) Line 3687: 07-02 17:00:35.755 745 8649 V MediaCodec: [OMX.google.amrnb.encoder] configured as input format: AMessage(what = 0x00000000) = &#123; Line 3688: 07-02 17:00:35.755 745 8649 V MediaCodec: string mime = &quot;audio/raw&quot; Line 3689: 07-02 17:00:35.755 745 8649 V MediaCodec: int32_t channel-count = 1 Line 3690: 07-02 17:00:35.755 745 8649 V MediaCodec: int32_t sample-rate = 8000 Line 3691: 07-02 17:00:35.755 745 8649 V MediaCodec: int32_t pcm-encoding = 2 Line 3692: 07-02 17:00:35.755 745 8649 V MediaCodec: &#125;, output format: AMessage(what = 0x00000000) = &#123; Line 3693: 07-02 17:00:35.755 745 8649 V MediaCodec: int32_t bitrate = 12200 Line 3694: 07-02 17:00:35.755 745 8649 V MediaCodec: int32_t max-bitrate = 12200 Line 3695: 07-02 17:00:35.755 745 8649 V MediaCodec: int32_t channel-count = 1 Line 3696: 07-02 17:00:35.755 745 8649 V MediaCodec: string mime = &quot;audio/3gpp&quot; Line 3697: 07-02 17:00:35.755 745 8649 V MediaCodec: int32_t sample-rate = 8000 Line 3698: 07-02 17:00:35.755 745 8649 V MediaCodec: &#125; Line 3699: 07-02 17:00:35.757 745 8649 V ACodec : onStart Line 3700: 07-02 17:00:35.758 745 8649 V ACodec : [OMX.google.amrnb.encoder] Now Loaded-&gt;Idle Line 3701: 07-02 17:00:35.758 745 8649 V ACodec : [OMX.google.amrnb.encoder] Allocating 4 buffers of size 2048/2048 (from 2048 using Invalid) on input port Line 3702: 07-02 17:00:35.766 745 8649 V ACodec : [OMX.google.amrnb.encoder] Allocating 4 buffers of size 8192/8192 (from 8192 using Invalid) on output port Line 3703: 07-02 17:00:35.771 745 8649 V MediaCodec: input buffers allocated Line 3704: 07-02 17:00:35.771 745 8649 V MediaCodec: output buffers allocated Line 3705: 07-02 17:00:35.772 745 8646 I MediaCodecSource: MediaCodecSource (audio) starting Line 3706: 07-02 17:00:35.772 745 8649 V ACodec : [OMX.google.amrnb.encoder] Now Idle-&gt;Executing Line 3701: 07-02 17:00:35.758 745 8649 V ACodec : [OMX.google.amrnb.encoder] Allocating 4 buffers of size 2048/2048 (from 2048 using Invalid) on input port Line 3702: 07-02 17:00:35.766 745 8649 V ACodec : [OMX.google.amrnb.encoder] Allocating 4 buffers of size 8192/8192 (from 8192 using Invalid) on output port Line 3706: 07-02 17:00:35.772 745 8649 V ACodec : [OMX.google.amrnb.encoder] Now Idle-&gt;Executing Line 3707: 07-02 17:00:35.773 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 5 Line 3708: 07-02 17:00:35.773 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 6 Line 3711: 07-02 17:00:35.775 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 7 Line 3715: 07-02 17:00:35.775 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 8 Line 3717: 07-02 17:00:35.776 745 8649 V ACodec : [OMX.google.amrnb.encoder] Now Executing Line 3833: 07-02 17:00:35.902 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling emptyBuffer 1 w/ time 20000 us Line 3834: 07-02 17:00:35.904 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXEmptyBufferDone 1 Line 3835: 07-02 17:00:35.907 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 5 time 20000 us, flags = 0x00000010 Line 3836: 07-02 17:00:35.911 745 8649 V MediaCodec: [OMX.google.amrnb.encoder] output format changed to: AMessage(what = 0x00000000) = &#123; Line 3843: 07-02 17:00:35.913 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 5 Line 3844: 07-02 17:00:35.925 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling emptyBuffer 2 w/ time 40000 us Line 3845: 07-02 17:00:35.926 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXEmptyBufferDone 2 Line 3846: 07-02 17:00:35.927 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 6 time 40000 us, flags = 0x00000010 Line 3847: 07-02 17:00:35.927 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 6 Line 3848: 07-02 17:00:35.942 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling emptyBuffer 3 w/ time 60000 us Line 3849: 07-02 17:00:35.945 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXEmptyBufferDone 3 Line 3850: 07-02 17:00:35.945 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 7 time 60000 us, flags = 0x00000010 Line 3851: 07-02 17:00:35.946 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 7 Line 3852: 07-02 17:00:35.963 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling emptyBuffer 4 w/ time 80000 us Line 3853: 07-02 17:00:35.965 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXEmptyBufferDone 4 Line 3854: 07-02 17:00:35.966 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 8 time 80000 us, flags = 0x00000010 Line 3855: 07-02 17:00:35.967 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 8 Line 3856: 07-02 17:00:35.982 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling emptyBuffer 1 w/ time 100000 us Line 3857: 07-02 17:00:35.983 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXEmptyBufferDone 1 Line 3858: 07-02 17:00:35.985 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 5 time 100000 us, flags = 0x00000010...... Line 8217: 07-02 17:00:56.446 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 8 Line 8218: 07-02 17:00:56.462 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling emptyBuffer 1 w/ time 20580000 us Line 8219: 07-02 17:00:56.464 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXEmptyBufferDone 1 Line 8220: 07-02 17:00:56.466 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 5 time 20580000 us, flags = 0x00000010 Line 8221: 07-02 17:00:56.467 745 8649 V ACodec : [OMX.google.amrnb.encoder] calling fillBuffer 5 Line 8222: 07-02 17:00:56.468 745 8646 I MediaCodecSource: encoder (audio) stopping Line 8223: 07-02 17:00:56.483 745 8649 V ACodec : [OMX.google.amrnb.encoder] Now Executing-&gt;Idle Line 8224: 07-02 17:00:56.484 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 5 time 20580000 us, flags = 0x00000000 Line 8225: 07-02 17:00:56.484 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 6 time 20520000 us, flags = 0x00000000 Line 8226: 07-02 17:00:56.484 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 7 time 20540000 us, flags = 0x00000000 Line 8227: 07-02 17:00:56.484 745 8649 V ACodec : [OMX.google.amrnb.encoder] onOMXFillBufferDone 8 time 20560000 us, flags = 0x00000000 Line 8228: 07-02 17:00:56.496 745 8649 V ACodec : [OMX.google.amrnb.encoder] Now Idle-&gt;Loaded Line 8229: 07-02 17:00:56.496 745 8649 V ACodec : [OMX.google.amrnb.encoder] Now Loaded Line 8231: 07-02 17:00:56.499 745 8646 I MediaCodecSource: encoder (audio) stopped （四）、视频Recorder编码Encoder12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565707-03 11:43:44.947 6238 6238 V CAM_VideoModule: startVideoRecording07-03 11:43:44.974 6238 6238 D CameraStorage: External storage state=mounted07-03 11:43:44.995 6238 6238 D CameraStorage: External storage state=mounted07-03 11:43:44.998 6238 6238 V CAM_VideoModule: initializeRecorder07-03 11:43:45.003 741 1966 V MediaPlayerService: Create new media recorder client from pid 623807-03 11:43:45.009 6238 6238 I CAM_VideoModule: NOTE: hfr = off : hsr = off07-03 11:43:45.024 802 6282 E mm-camera: &lt;ISP &gt;&lt;ERROR&gt; 378: tintless40_algo_process_be: failed: update_func rc -407-03 11:43:45.024 802 6282 E mm-camera: &lt;ISP &gt;&lt;ERROR&gt; 851: tintless40_algo_execute: failed: tintless40_trigger_algo07-03 11:43:45.024 802 6282 E mm-camera: &lt;ISP &gt;&lt;ERROR&gt; 98: isp_algo_execute_internal_algo: failed to run algo tintless07-03 11:43:45.024 802 6282 E mm-camera: &lt;ISP &gt;&lt;ERROR&gt; 710: isp_parser_thread_func: failed: isp_parser_process07-03 11:43:45.031 6238 6238 D LocationManager: No location received yet.07-03 11:43:45.033 6238 6238 D LocationManager: No location received yet.07-03 11:43:45.033 6238 6238 V CAM_VideoModule: New video filename: /storage/emulated/0/DCIM/Camera/VID_20180703_114345.mp407-03 11:43:45.059 1391 2526 I MediaFocusControl: AudioFocus requestAudioFocus() from uid/pid 10025/6238 clientId=android.media.AudioManager@11f939d req=2 flags=0x007-03 11:43:45.062 4966 4966 D AudioManager: AudioManager dispatching onAudioFocusChange(-2) for android.media.AudioManager@e6fb066com.android.music.MediaPlaybackService$4@2315fa707-03 11:43:45.063 4966 4966 V MediaPlaybackService: AudioFocus: received AUDIOFOCUS_LOSS_TRANSIENT07-03 11:43:45.079 726 6266 E QCamera : &lt;HAL&gt;&lt;ERROR&gt; status_t qcamera::QCameraParameters::setSkinBeautify(const qcamera::QCameraParameters &amp;): 15184: gpw status_t qcamera::QCameraParameters::setSkinBeautify(const qcamera::QCameraParameters &amp;): str=off , prev_str=off07-03 11:43:45.079 726 6266 E QCamera : &lt;HAL&gt;&lt;ERROR&gt; int32_t qcamera::QCameraParameters::setAjustLevel(const qcamera::QCameraParameters &amp;): 15302: gpw -1 -1 -1 -1 -107-03 11:43:45.079 726 6266 E QCamera : &lt;HAL&gt;&lt;ERROR&gt; int32_t qcamera::QCameraParameters::setAjustLevel(int, int, int, int, int): 15232: ggw3 -1 -1 07-03 11:43:45.091 726 2209 E CameraClient: setVideoBufferMode: 535: videoBufferMode 2 is not supported.07-03 11:43:45.100 802 6282 E mm-camera: &lt;ISP &gt;&lt;ERROR&gt; 378: tintless40_algo_process_be: failed: update_func rc -407-03 11:43:45.101 802 6282 E mm-camera: &lt;ISP &gt;&lt;ERROR&gt; 851: tintless40_algo_execute: failed: tintless40_trigger_algo07-03 11:43:45.101 802 6282 E mm-camera: &lt;ISP &gt;&lt;ERROR&gt; 98: isp_algo_execute_internal_algo: failed to run algo tintless07-03 11:43:45.101 802 6282 E mm-camera: &lt;ISP &gt;&lt;ERROR&gt; 710: isp_parser_thread_func: failed: isp_parser_process07-03 11:43:45.105 802 6296 E mm-camera: &lt;IMGLIB&gt;&lt;ERROR&gt; 318: faceproc_comp_set_param: Error param=52307-03 11:43:45.111 726 6266 E QCamera : &lt;HAL&gt;&lt;ERROR&gt; status_t qcamera::QCameraParameters::setSkinBeautify(const qcamera::QCameraParameters &amp;): 15184: gpw status_t qcamera::QCameraParameters::setSkinBeautify(const qcamera::QCameraParameters &amp;): str=off , prev_str=off07-03 11:43:45.111 726 6266 E QCamera : &lt;HAL&gt;&lt;ERROR&gt; int32_t qcamera::QCameraParameters::setAjustLevel(const qcamera::QCameraParameters &amp;): 15302: gpw -1 -1 -1 -1 -107-03 11:43:45.111 726 6266 E QCamera : &lt;HAL&gt;&lt;ERROR&gt; int32_t qcamera::QCameraParameters::setAjustLevel(int, int, int, int, int): 15232: ggw3 -1 -1 07-03 11:43:45.121 741 6349 I MediaPlayerService: MediaPlayerService::getOMX07-03 11:43:45.122 741 6349 I OMXClient: MuxOMX ctor Line 5362: 07-03 11:43:41.712 6238 6238 V CAM_VideoModule: Video Encoder selected = 2 Line 5363: 07-03 11:43:41.712 6238 6238 V CAM_VideoModule: Audio Encoder selected = 3 Line 5449: 07-03 11:43:41.944 6238 6238 V CAM_VideoModule: Video Encoder selected = 2 Line 5450: 07-03 11:43:41.944 6238 6238 V CAM_VideoModule: Audio Encoder selected = 3 Line 6011: 07-03 11:43:45.123 732 2029 I OMXMaster: makeComponentInstance(OMX.qcom.video.encoder.avc) in mediacodec process Line 6014: 07-03 11:43:45.170 732 2029 I OMX-VENC: Component_init : OMX.qcom.video.encoder.avc : return = 0x0 Line 6017: 07-03 11:43:45.180 732 2168 E OMXNodeInstance: getParameter(2dc0040:qcom.encoder.avc, ParamConsumerUsageBits(0x6f800004)) ERROR: UnsupportedIndex(0x8000101a) Line 6019: 07-03 11:43:45.181 732 1863 W OMXNodeInstance: [2dc0040:qcom.encoder.avc] component does not support metadata mode; using fallback Line 6020: 07-03 11:43:45.181 741 6349 E ACodec : [OMX.qcom.video.encoder.avc] storeMetaDataInBuffers (output) failed w/ err -1010 Line 6021: 07-03 11:43:45.182 741 6349 I ExtendedACodec: setupVideoEncoder() Line 6026: 07-03 11:43:45.231 741 6349 I ACodec : setupAVCEncoderParameters with [profile: Baseline] [level: Level1] Line 6028: 07-03 11:43:45.241 732 1863 E OMXNodeInstance: getConfig(2dc0040:qcom.encoder.avc, ??(0x7f000062)) ERROR: UnsupportedSetting(0x80001019) Line 6029: 07-03 11:43:45.245 741 6349 I ACodec : [OMX.qcom.video.encoder.avc] cannot encode HDR static metadata. Ignoring. Line 6030: 07-03 11:43:45.245 741 6349 I ACodec : setupVideoEncoder succeeded Line 6031: 07-03 11:43:45.245 741 6349 I ExtendedACodec: [OMX.qcom.video.encoder.avc] configure, AMessage : AMessage(what = 'conf', target = 75) = &#123; Line 6044: 07-03 11:43:45.245 741 6349 I ExtendedACodec: int32_t encoder = 1 Line 6174: 07-03 11:43:45.455 732 2169 I OMXMaster: makeComponentInstance(OMX.qcom.audio.encoder.aac) in mediacodec process Line 6180: 07-03 11:43:45.460 732 2169 E QC_AACENC: component init: role = OMX.qcom.audio.encoder.aac Line 6182: 07-03 11:43:45.491 732 732 E OMXNodeInstance: setConfig(2dc0041:qcom.encoder.aac, ConfigPriority(0x6f800002)) ERROR: UnsupportedIndex(0x8000101a) Line 6184: 07-03 11:43:45.502 732 2169 E OMXNodeInstance: setConfig(2dc0041:qcom.encoder.aac, ConfigPriority(0x6f800002)) ERROR: UnsupportedIndex(0x8000101a) Line 6193: 07-03 11:43:45.537 741 6363 I CameraSource: Using encoder format: 0x22 Line 6194: 07-03 11:43:45.537 741 6363 I CameraSource: Using encoder data space: 0x104 Line 7431: 07-03 11:43:56.472 741 6347 I MediaCodecSource: encoder (video) stopping Line 7456: 07-03 11:43:56.611 741 6347 I MediaCodecSource: encoder (video) stopped Line 7494: 07-03 11:43:56.677 741 6347 I MediaCodecSource: encoder (audio) stopping Line 7534: 07-03 11:43:56.800 741 6347 I MediaCodecSource: encoder (audio) stopped （五）、音视频混合MediaMuxer源码分析12345678910111213sp&lt;MediaWriter&gt; mWriter;[-&gt;\\android\\frameworks\\av\\media\\libstagefright\\MediaMuxer.cpp]status_t MediaMuxer::start() &#123; Mutex::Autolock autoLock(mMuxerLock); if (mState == INITIALIZED) &#123; mState = STARTED; mFileMeta-&gt;setInt32(kKeyRealTimeRecording, false); return mWriter-&gt;start(mFileMeta.get()); &#125; else &#123; ALOGE(\"start() is called in invalid state %d\", mState); return INVALID_OPERATION; &#125;&#125; 可以看到具体还是调用的MediaWriter实现的。我们来分析具体实例：MPEG4Writer.cpp，即MP4文件的格式封装过程。 1) 录制开始时，写入文件头部。 2) 录制进行时，实时写入音视频轨迹的数据块。 3) 录制结束时，写入索引信息并更新头部参数。 索引负责描述音视频轨迹的特征，会随着音视频轨迹的存储而变化，所以通常做法会将录像文件索引信息放在音视频轨迹流后面，在媒体流数据写完（录像结束）后才能写入。可以看到，存放音视频数据的mdat box是位于第二位的，而负责检索音视频的moov box是位于最后的，这与通常的MP4封装的排列顺序不同，当然这是为了符合录制而产生的结果。因为 moov的大小是随着 mdat 变化的，而我们录制视频的时间预先是不知道的，所以需要先将mdat 数据写入，最后再写入moov，完成封装。 现有Android系统上录像都是录制是MP4或3GP格式，底层就是使用MPEG4Writer组合器类来完成的，它将编码后的音视频轨迹按照MPEG4规范进行封装，填入各个参数，就组合成完整的MP4格式文件。MPEG4Writer的组合功能主要由两种线程完成，一种是负责音视频数据写入封装文件的写线程（WriterThread），一种是音视频数据读取处理的轨迹线程（TrackThread）。轨迹线程一般有两个：视频轨迹数据读取线程和音频轨迹数据读取线程，而写线程只有一个，负责将轨迹线程中打包成Chunk的数据写入封装文件。 如下图所示，轨迹线程是以帧为单位获取数据帧（Sample），并将每帧中的信息及系统环境信息提取汇总存储在内存的trak表中，其中需要维持的信息有Chunk写入文件的偏移地址Stco（Chunk Offset）、Sample与Chunk的映射关系Stsc（Sample-to-Chunk）、关键帧Stss（Sync Sample）、每一帧的持续时间Stts（Time-to-Sample）等，这些信息是跟每一帧的信息密切相关的，由图可以看出trak表由各自的线程维护，当录像结束时trak表会就会写入封装文件。而每一帧的数据流会先存入一个链表缓存中，当帧的数量达到一定值时，轨迹线程会将这些帧数据打包成块（Chunk）并通知写线程写入到封装文件。写线程接到Chunk已准备好的通知后就马上搜索Chunk链表（链表个数与轨迹线程个数相关，一般有两个，音视频轨迹线程各有一个），将找到的第一个Chunk后便写入封装文件，并会将写入的偏移地址更新到相应的trak表的Stco项（但trak表中其它数据是由轨迹线程更新）。音视频的Chunk数据是存储于同一mdat box中，按添加到Chunk链表时间先后顺序排列。等到录像结束时，录像应用会调用MPEG4Writer的stop方法，此时就会将音视频的trak表分别写入moov。 其实看完上面的内容，应该对Android录制视频过程中，录制的视频的封装过程有一个大体了解，我们平时所说的视频后缀名.mp4/.mkv等等就是视频封装的各种格式。先看看构造函数：在这里将实现一些参数的初始化，fd是传进来的录制文件的文件描述符。 1234567891011121314151617181920212223242526272829303132333435363738[-&gt;\\android\\frameworks\\av\\media\\libstagefright\\MPEG4Writer.cpp]MPEG4Writer::MPEG4Writer(int fd) : mFd(dup(fd)), mInitCheck(mFd &lt; 0? NO_INIT: OK), mIsRealTimeRecording(true), mUse4ByteNalLength(true), mUse32BitOffset(true), mIsFileSizeLimitExplicitlyRequested(false), mPaused(false), mStarted(false), mWriterThreadStarted(false), mOffset(0), mMdatOffset(0), mMoovBoxBuffer(NULL), mMoovBoxBufferOffset(0), mWriteMoovBoxToMemory(false), mFreeBoxOffset(0), mStreamableFile(false), mEstimatedMoovBoxSize(0), mMoovExtraSize(0), mInterleaveDurationUs(1000000), mTimeScale(-1), mStartTimestampUs(-1ll), mLatitudex10000(0), mLongitudex10000(0), mAreGeoTagsAvailable(false), mStartTimeOffsetMs(-1), mMetaKeys(new AMessage()), mIsAudioAMR(false) &#123; addDeviceMeta(); // Verify mFd is seekable off64_t off = lseek64(mFd, 0, SEEK_SET); if (off &lt; 0) &#123; ALOGE(\"cannot seek mFd: %s (%d)\", strerror(errno), errno); release(); &#125;&#125; 接着从 MPEG4Writer.cpp 的start()函数开始：在start部分，我们看到在这一部分，writeFtypBox(param) 将实现录制文件文件头部信息的相关信息的写入操作；startWriterThread() 开启封装视频文件的写线程；startTracks(param) 开启视频数据的读线程，也就是前面文件部分所说的轨迹线程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849status_t MPEG4Writer::start(MetaData *param) &#123; ...... if (mStarted) &#123; if (mPaused) &#123; mPaused = false; return startTracks(param); &#125; return OK; &#125; ...... mWriteMoovBoxToMemory = false; mMoovBoxBuffer = NULL; mMoovBoxBufferOffset = 0; writeFtypBox(param); mFreeBoxOffset = mOffset; if (mEstimatedMoovBoxSize == 0) &#123; int32_t bitRate = -1; if (param) &#123; param-&gt;findInt32(kKeyBitRate, &amp;bitRate); &#125; mEstimatedMoovBoxSize = estimateMoovBoxSize(bitRate); &#125; CHECK_GE(mEstimatedMoovBoxSize, 8); if (mStreamableFile) &#123; // Reserve a &apos;free&apos; box only for streamable file lseek64(mFd, mFreeBoxOffset, SEEK_SET); writeInt32(mEstimatedMoovBoxSize); write(&quot;free&quot;, 4); mMdatOffset = mFreeBoxOffset + mEstimatedMoovBoxSize; &#125; else &#123; mMdatOffset = mOffset; &#125; mOffset = mMdatOffset; lseek64(mFd, mMdatOffset, SEEK_SET); ...... status_t err = startWriterThread(); ...... err = startTracks(param); ...... mStarted = true; return OK;&#125; 继续看下 startWriterThread（）部分，在startWriterThread（）函数中，将真正建立新的子线程，并在子线程中执行ThreadWrappe函数中的操作。 1234567891011121314151617181920212223status_t MPEG4Writer::startWriterThread() &#123; ALOGV(\"startWriterThread\"); mDone = false; mIsFirstChunk = true; mDriftTimeUs = 0; for (List&lt;Track *&gt;::iterator it = mTracks.begin(); it != mTracks.end(); ++it) &#123; ChunkInfo info; info.mTrack = *it; info.mPrevChunkTimestampUs = 0; info.mMaxInterChunkDurUs = 0; mChunkInfos.push_back(info); &#125; pthread_attr_t attr; pthread_attr_init(&amp;attr); pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE); pthread_create(&amp;mThread, &amp;attr, ThreadWrapper, this); pthread_attr_destroy(&amp;attr); mWriterThreadStarted = true; return OK;&#125; 接着继续看 ThreadWrapper（）函数,在这里new 了一个MPEGWriter对象，真正的操作在threadFunc()中体现 123456void *MPEG4Writer::ThreadWrapper(void *me) &#123; ALOGV(\"ThreadWrapper: %p\", me); MPEG4Writer *writer = static_cast&lt;MPEG4Writer *&gt;(me); writer-&gt;threadFunc(); return NULL;&#125; 下面看下threadFun()。在这个函数中，将根据变量mDone 进行while循环，一直检测是否有数据块Chunk可写。轨迹线程是一直将读数据的数据往buffer中写入，buffer到了一定量后，就是chunk,这时就会通过信号量 mChunkReadyCondition来通知封装文件的写线程去检测链表，然后将检索到的Chunk数据写入文件的数据区，当然写之前，肯定会去判断下是否真的有数据可写。 1234567891011121314151617181920212223242526void MPEG4Writer::threadFunc() &#123; ALOGV(\"threadFunc\"); prctl(PR_SET_NAME, (unsigned long)\"MPEG4Writer\", 0, 0, 0); Mutex::Autolock autoLock(mLock); while (!mDone) &#123; Chunk chunk; bool chunkFound = false; while (!mDone &amp;&amp; !(chunkFound = findChunkToWrite(&amp;chunk))) &#123; mChunkReadyCondition.wait(mLock); &#125; if (chunkFound) &#123; if (mIsRealTimeRecording) &#123; mLock.unlock(); &#125; writeChunkToFile(&amp;chunk); if (mIsRealTimeRecording) &#123; mLock.lock(); &#125; &#125; &#125; writeAllChunks();&#125; 下面看下writerChunkToFile(&amp;chunk);轨迹线程读数据时是以数据帧Sample为单位，所以这里将Chunk写入封装文件，也是以Sample为单位，遍历整个链表，将数据写入封装文件，真正的写入操作是addSamole_l(*it); 1234567891011121314151617181920212223242526272829303132333435363738void MPEG4Writer::writeAllChunks() &#123; ALOGV(\"writeAllChunks\"); size_t outstandingChunks = 0; Chunk chunk; while (findChunkToWrite(&amp;chunk)) &#123; writeChunkToFile(&amp;chunk); ++outstandingChunks; &#125; sendSessionSummary(); mChunkInfos.clear(); ALOGD(\"%zu chunks are written in the last batch\", outstandingChunks);&#125;void MPEG4Writer::writeChunkToFile(Chunk* chunk) &#123; ALOGV(\"writeChunkToFile: %\" PRId64 \" from %s track\", chunk-&gt;mTimeStampUs, chunk-&gt;mTrack-&gt;isAudio()? \"audio\": \"video\"); int32_t isFirstSample = true; while (!chunk-&gt;mSamples.empty()) &#123; List&lt;MediaBuffer *&gt;::iterator it = chunk-&gt;mSamples.begin(); off64_t offset = (chunk-&gt;mTrack-&gt;isAvc() || chunk-&gt;mTrack-&gt;isHevc()) ? addMultipleLengthPrefixedSamples_l(*it) : addSample_l(*it); if (isFirstSample) &#123; chunk-&gt;mTrack-&gt;addChunkOffset(offset); isFirstSample = false; &#125; (*it)-&gt;release(); (*it) = NULL; chunk-&gt;mSamples.erase(it); &#125; chunk-&gt;mSamples.clear();&#125; 下面看下addSamole_l(*it) 函数，wirte写入操作，mFd 是上层设置录制的文件路径传下来的文件描述符 1234567891011off64_t MPEG4Writer::addSample_l(MediaBuffer *buffer) &#123; off64_t old_offset = mOffset; ::write(mFd, (const uint8_t *)buffer-&gt;data() + buffer-&gt;range_offset(), buffer-&gt;range_length()); mOffset += buffer-&gt;range_length(); return old_offset;&#125; 到此，封装文件的写入线程的操作大体走完，下面看轨迹线程的操作。 startTracks(param) 轨迹线程的开启。文件的录制过程中是有2条轨迹线程，一个是视频的轨迹线程，另一条则是音频的轨迹线程，在starTrack（param）中是在for 循环中start了两条轨迹线程。 123456789101112131415161718192021status_t MPEG4Writer::startTracks(MetaData *params) &#123; if (mTracks.empty()) &#123; ALOGE(\"No source added\"); return INVALID_OPERATION; &#125; for (List&lt;Track *&gt;::iterator it = mTracks.begin(); it != mTracks.end(); ++it) &#123; status_t err = (*it)-&gt;start(params); if (err != OK) &#123; for (List&lt;Track *&gt;::iterator it2 = mTracks.begin(); it2 != it; ++it2) &#123; (*it2)-&gt;stop(); &#125; return err; &#125; &#125; return OK;&#125; （it)-&gt;start(params) 将会执行status_t MPEG4Writer::Track::start(MetaData params) {} 。在这边也是同样新建子线程，在子线程中执行轨迹线程的相应操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243status_t MPEG4Writer::Track::start(MetaData *params) &#123; int64_t startTimeUs; ...... mStartTimeRealUs = startTimeUs; int32_t rotationDegrees; ...... initTrackingProgressStatus(params); sp&lt;MetaData&gt; meta = new MetaData; if (mOwner-&gt;isRealTimeRecording() &amp;&amp; mOwner-&gt;numTracks() &gt; 1) &#123; int64_t startTimeOffsetUs = mOwner-&gt;getStartTimeOffsetMs() * 1000LL; if (startTimeOffsetUs &lt; 0) &#123; // Start time offset was not set startTimeOffsetUs = kInitialDelayTimeUs; &#125; startTimeUs += startTimeOffsetUs; ALOGI(\"Start time offset: %\" PRId64 \" us\", startTimeOffsetUs); &#125; meta-&gt;setInt64(kKeyTime, startTimeUs); status_t err = mSource-&gt;start(meta.get()); ...... pthread_attr_t attr; pthread_attr_init(&amp;attr); pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE); mDone = false; mStarted = true; mTrackDurationUs = 0; mReachedEOS = false; mEstimatedTrackSizeBytes = 0; mMdatSizeBytes = 0; mMaxChunkDurationUs = 0; mLastDecodingTimeUs = -1; pthread_create(&amp;mThread, &amp;attr, ThreadWrapper, this); pthread_attr_destroy(&amp;attr); return OK;&#125; 下面看下上面ThreadWrapper函数,真正的操作又是放到了threadEntry()中去执行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464void *MPEG4Writer::Track::ThreadWrapper(void *me) &#123; Track *track = static_cast&lt;Track *&gt;(me); status_t err = track-&gt;threadEntry(); return (void *) err;&#125;status_t MPEG4Writer::Track::threadEntry() &#123; int32_t count = 0; const int64_t interleaveDurationUs = mOwner-&gt;interleaveDuration(); const bool hasMultipleTracks = (mOwner-&gt;numTracks() &gt; 1); int64_t chunkTimestampUs = 0; int32_t nChunks = 0; int32_t nActualFrames = 0; // frames containing non-CSD data (non-0 length) int32_t nZeroLengthFrames = 0; int64_t lastTimestampUs = 0; // Previous sample time stamp int64_t lastDurationUs = 0; // Between the previous two samples int64_t currDurationTicks = 0; // Timescale based ticks int64_t lastDurationTicks = 0; // Timescale based ticks int32_t sampleCount = 1; // Sample count in the current stts table entry uint32_t previousSampleSize = 0; // Size of the previous sample int64_t previousPausedDurationUs = 0; int64_t timestampUs = 0; int64_t cttsOffsetTimeUs = 0; int64_t currCttsOffsetTimeTicks = 0; // Timescale based ticks int64_t lastCttsOffsetTimeTicks = -1; // Timescale based ticks int32_t cttsSampleCount = 0; // Sample count in the current ctts table entry uint32_t lastSamplesPerChunk = 0; if (mIsAudio) &#123; prctl(PR_SET_NAME, (unsigned long)\"AudioTrackEncoding\", 0, 0, 0); &#125; else &#123; prctl(PR_SET_NAME, (unsigned long)\"VideoTrackEncoding\", 0, 0, 0); &#125; if (mOwner-&gt;isRealTimeRecording()) &#123; androidSetThreadPriority(0, ANDROID_PRIORITY_AUDIO); &#125; sp&lt;MetaData&gt; meta_data; status_t err = OK; MediaBuffer *buffer; const char *trackName = mIsAudio ? \"Audio\" : \"Video\"; while (!mDone &amp;&amp; (err = mSource-&gt;read(&amp;buffer)) == OK) &#123; if (buffer-&gt;range_length() == 0) &#123; buffer-&gt;release(); buffer = NULL; ++nZeroLengthFrames; continue; &#125; // If the codec specific data has not been received yet, delay pause. // After the codec specific data is received, discard what we received // when the track is to be paused. if (mPaused &amp;&amp; !mResumed) &#123; buffer-&gt;release(); buffer = NULL; continue; &#125; ++count; int32_t isCodecConfig; if (buffer-&gt;meta_data()-&gt;findInt32(kKeyIsCodecConfig, &amp;isCodecConfig) &amp;&amp; isCodecConfig) &#123; // if config format (at track addition) already had CSD, keep that // UNLESS we have not received any frames yet. // TODO: for now the entire CSD has to come in one frame for encoders, even though // they need to be spread out for decoders. if (mGotAllCodecSpecificData &amp;&amp; nActualFrames &gt; 0) &#123; ALOGI(\"ignoring additional CSD for video track after first frame\"); &#125; else &#123; mMeta = mSource-&gt;getFormat(); // get output format after format change if (mIsAvc) &#123; status_t err = makeAVCCodecSpecificData( (const uint8_t *)buffer-&gt;data() + buffer-&gt;range_offset(), buffer-&gt;range_length()); CHECK_EQ((status_t)OK, err); &#125; else if (mIsHevc) &#123; status_t err = makeHEVCCodecSpecificData( (const uint8_t *)buffer-&gt;data() + buffer-&gt;range_offset(), buffer-&gt;range_length()); CHECK_EQ((status_t)OK, err); &#125; else if (mIsMPEG4) &#123; copyCodecSpecificData((const uint8_t *)buffer-&gt;data() + buffer-&gt;range_offset(), buffer-&gt;range_length()); &#125; &#125; if (!mIsAudio) &#123; int32_t fps; mMeta-&gt;findInt32(kKeyFrameRate, &amp;fps); int64_t cttsOffsetTimeUs = 1000000LL/fps; mCttsOffsetTimeUs = cttsOffsetTimeUs + kMinCttsOffsetTimeUs; //delta factor &#125; buffer-&gt;release(); buffer = NULL; mGotAllCodecSpecificData = true; continue; &#125; ++nActualFrames; MediaBuffer *copy = NULL; // Check if the upstream source hints it is OK to hold on to the // buffer without releasing immediately and avoid cloning the buffer if (AVUtils::get()-&gt;canDeferRelease(buffer-&gt;meta_data())) &#123; copy = buffer; meta_data = new MetaData(*buffer-&gt;meta_data().get()); &#125; else &#123; // Make a deep copy of the MediaBuffer and Metadata and release // the original as soon as we can copy = new MediaBuffer(buffer-&gt;range_length()); memcpy(copy-&gt;data(), (uint8_t *)buffer-&gt;data() + buffer-&gt;range_offset(), buffer-&gt;range_length()); copy-&gt;set_range(0, buffer-&gt;range_length()); meta_data = new MetaData(*buffer-&gt;meta_data().get()); buffer-&gt;release(); buffer = NULL; &#125; if (mIsAvc || mIsHevc) StripStartcode(copy); size_t sampleSize = copy-&gt;range_length(); if (mIsAvc || mIsHevc) &#123; if (mOwner-&gt;useNalLengthFour()) &#123; sampleSize += 4; &#125; else &#123; sampleSize += 2; &#125; &#125; // Max file size or duration handling mMdatSizeBytes += sampleSize; updateTrackSizeEstimate(); if (mOwner-&gt;exceedsFileSizeLimit()) &#123; ALOGW(\"Recorded file size exceeds limit %\" PRId64 \"bytes\", mOwner-&gt;mMaxFileSizeLimitBytes); mOwner-&gt;notify(MEDIA_RECORDER_EVENT_INFO, MEDIA_RECORDER_INFO_MAX_FILESIZE_REACHED, 0); copy-&gt;release(); mSource-&gt;stop(); break; &#125; if (mOwner-&gt;exceedsFileDurationLimit()) &#123; ALOGW(\"Recorded file duration exceeds limit %\" PRId64 \"microseconds\", mOwner-&gt;mMaxFileDurationLimitUs); mOwner-&gt;notify(MEDIA_RECORDER_EVENT_INFO, MEDIA_RECORDER_INFO_MAX_DURATION_REACHED, 0); copy-&gt;release(); mSource-&gt;stop(); break; &#125; int32_t isSync = false; meta_data-&gt;findInt32(kKeyIsSyncFrame, &amp;isSync); CHECK(meta_data-&gt;findInt64(kKeyTime, &amp;timestampUs));//////////////////////////////////////////////////////////////////////////////// if (mStszTableEntries-&gt;count() == 0) &#123; mFirstSampleTimeRealUs = systemTime() / 1000; mStartTimestampUs = timestampUs; mOwner-&gt;setStartTimestampUs(mStartTimestampUs); previousPausedDurationUs = mStartTimestampUs; &#125; if (mResumed) &#123; int64_t durExcludingEarlierPausesUs = timestampUs - previousPausedDurationUs; if (WARN_UNLESS(durExcludingEarlierPausesUs &gt;= 0ll, \"for %s track\", trackName)) &#123; copy-&gt;release(); mSource-&gt;stop(); mIsMalformed = true; break; &#125; int64_t pausedDurationUs = durExcludingEarlierPausesUs - mTrackDurationUs; if (WARN_UNLESS(pausedDurationUs &gt;= lastDurationUs, \"for %s track\", trackName)) &#123; copy-&gt;release(); mSource-&gt;stop(); mIsMalformed = true; break; &#125; previousPausedDurationUs += pausedDurationUs - lastDurationUs; mResumed = false; &#125; timestampUs -= previousPausedDurationUs; if (WARN_UNLESS(timestampUs &gt;= 0ll, \"for %s track\", trackName)) &#123; copy-&gt;release(); mSource-&gt;stop(); mIsMalformed = true; break; &#125; if (!mIsAudio) &#123; /* * Composition time: timestampUs * Decoding time: decodingTimeUs * Composition time offset = composition time - decoding time */ int64_t decodingTimeUs; CHECK(meta_data-&gt;findInt64(kKeyDecodingTime, &amp;decodingTimeUs)); decodingTimeUs -= previousPausedDurationUs; // ensure non-negative, monotonic decoding time if (mLastDecodingTimeUs &lt; 0) &#123; decodingTimeUs = std::max((int64_t)0, decodingTimeUs); &#125; else &#123; // increase decoding time by at least 1 tick decodingTimeUs = std::max( mLastDecodingTimeUs + divUp(1000000, mTimeScale), decodingTimeUs); &#125; mLastDecodingTimeUs = decodingTimeUs; cttsOffsetTimeUs = timestampUs + mCttsOffsetTimeUs - decodingTimeUs; if (cttsOffsetTimeUs &lt; 0) &#123; cttsOffsetTimeUs = 0; &#125; if (WARN_UNLESS(cttsOffsetTimeUs &gt;= 0ll, \"for %s track\", trackName)) &#123; copy-&gt;release(); mSource-&gt;stop(); mIsMalformed = true; break; &#125; timestampUs = decodingTimeUs; ALOGV(\"decoding time: %\" PRId64 \" and ctts offset time: %\" PRId64, timestampUs, cttsOffsetTimeUs); // Update ctts box table if necessary currCttsOffsetTimeTicks = (cttsOffsetTimeUs * mTimeScale + 500000LL) / 1000000LL; if (WARN_UNLESS(currCttsOffsetTimeTicks &lt;= 0x0FFFFFFFFLL, \"for %s track\", trackName)) &#123; copy-&gt;release(); mSource-&gt;stop(); mIsMalformed = true; break; &#125; if (mStszTableEntries-&gt;count() == 0) &#123; lastCttsOffsetTimeTicks = currCttsOffsetTimeTicks; //addOneCttsTableEntry(1, currCttsOffsetTimeTicks); //cttsSampleCount = 0; // No sample in ctts box is pending cttsSampleCount = 1; &#125; else &#123; if (currCttsOffsetTimeTicks != lastCttsOffsetTimeTicks) &#123; addOneCttsTableEntry(cttsSampleCount, lastCttsOffsetTimeTicks); lastCttsOffsetTimeTicks = currCttsOffsetTimeTicks; cttsSampleCount = 1; // One sample in ctts box is pending &#125; else &#123; ++cttsSampleCount; &#125; &#125; // Update ctts time offset range if (mStszTableEntries-&gt;count() == 0) &#123; mMinCttsOffsetTimeUs = currCttsOffsetTimeTicks; mMaxCttsOffsetTimeUs = currCttsOffsetTimeTicks; &#125; else &#123; if (currCttsOffsetTimeTicks &gt; mMaxCttsOffsetTimeUs) &#123; mMaxCttsOffsetTimeUs = currCttsOffsetTimeTicks; &#125; else if (currCttsOffsetTimeTicks &lt; mMinCttsOffsetTimeUs) &#123; mMinCttsOffsetTimeUs = currCttsOffsetTimeTicks; &#125; &#125; &#125; if (mOwner-&gt;isRealTimeRecording()) &#123; if (mIsAudio) &#123; updateDriftTime(meta_data); &#125; &#125; if (WARN_UNLESS(timestampUs &gt;= 0ll, \"for %s track\", trackName)) &#123; copy-&gt;release(); mSource-&gt;stop(); mIsMalformed = true; break; &#125; ALOGV(\"%s media time stamp: %\" PRId64 \" and previous paused duration %\" PRId64, trackName, timestampUs, previousPausedDurationUs); if (timestampUs &gt; mTrackDurationUs) &#123; mTrackDurationUs = timestampUs; &#125; // We need to use the time scale based ticks, rather than the // timestamp itself to determine whether we have to use a new // stts entry, since we may have rounding errors. // The calculation is intended to reduce the accumulated // rounding errors. currDurationTicks = ((timestampUs * mTimeScale + 500000LL) / 1000000LL - (lastTimestampUs * mTimeScale + 500000LL) / 1000000LL); if (currDurationTicks &lt; 0ll) &#123; ALOGE(\"do not support out of order frames (timestamp: %lld &lt; last: %lld for %s track\", (long long)timestampUs, (long long)lastTimestampUs, trackName); copy-&gt;release(); mSource-&gt;stop(); mIsMalformed = true; break; &#125; // if the duration is different for this sample, see if it is close enough to the previous // duration that we can fudge it and use the same value, to avoid filling the stts table // with lots of near-identical entries. // \"close enough\" here means that the current duration needs to be adjusted by less // than 0.1 milliseconds if (lastDurationTicks &amp;&amp; (currDurationTicks != lastDurationTicks)) &#123; int64_t deltaUs = ((lastDurationTicks - currDurationTicks) * 1000000LL + (mTimeScale / 2)) / mTimeScale; if (deltaUs &gt; -100 &amp;&amp; deltaUs &lt; 100) &#123; // use previous ticks, and adjust timestamp as if it was actually that number // of ticks currDurationTicks = lastDurationTicks; timestampUs += deltaUs; &#125; &#125; mStszTableEntries-&gt;add(htonl(sampleSize)); if (mStszTableEntries-&gt;count() &gt; 2) &#123; // Force the first sample to have its own stts entry so that // we can adjust its value later to maintain the A/V sync. if (mStszTableEntries-&gt;count() == 3 || currDurationTicks != lastDurationTicks) &#123; addOneSttsTableEntry(sampleCount, lastDurationTicks); sampleCount = 1; &#125; else &#123; ++sampleCount; &#125; &#125; if (mSamplesHaveSameSize) &#123; if (mStszTableEntries-&gt;count() &gt;= 2 &amp;&amp; previousSampleSize != sampleSize) &#123; mSamplesHaveSameSize = false; &#125; previousSampleSize = sampleSize; &#125; ALOGV(\"%s timestampUs/lastTimestampUs: %\" PRId64 \"/%\" PRId64, trackName, timestampUs, lastTimestampUs); lastDurationUs = timestampUs - lastTimestampUs; lastDurationTicks = currDurationTicks; lastTimestampUs = timestampUs; if (isSync != 0) &#123; addOneStssTableEntry(mStszTableEntries-&gt;count()); &#125; if (mTrackingProgressStatus) &#123; if (mPreviousTrackTimeUs &lt;= 0) &#123; mPreviousTrackTimeUs = mStartTimestampUs; &#125; trackProgressStatus(timestampUs); &#125; if (!hasMultipleTracks) &#123; off64_t offset = (mIsAvc || mIsHevc) ? mOwner-&gt;addMultipleLengthPrefixedSamples_l(copy) : mOwner-&gt;addSample_l(copy); uint32_t count = (mOwner-&gt;use32BitFileOffset() ? mStcoTableEntries-&gt;count() : mCo64TableEntries-&gt;count()); if (count == 0) &#123; addChunkOffset(offset); &#125; copy-&gt;release(); copy = NULL; continue; &#125; mChunkSamples.push_back(copy); if (interleaveDurationUs == 0) &#123; addOneStscTableEntry(++nChunks, 1); bufferChunk(timestampUs); &#125; else &#123; if (chunkTimestampUs == 0) &#123; chunkTimestampUs = timestampUs; &#125; else &#123; int64_t chunkDurationUs = timestampUs - chunkTimestampUs; if (chunkDurationUs &gt; interleaveDurationUs) &#123; if (chunkDurationUs &gt; mMaxChunkDurationUs) &#123; mMaxChunkDurationUs = chunkDurationUs; &#125; ++nChunks; if (nChunks == 1 || // First chunk lastSamplesPerChunk != mChunkSamples.size()) &#123; lastSamplesPerChunk = mChunkSamples.size(); addOneStscTableEntry(nChunks, lastSamplesPerChunk); &#125; bufferChunk(timestampUs); chunkTimestampUs = timestampUs; &#125; &#125; &#125; &#125; if (isTrackMalFormed()) &#123; err = ERROR_MALFORMED; &#125; mOwner-&gt;trackProgressStatus(mTrackId, -1, err); // Last chunk if (!hasMultipleTracks) &#123; addOneStscTableEntry(1, mStszTableEntries-&gt;count()); &#125; else if (!mChunkSamples.empty()) &#123; addOneStscTableEntry(++nChunks, mChunkSamples.size()); bufferChunk(timestampUs); &#125; // We don't really know how long the last frame lasts, since // there is no frame time after it, just repeat the previous // frame's duration. if (mStszTableEntries-&gt;count() == 1) &#123; lastDurationUs = 0; // A single sample's duration lastDurationTicks = 0; &#125; else &#123; ++sampleCount; // Count for the last sample &#125; if (mStszTableEntries-&gt;count() &lt;= 2) &#123; addOneSttsTableEntry(1, lastDurationTicks); if (sampleCount - 1 &gt; 0) &#123; addOneSttsTableEntry(sampleCount - 1, lastDurationTicks); &#125; &#125; else &#123; addOneSttsTableEntry(sampleCount, lastDurationTicks); &#125; // The last ctts box may not have been written yet, and this // is to make sure that we write out the last ctts box. if (currCttsOffsetTimeTicks == lastCttsOffsetTimeTicks) &#123; if (cttsSampleCount &gt; 0) &#123; addOneCttsTableEntry(cttsSampleCount, lastCttsOffsetTimeTicks); &#125; &#125; mTrackDurationUs += lastDurationUs; mReachedEOS = true; sendTrackSummary(hasMultipleTracks); ALOGI(\"Received total/0-length (%d/%d) buffers and encoded %d frames. - %s\", count, nZeroLengthFrames, mStszTableEntries-&gt;count(), trackName); if (mIsAudio) &#123; ALOGI(\"Audio track drift time: %\" PRId64 \" us\", mOwner-&gt;getDriftTimeUs()); &#125; // if err is ERROR_IO (ex: during SSR), return OK to save the // recorded file successfully. Session tear down will happen as part of // client callback if ((err == ERROR_IO) || (err == ERROR_END_OF_STREAM)) &#123; return OK; &#125; return err;&#125; 下面看下，录制文件结束时的一些操作。录制文件结束时，上层应用分别是调用 MediaRecorder的stop()、reset()和release()法，下面看下MPEG4Writer.cpp中相对应的操作。 12345678910111213141516171819202122232425262728293031status_t MPEG4Writer::Track::stop() &#123; ALOGD(\"%s track stopping\", mIsAudio? \"Audio\": \"Video\"); if (!mStarted) &#123; ALOGE(\"Stop() called but track is not started\"); return ERROR_END_OF_STREAM; &#125; if (mDone) &#123; return OK; &#125; mDone = true; ALOGD(\"%s track source stopping\", mIsAudio? \"Audio\": \"Video\"); mSource-&gt;stop(); ALOGD(\"%s track source stopped\", mIsAudio? \"Audio\": \"Video\"); void *dummy; pthread_join(mThread, &amp;dummy); status_t err = static_cast&lt;status_t&gt;(reinterpret_cast&lt;uintptr_t&gt;(dummy)); ALOGD(\"%s track stopped\", mIsAudio? \"Audio\": \"Video\"); return err;&#125;void MPEG4Writer::release() &#123; close(mFd); mFd = -1; mInitCheck = NO_INIT; mStarted = false; free(mMoovBoxBuffer); mMoovBoxBuffer = NULL;&#125; （六）、参考资料(特别感谢各位前辈的分析和图示)：Camera#capture-videoAndroid NuPlayer播放框架 android ACodec MediaCodec NuPlayer flow - CSDN博客android MediaCodec ACodec - CSDN博客ffmpeg开发之旅(1)-(7)（总共七篇）深入理解Android音视频同步机制（总共五篇）Android硬编码——音频编码、视频编码及音视频混合Android 高通平台Camera录制–MPEG4Writer.cpp 简单跟读","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Video System（2）：音视频分离MediaExtractor、解码Decoder、渲染Renderer源码分析","slug":"Android Video System（2）：音视频分离MediaExtractor、解码Decoder、渲染Renderer源码分析","date":"2018-06-05T16:00:00.000Z","updated":"2018-05-17T16:23:42.126Z","comments":true,"path":"2018/06/06/Android Video System（2）：音视频分离MediaExtractor、解码Decoder、渲染Renderer源码分析/","link":"","permalink":"http://zhoujinjian.cc/2018/06/06/Android Video System（2）：音视频分离MediaExtractor、解码Decoder、渲染Renderer源码分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】【特别感谢 - Android NuPlayer播放框架】【特别感谢 - android ACodec MediaCodec NuPlayer flow】Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) ☯ V4l2 框架代码☯ kernel/drivers/media/v4l2-core/（文件前缀为 videobuf2） ☯ MSM 视频驱动程序文件☯ kernel/drivers/media/platform/msm/vidc/ ☯ 设备树☯ /kernel/arch/arm/boot/dts/qcom（Venus 的寄存器基址，时钟频率） ☯ Stagefright、libmedia、libmediaplayerservice、mediaserver☯ /frameworks/av/media/ ☯ OMX☯ /hardware/qcom/media/mam8996/mm-video-v4l2/vidc/ ☯ OMX 核心☯ /hardware/qcom/media/mm-core ☯ 软件编解码器路径☯ /vendor/qcom/proprietary/mm-video/omx_vpp(?)→ 解码器代码☯ /vendor/qcom/proprietary/mm-video/omx_vpp(?) → 编码器代码 Android在Java层中提供了一个MediaPlayer的类来作为播放媒体资源的接口，在使用中我们通常会编写以下的代码： 12345678910mMediaPlayer = new MediaPlayer();mMediaPlayer.setDataSource(Environment.getExternalStorageDirectory()+\"/test_video.mp4\");mMediaPlayer.setDisplay(...);mMediaPlayer.setAudioStreamType(AudioManager.STREAM_MUSIC);mMediaPlayer.prepareAsync();mMediaPlayer.start();mediaPlayer.pause(); mediaPlayer.stop();mediaPlayer.reset();mediaPlayer.release(); 前面第一章节已经分析过mMediaPlayer.setDataSource()、mMediaPlayer.setDisplay()下来的分析尝试分析解答如下疑问： 不同格式的多媒体文件如何探测并解析的？音视频数据缓冲区在哪里？（Source）音频解码线程、视频解码线程在哪里？ （DecoderBase）视频如何显示的？音频如何播放的？音视频同步在哪里？（Renderer） （一）、多媒体文件解析 - MediaExtractor分离音视频接下来继续分析mMediaPlayer.prepareAsync() 1.1、mMediaPlayer.prepareAsync()12[-&gt;\\frameworks\\base\\media\\java\\android\\media\\MediaPlayer.java]public native void prepareAsync() throws IllegalStateException; 通过JNI调用 1234567891011121314151617[-&gt;\\frameworks\\base\\media\\jni\\android_media_MediaPlayer.cpp]static voidandroid_media_MediaPlayer_prepareAsync(JNIEnv *env, jobject thiz)&#123; sp&lt;MediaPlayer&gt; mp = getMediaPlayer(env, thiz); if (mp == NULL ) &#123; jniThrowException(env, \"java/lang/IllegalStateException\", NULL); return; &#125; // Handle the case where the display surface was set before the mp was // initialized. We try again to make it stick. sp&lt;IGraphicBufferProducer&gt; st = getVideoSurfaceTexture(env, thiz); mp-&gt;setVideoSurfaceTexture(st); process_media_player_call( env, thiz, mp-&gt;prepareAsync(), \"java/io/IOException\", \"Prepare Async failed.\" );&#125; 首先设置视频的 display surface（关于IGraphicBufferProducer相关知识请参考：Android 7.1.2 (Android N) Android Graphics 系统 分析 [i.wonder~]）， 1.1.1、MediaPlayer.setVideoSurfaceTexture()123456789[-&gt;\\frameworks\\av\\media\\libmedia\\mediaplayer.cpp]status_t MediaPlayer::setVideoSurfaceTexture( const sp&lt;IGraphicBufferProducer&gt;&amp; bufferProducer)&#123; ALOGV(\"setVideoSurfaceTexture\"); Mutex::Autolock _l(mLock); if (mPlayer == 0) return NO_INIT; return mPlayer-&gt;setVideoSurfaceTexture(bufferProducer);&#125; 前面setDataSource()分析过，此处会调用NuPlayer的setVideoSurfaceTexture()函数 12345678910111213[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]void NuPlayer::setVideoSurfaceTextureAsync( const sp&lt;IGraphicBufferProducer&gt; &amp;bufferProducer) &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatSetVideoSurface, this); if (bufferProducer == NULL) &#123; msg-&gt;setObject(\"surface\", NULL); &#125; else &#123; msg-&gt;setObject(\"surface\", new Surface(bufferProducer, true /* controlledByApp */)); &#125; msg-&gt;post();&#125; 此处首先构造了一个AMessage消息，然后new Surface()，接下来看看消息处理过程。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]case kWhatSetVideoSurface: &#123; sp&lt;RefBase&gt; obj; CHECK(msg-&gt;findObject(\"surface\", &amp;obj)); sp&lt;Surface&gt; surface = static_cast&lt;Surface *&gt;(obj.get()); if (mSource == NULL || !mStarted || mSource-&gt;getFormat(false /* audio */) == NULL // NOTE: mVideoDecoder's mSurface is always non-null || (mVideoDecoder != NULL &amp;&amp; mVideoDecoder-&gt;setVideoSurface(surface) == OK)) &#123; performSetSurface(surface); break; &#125; mDeferredActions.push_back( new FlushDecoderAction(FLUSH_CMD_FLUSH /* audio */, FLUSH_CMD_SHUTDOWN /* video */)); mDeferredActions.push_back(new SetSurfaceAction(surface)); if (obj != NULL || mAudioDecoder != NULL) &#123; if (mStarted) &#123; int64_t currentPositionUs = 0; if (getCurrentPosition(&amp;currentPositionUs) == OK) &#123; mDeferredActions.push_back( new SeekAction(currentPositionUs)); &#125; &#125; mDeferredActions.push_back( new SimpleAction(&amp;NuPlayer::performScanSources)); &#125; mDeferredActions.push_back( new ResumeDecoderAction(false /* needNotify */)); processDeferredActions(); break; &#125;void NuPlayer::performSetSurface(const sp&lt;Surface&gt; &amp;surface) &#123; ALOGV(\"performSetSurface\"); mSurface = surface; // XXX - ignore error from setVideoScalingMode for now setVideoScalingMode(mVideoScalingMode); if (mDriver != NULL) &#123; sp&lt;NuPlayerDriver&gt; driver = mDriver.promote(); if (driver != NULL) &#123; driver-&gt;notifySetSurfaceComplete(); &#125; &#125;&#125; 可以看到将surface 赋值给NuPlayer的mSurface ，待视频解码后就可以在此surface 上渲染画面了，这个稍后再作分析。 1.1.2、MediaPlayer.prepareAsync()然后接着调用MediaPlayer prepareAsync()函数。12345678910111213141516171819202122[-&gt;\\frameworks\\av\\media\\libmedia\\mediaplayer.cpp]status_t MediaPlayer::prepareAsync()&#123; ALOGV(\"prepareAsync\"); Mutex::Autolock _l(mLock); return prepareAsync_l();&#125;status_t MediaPlayer::prepareAsync_l()&#123; if ( (mPlayer != 0) &amp;&amp; ( mCurrentState &amp; (MEDIA_PLAYER_INITIALIZED | MEDIA_PLAYER_STOPPED) ) ) &#123; if (mAudioAttributesParcel != NULL) &#123; mPlayer-&gt;setParameter(KEY_PARAMETER_AUDIO_ATTRIBUTES, *mAudioAttributesParcel); &#125; else &#123; mPlayer-&gt;setAudioStreamType(mStreamType); &#125; mCurrentState = MEDIA_PLAYER_PREPARING; return mPlayer-&gt;prepareAsync(); &#125; ALOGE(\"prepareAsync called in state %d, mPlayer(%p)\", mCurrentState, mPlayer.get()); return INVALID_OPERATION;&#125; 此处会调用NuPlayer的prepareAsync()函数，prepareAsync()发送了一个kWhatPrepare的AMessage，我们直接看看消息处理。 123456789[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]void NuPlayer::prepareAsync() &#123; (new AMessage(kWhatPrepare, this))-&gt;post();&#125; case kWhatPrepare:&#123; mSource-&gt;prepareAsync(); break;&#125; 此处又调用了GenericSource的prepareAsync()函数，发送了一个kWhatPrepareAsync消息。直接看看GenericSource如何处理的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\GenericSource.cpp]void NuPlayer::GenericSource::prepareAsync() &#123; if (mLooper == NULL) &#123; mLooper = new ALooper; mLooper-&gt;setName(\"generic\"); mLooper-&gt;start(); mLooper-&gt;registerHandler(this); &#125; sp&lt;AMessage&gt; msg = new AMessage(kWhatPrepareAsync, this); msg-&gt;post();&#125; switch (msg-&gt;what()) &#123; case kWhatPrepareAsync: &#123; onPrepareAsync(); break; &#125;void NuPlayer::GenericSource::onPrepareAsync() &#123; // delayed data source creation if (mDataSource == NULL) &#123; // set to false first, if the extractor // comes back as secure, set it to true then. mIsSecure = false; if (!mUri.empty()) &#123; const char* uri = mUri.c_str(); String8 contentType; mIsWidevine = !strncasecmp(uri, \"widevine://\", 11); if (!strncasecmp(\"http://\", uri, 7) || !strncasecmp(\"https://\", uri, 8) || mIsWidevine) &#123; mHttpSource = DataSource::CreateMediaHTTP(mHTTPService); ...... &#125; mDataSource = DataSource::CreateFromURI( mHTTPService, uri, &amp;mUriHeaders, &amp;contentType, static_cast&lt;HTTPBase *&gt;(mHttpSource.get())); &#125; else &#123; mIsWidevine = false; mDataSource = new FileSource(mFd, mOffset, mLength); mFd = -1; &#125; ...... &#125; if (mDataSource-&gt;flags() &amp; DataSource::kIsCachingDataSource) &#123; mCachedSource = static_cast&lt;NuCachedSource2 *&gt;(mDataSource.get()); &#125; mIsStreaming = (mIsWidevine || mCachedSource != NULL); // init extractor from data source status_t err = initFromDataSource(); if (mVideoTrack.mSource != NULL) &#123; sp&lt;MetaData&gt; meta = doGetFormatMeta(false /* audio */); sp&lt;AMessage&gt; msg = new AMessage; err = convertMetaDataToMessage(meta, &amp;msg); ...... notifyVideoSizeChanged(msg); &#125; ...... if (mIsSecure) &#123; // secure decoders must be instantiated before starting widevine source sp&lt;AMessage&gt; reply = new AMessage(kWhatSecureDecodersInstantiated, this); notifyInstantiateSecureDecoders(reply); &#125; else &#123; finishPrepareAsync(); &#125;&#125; 首先构造了 mDataSource = new FileSource，然后调用了initFromDataSource()，这里面包含多媒体文件格式探测，。 1.1.3、GenericSource.initFromDataSource()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\GenericSource.cpp]status_t NuPlayer::GenericSource::initFromDataSource() &#123; sp&lt;IMediaExtractor&gt; extractor; String8 mimeType; float confidence; sp&lt;AMessage&gt; dummy; bool isWidevineStreaming = false; CHECK(mDataSource != NULL); if (mIsWidevine) &#123; ...... &#125; else if (mIsStreaming) &#123; if (!mDataSource-&gt;sniff(&amp;mimeType, &amp;confidence, &amp;dummy)) &#123; return UNKNOWN_ERROR; &#125; isWidevineStreaming = !strcasecmp( mimeType.string(), MEDIA_MIMETYPE_CONTAINER_WVM); &#125; if (isWidevineStreaming) &#123; ...... &#125; else &#123; extractor = MediaExtractor::Create(mDataSource, mimeType.isEmpty() ? NULL : mimeType.string()); &#125; ...... if (extractor-&gt;getDrmFlag()) &#123; checkDrmStatus(mDataSource); &#125; mFileMeta = extractor-&gt;getMetaData(); if (mFileMeta != NULL) &#123; int64_t duration; if (mFileMeta-&gt;findInt64(kKeyDuration, &amp;duration)) &#123; mDurationUs = duration; &#125;...... &#125; int32_t totalBitrate = 0; size_t numtracks = extractor-&gt;countTracks(); for (size_t i = 0; i &lt; numtracks; ++i) &#123; sp&lt;IMediaSource&gt; track = extractor-&gt;getTrack(i); sp&lt;MetaData&gt; meta = extractor-&gt;getTrackMetaData(i); const char *mime; CHECK(meta-&gt;findCString(kKeyMIMEType, &amp;mime)); // Do the string compare immediately with \"mime\", // we can't assume \"mime\" would stay valid after another // extractor operation, some extractors might modify meta // during getTrack() and make it invalid. if (!strncasecmp(mime, \"audio/\", 6)) &#123; if (mAudioTrack.mSource == NULL) &#123; mAudioTrack.mIndex = i; mAudioTrack.mSource = track; mAudioTrack.mPackets = new AnotherPacketSource(mAudioTrack.mSource-&gt;getFormat()); if (!strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_VORBIS)) &#123; mAudioIsVorbis = true; &#125; else &#123; mAudioIsVorbis = false; &#125; &#125; &#125; else if (!strncasecmp(mime, \"video/\", 6)) &#123; if (mVideoTrack.mSource == NULL) &#123; mVideoTrack.mIndex = i; mVideoTrack.mSource = track; mVideoTrack.mPackets = new AnotherPacketSource(mVideoTrack.mSource-&gt;getFormat()); // check if the source requires secure buffers int32_t secure; if (meta-&gt;findInt32(kKeyRequiresSecureBuffers, &amp;secure) &amp;&amp; secure) &#123; mIsSecure = true; if (mUIDValid) &#123; extractor-&gt;setUID(mUID); &#125; &#125; &#125; &#125; mSources.push(track); int64_t durationUs; if (meta-&gt;findInt64(kKeyDuration, &amp;durationUs)) &#123; if (durationUs &gt; mDurationUs) &#123; mDurationUs = durationUs; &#125; &#125; int32_t bitrate; if (totalBitrate &gt;= 0 &amp;&amp; meta-&gt;findInt32(kKeyBitRate, &amp;bitrate)) &#123; totalBitrate += bitrate; &#125; else &#123; totalBitrate = -1; &#125; &#125; ...... mBitrate = totalBitrate; return OK;&#125; 可以看到通过MediaExtractor::Create()得到MediaExtractor，然后将数据解析成track 赋值给mAudioTrack.mSource、mVideoTrack.mSource。 1.1.4、MediaExtractor::Create()123456789101112131415161718192021222324252627282930313233343536373839404142434445[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaExtractor.cpp]sp&lt;IMediaExtractor&gt; MediaExtractor::Create( const sp&lt;DataSource&gt; &amp;source, const char *mime) &#123; ALOGV(\"MediaExtractor::Create %s\", mime); char value[PROPERTY_VALUE_MAX]; if (property_get(\"media.stagefright.extractremote\", value, NULL) &amp;&amp; (!strcmp(\"0\", value) || !strcasecmp(\"false\", value))) &#123; // local extractor ALOGW(\"creating media extractor in calling process\"); return CreateFromService(source, mime); &#125; else &#123; // Check if it's WVM, since WVMExtractor needs to be created in the media server process, // not the extractor process. String8 mime8; float confidence; sp&lt;AMessage&gt; meta; if (SniffWVM(source, &amp;mime8, &amp;confidence, &amp;meta) &amp;&amp; !strcasecmp(mime8, MEDIA_MIMETYPE_CONTAINER_WVM)) &#123; return new WVMExtractor(source); &#125; ...... if (SniffDRM(source, &amp;mime8, &amp;confidence, &amp;meta)) &#123; const char *drmMime = mime8.string(); ALOGV(\"Detected media content as '%s' with confidence %.2f\", drmMime, confidence); if (!strncmp(drmMime, \"drm+es_based+\", 13)) &#123; // DRMExtractor sets container metadata kKeyIsDRM to 1 return new DRMExtractor(source, drmMime + 14); &#125; &#125; // remote extractor ALOGV(\"get service manager\"); sp&lt;IBinder&gt; binder = defaultServiceManager()-&gt;getService(String16(\"media.extractor\")); if (binder != 0) &#123; sp&lt;IMediaExtractorService&gt; mediaExService(interface_cast&lt;IMediaExtractorService&gt;(binder)); sp&lt;IMediaExtractor&gt; ex = mediaExService-&gt;makeExtractor(RemoteDataSource::wrap(source), mime); return ex; &#125; else &#123; ...... &#125; &#125; return NULL;&#125; 可以看到通过Binder通信获取”media.extractor”服务得到一个Extractor。 1.1.5、IMediaExtractor-&gt;getTrack()根据不同类别解析出不同的Track 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[-&gt;\\frameworks\\av\\media\\libstagefright\\]AACExtractor.cpp sp&lt;IMediaSource&gt; AACExtractor::getTrack(size_t index)AMRExtractor.cpp sp&lt;IMediaSource&gt; AMRExtractor::getTrack(size_t index)MP3Extractor.cpp sp&lt;IMediaSource&gt; MP3Extractor::getTrack(size_t index)NuMediaExtractor.cpp sp&lt;IMediaSource&gt; source = mImpl-&gt;getTrack(index)WAVExtractor.cpp sp&lt;IMediaSource&gt; WAVExtractor::getTrack(size_t index)FLACExtractor.cpp sp&lt;IMediaSource&gt; FLACExtractor::getTrack(size_t index) StagefrightMetadataRetriever.cpp sp&lt;IMediaSource&gt; source = mExtractor-&gt;getTrack(i)AVIExtractor.cpp sp&lt;MediaSource&gt; AVIExtractor::getTrack(size_t index)OggExtractor.cpp sp&lt;IMediaSource&gt; OggExtractor::getTrack(size_t index)MPEG4Extractor.cpp sp&lt;IMediaSource&gt; MPEG4Extractor::getTrack(size_t index)//MP3sp&lt;IMediaSource&gt; MP3Extractor::getTrack(size_t index) &#123; return new MP3Source( mMeta, mDataSource, mFirstFramePos, mFixedHeader, mSeeker);&#125;//MPEG4sp&lt;IMediaSource&gt; MPEG4Extractor::getTrack(size_t index) &#123; status_t err; ...... Track *track = mFirstTrack; while (index &gt; 0) &#123; if (track == NULL) &#123; return NULL; &#125; track = track-&gt;next; --index; &#125; ...... Trex *trex = NULL; int32_t trackId; if (track-&gt;meta-&gt;findInt32(kKeyTrackID, &amp;trackId)) &#123; for (size_t i = 0; i &lt; mTrex.size(); i++) &#123; Trex *t = &amp;mTrex.editItemAt(i); if (t-&gt;track_ID == (uint32_t) trackId) &#123; trex = t; break; &#125; &#125; &#125; else &#123; ...... &#125; const char *mime; ....... if (!strcasecmp(mime, MEDIA_MIMETYPE_VIDEO_AVC)) &#123; uint32_t type; const void *data; size_t size; if (!track-&gt;meta-&gt;findData(kKeyAVCC, &amp;type, &amp;data, &amp;size)) &#123; return NULL; &#125; const uint8_t *ptr = (const uint8_t *)data; ...... &#125; else if (!strcasecmp(mime, MEDIA_MIMETYPE_VIDEO_HEVC)) &#123; uint32_t type; const void *data; size_t size; if (!track-&gt;meta-&gt;findData(kKeyHVCC, &amp;type, &amp;data, &amp;size)) &#123; return NULL; &#125; const uint8_t *ptr = (const uint8_t *)data; ...... &#125; return new MPEG4Source(this, track-&gt;meta, mDataSource, track-&gt;timescale, track-&gt;sampleTable, mSidxEntries, trex, mMoofOffset);&#125; 得到不同格式的 MP3Extractor、MPEG4Source …… 还记的前面提出的第一点疑问吗，现在我们知道了如何分离音视频了并且得到了相应的文件Source了。图示（红线部分）： （二）、多媒体文件 - 音视频解码（Decoder）音频解码、视频解码在何处，答案就在mMediaPlayer.start()流程当中，先看看start()总体时序图，然后一步步分析 由于从Java层到JNI前面已多次分析，这里直接从NuPlayer::start()开始分析 2.1、NuPlayer::start()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]void NuPlayer::start() &#123; (new AMessage(kWhatStart, this))-&gt;post();&#125; case kWhatStart: &#123; ALOGV(\"kWhatStart\"); if (mStarted) &#123; // do not resume yet if the source is still buffering if (!mPausedForBuffering) &#123; onResume(); &#125; &#125; else &#123; onStart(); &#125; mPausedByClient = false; break; &#125;void NuPlayer::onStart(int64_t startPositionUs) &#123; if (!mSourceStarted) &#123; mSourceStarted = true; mSource-&gt;start(); &#125; if (startPositionUs &gt; 0) &#123; performSeek(startPositionUs); if (mSource-&gt;getFormat(false /* audio */) == NULL) &#123; return; &#125; &#125; mOffloadAudio = false; mAudioEOS = false; mVideoEOS = false; mStarted = true; mPaused = false; uint32_t flags = 0; if (mSource-&gt;isRealTime()) &#123; flags |= Renderer::FLAG_REAL_TIME; &#125; sp&lt;MetaData&gt; audioMeta = mSource-&gt;getFormatMeta(true /* audio */); sp&lt;MetaData&gt; videoMeta = mSource-&gt;getFormatMeta(false /* audio */); ...... ALOGV_IF(audioMeta == NULL, \"no metadata for audio source\"); // video only stream audio_stream_type_t streamType = AUDIO_STREAM_MUSIC; if (mAudioSink != NULL) &#123; streamType = mAudioSink-&gt;getAudioStreamType(); &#125; sp&lt;AMessage&gt; videoFormat = mSource-&gt;getFormat(false /* audio */); mOffloadAudio = canOffloadStream(audioMeta, (videoFormat != NULL), mSource-&gt;isStreaming(), streamType) &amp;&amp; (mPlaybackSettings.mSpeed == 1.f &amp;&amp; mPlaybackSettings.mPitch == 1.f); if (mOffloadAudio) &#123; flags |= Renderer::FLAG_OFFLOAD_AUDIO; &#125; sp&lt;AMessage&gt; notify = new AMessage(kWhatRendererNotify, this); ++mRendererGeneration; notify-&gt;setInt32(\"generation\", mRendererGeneration); mRenderer = new Renderer(mAudioSink, notify, flags); mRendererLooper = new ALooper; mRendererLooper-&gt;setName(\"NuPlayerRenderer\"); mRendererLooper-&gt;start(false, false, ANDROID_PRIORITY_AUDIO); mRendererLooper-&gt;registerHandler(mRenderer); status_t err = mRenderer-&gt;setPlaybackSettings(mPlaybackSettings); ...... float rate = getFrameRate(); if (rate &gt; 0) &#123; mRenderer-&gt;setVideoFrameRate(rate); &#125; if (mVideoDecoder != NULL) &#123; mVideoDecoder-&gt;setRenderer(mRenderer); &#125; if (mAudioDecoder != NULL) &#123; mAudioDecoder-&gt;setRenderer(mRenderer); &#125; postScanSources();&#125; 这里创建了名为NuPlayerRenderer的Renderer对象，然后启动循环，看看初始化 123456789101112131415161718192021222324[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]NuPlayer::Renderer::Renderer( const sp&lt;MediaPlayerBase::AudioSink&gt; &amp;sink, const sp&lt;AMessage&gt; &amp;notify, uint32_t flags) : mAudioSink(sink), mUseVirtualAudioSink(false), mNotify(notify), mFlags(flags), mNumFramesWritten(0), mDrainAudioQueuePending(false), mDrainVideoQueuePending(false), mAudioQueueGeneration(0), mVideoQueueGeneration(0), mAudioDrainGeneration(0), mVideoDrainGeneration(0), mAudioEOSGeneration(0), mPlaybackSettings(AUDIO_PLAYBACK_RATE_DEFAULT), ...... mWakeLock(new AWakeLock()) &#123; mMediaClock = new MediaClock; mPlaybackRate = mPlaybackSettings.mSpeed; mMediaClock-&gt;setPlaybackRate(mPlaybackRate);&#125; 2.2、postScanSources()1234567891011121314151617181920212223242526272829303132333435[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]void NuPlayer::postScanSources() &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatScanSources, this); msg-&gt;setInt32(\"generation\", mScanSourcesGeneration); msg-&gt;post(); mScanSourcesPending = true;&#125; case kWhatScanSources: &#123; int32_t generation; mScanSourcesPending = false; bool mHadAnySourcesBefore = (mAudioDecoder != NULL) || (mVideoDecoder != NULL); bool rescan = false; // initialize video before audio because successful initialization of // video may change deep buffer mode of audio. if (mSurface != NULL) &#123; if (instantiateDecoder(false, &amp;mVideoDecoder) == -EWOULDBLOCK) &#123; rescan = true; &#125; &#125; // Don't try to re-open audio sink if there's an existing decoder. if (mAudioSink != NULL &amp;&amp; mAudioDecoder == NULL) &#123; if (instantiateDecoder(true, &amp;mAudioDecoder) == -EWOULDBLOCK) &#123; rescan = true; &#125; &#125; ...... &#125; 此处调用了instantiateDecoder()来初始化音视频解码器Decoder 2.3、instantiateDecoder()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]status_t NuPlayer::instantiateDecoder( bool audio, sp&lt;DecoderBase&gt; *decoder, bool checkAudioModeChange) &#123; ...... sp&lt;AMessage&gt; format = mSource-&gt;getFormat(audio); format-&gt;setInt32(\"priority\", 0 /* realtime */); ...... if (audio) &#123; sp&lt;AMessage&gt; notify = new AMessage(kWhatAudioNotify, this); ++mAudioDecoderGeneration; notify-&gt;setInt32(\"generation\", mAudioDecoderGeneration); if (checkAudioModeChange) &#123; determineAudioModeChange(format); &#125; if (mOffloadAudio) &#123; mSource-&gt;setOffloadAudio(true /* offload */); const bool hasVideo = (mSource-&gt;getFormat(false /*audio */) != NULL); format-&gt;setInt32(\"has-video\", hasVideo); *decoder = new DecoderPassThrough(notify, mSource, mRenderer); &#125; else &#123; mSource-&gt;setOffloadAudio(false /* offload */); *decoder = new Decoder(notify, mSource, mPID, mRenderer); &#125; &#125; else &#123; sp&lt;AMessage&gt; notify = new AMessage(kWhatVideoNotify, this); ++mVideoDecoderGeneration; notify-&gt;setInt32(\"generation\", mVideoDecoderGeneration); *decoder = new Decoder( notify, mSource, mPID, mRenderer, mSurface, mCCDecoder); // enable FRC if high-quality AV sync is requested, even if not // directly queuing to display, as this will even improve textureview // playback. &#123; char value[PROPERTY_VALUE_MAX]; if (property_get(\"persist.sys.media.avsync\", value, NULL) &amp;&amp; (!strcmp(\"1\", value) || !strcasecmp(\"true\", value))) &#123; format-&gt;setInt32(\"auto-frc\", 1); &#125; &#125; &#125; (*decoder)-&gt;init(); (*decoder)-&gt;configure(format); ...... return OK;&#125; 2.3.1、创建音视频解码器new Decoder()123456789101112131415161718192021222324[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerDecoder.cpp]NuPlayer::Decoder::Decoder( const sp&lt;AMessage&gt; &amp;notify, const sp&lt;Source&gt; &amp;source, pid_t pid, const sp&lt;Renderer&gt; &amp;renderer, const sp&lt;Surface&gt; &amp;surface, const sp&lt;CCDecoder&gt; &amp;ccDecoder) : DecoderBase(notify), mSurface(surface), mSource(source), mRenderer(renderer), mCCDecoder(ccDecoder), ...... mVideoWidth(0), mVideoHeight(0), mIsAudio(true), ...... mComponentName(\"decoder\") &#123; mCodecLooper = new ALooper; mCodecLooper-&gt;setName(\"NPDecoder-CL\"); mCodecLooper-&gt;start(false, false, ANDROID_PRIORITY_AUDIO); mVideoTemporalLayerAggregateFps[0] = mFrameRateTotal;&#125; 创建音视频解码器（NuPlayer::Decoder），为其创建名为NPDecoder-CL的mCodecLooper 【其父类NuPlayer::DecoderBase的构造中则会创建NPDecoder】 12345678910111213[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerDecoderBase.cpp]NuPlayer::DecoderBase::DecoderBase(const sp&lt;AMessage&gt; &amp;notify) : mNotify(notify), mBufferGeneration(0), mPaused(false), mStats(new AMessage), mRequestInputBuffersPending(false) &#123; // Every decoder has its own looper because MediaCodec operations // are blocking, but NuPlayer needs asynchronous operations. mDecoderLooper = new ALooper; mDecoderLooper-&gt;setName(\"NPDecoder\"); mDecoderLooper-&gt;start(false, false, ANDROID_PRIORITY_AUDIO);&#125; 2.3.2、初始化Decoder-&gt;init()对该解码器进行init()操作，调用NuPlayer::DecoderBase::init()为mDecoderLooper注册handler【init()和configure()都是NuPlayerDecoder继承自NuPlayer::DecoderBase的方法】 123456789101112131415161718[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerDecoderBase.cpp]void NuPlayer::DecoderBase::configure(const sp&lt;AMessage&gt; &amp;format) &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatConfigure, this); msg-&gt;setMessage(\"format\", format); msg-&gt;post();&#125;void NuPlayer::DecoderBase::init() &#123; mDecoderLooper-&gt;registerHandler(this);&#125; case kWhatConfigure: &#123; sp&lt;AMessage&gt; format; CHECK(msg-&gt;findMessage(\"format\", &amp;format)); onConfigure(format); break; &#125; 对该解码器进行configure(format)操作，调用NuPlayer::DecoderBase::configure(…)产生一个kWhatConfigure消息，然后消息处理中调用NuPlayer::Decoder::onConfigure(…) 2.3.3、配置Decoder-&gt;configure()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerDecoder.cpp]void NuPlayer::Decoder::onConfigure(const sp&lt;AMessage&gt; &amp;format) &#123; mFormatChangePending = false; mTimeChangePending = false; ++mBufferGeneration; AString mime; mIsAudio = !strncasecmp(\"audio/\", mime.c_str(), 6); mIsVideoAVC = !strcasecmp(MEDIA_MIMETYPE_VIDEO_AVC, mime.c_str()); mComponentName = mime; mComponentName.append(\" decoder\"); ALOGV(\"[%s] onConfigure (surface=%p)\", mComponentName.c_str(), mSurface.get()); mCodec = MediaCodec::CreateByType( mCodecLooper, mime.c_str(), false /* encoder */, NULL /* err */, mPid); int32_t secure = 0; if (format-&gt;findInt32(\"secure\", &amp;secure) &amp;&amp; secure != 0) &#123; if (mCodec != NULL) &#123; mCodec-&gt;getName(&amp;mComponentName); mComponentName.append(\".secure\"); mCodec-&gt;release(); ALOGI(\"[%s] creating\", mComponentName.c_str()); mCodec = MediaCodec::CreateByComponentName( mCodecLooper, mComponentName.c_str(), NULL /* err */, mPid); &#125; &#125; ...... mIsSecure = secure; mCodec-&gt;getName(&amp;mComponentName); status_t err; if (mSurface != NULL) &#123; // disconnect from surface as MediaCodec will reconnect err = native_window_api_disconnect( mSurface.get(), NATIVE_WINDOW_API_MEDIA); // We treat this as a warning, as this is a preparatory step. // Codec will try to connect to the surface, which is where // any error signaling will occur. ALOGW_IF(err != OK, \"failed to disconnect from surface: %d\", err); &#125; err = mCodec-&gt;configure( format, mSurface, NULL /* crypto */, 0 /* flags */); ...... rememberCodecSpecificData(format); mStats-&gt;setString(\"mime\", mime.c_str()); mStats-&gt;setString(\"component-name\", mComponentName.c_str()); if (!mIsAudio) &#123; int32_t width, height; if (mOutputFormat-&gt;findInt32(\"width\", &amp;width) &amp;&amp; mOutputFormat-&gt;findInt32(\"height\", &amp;height)) &#123; mStats-&gt;setInt32(\"width\", width); mStats-&gt;setInt32(\"height\", height); &#125; &#125; sp&lt;AMessage&gt; reply = new AMessage(kWhatCodecNotify, this); mCodec-&gt;setCallback(reply); err = mCodec-&gt;start(); releaseAndResetMediaBuffers(); mPaused = false; mResumePending = false;&#125; 在onConfigure中，首先会调用MediaCodec::CreateByType(…)或者MediaCodec::CreateByComponentName(…)根据情况创建MediaCodec，接着调用MediaCodec::init(…)，随后调用MediaCodec::configure(…)对MediaCodec进行配置使其转入Configured状态;然后又调用MediaCodec::start()使MediaCodec转入Executing状态。 2.3.4、MediaCodec::init(…)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaCodec.cpp]sp&lt;MediaCodec&gt; MediaCodec::CreateByType( const sp&lt;ALooper&gt; &amp;looper, const AString &amp;mime, bool encoder, status_t *err, pid_t pid) &#123; sp&lt;MediaCodec&gt; codec = new MediaCodec(looper, pid); const status_t ret = codec-&gt;init(mime, true /* nameIsType */, encoder); return ret == OK ? codec : NULL; // NULL deallocates codec.&#125;sp&lt;MediaCodec&gt; MediaCodec::CreateByComponentName( const sp&lt;ALooper&gt; &amp;looper, const AString &amp;name, status_t *err, pid_t pid) &#123; sp&lt;MediaCodec&gt; codec = new MediaCodec(looper, pid); const status_t ret = codec-&gt;init(name, false /* nameIsType */, false /* encoder */); return ret == OK ? codec : NULL; // NULL deallocates codec.&#125;sp&lt;MediaCodec&gt; MediaCodec::CreateByType( const sp&lt;ALooper&gt; &amp;looper, const AString &amp;mime, bool encoder, status_t *err, pid_t pid) &#123; sp&lt;MediaCodec&gt; codec = new MediaCodec(looper, pid); const status_t ret = codec-&gt;init(mime, true /* nameIsType */, encoder); if (err != NULL) &#123; *err = ret; &#125; return ret == OK ? codec : NULL; // NULL deallocates codec.&#125;status_t MediaCodec::init(const AString &amp;name, bool nameIsType, bool encoder) &#123; mResourceManagerService-&gt;init(); // save init parameters for reset mInitName = name; mInitNameIsType = nameIsType; mInitIsEncoder = encoder; // Current video decoders do not return from OMX_FillThisBuffer // quickly, violating the OpenMAX specs, until that is remedied // we need to invest in an extra looper to free the main event // queue. mCodec = GetCodecBase(name, nameIsType); ...... bool secureCodec = false; if (nameIsType &amp;&amp; !strncasecmp(name.c_str(), \"video/\", 6)) &#123; mIsVideo = true; &#125; else &#123; AString tmp = name; if (tmp.endsWith(\".secure\")) &#123; secureCodec = true; tmp.erase(tmp.size() - 7, 7); &#125; const sp&lt;IMediaCodecList&gt; mcl = MediaCodecList::getInstance(); ...... ssize_t codecIdx = mcl-&gt;findCodecByName(tmp.c_str()); if (codecIdx &gt;= 0) &#123; const sp&lt;MediaCodecInfo&gt; info = mcl-&gt;getCodecInfo(codecIdx); Vector&lt;AString&gt; mimes; info-&gt;getSupportedMimes(&amp;mimes); for (size_t i = 0; i &lt; mimes.size(); i++) &#123; if (mimes[i].startsWith(\"video/\")) &#123; mIsVideo = true; break; &#125; &#125; &#125; &#125; if (mIsVideo) &#123; // video codec needs dedicated looper if (mCodecLooper == NULL) &#123; mCodecLooper = new ALooper; mCodecLooper-&gt;setName(\"CodecLooper\"); mCodecLooper-&gt;start(false, false, ANDROID_PRIORITY_AUDIO); &#125; mCodecLooper-&gt;registerHandler(mCodec); &#125; else &#123; mLooper-&gt;registerHandler(mCodec); &#125; mLooper-&gt;registerHandler(this); mCodec-&gt;setNotificationMessage(new AMessage(kWhatCodecNotify, this)); sp&lt;AMessage&gt; msg = new AMessage(kWhatInit, this); msg-&gt;setString(\"name\", name); msg-&gt;setInt32(\"nameIsType\", nameIsType); if (nameIsType) &#123; msg-&gt;setInt32(\"encoder\", encoder); &#125; status_t err; Vector&lt;MediaResource&gt; resources; MediaResource::Type type = secureCodec ? MediaResource::kSecureCodec : MediaResource::kNonSecureCodec; MediaResource::SubType subtype = mIsVideo ? MediaResource::kVideoCodec : MediaResource::kAudioCodec; resources.push_back(MediaResource(type, subtype, 1)); for (int i = 0; i &lt;= kMaxRetry; ++i) &#123; sp&lt;AMessage&gt; response; err = PostAndAwaitResponse(msg, &amp;response); &#125; return err;&#125; 2.3.4.1、GetCodecBase当编解码以”omx.”开头则创建ACodec对象。1234567891011[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaCodec.cpp]sp&lt;CodecBase&gt; MediaCodec::GetCodecBase(const AString &amp;name, bool nameIsType) &#123; // at this time only ACodec specifies a mime type. if (nameIsType || name.startsWithIgnoreCase(\"omx.\")) &#123; return new ACodec; &#125; else if (name.startsWithIgnoreCase(\"android.filter.\")) &#123; return new MediaFilter; &#125; else &#123; return NULL; &#125;&#125; 2.3.4.2、MediaCodecList::getInstance()1234567891011121314[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaCodecList.cpp]sp&lt;IMediaCodecList&gt; MediaCodecList::getInstance() &#123; Mutex::Autolock _l(sRemoteInitMutex); if (sRemoteList == NULL) &#123; sp&lt;IBinder&gt; binder = defaultServiceManager()-&gt;getService(String16(\"media.player\")); sp&lt;IMediaPlayerService&gt; service = interface_cast&lt;IMediaPlayerService&gt;(binder); if (service.get() != NULL) &#123; sRemoteList = service-&gt;getCodecList(); &#125; &#125; return sRemoteList;&#125; 通过Binder通信获取MediaCodec列表。getCodecList()函数实现在 1234[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\MediaPlayerService.cpp]sp&lt;IMediaCodecList&gt; MediaPlayerService::getCodecList() const &#123; return MediaCodecList::getLocalInstance();&#125; 1234567891011121314151617[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaCodecList.cpp]sp&lt;IMediaCodecList&gt; MediaCodecList::getLocalInstance() &#123; Mutex::Autolock autoLock(sInitMutex); if (sCodecList == NULL) &#123; MediaCodecList *codecList = new MediaCodecList; ...... &#125; return sCodecList;&#125;MediaCodecList::MediaCodecList() : mInitCheck(NO_INIT), mUpdate(false), mGlobalSettings(new AMessage()) &#123; parseTopLevelXMLFile(\"/etc/media_codecs.xml\"); parseTopLevelXMLFile(\"/etc/media_codecs_performance.xml\", true/* ignore_errors */); parseTopLevelXMLFile(kProfilingResults, true/* ignore_errors */);&#125; O(∩_∩)O哈哈~，终于分析到Codecs加载的地方了。还记得第一章节分析的附录吗，高通的音视频硬解码，这里再贴一下。 1234567891011121314151617181920212223242526272829303132333435363738[AOSP/device/qcom/msm8996/media_codecs.xml(system/etc/media_codecs.xml)] &lt;Decoders&gt; &lt;!-- Video Hardware --&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.avc\" type=\"video/avc\" &gt; &lt;Quirk name=\"requires-allocate-on-input-ports\" /&gt; &lt;Quirk name=\"requires-allocate-on-output-ports\" /&gt; &lt;Limit name=\"size\" min=\"64x64\" max=\"4096x2160\" /&gt; &lt;Limit name=\"alignment\" value=\"2x2\" /&gt; &lt;Limit name=\"block-size\" value=\"16x16\" /&gt; &lt;Limit name=\"blocks-per-second\" min=\"1\" max=\"1958400\" /&gt; &lt;Limit name=\"bitrate\" range=\"1-100000000\" /&gt; &lt;Limit name=\"frame-rate\" range=\"1-240\" /&gt; &lt;Limit name=\"vt-version\" value=\"65537\" /&gt; &lt;Limit name=\"vt-low-latency\" value=\"1\" /&gt; &lt;Limit name=\"vt-max-macroblock-processing-rate\" value=\"972000\" /&gt; &lt;Limit name=\"vt-max-level\" value=\"52\" /&gt; &lt;Limit name=\"vt-max-instances\" value=\"16\" /&gt; &lt;Feature name=\"adaptive-playback\" /&gt; &lt;Limit name=\"concurrent-instances\" max=\"16\" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.avc.secure\" type=\"video/avc\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.mpeg4\" type=\"video/mp4v-es\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.h263\" type=\"video/3gpp\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vc1\" type=\"video/x-ms-wmv\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vc1.secure\" type=\"video/x-ms-wmv\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.divx\" type=\"video/divx\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.divx311\" type=\"video/divx311\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.divx4\" type=\"video/divx4\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vp8\" type=\"video/x-vnd.on2.vp8\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vp9\" type=\"video/x-vnd.on2.vp9\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vp9.secure\" type=\"video/x-vnd.on2.vp9\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.hevc\" type=\"video/hevc\" &gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.hevc.secure\" type=\"video/hevc\" &gt; &lt;!-- Audio Software --&gt; &lt;MediaCodec name=\"OMX.qti.audio.decoder.flac\" type=\"audio/flac\" /&gt; &lt;/Decoders&gt; &lt;Include href=\"media_codecs_google_video.xml\" /&gt; 2.3.5、MediaCodec-&gt;configure()产生kWhatConfigure消息，在消息处理中调用ACodec::initiateConfigureComponent(…)又产生消息kWhatConfigureComponent，然后该消息处理中又调用了ACodec::LoadedState::onConfigureComponent(…)。然后在其中又会先调用ACodec::configureCodec(…)，在configureCodec中会对IOMX进行一系列的设置以及配置操作，通过Binder通信就对OMXNodeInstance进行相应的设置和配置操作，最终就对OMX组件进行了相应的设置和配置。然后向MediaCodec发送kWhatComponentConfigured消息，在消息处理中将MediaCodec状态设为CONFIGURED； 1234567891011121314151617181920212223242526272829[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaCodec.cpp]status_t MediaCodec::configure( const sp&lt;AMessage&gt; &amp;format, const sp&lt;Surface&gt; &amp;surface, const sp&lt;ICrypto&gt; &amp;crypto, uint32_t flags) &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatConfigure, this); case kWhatConfigure: &#123; sp&lt;AReplyToken&gt; replyID; ...... sp&lt;RefBase&gt; obj; sp&lt;AMessage&gt; format; ...... if (obj != NULL) &#123; format-&gt;setObject(\"native-window\", obj); status_t err = handleSetSurface(static_cast&lt;Surface *&gt;(obj.get())); &#125; else &#123; handleSetSurface(NULL); &#125; mReplyID = replyID; setState(CONFIGURING); ...... extractCSD(format); mCodec-&gt;initiateConfigureComponent(format); break; &#125; 1234567891011121314151617181920212223242526272829303132[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]void ACodec::initiateConfigureComponent(const sp&lt;AMessage&gt; &amp;msg) &#123; msg-&gt;setWhat(kWhatConfigureComponent); msg-&gt;setTarget(this); msg-&gt;post();&#125; case ACodec::kWhatConfigureComponent: &#123; onConfigureComponent(msg); handled = true; break; &#125;bool ACodec::LoadedState::onConfigureComponent( const sp&lt;AMessage&gt; &amp;msg) &#123; status_t err = OK; AString mime; if (!msg-&gt;findString(\"mime\", &amp;mime)) &#123; err = BAD_VALUE; &#125; else &#123; err = mCodec-&gt;configureCodec(mime.c_str(), msg); &#125; ...... &#123; sp&lt;AMessage&gt; notify = mCodec-&gt;mNotify-&gt;dup(); notify-&gt;setInt32(\"what\", CodecBase::kWhatComponentConfigured); notify-&gt;setMessage(\"input-format\", mCodec-&gt;mInputFormat); notify-&gt;setMessage(\"output-format\", mCodec-&gt;mOutputFormat); notify-&gt;post(); &#125; return true;&#125; 2.3.6、MediaCodec-&gt;start()产生kWhatStart消息，消息处理中先将MediaCodec状态设为STARTING，然后调用ACodec::initiateStart()产生kWhatStart消息，在其消息处理中又调用ACodec::LoadedState::onStart()，然后在其中首先向IOMX发送状态转换命令，经过OMXNodeInstance最终对将OMX组件状态转换成Idle（转换完成时OMX会发送OMX_EventCmdComplete事件），接着对ACodec进行changeState至LoadedToIdleState。而在changeState过程中会调用ACodec::LoadedToIdleState::stateEntered() =&gt; ACodec::LoadedToIdleState::allocateBuffers() =&gt; ACodec::allocateBuffersOnPort(…)，其中会为OMX组件端口分配缓冲，并向MediaCodec发送消息kWhatBuffersAllocated，消息处理中将MediaCodec状态设为STARTED而若allocateBuffers失败则由IOMX经OMXNodeInstance将OMX组件转换回Loaded状态，同时把ACodec状态转换回LoadedState 1234567891011121314[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaCodec.cpp]status_t MediaCodec::start() &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatStart, this);......&#125; case kWhatStart: &#123; sp&lt;AReplyToken&gt; replyID; ...... setState(STARTING); mCodec-&gt;initiateStart(); break; &#125; 1234567891011121314151617181920[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]void ACodec::initiateStart() &#123; (new AMessage(kWhatStart, this))-&gt;post();&#125; case ACodec::kWhatStart: &#123; onStart(); handled = true; break; &#125;void ACodec::LoadedState::onStart() &#123; status_t err = mCodec-&gt;mOMX-&gt;sendCommand(mCodec-&gt;mNode, OMX_CommandStateSet, OMX_StateIdle); if (err != OK) &#123; mCodec-&gt;signalError(OMX_ErrorUndefined, makeNoSideEffectStatus(err)); &#125; else &#123; mCodec-&gt;changeState(mCodec-&gt;mLoadedToIdleState); &#125;&#125; 一旦缓冲区成功分配到输入和输出端口，OMX组件（编解码）会为Loaded-to-Idle状态生成OMX_EventCmdComplete事件转换并使用EventHandlerCallback将其发送给客户端。 （三）、音视频解码数据处理3.1、音视频解码数据处理-emptyBuffer还是老样子，先看看时序图，然后一步步分析 1、 MediaCodec::start()之后ACodec是在LoadedToIdleState状态，此时若ACodec::LoadedToIdleState::onOMXEvent(…)接收到组件转换至Idle状态后的OMX_EventCmdComplete事件，会向IOMX发送状态转换命令，经过OMXNodeInstance最终对将OMX组件状态转换成Executing状态（这里OMX会发送OMX_EventCmdComplete事件），然后ACodec进行changeState至IdleToExecutingState。2、 此时ACodec::IdleToExecutingState::onOMXEvent(…)检测到上面的OMX_EventCmdComplete事件后，会首先调用函数ACodec::ExecutingState::resume()，然后对ACodec进行changeState至ExecutingState。 3.1.1、ACodec::ExecutingState::resume()123456789101112131415161718192021222324252627282930313233343536[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]void ACodec::ExecutingState::resume() &#123; submitOutputBuffers(); ...... for (size_t i = 0; i &lt; mCodec-&gt;mBuffers[kPortIndexInput].size(); i++) &#123; BufferInfo *info = &amp;mCodec-&gt;mBuffers[kPortIndexInput].editItemAt(i); if (info-&gt;mStatus == BufferInfo::OWNED_BY_US) &#123; postFillThisBuffer(info); &#125; &#125; mActive = true;&#125;void ACodec::BaseState::postFillThisBuffer(BufferInfo *info) &#123; if (mCodec-&gt;mPortEOS[kPortIndexInput]) &#123; return; &#125; CHECK_EQ((int)info-&gt;mStatus, (int)BufferInfo::OWNED_BY_US); sp&lt;AMessage&gt; notify = mCodec-&gt;mNotify-&gt;dup(); notify-&gt;setInt32(\"what\", CodecBase::kWhatFillThisBuffer); notify-&gt;setInt32(\"buffer-id\", info-&gt;mBufferID); info-&gt;mData-&gt;meta()-&gt;clear(); notify-&gt;setBuffer(\"buffer\", info-&gt;mData); sp&lt;AMessage&gt; reply = new AMessage(kWhatInputBufferFilled, mCodec); reply-&gt;setInt32(\"buffer-id\", info-&gt;mBufferID); notify-&gt;setMessage(\"reply\", reply); notify-&gt;post(); info-&gt;mStatus = BufferInfo::OWNED_BY_UPSTREAM;&#125; 在函数ACodec::ExecutingState::resume()中会调用ACodec::BaseState::postFillThisBuffer(…)，然后其中会先向MediaCodec发送kWhatFillThisBuffer消息，消息处理中在满足相应的条件下就会去调用函数MediaCodec::onInputBufferAvailable()来通知NuPlayer::Decoder有可用的inputbuffer；然后再生成kWhatInputBufferFilled消息，消息处理中调用ACodec::BaseState::onInputBufferFilled(…)。【产生两个消息，一个向上(MediaCodec)处理，一个向下(OMX)处理】 3.1.1.1、kWhatFillThisBuffer消息处理12345678910111213141516171819202122232425262728293031[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaCodec.cpp] case CodecBase::kWhatFillThisBuffer: &#123; /* size_t index = */updateBuffers(kPortIndexInput, msg); ...... if (mFlags &amp; kFlagIsAsync) &#123; if (!mHaveInputSurface) &#123; if (mState == FLUSHED) &#123; mHavePendingInputBuffers = true; &#125; else &#123; onInputBufferAvailable(); &#125; &#125; &#125; else if (mFlags &amp; kFlagDequeueInputPending) &#123; ++mDequeueInputTimeoutGeneration; mFlags &amp;= ~kFlagDequeueInputPending; mDequeueInputReplyID = 0; &#125; else &#123; postActivityNotificationIfPossible(); &#125; break; &#125;void MediaCodec::onInputBufferAvailable() &#123; int32_t index; while ((index = dequeuePortBuffer(kPortIndexInput)) &gt;= 0) &#123; sp&lt;AMessage&gt; msg = mCallback-&gt;dup(); msg-&gt;setInt32(\"callbackID\", CB_INPUT_AVAILABLE); msg-&gt;setInt32(\"index\", index); msg-&gt;post(); &#125;&#125; P.S. 1：MediaCodec::onInputBufferAvailable()的调用：其中会先调用函数MediaCodec::dequeuePortBuffer(…)获取buffer的索引，然后将一个新消息发送给NuPlayer::Decoder，并设置消息的callbackID为CB_INPUT_AVAILABLE，同时设置index，接着NuPlayer::Decoder接收到该CB_INPUT_AVAILABLE消息，在消息处理中调用NuPlayer::Decoder::handleAnInputBuffer(…)，其会： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerDecoder.cpp]case MediaCodec::CB_INPUT_AVAILABLE:&#123; int32_t index; CHECK(msg-&gt;findInt32(\"index\", &amp;index)); handleAnInputBuffer(index); break;&#125;bool NuPlayer::Decoder::handleAnInputBuffer(size_t index) &#123; sp&lt;ABuffer&gt; buffer; mCodec-&gt;getInputBuffer(index, &amp;buffer); if (index &gt;= mInputBuffers.size()) &#123; for (size_t i = mInputBuffers.size(); i &lt;= index; ++i) &#123; mInputBuffers.add(); mMediaBuffers.add(); mInputBufferIsDequeued.add(); mMediaBuffers.editItemAt(i) = NULL; mInputBufferIsDequeued.editItemAt(i) = false; &#125; &#125; mInputBuffers.editItemAt(index) = buffer; if (mMediaBuffers[index] != NULL) &#123; mMediaBuffers[index]-&gt;release(); mMediaBuffers.editItemAt(index) = NULL; &#125; mInputBufferIsDequeued.editItemAt(index) = true; if (!mCSDsToSubmit.isEmpty()) &#123; sp&lt;AMessage&gt; msg = new AMessage(); msg-&gt;setSize(\"buffer-ix\", index); sp&lt;ABuffer&gt; buffer = mCSDsToSubmit.itemAt(0); msg-&gt;setBuffer(\"buffer\", buffer); mCSDsToSubmit.removeAt(0); return true; &#125; while (!mPendingInputMessages.empty()) &#123; sp&lt;AMessage&gt; msg = *mPendingInputMessages.begin(); if (!onInputBufferFetched(msg)) &#123; break; &#125; mPendingInputMessages.erase(mPendingInputMessages.begin()); &#125; if (!mInputBufferIsDequeued.editItemAt(index)) &#123; return true; &#125; mDequeuedInputBuffers.push_back(index); onRequestInputBuffers(); return true;&#125; ○1、先通过MediaCodec::getInputBuffer(…) -&gt; MediaCodec::getBufferAndFormat(…)获取该buffer ○2、然后调用NuPlayer::Decoder::onInputBufferFetched(…)执行内存拷贝将buffer拷贝到编解码器，然后又调用了MediaCodec::queueInputBuffer(…)将buffer提交给解码器，其会产生消息kWhatQueueInputBuffer，消息处理中调用MediaCodec::onQueueInputBuffer(…) ○3、之后调用函数NuPlayer::DecoderBase::onRequestInputBuffers()，处理是否需要更多的数据。其中会调用NuPlayer::Decoder::doRequestBuffers，若返回true则需要更多的数据，则会产生新消息kWhatRequestInputBuffers，消息处理中又将调用onRequestInputBuffers。（实际获取更多缓冲的操作在下面ACodec部分完成） 3.1.1.2、kWhatInputBufferFilled消息处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp] case kWhatInputBufferFilled: &#123; onInputBufferFilled(msg); break; &#125; void ACodec::BaseState::onInputBufferFilled(const sp&lt;AMessage&gt; &amp;msg) &#123; IOMX::buffer_id bufferID; CHECK(msg-&gt;findInt32(\"buffer-id\", (int32_t*)&amp;bufferID)); sp&lt;ABuffer&gt; buffer; int32_t err = OK; bool eos = false; PortMode mode = getPortMode(kPortIndexInput); int32_t tmp; BufferInfo *info = mCodec-&gt;findBufferByID(kPortIndexInput, bufferID); BufferInfo::Status status = BufferInfo::getSafeStatus(info); info-&gt;mStatus = BufferInfo::OWNED_BY_US; switch (mode) &#123; case KEEP_BUFFERS: &#123; if (eos) &#123; if (!mCodec-&gt;mPortEOS[kPortIndexInput]) &#123; mCodec-&gt;mPortEOS[kPortIndexInput] = true; mCodec-&gt;mInputEOSResult = err; &#125; &#125; break; &#125; case RESUBMIT_BUFFERS: &#123; if (buffer != NULL &amp;&amp; !mCodec-&gt;mPortEOS[kPortIndexInput]) &#123; int64_t timeUs; CHECK(buffer-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;timeUs)); OMX_U32 flags = OMX_BUFFERFLAG_ENDOFFRAME; MetadataBufferType metaType = mCodec-&gt;mInputMetadataType; int32_t isCSD = 0; if (buffer-&gt;meta()-&gt;findInt32(\"csd\", &amp;isCSD) &amp;&amp; isCSD != 0) &#123; if (mCodec-&gt;mIsLegacyVP9Decoder) &#123; postFillThisBuffer(info); break; &#125; flags |= OMX_BUFFERFLAG_CODECCONFIG; metaType = kMetadataBufferTypeInvalid; &#125; ...... if (buffer != info-&gt;mCodecData) &#123; sp&lt;DataConverter&gt; converter = mCodec-&gt;mConverter[kPortIndexInput]; status_t err = converter-&gt;convert(buffer, info-&gt;mCodecData); &#125; ...... info-&gt;checkReadFence(\"onInputBufferFilled\"); status_t err2 = OK; switch (metaType) &#123; case kMetadataBufferTypeInvalid: break;#ifndef OMX_ANDROID_COMPILE_AS_32BIT_ON_64BIT_PLATFORMS case kMetadataBufferTypeNativeHandleSource: if (info-&gt;mCodecData-&gt;size() &gt;= sizeof(VideoNativeHandleMetadata)) &#123; VideoNativeHandleMetadata *vnhmd = (VideoNativeHandleMetadata*)info-&gt;mCodecData-&gt;base(); err2 = mCodec-&gt;mOMX-&gt;updateNativeHandleInMeta( mCodec-&gt;mNode, kPortIndexInput, NativeHandle::create(vnhmd-&gt;pHandle, false /* ownsHandle */), bufferID); &#125; break; case kMetadataBufferTypeANWBuffer: if (info-&gt;mCodecData-&gt;size() &gt;= sizeof(VideoNativeMetadata)) &#123; VideoNativeMetadata *vnmd = (VideoNativeMetadata*)info-&gt;mCodecData-&gt;base(); err2 = mCodec-&gt;mOMX-&gt;updateGraphicBufferInMeta( mCodec-&gt;mNode, kPortIndexInput, new GraphicBuffer(vnmd-&gt;pBuffer, false /* keepOwnership */), bufferID); &#125; break;#endif default: err2 = ERROR_UNSUPPORTED; break; &#125; if (err2 == OK) &#123; err2 = mCodec-&gt;mOMX-&gt;emptyBuffer( mCodec-&gt;mNode, bufferID, 0, info-&gt;mCodecData-&gt;size(), flags, timeUs, info-&gt;mFenceFd); &#125; info-&gt;mFenceFd = -1; ...... info-&gt;mStatus = BufferInfo::OWNED_BY_COMPONENT; if (!eos &amp;&amp; err == OK) &#123; getMoreInputDataIfPossible(); &#125; else &#123; ALOGV(\"[%s] Signalled EOS (%d) on the input port\", mCodec-&gt;mComponentName.c_str(), err); mCodec-&gt;mPortEOS[kPortIndexInput] = true; mCodec-&gt;mInputEOSResult = err; &#125; &#125; else if (!mCodec-&gt;mPortEOS[kPortIndexInput]) &#123; ...... info-&gt;checkReadFence(\"onInputBufferFilled\"); status_t err2 = mCodec-&gt;mOMX-&gt;emptyBuffer( mCodec-&gt;mNode, bufferID, 0, 0, OMX_BUFFERFLAG_EOS, 0, info-&gt;mFenceFd); info-&gt;mFenceFd = -1; ...... info-&gt;mStatus = BufferInfo::OWNED_BY_COMPONENT; mCodec-&gt;mPortEOS[kPortIndexInput] = true; mCodec-&gt;mInputEOSResult = err; &#125; break; &#125; ...... &#125;&#125; P.S. 2：ACodec::BaseState::onInputBufferFilled(…)的调用：因为当前ACodec在ExecutingState，所以PortMode为RESUBMIT_BUFFERS，故会调用IOMX的emptyBuffer(…)方法，经过进程间通信调用到OMX::emptyBuffer(…)，并最终调用OMXNodeInstance::emptyBuffer(…)，其中又会调用到函数OMXNodeInstance::emptyBuffer_l(…)，其则会调用OMX_EmptyThisBuffer宏对OMX组件进行相关的操作（根据需要选择相应的软解组件或者硬解组件）。对于软解组件SoftOMXComponent ○1、其的构造函数的初始化列表中有mComponent-&gt;EmptyThisBuffer = EmptyThisBufferWrapper;故实际会调用其EmptyThisBufferWrapper(…)函数，而其中调用SoftOMXComponent的虚函数emptyThisBuffer。 ○2、所以调用子类的emptyThisBuffer即SimpleSoftOMXComponent::emptyThisBuffer(…)产生kWhatEmptyThisBuffer消息，消息处理中实际的解码器就要调用onQueueFilled(…)函数【实际组件继承自SimpleSoftOMXComponent】 ○3、接着会调用SoftOMXComponent::notifyEmptyBufferDone(…)使用OMX的回调机制，闭环发送消息到OMX客户端ACodec。 ○4、调用到OMXNodeInstance::OnEmptyBufferDone(…)，其又会调用OMX::OnEmptyBufferDone(…)，然后在其中会发送omx_message::EMPTY_BUFFER_DONE消息，ACodec中收到该消息【CodecObserver中先收到，但只设置消息】调用ACodec::BaseState::onOMXEmptyBufferDone(…) ○5、在onOMXEmptyBufferDone中获取PortMode，为RESUBMIT_BUFFERS则ACodec::BaseState::postFillThisBuffer(…)被调用，从而又从3中的postFillThisBuffer开始循环执行相关操作以处理更多的输入缓冲。 3.2、音视频解码数据处理-fillBuffer还是老样子，先看看时序图，然后一步步分析 3.2.1、ACodec::ExecutingState::resume()12345678910111213141516171819202122232425262728293031323334353637[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]void ACodec::ExecutingState::resume() &#123; submitOutputBuffers(); for (size_t i = 0; i &lt; mCodec-&gt;mBuffers[kPortIndexInput].size(); i++) &#123; BufferInfo *info = &amp;mCodec-&gt;mBuffers[kPortIndexInput].editItemAt(i); if (info-&gt;mStatus == BufferInfo::OWNED_BY_US) &#123; postFillThisBuffer(info); &#125; &#125; mActive = true;&#125;void ACodec::ExecutingState::submitOutputBuffers() &#123; submitRegularOutputBuffers(); if (mCodec-&gt;storingMetadataInDecodedBuffers()) &#123; submitOutputMetaBuffers(); &#125;&#125;void ACodec::ExecutingState::submitRegularOutputBuffers() &#123; bool failed = false; for (size_t i = 0; i &lt; mCodec-&gt;mBuffers[kPortIndexOutput].size(); ++i) &#123; BufferInfo *info = &amp;mCodec-&gt;mBuffers[kPortIndexOutput].editItemAt(i); if (mCodec-&gt;mNativeWindow != NULL) &#123; ...... &#125; else &#123; ...... &#125; ...... info-&gt;checkWriteFence(\"submitRegularOutputBuffers\"); status_t err = mCodec-&gt;mOMX-&gt;fillBuffer(mCodec-&gt;mNode, info-&gt;mBufferID, info-&gt;mFenceFd); info-&gt;mFenceFd = -1; ...... info-&gt;mStatus = BufferInfo::OWNED_BY_COMPONENT; &#125; ......&#125; 1、ACodec::ExecutingState::resume()函数，在resume()中调用ACodec::BaseState::postFillThisBuffer(…)前会先调用函数ACodec::ExecutingState::submitOutputBuffers()，即在获取输入数据前会先把输出端的数据提交出去。 2、在submitOutputBuffers()中调用ACodec::ExecutingState::submitRegularOutputBuffers()，其中又会调用到IOMX的fillBuffer (…)方法，经过进程间通信调用到OMX:: fillBuffer (…)，并最终调用OMXNodeInstance:: fillBuffer (…)，其中又会调用到OMX_FillThisBuffer宏对OMX组件进行相关的操作（同样根据需要选择相应的软解组件或者硬解组件）。对于软解组件SoftOMXComponent：（下面的操作与emptyBuffer时类似） ○1、在其构造函数的初始化列表中有mComponent-&gt;FillThisBuffer = FillThisBufferWrapper;所以实际会调用到其FillThisBufferWrapper (…)函数○2、然后调用SimpleSoftOMXComponent::fillThisBuffer(…)产生kWhatFillThisBuffer消息，消息处理中实际的组件就要调用onQueueFilled(…)函数【实际组件继承自SimpleSoftOMXComponent】 ○3、接着会调用SoftOMXComponent::notifyFillBufferDone(…)使用OMX的回调机制，闭环发送消息到OMX客户端ACodec。○4之后调用到OMXNodeInstance:: OnFillBufferDone (…)函数，其又会调用OMX:: OnFillBufferDone (…)，然后在其中会发送omx_message:: FILL_BUFFER_DONE消息，ACodec中收到该消息【CodecObserver中先收到，但只设置消息】调用ACodec::BaseState:: onOMXFillBufferDone (…)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]bool ACodec::BaseState::onOMXFillBufferDone( IOMX::buffer_id bufferID, size_t rangeOffset, size_t rangeLength, OMX_U32 flags, int64_t timeUs, int fenceFd) &#123; ALOGV(\"[%s] onOMXFillBufferDone %u time %\" PRId64 \" us, flags = 0x%08x\", mCodec-&gt;mComponentName.c_str(), bufferID, timeUs, flags); ssize_t index; status_t err= OK;#if TRACK_BUFFER_TIMING index = mCodec-&gt;mBufferStats.indexOfKey(timeUs); if (index &gt;= 0) &#123; ACodec::BufferStats *stats = &amp;mCodec-&gt;mBufferStats.editValueAt(index); stats-&gt;mFillBufferDoneTimeUs = ALooper::GetNowUs(); ALOGI(\"frame PTS %lld: %lld\", timeUs, stats-&gt;mFillBufferDoneTimeUs - stats-&gt;mEmptyBufferTimeUs); mCodec-&gt;mBufferStats.removeItemsAt(index); stats = NULL; &#125;#endif BufferInfo *info = mCodec-&gt;findBufferByID(kPortIndexOutput, bufferID, &amp;index); BufferInfo::Status status = BufferInfo::getSafeStatus(info); if (status != BufferInfo::OWNED_BY_COMPONENT) &#123; ALOGE(\"Wrong ownership in FBD: %s(%d) buffer #%u\", _asString(status), status, bufferID); mCodec-&gt;dumpBuffers(kPortIndexOutput); mCodec-&gt;signalError(OMX_ErrorUndefined, FAILED_TRANSACTION); if (fenceFd &gt;= 0) &#123; ::close(fenceFd); &#125; return true; &#125; info-&gt;mDequeuedAt = ++mCodec-&gt;mDequeueCounter; info-&gt;mStatus = BufferInfo::OWNED_BY_US; if (info-&gt;mRenderInfo != NULL) &#123; // The fence for an emptied buffer must have signaled, but there still could be queued // or out-of-order dequeued buffers in the render queue prior to this buffer. Drop these, // as we will soon requeue this buffer to the surface. While in theory we could still keep // track of buffers that are requeued to the surface, it is better to add support to the // buffer-queue to notify us of released buffers and their fences (in the future). mCodec-&gt;notifyOfRenderedFrames(true /* dropIncomplete */); &#125; // byte buffers cannot take fences, so wait for any fence now if (mCodec-&gt;mNativeWindow == NULL) &#123; (void)mCodec-&gt;waitForFence(fenceFd, \"onOMXFillBufferDone\"); fenceFd = -1; &#125; info-&gt;setReadFence(fenceFd, \"onOMXFillBufferDone\"); PortMode mode = getPortMode(kPortIndexOutput); switch (mode) &#123; case KEEP_BUFFERS: break; case RESUBMIT_BUFFERS: &#123; if (rangeLength == 0 &amp;&amp; (!(flags &amp; OMX_BUFFERFLAG_EOS) || mCodec-&gt;mPortEOS[kPortIndexOutput])) &#123; ...... err = mCodec-&gt;mOMX-&gt;fillBuffer(mCodec-&gt;mNode, info-&gt;mBufferID, info-&gt;mFenceFd); info-&gt;mFenceFd = -1; ...... info-&gt;mStatus = BufferInfo::OWNED_BY_COMPONENT; break; &#125; sp&lt;AMessage&gt; reply = new AMessage(kWhatOutputBufferDrained, mCodec); if (mCodec-&gt;mOutputFormat != mCodec-&gt;mLastOutputFormat &amp;&amp; rangeLength &gt; 0) &#123; // pretend that output format has changed on the first frame (we used to do this) if (mCodec-&gt;mBaseOutputFormat == mCodec-&gt;mOutputFormat) &#123; mCodec-&gt;onOutputFormatChanged(mCodec-&gt;mOutputFormat); &#125; mCodec-&gt;addKeyFormatChangesToRenderBufferNotification(reply); mCodec-&gt;sendFormatChange(); &#125; else if (rangeLength &gt; 0 &amp;&amp; mCodec-&gt;mNativeWindow != NULL) &#123; // If potentially rendering onto a surface, always save key format data (crop &amp; // data space) so that we can set it if and once the buffer is rendered. mCodec-&gt;addKeyFormatChangesToRenderBufferNotification(reply); &#125; if (mCodec-&gt;usingMetadataOnEncoderOutput()) &#123; native_handle_t *handle = NULL; VideoNativeHandleMetadata &amp;nativeMeta = *(VideoNativeHandleMetadata *)info-&gt;mData-&gt;data(); if (info-&gt;mData-&gt;size() &gt;= sizeof(nativeMeta) &amp;&amp; nativeMeta.eType == kMetadataBufferTypeNativeHandleSource) &#123;#ifdef OMX_ANDROID_COMPILE_AS_32BIT_ON_64BIT_PLATFORMS // handle is only valid on 32-bit/mediaserver process handle = NULL;#else handle = (native_handle_t *)nativeMeta.pHandle;#endif &#125; info-&gt;mData-&gt;meta()-&gt;setPointer(\"handle\", handle); info-&gt;mData-&gt;meta()-&gt;setInt32(\"rangeOffset\", rangeOffset); info-&gt;mData-&gt;meta()-&gt;setInt32(\"rangeLength\", rangeLength); &#125; else if (info-&gt;mData == info-&gt;mCodecData) &#123; info-&gt;mData-&gt;setRange(rangeOffset, rangeLength); &#125; else &#123; info-&gt;mCodecData-&gt;setRange(rangeOffset, rangeLength); // in this case we know that mConverter is not null status_t err = mCodec-&gt;mConverter[kPortIndexOutput]-&gt;convert( info-&gt;mCodecData, info-&gt;mData); if (err != OK) &#123; mCodec-&gt;signalError(OMX_ErrorUndefined, makeNoSideEffectStatus(err)); return true; &#125; &#125; if (mCodec-&gt;mSkipCutBuffer != NULL) &#123; mCodec-&gt;mSkipCutBuffer-&gt;submit(info-&gt;mData); &#125; info-&gt;mData-&gt;meta()-&gt;setInt64(\"timeUs\", timeUs); sp&lt;AMessage&gt; notify = mCodec-&gt;mNotify-&gt;dup(); notify-&gt;setInt32(\"what\", CodecBase::kWhatDrainThisBuffer); notify-&gt;setInt32(\"buffer-id\", info-&gt;mBufferID); notify-&gt;setBuffer(\"buffer\", info-&gt;mData); notify-&gt;setInt32(\"flags\", flags); reply-&gt;setInt32(\"buffer-id\", info-&gt;mBufferID); notify-&gt;setMessage(\"reply\", reply); notify-&gt;post(); info-&gt;mStatus = BufferInfo::OWNED_BY_DOWNSTREAM; if (flags &amp; OMX_BUFFERFLAG_EOS) &#123; ALOGV(\"[%s] saw output EOS\", mCodec-&gt;mComponentName.c_str()); sp&lt;AMessage&gt; notify = mCodec-&gt;mNotify-&gt;dup(); notify-&gt;setInt32(\"what\", CodecBase::kWhatEOS); notify-&gt;setInt32(\"err\", mCodec-&gt;mInputEOSResult); notify-&gt;post(); mCodec-&gt;mPortEOS[kPortIndexOutput] = true; &#125; break; &#125; case FREE_BUFFERS: err = mCodec-&gt;freeBuffer(kPortIndexOutput, index); if (err != OK) &#123; mCodec-&gt;signalError(OMX_ErrorUndefined, makeNoSideEffectStatus(err)); return true; &#125; break; default: ALOGE(\"Invalid port mode: %d\", mode); return false; &#125; return true;&#125; ○5、在onOMXFillBufferDone中获取PortMode，为RESUBMIT_BUFFERS则首先如果需要继续调用到IOMX的fillBuffer (…)填充输出缓冲重复做相关操作，接着ACodec又会生成一个kWhatOutputBufferDrained消息存在reply中，作为kWhatDrainThisBuffer消息的返回消息【notify-&gt;setMessage(“reply”, reply);】，然后向MediaCodec发送消息kWhatDrainThisBuffer，消息处理中调用函数MediaCodec::onOutputBufferAvailable()通知NuPlayer::Decoder有可用的output buffer，其中会设置消息的callbackID为CB_OUTPUT_AVAILABLE，同时设置index，接着NuPlayer::Decoder接收到该CB_OUTPUT_AVAILABLE消息，在消息处理中调用NuPlayer::Decoder::handleAnOutputBuffer(…)，在其中会进行如下处理： 123456789101112131415161718192021222324252627282930313233343536[-&gt;\\frameworks\\av\\media\\libstagefright\\MediaCodec.cpp]void MediaCodec::onOutputBufferAvailable() &#123; int32_t index; while ((index = dequeuePortBuffer(kPortIndexOutput)) &gt;= 0) &#123; const sp&lt;ABuffer&gt; &amp;buffer = mPortBuffers[kPortIndexOutput].itemAt(index).mData; sp&lt;AMessage&gt; msg = mCallback-&gt;dup(); msg-&gt;setInt32(\"callbackID\", CB_OUTPUT_AVAILABLE); msg-&gt;setInt32(\"index\", index); msg-&gt;setSize(\"offset\", buffer-&gt;offset()); msg-&gt;setSize(\"size\", buffer-&gt;size()); int64_t timeUs; CHECK(buffer-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;timeUs)); msg-&gt;setInt64(\"timeUs\", timeUs); int32_t omxFlags; CHECK(buffer-&gt;meta()-&gt;findInt32(\"omxFlags\", &amp;omxFlags)); uint32_t flags = 0; if (omxFlags &amp; OMX_BUFFERFLAG_SYNCFRAME) &#123; flags |= BUFFER_FLAG_SYNCFRAME; &#125; if (omxFlags &amp; OMX_BUFFERFLAG_CODECCONFIG) &#123; flags |= BUFFER_FLAG_CODECCONFIG; &#125; if (omxFlags &amp; OMX_BUFFERFLAG_EOS) &#123; flags |= BUFFER_FLAG_EOS; &#125; msg-&gt;setInt32(\"flags\", flags); msg-&gt;post(); &#125;&#125; （四）、多媒体文件 - 音视频渲染（Renderer）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerDecoder.cpp]bool NuPlayer::Decoder::handleAnOutputBuffer( size_t index, size_t offset, size_t size, int64_t timeUs, int32_t flags) &#123; sp&lt;ABuffer&gt; buffer; mCodec-&gt;getOutputBuffer(index, &amp;buffer); if (index &gt;= mOutputBuffers.size()) &#123; for (size_t i = mOutputBuffers.size(); i &lt;= index; ++i) &#123; mOutputBuffers.add(); &#125; &#125; mOutputBuffers.editItemAt(index) = buffer; buffer-&gt;setRange(offset, size); buffer-&gt;meta()-&gt;clear(); buffer-&gt;meta()-&gt;setInt64(\"timeUs\", timeUs); bool eos = flags &amp; MediaCodec::BUFFER_FLAG_EOS; // we do not expect CODECCONFIG or SYNCFRAME for decoder sp&lt;AMessage&gt; reply = new AMessage(kWhatRenderBuffer, this); reply-&gt;setSize(\"buffer-ix\", index); reply-&gt;setInt32(\"generation\", mBufferGeneration); if (eos) &#123; buffer-&gt;meta()-&gt;setInt32(\"eos\", true); reply-&gt;setInt32(\"eos\", true); &#125; else if (mSkipRenderingUntilMediaTimeUs &gt;= 0) &#123; if (timeUs &lt; mSkipRenderingUntilMediaTimeUs) &#123; reply-&gt;post(); return true; &#125; mSkipRenderingUntilMediaTimeUs = -1; &#125; mNumFramesTotal += !mIsAudio; // wait until 1st frame comes out to signal resume complete notifyResumeCompleteIfNecessary(); if (mRenderer != NULL) &#123; // send the buffer to renderer. mRenderer-&gt;queueBuffer(mIsAudio, buffer, reply); if (eos &amp;&amp; !isDiscontinuityPending()) &#123; mRenderer-&gt;queueEOS(mIsAudio, ERROR_END_OF_STREAM); &#125; &#125; return true;&#125; a. 在kWhatRenderBuffer消息处理中会调用NuPlayer::Decoder::onRenderBuffer(…)，在其中根据情况调用函数MediaCodec::renderOutputBufferAndRelease(..)渲染并释放，或者调用MediaCodec::releaseOutputBuffer(…)不渲染直接释放，两中情况都会产生kWhatReleaseOutputBuffer消息，该消息处理中调用函数MediaCodec::onReleaseOutputBuffer(…)，其中判断若SoftRenderer非空则进行软件渲染，不然就会通过○5中的reply让ACodec去硬件渲染，在kWhatOutputBufferDrained消息处理就会中调用到函数ACodec::BaseState::onOutputBufferDrained(…)进行真正的硬件渲染。 12345678910111213141516171819202122232425262728[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerDecoder.cpp]void NuPlayer::Decoder::onRenderBuffer(const sp&lt;AMessage&gt; &amp;msg) &#123; status_t err; int32_t render; size_t bufferIx; int32_t eos; CHECK(msg-&gt;findSize(\"buffer-ix\", &amp;bufferIx)); if (!mIsAudio) &#123; int64_t timeUs; sp&lt;ABuffer&gt; buffer = mOutputBuffers[bufferIx]; buffer-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;timeUs); if (mCCDecoder != NULL &amp;&amp; mCCDecoder-&gt;isSelected()) &#123; mCCDecoder-&gt;display(timeUs); &#125; &#125; if (msg-&gt;findInt32(\"render\", &amp;render) &amp;&amp; render) &#123; int64_t timestampNs; CHECK(msg-&gt;findInt64(\"timestampNs\", &amp;timestampNs)); err = mCodec-&gt;renderOutputBufferAndRelease(bufferIx, timestampNs); &#125; else &#123; mNumOutputFramesDropped += !mIsAudio; err = mCodec-&gt;releaseOutputBuffer(bufferIx); &#125; ......&#125; b. MediaCodec:: getOutputBuffer (…) -&gt; MediaCodec::getBufferAndFormat(…)获取该buffer的信息c. 若Renderer非空则会调用NuPlayer::Renderer::queueBuffer(…)进行Renderer的相关处理同时消耗产生的kWhatRenderBuffer消息。queueBuffer()会产生kWhatQueueBuffer消息，消息处理中会调用函数NuPlayer::Renderer::onQueueBuffer(…) –&gt; NuPlayer::Renderer::postDrainVideoQueue() 【另外有audio的相关处理】，其中产生kWhatDrainVideoQueue消息，消息处理中调用先NuPlayer::Renderer::onDrainVideoQueue()在VideoQueue中取相关数据，再调用NuPlayer::Renderer::postDrainVideoQueue()循环取video数据，接着还会发送kWhatRenderBuffer消息。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayerRenderer.cpp]void NuPlayer::Renderer::onQueueBuffer(const sp&lt;AMessage&gt; &amp;msg) &#123; int32_t audio; if (audio) &#123; mHasAudio = true; &#125; else &#123; mHasVideo = true; &#125; if (mHasVideo) &#123; if (mVideoScheduler == NULL) &#123; mVideoScheduler = new VideoFrameScheduler(); mVideoScheduler-&gt;init(); &#125; &#125; sp&lt;ABuffer&gt; buffer; CHECK(msg-&gt;findBuffer(\"buffer\", &amp;buffer)); sp&lt;AMessage&gt; notifyConsumed; CHECK(msg-&gt;findMessage(\"notifyConsumed\", &amp;notifyConsumed)); QueueEntry entry; entry.mBuffer = buffer; entry.mNotifyConsumed = notifyConsumed; entry.mOffset = 0; entry.mFinalResult = OK; entry.mBufferOrdinal = ++mTotalBuffersQueued; if (audio) &#123; Mutex::Autolock autoLock(mLock); mAudioQueue.push_back(entry); postDrainAudioQueue_l(); &#125; else &#123; mVideoQueue.push_back(entry); postDrainVideoQueue(); &#125; Mutex::Autolock autoLock(mLock); if (!mSyncQueues || mAudioQueue.empty() || mVideoQueue.empty()) &#123; return; &#125; sp&lt;ABuffer&gt; firstAudioBuffer = (*mAudioQueue.begin()).mBuffer; sp&lt;ABuffer&gt; firstVideoBuffer = (*mVideoQueue.begin()).mBuffer; if (firstAudioBuffer == NULL || firstVideoBuffer == NULL) &#123; syncQueuesDone_l(); return; &#125; int64_t firstAudioTimeUs; int64_t firstVideoTimeUs; int64_t diff = firstVideoTimeUs - firstAudioTimeUs; if (diff &gt; 100000ll) &#123; (*mAudioQueue.begin()).mNotifyConsumed-&gt;post(); mAudioQueue.erase(mAudioQueue.begin()); return; &#125; syncQueuesDone_l();&#125; （五）、视频解码输出到SurfaceFlinger123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]void ACodec::BaseState::onOutputBufferDrained(const sp&lt;AMessage&gt; &amp;msg) &#123; IOMX::buffer_id bufferID; CHECK(msg-&gt;findInt32(\"buffer-id\", (int32_t*)&amp;bufferID)); ssize_t index; BufferInfo *info = mCodec-&gt;findBufferByID(kPortIndexOutput, bufferID, &amp;index); BufferInfo::Status status = BufferInfo::getSafeStatus(info); if (status != BufferInfo::OWNED_BY_DOWNSTREAM) &#123; ALOGE(\"Wrong ownership in OBD: %s(%d) buffer #%u\", _asString(status), status, bufferID); mCodec-&gt;dumpBuffers(kPortIndexOutput); mCodec-&gt;signalError(OMX_ErrorUndefined, FAILED_TRANSACTION); return; &#125; android_native_rect_t crop; if (msg-&gt;findRect(\"crop\", &amp;crop.left, &amp;crop.top, &amp;crop.right, &amp;crop.bottom) &amp;&amp; memcmp(&amp;crop, &amp;mCodec-&gt;mLastNativeWindowCrop, sizeof(crop)) != 0) &#123; mCodec-&gt;mLastNativeWindowCrop = crop; status_t err = native_window_set_crop(mCodec-&gt;mNativeWindow.get(), &amp;crop); ALOGW_IF(err != NO_ERROR, \"failed to set crop: %d\", err); &#125; int32_t dataSpace; if (msg-&gt;findInt32(\"dataspace\", &amp;dataSpace) &amp;&amp; dataSpace != mCodec-&gt;mLastNativeWindowDataSpace) &#123; status_t err = native_window_set_buffers_data_space( mCodec-&gt;mNativeWindow.get(), (android_dataspace)dataSpace); mCodec-&gt;mLastNativeWindowDataSpace = dataSpace; ALOGW_IF(err != NO_ERROR, \"failed to set dataspace: %d\", err); &#125; int32_t render; if (mCodec-&gt;mNativeWindow != NULL &amp;&amp; msg-&gt;findInt32(\"render\", &amp;render) &amp;&amp; render != 0 &amp;&amp; info-&gt;mData != NULL &amp;&amp; info-&gt;mData-&gt;size() != 0) &#123; ATRACE_NAME(\"render\"); // The client wants this buffer to be rendered. // save buffers sent to the surface so we can get render time when they return int64_t mediaTimeUs = -1; info-&gt;mData-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;mediaTimeUs); if (mediaTimeUs &gt;= 0) &#123; mCodec-&gt;mRenderTracker.onFrameQueued( mediaTimeUs, info-&gt;mGraphicBuffer, new Fence(::dup(info-&gt;mFenceFd))); &#125; int64_t timestampNs = 0; if (!msg-&gt;findInt64(\"timestampNs\", &amp;timestampNs)) &#123; // use media timestamp if client did not request a specific render timestamp if (info-&gt;mData-&gt;meta()-&gt;findInt64(\"timeUs\", &amp;timestampNs)) &#123; ALOGV(\"using buffer PTS of %lld\", (long long)timestampNs); timestampNs *= 1000; &#125; &#125; status_t err; err = native_window_set_buffers_timestamp(mCodec-&gt;mNativeWindow.get(), timestampNs); ALOGW_IF(err != NO_ERROR, \"failed to set buffer timestamp: %d\", err); info-&gt;checkReadFence(\"onOutputBufferDrained before queueBuffer\"); err = mCodec-&gt;mNativeWindow-&gt;queueBuffer( mCodec-&gt;mNativeWindow.get(), info-&gt;mGraphicBuffer.get(), info-&gt;mFenceFd); info-&gt;mFenceFd = -1; if (err == OK) &#123; info-&gt;mStatus = BufferInfo::OWNED_BY_NATIVE_WINDOW; &#125; else &#123; ALOGE(\"queueBuffer failed in onOutputBufferDrained: %d\", err); mCodec-&gt;signalError(OMX_ErrorUndefined, makeNoSideEffectStatus(err)); info-&gt;mStatus = BufferInfo::OWNED_BY_US; // keeping read fence as write fence to avoid clobbering info-&gt;mIsReadFence = false; &#125; &#125; else &#123; if (mCodec-&gt;mNativeWindow != NULL &amp;&amp; (info-&gt;mData == NULL || info-&gt;mData-&gt;size() != 0)) &#123; // move read fence into write fence to avoid clobbering info-&gt;mIsReadFence = false; ATRACE_NAME(\"frame-drop\"); &#125; info-&gt;mStatus = BufferInfo::OWNED_BY_US; &#125; ......&#125; 5.1、Surfaceflinger 视频解码缓存申请前面2.3.6、MediaCodec-&gt;start()分析过：产生kWhatStart消息，消息处理中先将MediaCodec状态设为STARTING，然后调用ACodec::initiateStart()产生kWhatStart消息，在其消息处理中又调用ACodec::LoadedState::onStart()，然后在其中首先向IOMX发送状态转换命令，经过OMXNodeInstance最终对将OMX组件状态转换成Idle（转换完成时OMX会发送OMX_EventCmdComplete事件），接着对ACodec进行changeState至LoadedToIdleState。而在changeState过程中会调用ACodec::LoadedToIdleState::stateEntered() =&gt; ACodec::LoadedToIdleState::allocateBuffers() =&gt; ACodec::allocateBuffersOnPort(…)，其中会为OMX组件端口分配缓冲，并向MediaCodec发送消息kWhatBuffersAllocated，消息处理中将MediaCodec状态设为STARTED而若allocateBuffers失败则由IOMX经OMXNodeInstance将OMX组件转换回Loaded状态，同时把ACodec状态转换回LoadedState12345678910111213141516171819202122232425[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]//使用surface渲染，为输出分配图形缓存GraphicBuffer status_t ACodec::LoadedToIdleState::allocateBuffers() &#123; status_t err = mCodec-&gt;allocateBuffersOnPort(kPortIndexInput); if (err != OK) &#123; return err; &#125; return mCodec-&gt;allocateBuffersOnPort(kPortIndexOutput);&#125;status_t ACodec::allocateBuffersOnPort(OMX_U32 portIndex) &#123; CHECK(portIndex == kPortIndexInput || portIndex == kPortIndexOutput); CHECK(mDealer[portIndex] == NULL); CHECK(mBuffers[portIndex].isEmpty()); status_t err; if (mNativeWindow != NULL &amp;&amp; portIndex == kPortIndexOutput) &#123; if (storingMetadataInDecodedBuffers()) &#123; err = allocateOutputMetadataBuffers(); &#125; else &#123; err = allocateOutputBuffersFromNativeWindow(); &#125; &#125; ......&#125; 5.1.1、allocateOutputBuffersFromNativeWindow()的实现12345678910111213141516171819202122232425262728293031323334353637383940414243[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]status_t ACodec::allocateOutputBuffersFromNativeWindow() &#123; OMX_U32 bufferCount, bufferSize, minUndequeuedBuffers; status_t err = configureOutputBuffersFromNativeWindow( &amp;bufferCount, &amp;bufferSize, &amp;minUndequeuedBuffers, true /* preregister */); if (err != 0) return err; mNumUndequeuedBuffers = minUndequeuedBuffers; if (!storingMetadataInDecodedBuffers()) &#123; static_cast&lt;Surface*&gt;(mNativeWindow.get()) -&gt;getIGraphicBufferProducer()-&gt;allowAllocation(true); &#125; ...... // Dequeue buffers and send them to OMX for (OMX_U32 i = 0; i &lt; bufferCount; i++) &#123; ANativeWindowBuffer *buf; int fenceFd; err = mNativeWindow-&gt;dequeueBuffer(mNativeWindow.get(), &amp;buf, &amp;fenceFd); ...... sp&lt;GraphicBuffer&gt; graphicBuffer(new GraphicBuffer(buf, false)); BufferInfo info; info.mStatus = BufferInfo::OWNED_BY_US; info.mFenceFd = fenceFd; info.mIsReadFence = false; info.mRenderInfo = NULL; info.mData = new ABuffer(NULL /* data */, bufferSize /* capacity */); info.mCodecData = info.mData; info.mGraphicBuffer = graphicBuffer; mBuffers[kPortIndexOutput].push(info); IOMX::buffer_id bufferId; err = mOMX-&gt;useGraphicBuffer(mNode, kPortIndexOutput, graphicBuffer, &amp;bufferId); ...... mBuffers[kPortIndexOutput].editItemAt(i).mBufferID = bufferId; ...... &#125; ...... return err;&#125; 5.1.1.1、首先为视频编码输出准备Surface此处通过Binder通信使用IGraphicBufferProducer请求分配一个Native Surface12static_cast&lt;Surface*&gt;(mNativeWindow.get()) -&gt;getIGraphicBufferProducer()-&gt;allowAllocation(true); 5.1.1.2、Surface-&gt;dequeueBuffer为Surface分配Buffer，提供给视频解码后数据使用12345678[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]status_t ACodec::allocateOutputBuffersFromNativeWindow() &#123; for (OMX_U32 i = 0; i &lt; bufferCount; i++) &#123; ANativeWindowBuffer *buf; int fenceFd; err = mNativeWindow-&gt;dequeueBuffer(mNativeWindow.get(), &amp;buf, &amp;fenceFd); ......&#125; 5.2、Surface-&gt;queueBuffer()待视频解码后，使用queueBuffer()交给SurfaceFlinger渲染，就可以在屏幕上看到视频画面了。123456[-&gt;\\frameworks\\av\\media\\libstagefright\\ACodec.cpp]void ACodec::BaseState::onOutputBufferDrained(const sp&lt;AMessage&gt; &amp;msg) &#123; err = mCodec-&gt;mNativeWindow-&gt;queueBuffer( mCodec-&gt;mNativeWindow.get(), info-&gt;mGraphicBuffer.get(), info-&gt;mFenceFd); ...&#125; 关于SurfaceFlinger的知识请参考：【Android 7.1.2 (Android N) Android Graphics 系统分析】 （ ͡° ͜ʖ ͡°）、（ಡωಡ）累~~~，有时间再继续Todo的分析吧，(๑乛◡乛๑) ！！！Todo：Android OpenMax机制 实现分析Todo：Android 音视频同步机制 源码分析Todo：Android 音视频录制（Recoder）、编码（Encode）、混合（MediaMuxer）源码分析 （六）、参考资料(特别感谢各位前辈的分析和图示)：Android NuPlayer播放框架 专栏：MultiMedia框架总结(基于6.0源码) - CSDN博客Android多媒体开发-归档 | April is your lieAndroid-7.0-Nuplayer概述 - CSDN博客Android-7.0-MediaPlayer状态机 - CSDN博客Android-7.0-Nuplayer-启动流程 - CSDN博客YUV - 维基百科，自由的百科全书Android Media Player 框架分析-Nuplayer（1） - CSDN博客Android Media Player 框架分析-AHandler AMessage ALooper - CSDN博客Android 4.2.2 stagefright架构 - CSDN博客android4.2.2的stagefright架构下基于SurfaceFlinger的视频解码输出缓存创建机制 - CSDN博客husanlim 的专栏 参考 - CSDN博客android ACodec MediaCodec NuPlayer flow - CSDN博客android MediaCodec ACodec - CSDN博客ffmpeg开发之旅(1)-(7)（总共七篇）深入理解Android音视频同步机制（总共五篇）Android硬编码——音频编码、视频编码及音视频混合","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android Video System（1）：Video System(视频系统)框架分析","slug":"Android Video System（1）：Video System[视频系统]框架分析","date":"2018-05-31T16:00:00.000Z","updated":"2018-05-17T16:21:51.537Z","comments":true,"path":"2018/06/01/Android Video System（1）：Video System[视频系统]框架分析/","link":"","permalink":"http://zhoujinjian.cc/2018/06/01/Android Video System（1）：Video System[视频系统]框架分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】【特别感谢 - Android NuPlayer播放框架】【特别感谢 - android ACodec MediaCodec NuPlayer flow】 Google Pixel、Pixel XL 内核代码（文章基于 Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) ☯ V4l2 框架代码☯ kernel/drivers/media/v4l2-core/（文件前缀为 videobuf2） ☯ MSM 视频驱动程序文件☯ kernel/drivers/media/platform/msm/vidc/ ☯ 设备树☯ /kernel/arch/arm/boot/dts/qcom（Venus 的寄存器基址，时钟频率） ☯ Stagefright、libmedia、libmediaplayerservice、mediaserver☯ /frameworks/av/media/ ☯ OMX☯ /hardware/qcom/media/mam8996/mm-video-v4l2/vidc/ ☯ OMX 核心☯ /hardware/qcom/media/mm-core ☯ 软件编解码器路径☯ /vendor/qcom/proprietary/mm-video/omx_vpp(?)→ 解码器代码☯ /vendor/qcom/proprietary/mm-video/omx_vpp(?) → 编码器代码 (一)、Android Video Overview基于 OpenMAX 的视频解码 – 数据流 YUV，是一种颜色编码方法。常使用在各个视频处理组件中。 YUV在对照片或视频编码时，考虑到人类的感知能力，允许降低色度的带宽。YUVVPU，Video processing unit 基于 OpenMAX 的视频编码 – 数据流 视频框架： 组件描述： 总结： 从视频框架可以了解到。视频文件先经Stagefright传到OMX decoder解码（软解或硬解）、OMX decoder将解码后的YUV数据回传到Stagefright，不断循环播放同时经由SurfaceFlinger渲染到LCD屏幕上。 (二)、Android MediaPlayer &amp; Nuplayer 框架分析2.1、MediaPlayerAndroid在Java层中提供了一个MediaPlayer的类来作为播放媒体资源的接口，在使用中我们通常会编写以下的代码： 12345678910mMediaPlayer = new MediaPlayer();mMediaPlayer.setDataSource(Environment.getExternalStorageDirectory()+\"/test_video.mp4\");mMediaPlayer.setDisplay(...);mMediaPlayer.setAudioStreamType(AudioManager.STREAM_MUSIC);mMediaPlayer.prepareAsync();mMediaPlayer.start();mediaPlayer.pause(); mediaPlayer.stop();mediaPlayer.reset();mediaPlayer.release(); 通常MediaPlayer的调用逻辑是，构造函数-&gt; setDataSource -&gt; SetVideoSurfaceTexture-&gt; prepare/prepareAsync -&gt; start-&gt; stop-&gt; reset-&gt; 析构函数，按照实际需求还会调用pause、isPlaying、getDuration、getCurrentPosition、setLooping、seekTo等方法。 2.1.1、MediaPlayer状态图: ☯ Idle状态调用new或reset()方法创建MediaPlayer后进入空闲☯ End状态调用release()后就结束☯ Error状态播放控制操作出错或无效状态下调用播放控制操作☯ Initialized状态 调用setDataSource之后完成初始化☯ Prepared状态同步prepare()或异步prepareAsync()完成准备☯ Preparing状态是一种瞬时状态，调用prepareAsync()时会先进入此状态☯ Started 状态要开始播放必须调用start()☯ Paused 状态调用pause()并成功返回后播放可以被暂停☯ Stopped状态调用stop()会停止播放☯ PlaybackCompleted状态当播放到达流末端时，播放完成 2.1.2、MediaPlayer和MediaPlayerServicemediaserver 启动后会把media相关一些服务添加到servicemanager中，其中就有mediaPlayerService。这样应用启动前，系统就有了mediaPlayerService这个服务程序。 1[-&gt;\\frameworks\\av\\media\\mediaserver\\main_mediaserver.cpp] 2.1.3、创建MediaPlayer☯ Java应用程序中创建MediaPlayer对象MediaPlayer mediaPlayer = new MediaPlayer();☯ MediaPlayer的构造函数中比较重要的就是本地的native函数：native_setup()其对应的JNI函数为android_media_MediaPlayer_native_setup() 1[-&gt;\\frameworks\\base\\media\\jni\\android_media_MediaPlayer.cpp] 构造Native层的MediaPlayer对象的时候【MediaPlayer.cpp】，也会构造其父类的对象。在MediaPlayer的父类IMediaDeathNotifier中有个很重要的方法getMediaPlayerService()来获取MediaPlayerService，其关系到MediaPlayer和MediaPlayerService之间的通信。 2.1.4、setDataSource()设置播放资源在整个应用程序的进程中，Mediaplayer.cpp 中 setDataSource会从service manager中获得mediaPlayerService 服务，然后通过服务来创建player，这个player就是播放器的真实实例，同时也使MediaPlayer和MediaPlayerService建立了联系。在java层MediaPlayer.java中的setDataSource最终会调用_setDataSource方法，对应native层MediaPlayer.cpp中的setDataSource方法。 通过 getMediaPlayerService 得到的BpMediaPlayerService类型的service，和mediaPlayerService进程中的BnMediaPlayerService 相对应负责binder通讯。 在create函数中创建了一个MediaPlayerService::Client的实例，是MediaPlayerService的内部类，也就是说MediaPlayerService会为每个client应用进程创建一个相应的MediaPlayerService::Client的实例，来实现播放以及播放过程的控制，向MediaPlayer发事件通知。到这里，在Server端的对象就创建完成了。 然后在MediaPlayer.cpp中就得到了一个sever端的player实例，它和本地其他类的实例没什么用法上的区别，而实际上则是通过binder机制运行在另外一个进程中的。获得此实例后继续player-&gt;setDataSource操作。 小结：Java应用程序中使用MediaPlayer.java的setDataSource()会传递到Native层中MediaPlayer.cpp的setDataSource()去执行，而MediaPlayer.cpp又会把这个方法交给MediaPlayerservice去执行。MediaPlayerService则是使用NuPlayer实现的，最后， setDataSource还是交给了NuPlayer去执行了。这个过程把MediaPlayer和MediaPlayerService之间的联系建立起来，同时又把MediaPlayerService和NuPlayer的关系建立了起来。 2.1.5、setDisplay() 下一步就是java层的setDisplay，依然查看java层MediaPlayer： 123456789101112[-&gt;\\frameworks\\base\\media\\java\\android\\media\\MediaPlayer.java] public void setDisplay(SurfaceHolder sh) &#123; mSurfaceHolder = sh; Surface surface; if (sh != null) &#123; surface = sh.getSurface(); &#125; else &#123; surface = null; &#125; _setVideoSurface(surface); updateSurfaceScreenOn(); &#125; 最后会调用本地方法_setVideoSurface，我们继续找到它的jni实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[-&gt;\\frameworks\\base\\media\\jni\\android_media_MediaPlayer.cpp]static void android_media_MediaPlayer_setVideoSurface(JNIEnv *env, jobject thiz, jobject jsurface)&#123; setVideoSurface(env, thiz, jsurface, true /* mediaPlayerMustBeAlive */);&#125;static void setVideoSurface(JNIEnv *env, jobject thiz, jobject jsurface, jboolean mediaPlayerMustBeAlive)&#123; sp&lt;MediaPlayer&gt; mp = getMediaPlayer(env, thiz);//获取C++的MediaPlayer if (mp == NULL) &#123; if (mediaPlayerMustBeAlive) &#123; jniThrowException(env, \"java/lang/IllegalStateException\", NULL); &#125; return; &#125; //将旧的IGraphicBufferProducer的强引用减一 decVideoSurfaceRef(env, thiz); //IGraphicBufferProducer图层缓冲区合成器 sp&lt;IGraphicBufferProducer&gt; new_st; if (jsurface) &#123; //得到java层的surface sp&lt;Surface&gt; surface(android_view_Surface_getSurface(env, jsurface)); if (surface != NULL) &#123; //获取IGraphicBufferProducer new_st = surface-&gt;getIGraphicBufferProducer(); if (new_st == NULL) &#123; jniThrowException(env, \"java/lang/IllegalArgumentException\", \"The surface does not have a binding SurfaceTexture!\"); return; &#125; //增加IGraphicBufferProducer的强引用+1 new_st-&gt;incStrong((void*)decVideoSurfaceRef); &#125; else &#123; jniThrowException(env, \"java/lang/IllegalArgumentException\", \"The surface has been released\"); return; &#125; &#125; //上面我们在native_init方法中将java层mNativeSurfaceTexture查找给了jni层，正好，在这里将IGraphicBufferProducer赋给它 env-&gt;SetLongField(thiz, fields.surface_texture, (jlong)new_st.get()); // This will fail if the media player has not been initialized yet. This // can be the case if setDisplay() on MediaPlayer.java has been called // before setDataSource(). The redundant call to setVideoSurfaceTexture() // in prepare/prepareAsync covers for this case. //如果MediaPlayer没有初始化，这一步会失败。原因可能是setDisplay在setDataSource之前。如果在prepare/prepareAsync 时想规避这个错误而去调用setVideoSurfaceTexture是多余的。 //最终会调用C++层的setVideoSurfaceTexture方法，下一节在分析 mp-&gt;setVideoSurfaceTexture(new_st);&#125;//将旧的IGraphicBufferProducer的强引用减一static void decVideoSurfaceRef(JNIEnv *env, jobject thiz)&#123; sp&lt;MediaPlayer&gt; mp = getMediaPlayer(env, thiz); if (mp == NULL) &#123; return; &#125; sp&lt;IGraphicBufferProducer&gt; old_st = getVideoSurfaceTexture(env, thiz); if (old_st != NULL) &#123; old_st-&gt;decStrong((void*)decVideoSurfaceRef); &#125;&#125; 这一步主要是对图像显示的surface的保存，然后将旧的IGraphicBufferProducer强引用减一，再获得新的IGraphicBufferProducer，最后会调用C++的MediaPlayer的setVideoSurfaceTexture将它折纸进去。 IGraphicBufferProducer是SurfaceFlinger的内容，一个UI完全显示到diplay的过程，SurfaceFlinger扮演着重要的角色但是它的职责是“Flinger”，即把系统中所有应用程序的最终的“绘图结果”进行“混合”，然后统一显示到物理屏幕上，而其他方面比如各个程序的绘画过程，就由其他东西来担任了。这个光荣的任务自然而然地落在了BufferQueue的肩膀上，它是每个应用程序“一对一”的辅导老师，指导着UI程序的“画板申请”、“作画流程”等一系列细节。下面的图描述了这三者的关系： 虽说是三者的关系，但是他们所属的层却只有两个，app属于Java层，BufferQueue/SurfaceFlinger属于native层。也就是说BufferQueue也是隶属SurfaceFlinger，所有工作围绕SurfaceFlinger展开。 这里IGraphicBufferProducer就是app和BufferQueue重要桥梁，GraphicBufferProducer承担着单个应用进程中的UI显示需求，与BufferQueue打交道的就是它。 2.1.6、播放器基本模型NuPlayer不管有多么神秘，说到底还是个播放器。在播放器的基本模型上，他与VCL、mplayer、ffmpeg等开源的结构是一致的。只是组织实现的方式不同。深入了解NuPlayer之前，把播放器的基本模型总结一下，然后按照模型的各个部分来深入研究NuPlayer的实现方式。 ☯ datasource数据源：数据源，数据的来源不一定都是本地file，也有可能是网路上的各种协议例如：http、rtsp、HLS等。source的任务就是把数据源抽象出来，为下一个demux模块提供它需要的稳定的数据流。demux不用关信数据到底是从什么地方来的。 ☯ demuxer解复用：视频文件一般情况下都是把音视频的ES流交织的通过某种规则放在一起。这种规则就是容器规则。现在有很多不同的容器格式。如ts、mp4、flv、mkv、avi、rmvb等等。demux的功能就是把音视频的ES流从容器中剥离出来，然后分别送到不同的解码器中。其实音频和视频本身就是2个独立的系统。容器把它们包在了一起。但是他们都是独立解码的，所以解码之前，需要把它分别 独立出来。demux就是干这活的，他为下一步decoder解码提供了数据流。 ☯ decoder解码：解码器—-播放器的核心模块。分为音频和视频解码器。影像在录制后, 原始的音视频都是占用大量空间, 而且是冗余度较高的数据. 因此, 通常会在制作的时候就会进行某种压缩 ( 压缩技术就是将数据中的冗余信息去除数据之间的相关性 ). 这就是我们熟知的音视频编码格式, 包括MPEG1（VCD）\\ MPEG2（DVD）\\ MPEG4 \\ H.264 等等. 音视频解码器的作用就是把这些压缩了的数据还原成原始的音视频数据. 当然, 编码解码过程基本上都是有损的 .解码器的作用就是把编码后的数据还原成原始数据。 ☯ output输出：输出部分分为音频和视频输出。解码后的音频（pcm）和视频（yuv）的原始数据需要得到音视频的output模块的支持才能真正的让人的感官系统（眼和耳）辨识到。 所以，播放器大致分成上述4部分。怎么抽象的实现这4大部分、以及找到一种合理的方式将这几部分组织并运动起来。是每个播放器不同的实现方式而已。接下来就围绕这4大部分做深入学习，看看NuPlayer的工作原理。 2.2、NuPlayer分析2.2.0、NuPlayer简介Android2.3时引入流媒体框架，而流媒体框架的核心是NuPlayer。在之前的版本中一般认为Local Playback就用Stagefrightplayer+Awesomeplayer，流媒体用NuPlayer。Android4.0之后HttpLive和RTSP协议开始使用NuPlayer播放器，Android5.0（L版本）之后本地播放也开始使用NuPlayer播放器。 Android7.0(N版本)则完全去掉了Awesomeplayer。通俗点说，NuPlayer是AOSP中提供的多媒体播放框架，能够支持本地文件、HTTP（HLS）、RTSP等协议的播放，通常支持H.264、H.265/HEVC、AAC编码格式，支持MP4、MPEG-TS封装。在实现上NuPlayer和Awesomeplayer不同，NuPlayer基于StagefrightPlayer的基础类构建，利用了更底层的ALooper/AHandler机制来异步地处理请求，ALooper列队消息请求，AHandler中去处理，所以有更少的Mutex/Lock在NuPlayer中。Awesomeplayer中利用了omxcodec而NuPlayer中利用了Acodec。 2.2.1、NuPlayer整体类关系图 NuPlayer由NuPlayerDriver封装，利用了底层的ALooper/AHandler机制来异步地处理请求，ALooper保存消息请求，然后在AHandler中处理。另外，NuPlayer中利用到了Acodec。 ☯ NuPlayer::Source解析模块（parser，功能类似FFmpeg的avformat）。其接口与MediaExtractor和MediaSource组合的接口差不多，同时提供了用于快速定位的seekTo接口。 ☯ NuPlayer::Decoder解码模块（decoder，功能类似FFmpeg的avcodec），封装了用于AVC、AAC解码的接口，通过ACodec实现解码（包含OMX硬解码和软解码）。 ☯ NuPlayer::Render渲染模块（render，功能类似声卡驱动和显卡驱动），主要用于音视频渲染和同步，与NativeWindow有关。 ☯ NuPlayer 是播放框架中连接Source、Decoder、Renderer的纽带 ☯ NuPlayerDriver作为NuPlayer类的封装，直接调用NuPlayer。 NuPlayer框架中最顶层的类是NuPlayerDriver，继承自MediaPlayerInterface，主要提供一个状态转换机制，作为NuPlayer类的Wrapper。NuPlayerDriver类中最重要的成员是以下几个： 123&gt; State mState 播放器状体标志 &gt; sp &lt;ALooper&gt; mLooper 内部消息驱动机制 &gt; sp &lt;NuPlayer&gt; mPlayer 真正完成播放器的类 NuPlayerDriver主要是 构造函数-&gt; setDataSource -&gt; SetVideoSurfaceTexture-&gt; prepare/prepareAsync -&gt; start-&gt; stop-&gt; reset-&gt; 析构函数，实际需求pause、isPlaying、getDuration、getCurrentPosition、setLooping、seekTo等方法 2.2.2、NuPlayer框架需要关注知识点123456789101112NuPlayer的框架，其内部实现逻辑。那么最终就落实到如何从一个类中提取出需要的框架及知识点。那么一个类的对外接口部分通常包括：--- 构造函数和析构函数--- 必须调用的接口--- 可选的调用接口在多媒体播放中，通过关注的点有：--- 如何实现解复用，得到音频、视频、字幕等数据--- 如何实现解码--- 如何实现音视频同步--- 如何渲染视频--- 如何播放音频--- 如何实现快速定位 (三)、Android MediaPlayer框架分析 - AHandler AMessage ALooper前文中提到过NuPlayer基于StagefrightPlayer的基础类构建，利用了更底层的ALooper/AHandler机制来异步地处理请求，ALooper保存消息请求，然后调用AHandler接口去处理。实际上在代码中NuPlayer本身继承自AHandler类，而ALooper对象保存在NuPlayerDriver中。ALooper/AHandler机制是模拟的消息循环处理方式，通常有三个主要部分：消息（message，通常包含Handler）、消息队列（queue）、消息处理线程（looper thread）。 对于handler消息机制，构成就必须包括一个Loop，message。那么对应的AHandler，也应该有对应的ALooper、AMessage。因此本小节主要涉及到三个类ALooper、AHandler、AMessage。 3.1、AHandler接口分析（消息处理类）下面代码是AHandler接口: 1234567891011121314151617181920212223\"./frameworks/av/include/media/stagefright/AHandler.h\"struct AHandler : public RefBase &#123; AHandler(); ALooper::handler_id id() const; sp&lt;ALooper&gt; looper() const; wp&lt;ALooper&gt; getLooper() const; wp&lt;AHandler&gt; getHandler() const;protected: virtual void onMessageReceived(const sp&lt;AMessage&gt; &amp;msg) = 0;private: friend struct AMessage; // deliverMessage() friend struct ALooperRoster; // setID() uint32_t mMessageCounter; KeyedVector&lt;uint32_t, uint32_t&gt; mMessages; void setID(ALooper::handler_id id, wp&lt;ALooper&gt; looper); void deliverMessage(const sp&lt;AMessage&gt; &amp;msg);&#125;; 看上面接口，初步印象是AHandler没有直接对外的接口（只有获取成员变量的接口），基本上只有一个onMessageReceived用于子类继承，deliverMessage用于给类AMessage使用，setID用于给友元类ALooperRoster使用。从这点来说，真正代码应该在AMessage里边。 3.2、AMessage接口分析（消息载体）下面代码是AMessage的声明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768\"./frameworks/av/include/media/stagefright/AMessage.h\"struct AMessage : public RefBase &#123; AMessage(); AMessage(uint32_t what, const sp&lt;const AHandler&gt; &amp;handler); // 代码中常用的构造函数 static sp&lt;AMessage&gt; FromParcel(const Parcel &amp;parcel, size_t maxNestingLevel = 255); // Write this AMessage to a parcel. // All items in the AMessage must have types that are recognized by // FromParcel(); otherwise, TRESPASS error will occur. void writeToParcel(Parcel *parcel) const; void setWhat(uint32_t what); uint32_t what() const; // 注意这是一个AHandler，通过这个可以获得ALooper对象引用 void setTarget(const sp&lt;const AHandler&gt; &amp;handler); // 清除所有设置的消息属性参数 void clear(); // 一系列设置/获取 Message 属性的函数。。。 void setInt32/setInt64/setSize/setFloat/setDouble/setPointer/setPointer/setString/setRect/setObject/setBuffer/setMessage(...); bool findInt32/findInt64/findSize/findFloat/findDouble/findPointer/findString/findObject/findBuffer/findMessage/findRect(...) const; // 通过这个函数检索下指定名称的消息属性是否存在 bool contains(const char *name) const; // 投递消息的接口，顾名思义直接投递给构造函数的ALooper，注意支持延时消息，但不支持提前消息，delayUS &gt; 0 status_t post(int64_t delayUs = 0); // 投递消息并等待执行结束后发送response消息 status_t postAndAwaitResponse(sp&lt;AMessage&gt; *response); // If this returns true, the sender of this message is synchronously // awaiting a response and the reply token is consumed from the message // and stored into replyID. The reply token must be used to send the response // using \"postReply\" below. bool senderAwaitsResponse(sp&lt;AReplyToken&gt; *replyID); // Posts the message as a response to a reply token. A reply token can // only be used once. Returns OK if the response could be posted; otherwise, // an error. status_t postReply(const sp&lt;AReplyToken&gt; &amp;replyID); // 深拷贝 sp&lt;AMessage&gt; dup() const; // 比较两个消息，并返回差异 sp&lt;AMessage&gt; changesFrom(const sp&lt;const AMessage&gt; &amp;other, bool deep = false) const; // 获取消息属性存储的个数及特定索引上的消息属性参数 size_t countEntries() const; const char *getEntryNameAt(size_t index, Type *type) const;protected: virtual ~AMessage();private: friend struct ALooper; // deliver() uint32_t mWhat; wp&lt;AHandler&gt; mHandler; wp&lt;ALooper&gt; mLooper; // 用于ALooper调用的，发送消息的接口 void deliver();&#125;; 从上面的接口可以看出在使用AMessage是只需要指定消息的id和要处理该消息的AHandler即可，可以通过构造函数，也可以单独调用setWhat和setTarget接口。AMessage构造完成之后，可以调用setXXX设置对应的参数，通过findXXX获取传递的参数。最后通过post即可将消息投递到AHandler的消息队列中。 3.3、ALooper接口分析（消息处理循环及后台线程）其简化的声明如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354\"./frameworks/av/include/media/stagefright/ALooper.h\"struct ALooper : public RefBase &#123; ALooper(); // Takes effect in a subsequent call to start(). void setName(const char *name); const char *getName() const; handler_id registerHandler(const sp&lt;AHandler&gt; &amp;handler); void unregisterHandler(handler_id handlerID); status_t start(bool runOnCallingThread = false, bool canCallJava = false, int32_t priority = PRIORITY_DEFAULT); status_t stop(); static int64_t GetNowUs(); protected: virtual ~ALooper();private: friend struct AMessage; // post() AString mName; struct Event &#123; int64_t mWhenUs; sp&lt;AMessage&gt; mMessage; &#125;; List&lt;Event&gt; mEventQueue; struct LooperThread; sp&lt;LooperThread&gt; mThread; bool mRunningLocally; // START --- methods used only by AMessage // posts a message on this looper with the given timeout void post(const sp&lt;AMessage&gt; &amp;msg, int64_t delayUs); // creates a reply token to be used with this looper sp&lt;AReplyToken&gt; createReplyToken(); // waits for a response for the reply token. If status is OK, the response // is stored into the supplied variable. Otherwise, it is unchanged. status_t awaitResponse(const sp&lt;AReplyToken&gt; &amp;replyToken, sp&lt;AMessage&gt; *response); // posts a reply for a reply token. If the reply could be successfully posted, // it returns OK. Otherwise, it returns an error value. status_t postReply(const sp&lt;AReplyToken&gt; &amp;replyToken, const sp&lt;AMessage&gt; &amp;msg); // END --- methods used only by AMessage bool loop();&#125;; ALooper对外接口比较简单，通常就是NuPlayerDriver构造函数中的调用逻辑。先创建一个ALooper对象，然后调用setName和start接口，之后调用registerHandler设置一个AHandler，这样就完成了初始化。在析构之前需要调用stop接口。这里需要说明下，ALooper::start接口会启动一个线程，并调用ALooper::loop函数，该函数主要实现消息的实际执行。代码如下： 123456789101112131415161718192021222324252627282930313233bool ALooper::loop() &#123; Event event; &#123; Mutex::Autolock autoLock(mLock); if (mThread == NULL &amp;&amp; !mRunningLocally) &#123; return false; &#125; // 从mEventQueue取出消息，判断是否需要执行，不需要的话就等待 // 需要的话就调用handler执行，并删除对应消息 if (mEventQueue.empty()) &#123; mQueueChangedCondition.wait(mLock); return true; &#125; int64_t whenUs = (*mEventQueue.begin()).mWhenUs; int64_t nowUs = GetNowUs(); if (whenUs &gt; nowUs) &#123; int64_t delayUs = whenUs - nowUs; mQueueChangedCondition.waitRelative(mLock, delayUs * 1000ll); return true; &#125; event = *mEventQueue.begin(); mEventQueue.erase(mEventQueue.begin()); &#125; event.mMessage-&gt;deliver(); return true;&#125; 那么消息是通过那个函数添加进来的呢？ 这就是友元类AMessage的作用，通过调用ALooper::post接口，将AMessage添加到mEventQueue中。 3.4、一个调用实例以NuPlayer::setVideoSurfaceTextureAsync为示例分析下ALooper/AHandler机制。这里不解释ALooper的初始化过程，有兴趣的可以参考资料Android Native层异步消息处理框架的内容。下面是setVideoSurfaceTextureAsync的代码。 12345678910111213[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]void NuPlayer::setVideoSurfaceTextureAsync( const sp&lt;IGraphicBufferProducer&gt; &amp;bufferProducer) &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatSetVideoSurface, this); if (bufferProducer == NULL) &#123; msg-&gt;setObject(\"surface\", NULL); &#125; else &#123; msg-&gt;setObject(\"surface\", new Surface(bufferProducer, true /* controlledByApp */)); &#125; msg-&gt;post();&#125; 这段代码功能很简单，创建一个AMessage对象，并设置下参数，参数类型为Object，名称是”surface”，然后通过AMessage::post接口，间接调用ALooper::post接口，将消息发送给ALooper-NuPlayerDriver::mLooper；ALooper的消息循环线程检测到这个消息，在ALooper::loop函数中通过AMessage的deliver接口，调用AHandler::deliverMessage接口，这个函数会调动NuPlayer::onMessageReceived（通过继承机制实现）接口。这样绕了一圈。我们就可以通过ALooper/AHandler机制处理消息了。具体处理代码如下 123456789101112131415161718192021222324252627[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]void NuPlayer::onMessageReceived(const sp&lt;AMessage&gt; &amp;msg) &#123; switch (msg-&gt;what()) &#123; case kWhatSetVideoSurface: &#123; sp&lt;RefBase&gt; obj; CHECK(msg-&gt;findObject(\"surface\", &amp;obj)); sp&lt;Surface&gt; surface = static_cast&lt;Surface *&gt;(obj.get()); ALOGD(\"onSetVideoSurface(%p video decoder)\", surface.get()); // Need to check mStarted before calling mSource-&gt;getFormat because NuPlayer might // be in preparing state and it could take long time. // When mStarted is true, mSource must have been set. if (mSource == NULL || !mStarted || mSource-&gt;getFormat(false /* audio */) == NULL // NOTE: mVideoDecoder's mSurface is always non-null || (mVideoDecoder != NULL &amp;&amp; mVideoDecoder-&gt;setVideoSurface(surface) == OK)) &#123; performSetSurface(surface); break; &#125; &#125; // ... 省略其他部分代码 &#125;&#125; (四)、NuPlayer源码分析这次我们需要深入分析的是NuPlayer类，相比于NuPlayerDriver的接口功能，NuPlayer继承自AHandler类，是AOSP播放框架中连接Source、Decoder、Render的纽带。 4.1、主要接口和核心的类成员NuPlayer类被NuPlayerDriver直接调用，其主要接口如下： 12345678910111213141516171819202122232425262728293031// code from NuPlayer.h (~/frameworks/av/media/libmediaplayerservice/nuplayer/)struct NuPlayer : public AHandler &#123; NuPlayer(pid_t pid); void setUID(uid_t uid); void setDriver(const wp&lt;NuPlayerDriver&gt; &amp;driver); void setDataSourceAsync(...); void prepareAsync(); void setVideoSurfaceTextureAsync(const sp&lt;IGraphicBufferProducer&gt; &amp;bufferProducer); void start(); void pause(); // Will notify the driver through \"notifyResetComplete\" once finished. void resetAsync(); // Will notify the driver through \"notifySeekComplete\" once finished // and needNotify is true. void seekToAsync(int64_t seekTimeUs, bool needNotify = false); status_t setVideoScalingMode(int32_t mode); status_t getTrackInfo(Parcel* reply) const; status_t getSelectedTrack(int32_t type, Parcel* reply) const; status_t selectTrack(size_t trackIndex, bool select, int64_t timeUs); status_t getCurrentPosition(int64_t *mediaUs); sp&lt;MetaData&gt; getFileMeta(); float getFrameRate();protected: virtual ~NuPlayer(); virtual void onMessageReceived(const sp&lt;AMessage&gt; &amp;msg);&#125; 接口分类下，无外乎几个分类： ☯ 用于初始化的（比如构造函数、setDriver/setDataSourceAsync/prepareAsync/setVideoSurfaceTextureAsync）☯ 用于销毁的（比如析构函数、resetAsync）☯ 用于播放控制的（比如start/pause/seekToAsync）☯ 用于状态获取的（比如getCurrentPosition/getFileMeta）下面是主要的类成员部分 123456789[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.h]wp&lt;NuPlayerDriver&gt; mDriver; // 接口调用方sp&lt;Source&gt; mSource; // 相当于FFmpeg中的demuxersp&lt;Surface&gt; mSurface; // 显示用的Surfacesp&lt;DecoderBase&gt; mVideoDecoder; // 视频解码器sp&lt;DecoderBase&gt; mAudioDecoder; // 音频解码器sp&lt;CCDecoder&gt; mCCDecoder; sp&lt;Renderer&gt; mRenderer; // 渲染器sp&lt;ALooper&gt; mRendererLooper; 4.2、setDataSourceAsync()现分析这个函数有多重不同的重载形式，如下： 123456[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.h]void setDataSourceAsync(const sp&lt;IStreamSource&gt; &amp;source);void setDataSourceAsync(const sp&lt;IMediaHTTPService&gt; &amp;httpService, const char *url, const KeyedVector&lt;String8, String8&gt; *headers);void setDataSourceAsync(int fd, int64_t offset, int64_t length);void setDataSourceAsync(const sp&lt;DataSource&gt; &amp;source); 需要根据实际情况选择，这里以第三个接口为例，说明下多本地媒体文件是如何处理的。下面是这个函数的实现代码： 1234567891011121314[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]void NuPlayer::setDataSourceAsync(int fd, int64_t offset, int64_t length) &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatSetDataSource, this); sp&lt;AMessage&gt; notify = new AMessage(kWhatSourceNotify, this); // 创建对象用于读取本地文件 sp&lt;GenericSource&gt; source = new GenericSource(notify, mUIDValid, mUID); // 实际干活的的代码 status_t err = source-&gt;setDataSource(fd, offset, length); msg-&gt;setObject(\"source\", source); msg-&gt;post();&#125; 看实现很简单，创建GenericSource对象，并调用其setDataSource接口，然后发送kWhatSetDataSource消息。我们看看如何处理然后发送kWhatSetDataSource消息呢？代码如下：12345678910111213141516171819202122[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]case kWhatSetDataSource:&#123; CHECK(mSource == NULL); status_t err = OK; sp&lt;RefBase&gt; obj; CHECK(msg-&gt;findObject(\"source\", &amp;obj)); if (obj != NULL) &#123; Mutex::Autolock autoLock(mSourceLock); mSource = static_cast&lt;Source *&gt;(obj.get()); &#125; else &#123; err = UNKNOWN_ERROR; &#125; // 通知Driver函数调用完成 CHECK(mDriver != NULL); sp&lt;NuPlayerDriver&gt; driver = mDriver.promote(); if (driver != NULL) &#123; driver-&gt;notifySetDataSourceCompleted(err); &#125; break;&#125; 看到这里发现，其实没做什么就是直接通知NuPlayerDriver。我们还注意到这里构建了一个特殊消息（AMessage）notify，这个消息用于在Source和NuPlayer直接传递。下面这是消息循环中的处理函数： 123456[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]case kWhatSourceNotify:&#123; onSourceNotify(msg); break;&#125; 在后续讨论Source的时候详细说明这个消息通知的意义。 4.3、prepareAsync()这个函数实现的功能对应于MediaPlayerBase::prepare/prepareAsync接口，实现异步的prepare功能，一般就是做一些额外的初始化工作。那么直接看一下实现： 1234[-&gt;\\frameworks\\av\\media\\libmediaplayerservice\\nuplayer\\NuPlayer.cpp]void NuPlayer::prepareAsync() &#123; (new AMessage(kWhatPrepare, this))-&gt;post();&#125; 代码就是发了一个kWhatPrepare的消息。接下来是如何处理这个消息。 12345case kWhatPrepare:&#123; mSource-&gt;prepareAsync(); break;&#125; 最终还是调用了Source::prepareAsync接口。后面会解释其功能。（这里面可能会解析下码流，读取音频、视频、字幕流信息，读取时长、元数据等）。 4.4、setVideoSurfaceTextureAsync()调用这个接口主要为了设置视频渲染窗口。其实现相对简单，创建一个Surface，然后发送异步的kWhatSetVideoSurface消息。代码如下： 1234567891011void NuPlayer::setVideoSurfaceTextureAsync( const sp&lt;IGraphicBufferProducer&gt; &amp;bufferProducer) &#123; sp&lt;AMessage&gt; msg = new AMessage(kWhatSetVideoSurface, this); if (bufferProducer == NULL) &#123; msg-&gt;setObject(\"surface\", NULL); &#125; else &#123; msg-&gt;setObject(\"surface\", new Surface(bufferProducer, true /* controlledByApp */)); &#125; msg-&gt;post();&#125; 那么看看如何处理kWhatSetVideoSurface消息呢？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546case kWhatSetVideoSurface: &#123; sp&lt;RefBase&gt; obj; CHECK(msg-&gt;findObject(\"surface\", &amp;obj)); sp&lt;Surface&gt; surface = static_cast&lt;Surface *&gt;(obj.get()); // Need to check mStarted before calling mSource-&gt;getFormat because NuPlayer might // be in preparing state and it could take long time. // When mStarted is true, mSource must have been set. if (mSource == NULL || !mStarted || mSource-&gt;getFormat(false /* audio */) == NULL // NOTE: mVideoDecoder's mSurface is always non-null || (mVideoDecoder != NULL &amp;&amp; mVideoDecoder-&gt;setVideoSurface(surface) == OK)) &#123; performSetSurface(surface); // 通知NuPlayerDriver设置完成 break; &#125; // 清空音频、视频缓冲 mDeferredActions.push_back( new FlushDecoderAction(FLUSH_CMD_FLUSH /* audio */,FLUSH_CMD_SHUTDOWN /* video */)); // 最终调用NuPlayer::performSetSurface接口 mDeferredActions.push_back(new SetSurfaceAction(surface)); if (obj != NULL || mAudioDecoder != NULL) &#123; if (mStarted) &#123; // Issue a seek to refresh the video screen only if started otherwise // the extractor may not yet be started and will assert. // If the video decoder is not set (perhaps audio only in this case) // do not perform a seek as it is not needed. int64_t currentPositionUs = 0; if (getCurrentPosition(&amp;currentPositionUs) == OK) &#123; mDeferredActions.push_back( new SeekAction(currentPositionUs)); &#125; &#125; // 对于新的surface设置，重置下解码器 mDeferredActions.push_back(new SimpleAction(&amp;NuPlayer::performScanSources)); &#125; // After a flush without shutdown, decoder is paused. // Don't resume it until source seek is done, otherwise it could // start pulling stale data too soon. mDeferredActions.push_back( new ResumeDecoderAction(false /* needNotify */)); // 把上面mDeferredActions中缓存的所有Action处理下，并清空 processDeferredActions(); break;&#125; 这里的代码相对复杂点，涉及到很多，其实主要是为了设置Surface之后，可以正常解码显示，因为某些情况下解码器初始化需要依赖于具体的Surface。当然，里边还涉及到NuPlayer状态及初始化判断。 4.5、start()/pause()start函数实现很简单，实际就发送了kWhatStart消息。 123void NuPlayer::start() &#123; (new AMessage(kWhatStart, this))-&gt;post();&#125; 在消息处理函数中的处理如下： 12345678910111213case kWhatStart:&#123; if (mStarted) &#123; // do not resume yet if the source is still buffering if (!mPausedForBuffering) &#123; onResume(); &#125; &#125; else &#123; onStart(); &#125; mPausedByClient = false; break;&#125; 直接调用了OnStart/OnResume函数。pause函数实现类似，只是发送的是kWhatPause消息。在消息处理函数中的代码如下： 123456case kWhatPause:&#123; onPause(); mPausedByClient = true; break;&#125; 直接调用的onPause函数。下面单独分析下这三个函数。先从简单的函数开始OnPause/onResume NuPlayer::onPause这个函数实现暂停功能，总体来说就是把Source和Render暂停就可以了，代码如下： 123456789101112void NuPlayer::onPause() &#123; if (mPaused) &#123; return; &#125; mPaused = true; if (mSource != NULL) &#123; mSource-&gt;pause(); &#125; if (mRenderer != NULL) &#123; mRenderer-&gt;pause(); &#125;&#125; NuPlayer::onResume这个函数实现恢复功能，代码逻辑跟onPause差不多，把Source和Render恢复，还可能涉及其它操作。代码如下： 1234567891011121314151617void NuPlayer::onResume() &#123; if (!mPaused || mResetting) &#123; return; &#125; mPaused = false; if (mSource != NULL) &#123; mSource-&gt;resume(); &#125; // |mAudioDecoder| may have been released due to the pause timeout, so re-create it if // needed. if (audioDecoderStillNeeded() &amp;&amp; mAudioDecoder == NULL) &#123; instantiateDecoder(true /* audio */, &amp;mAudioDecoder); &#125; if (mRenderer != NULL) &#123; mRenderer-&gt;resume(); &#125;&#125; NuPlayer::onStart这个接口实现启动的操作，相对复杂点，需要初始化解码器、初始化Render、设置Source状态，并将三者关联起来。代码如下： 123456789101112131415161718192021222324252627282930313233void NuPlayer::onStart(int64_t startPositionUs) &#123; if (!mSourceStarted) &#123; mSourceStarted = true; mSource-&gt;start(); // 设置Source状态 &#125; // ... (省略部分代码) sp&lt;AMessage&gt; notify = new AMessage(kWhatRendererNotify, this); ++mRendererGeneration; // 创建Render和RenderLooper，属性设置、与解码器关联 notify-&gt;setInt32(\"generation\", mRendererGeneration); mRenderer = new Renderer(mAudioSink, notify, flags); mRendererLooper = new ALooper; mRendererLooper-&gt;setName(\"NuPlayerRenderer\"); mRendererLooper-&gt;start(false, false, ANDROID_PRIORITY_AUDIO); mRendererLooper-&gt;registerHandler(mRenderer); status_t err = mRenderer-&gt;setPlaybackSettings(mPlaybackSettings); float rate = getFrameRate(); if (rate &gt; 0) &#123; mRenderer-&gt;setVideoFrameRate(rate); &#125; if (mVideoDecoder != NULL) &#123; mVideoDecoder-&gt;setRenderer(mRenderer); &#125; if (mAudioDecoder != NULL) &#123; mAudioDecoder-&gt;setRenderer(mRenderer); &#125; postScanSources();&#125; 上面代码中没有解码器的初始化，那只能继续看看postScanSources代码了。看实现发现就是发送了kWhatScanSources消息。那么消息循环里边是怎么处理的呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657case kWhatScanSources:&#123; int32_t generation; CHECK(msg-&gt;findInt32(\"generation\", &amp;generation)); if (generation != mScanSourcesGeneration) &#123; // Drop obsolete msg. break; &#125; mScanSourcesPending = false; bool mHadAnySourcesBefore = (mAudioDecoder != NULL) || (mVideoDecoder != NULL); bool rescan = false; // initialize video before audio because successful initialization of // video may change deep buffer mode of audio. if (mSurface != NULL) &#123; // 初始化视频解码器 if (instantiateDecoder(false, &amp;mVideoDecoder) == -EWOULDBLOCK) &#123; rescan = true; &#125; &#125; // Don't try to re-open audio sink if there's an existing decoder. if (mAudioSink != NULL &amp;&amp; mAudioDecoder == NULL) &#123; // 初始化音频解码器 if (instantiateDecoder(true, &amp;mAudioDecoder) == -EWOULDBLOCK) &#123; rescan = true; &#125; &#125; if (!mHadAnySourcesBefore &amp;&amp; (mAudioDecoder != NULL || mVideoDecoder != NULL)) &#123; // This is the first time we've found anything playable. // 设置定期查询时长 if (mSourceFlags &amp; Source::FLAG_DYNAMIC_DURATION) &#123; schedulePollDuration(); &#125; &#125; status_t err; // 一些异常处理逻辑 if ((err = mSource-&gt;feedMoreTSData()) != OK) &#123; if (mAudioDecoder == NULL &amp;&amp; mVideoDecoder == NULL) &#123; // We're not currently decoding anything (no audio or // video tracks found) and we just ran out of input data. if (err == ERROR_END_OF_STREAM) &#123; notifyListener(MEDIA_PLAYBACK_COMPLETE, 0, 0); &#125; else &#123; notifyListener(MEDIA_ERROR, MEDIA_ERROR_UNKNOWN, err); &#125; &#125; break; &#125; // 如果需要的话，重新扫描Source if (rescan) &#123; msg-&gt;post(100000ll); mScanSourcesPending = true; &#125; break;&#125; 此外还有seekToAsync()、resetAsync()、getCurrentPosition()、getFileMeta()。由于实现类似，就不一一介绍了。 4.6、小结结和疑问到这里，我们已经把NuPlayer主要的函数分析完了，但是问题依旧在。比如下面几个： 不同格式的多媒体文件如何探测并解析的？音视频数据缓冲区在哪里？（Source）视频如何显示的？音频如何播放的？音视频同步在哪里？（Renderer）音频解码线程、视频解码线程在哪里？ （DecoderBase） 我想接下来的分析就是解决这些疑问的。 4.7、Codec Encoder 、Decoder列表附录123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138[AOSP/device/qcom/msm8996/media_codecs.xml(system/etc/media_codecs.xml)]&lt;!-- 8996 Decoder capabilities __________________________________________________________________ | Codec | W H fps Mbps MB/s | Secure-dec | |__________|_________________________________________|____________| | h264 | 3840 2160 60 100 1958400 | Y | | | (4096) (2160) (56) (100) | | | hevc | 3840 2160 60 100 1958400 | Y | | | (4096) (2160) (56) (100) | | | mpeg4 | 1920 1088 60 60 489600 | N | | vc1 | 1920 1088 60 60 489600 | Y | | vp8 | 3840 2160 30 100 979200 | N | | vp9 | 3840 2160 30 100 979200 | Y | | divx3 | 720 480 30 2 40500 | N | | div4/5/6 | 1920 1088 30 10 244800 | N | | h263 | 864 480 30 2 48600 | N | | mpeg2 | 1920 1088 30 40 244800 | Y | |__________|_________________________________________|____________| 8996 Encoder capabilities ______________________________________________________ | Codec | W H fps Mbps MB/s | |__________|_________________________________________| | h264 | 3840 2160 30 100 979200 | | hevc | 3840 2160 30 100 979200 | | mpeg4 | 1920 1088 60 60 489600 | | vp8 | 3840 2160 30 100 979200 | | h263 | 864 480 30 2 48600 | |__________|_________________________________________|--&gt;&lt;MediaCodecs&gt; &lt;Include href=\"media_codecs_google_audio.xml\" /&gt; &lt;Include href=\"media_codecs_google_telephony.xml\" /&gt; &lt;Settings&gt; &lt;Setting name=\"max-video-encoder-input-buffers\" value=\"11\" /&gt; &lt;/Settings&gt; &lt;Encoders&gt; &lt;!-- Audio Hardware --&gt; &lt;!-- Audio Software --&gt; &lt;!-- Video Hardware --&gt; &lt;MediaCodec name=\"OMX.qcom.video.encoder.avc\" type=\"video/avc\" &gt; &lt;Quirk name=\"requires-allocate-on-input-ports\" /&gt; &lt;Quirk name=\"requires-allocate-on-output-ports\" /&gt; &lt;Quirk name=\"requires-loaded-to-idle-after-allocation\" /&gt; &lt;Limit name=\"size\" min=\"96x64\" max=\"4096x2160\" /&gt; &lt;Limit name=\"alignment\" value=\"2x2\" /&gt; &lt;Limit name=\"block-size\" value=\"16x16\" /&gt; &lt;Limit name=\"blocks-per-second\" min=\"1\" max=\"979200\" /&gt; &lt;Limit name=\"bitrate\" range=\"1-100000000\" /&gt; &lt;Limit name=\"frame-rate\" range=\"1-240\" /&gt; &lt;Limit name=\"concurrent-instances\" max=\"16\" /&gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.encoder.mpeg4\" type=\"video/mp4v-es\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.encoder.h263\" type=\"video/3gpp\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.encoder.vp8\" type=\"video/x-vnd.on2.vp8\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.encoder.hevc\" type=\"video/hevc\" &gt; ...... &lt;/MediaCodec&gt; &lt;/Encoders&gt; &lt;Decoders&gt; &lt;!-- Video Hardware --&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.avc\" type=\"video/avc\" &gt; &lt;Quirk name=\"requires-allocate-on-input-ports\" /&gt; &lt;Quirk name=\"requires-allocate-on-output-ports\" /&gt; &lt;Limit name=\"size\" min=\"64x64\" max=\"4096x2160\" /&gt; &lt;Limit name=\"alignment\" value=\"2x2\" /&gt; &lt;Limit name=\"block-size\" value=\"16x16\" /&gt; &lt;Limit name=\"blocks-per-second\" min=\"1\" max=\"1958400\" /&gt; &lt;Limit name=\"bitrate\" range=\"1-100000000\" /&gt; &lt;Limit name=\"frame-rate\" range=\"1-240\" /&gt; &lt;Limit name=\"vt-version\" value=\"65537\" /&gt; &lt;Limit name=\"vt-low-latency\" value=\"1\" /&gt; &lt;Limit name=\"vt-max-macroblock-processing-rate\" value=\"972000\" /&gt; &lt;Limit name=\"vt-max-level\" value=\"52\" /&gt; &lt;Limit name=\"vt-max-instances\" value=\"16\" /&gt; &lt;Feature name=\"adaptive-playback\" /&gt; &lt;Limit name=\"concurrent-instances\" max=\"16\" /&gt; &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.avc.secure\" type=\"video/avc\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.mpeg4\" type=\"video/mp4v-es\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.mpeg2\" type=\"video/mpeg2\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.mpeg2.secure\" type=\"video/mpeg2\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.h263\" type=\"video/3gpp\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vc1\" type=\"video/x-ms-wmv\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vc1.secure\" type=\"video/x-ms-wmv\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.divx\" type=\"video/divx\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.divx311\" type=\"video/divx311\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.divx4\" type=\"video/divx4\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vp8\" type=\"video/x-vnd.on2.vp8\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vp9\" type=\"video/x-vnd.on2.vp9\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.vp9.secure\" type=\"video/x-vnd.on2.vp9\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.hevc\" type=\"video/hevc\" &gt; ...... &lt;/MediaCodec&gt; &lt;MediaCodec name=\"OMX.qcom.video.decoder.hevc.secure\" type=\"video/hevc\" &gt; ...... &lt;/MediaCodec&gt; &lt;!-- Audio Software --&gt; &lt;MediaCodec name=\"OMX.qti.audio.decoder.flac\" type=\"audio/flac\" /&gt; &lt;/Decoders&gt; &lt;Include href=\"media_codecs_google_video.xml\" /&gt;&lt;/MediaCodecs&gt; （五）、参考资料(特别感谢各位前辈的分析和图示)：Android NuPlayer播放框架 专栏：MultiMedia框架总结(基于6.0源码) - CSDN博客Android多媒体开发-归档 | April is your lieAndroid-7.0-Nuplayer概述 - CSDN博客Android-7.0-MediaPlayer状态机 - CSDN博客Android-7.0-Nuplayer-启动流程 - CSDN博客YUV - 维基百科，自由的百科全书Android Media Player 框架分析-Nuplayer（1） - CSDN博客Android Media Player 框架分析-AHandler AMessage ALooper - CSDN博客Android 4.2.2 stagefright架构 - CSDN博客android4.2.2的stagefright架构下基于SurfaceFlinger的视频解码输出缓存创建机制 - CSDN博客husanlim 的专栏 参考 - CSDN博客android ACodec MediaCodec NuPlayer flow - CSDN博客android MediaCodec ACodec - CSDN博客","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Audio System（3）：Android audio system(音频系统)分析","slug":"Audio System（3）：Android audio system[音频系统]分析","date":"2018-05-24T16:00:00.000Z","updated":"2018-05-09T15:03:52.335Z","comments":true,"path":"2018/05/25/Audio System（3）：Android audio system[音频系统]分析/","link":"","permalink":"http://zhoujinjian.cc/2018/05/25/Audio System（3）：Android audio system[音频系统]分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】【特别感谢 - 林学森的Android专栏】【特别感谢 - Yangwen123 - 深入剖析Android音频系统】【特别感谢 - Zyuanyun - Android 音频系统：从 AudioTrack 到 AudioFlinger】Google Pixel、Pixel XL 内核代码（Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（文章基于 Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) 源码（主要源码路径）： User space audio code 源码： • /hardware/qcom/audio/hal/ – (Audio 高通HAL 源码) • /libhardware/modules/audio/ – (Audio 原生HAL 源码) • /external/tinyalsa/ – (tinymix, tinyplay, tinycap 源码) • /vendor/qcom/proprietary/mm-audio/ – (QTI OMX audio encoder and decoders 源码，未公开) • /frameworks/av/media/audioserver/ – (Audioserver 源码) • /hardware/libhardware_legacy/audio – (Audio legacy 源码) • /frameworks/av/media/libstagefright/ – (Google Stagefright 多媒体框架源码) • /frameworks/av/services/audioflinger/ – (Audioflinger 相关源码) • /external/bluetooth/bluedroid/ – (A2DP audio HAL 相关源码)/ • /hardware/libhardware/modules/usbaudio/ – (USB HAL 源码)/ • /vendor/qcom/proprietary/wfd/mm/source/framework/src/ – (Wi-Fi Display (WFD)、 WFDMMSourceAudioSource.cpp，未公开) • /system/core/include/system/ – (audio.h)/ (一)、深入剖析Android音频之AudioFlinger1.0、总体框架图 系统启动时将执行 /system/etc/init/audioserver.rc ，运行 /system/bin/ 目录下的 audioserver 服务。audioserver.rc 内容如下： 12345678[-&gt;\\frameworks\\av\\media\\audioserver\\audioserver.rc]service audioserver /system/bin/audioserver class main user audioserver # media gid needed for /dev/fm (radio ) and for /data/misc/media (tee) group audio radio camera drmpc inet media mediarm net_bt net_bt_admin net_bw_acct ioprio rt 4 writepid /dev/cpuset/forground/tasks /dev/stune/foreground/tasks audioserver 是由同目录下main_audioserver编译生成的。 1.1、AudioFlingerAudioFlinger是整个音频系统的核心与难点。作为Android系统中的音频中枢，它同时也是一个系统服务，启到承上(为上层提供访问接口)启下(通过HAL来管理音频设备)的作用。只有理解了AudioFlinger，才能以此为基础更好地深入到其它模块，并且Audioserver最先启动的也是AudioFlinger，因而我们把它放在前面进行分析。 1234567891011121314151617181920212223[-&gt;\\frameworks\\av\\media\\audioserver\\main_audioserver.cpp]int main(int argc __unused, char **argv)&#123; ...... if (doLog &amp;&amp; (childPid = fork()) != 0) &#123; ...... &#125; else &#123; // all other services if (doLog) &#123; prctl(PR_SET_PDEATHSIG, SIGKILL); // if parent media.log dies before me, kill me also setpgid(0, 0); // but if I die first, don't kill my parent &#125; sp&lt;ProcessState&gt; proc(ProcessState::self()); sp&lt;IServiceManager&gt; sm = defaultServiceManager(); ALOGI(\"ServiceManager: %p\", sm.get()); AudioFlinger::instantiate(); AudioPolicyService::instantiate(); RadioService::instantiate(); SoundTriggerHwService::instantiate(); ProcessState::self()-&gt;startThreadPool(); IPCThreadState::self()-&gt;joinThreadPool(); &#125;&#125; AudioFlinger继承了模板类BinderService，该类用于注册native service。BinderService是一个模板类，该类的publish函数就是完成向ServiceManager注册服务。AudioFlinger注册名为”media.audio_flinger”的服务。1static const char* getServiceName() ANDROID_API &#123; return \"media.audio_flinger\"; &#125; 1.1.1 AudioFlinger服务的启动和运行AudioFlinger的构造函数，发现它只是简单地为内部一些变量做了初始化，除此之外就没有任何代码了： 1234567891011121314151617181920212223[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]AudioFlinger::AudioFlinger() : BnAudioFlinger(), mPrimaryHardwareDev(NULL), mAudioHwDevs(NULL), mHardwareStatus(AUDIO_HW_IDLE), mMasterVolume(1.0f), mMasterMute(false), // mNextUniqueId(AUDIO_UNIQUE_ID_USE_MAX), mMode(AUDIO_MODE_INVALID), mBtNrecIsOff(false), mIsLowRamDevice(true), mIsDeviceTypeKnown(false), mGlobalEffectEnableTime(0), mSystemReady(false)&#123; // unsigned instead of audio_unique_id_use_t, because ++ operator is unavailable for enum for (unsigned use = AUDIO_UNIQUE_ID_USE_UNSPECIFIED; use &lt; AUDIO_UNIQUE_ID_USE_MAX; use++) &#123; // zero ID has a special meaning, so unavailable mNextUniqueIds[use] = AUDIO_UNIQUE_ID_USE_MAX; &#125; ......&#125; BnAudioFlinger是由RefBase层层继承而来的，并且IServiceManager::addService的第二个参数实际上是一个强指针引用(constsp&amp;),因而AudioFlinger具备了强指针被第一次引用时调用onFirstRef的程序逻辑。 1234567891011121314151617181920212223[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]void AudioFlinger::onFirstRef()&#123; Mutex::Autolock _l(mLock); /* TODO: move all this work into an Init() function */ char val_str[PROPERTY_VALUE_MAX] = &#123; 0 &#125;; if (property_get(\"ro.audio.flinger_standbytime_ms\", val_str, NULL) &gt;= 0) &#123; uint32_t int_val; if (1 == sscanf(val_str, \"%u\", &amp;int_val)) &#123; mStandbyTimeInNsecs = milliseconds(int_val); ALOGI(\"Using %u mSec as standby time.\", int_val); &#125; else &#123; mStandbyTimeInNsecs = kDefaultStandbyTimeInNsecs; ALOGI(\"Using default %u mSec as standby time.\", (uint32_t)(mStandbyTimeInNsecs / 1000000)); &#125; &#125; mPatchPanel = new PatchPanel(this); mMode = AUDIO_MODE_NORMAL;&#125; 从这时开始，AudioFlinger就是一个“有意义”的实体了 1.2、音频设备的管理虽然AudioFlinger实体已经成功创建并初始化，但到目前为止它还是一块静态的内存空间，没有涉及到具体的工作。 从职能分布上来讲，AudioPolicyService是策略的制定者，比如什么时候打开音频接口设备、某种Stream类型的音频对应什么设备等等。而AudioFlinger则是策略的执行者，例如具体如何与音频设备通信，如何维护现有系统中的音频设备，以及多个音频流的混音如何处理等等都得由它来完成。 目前Audio系统中支持的音频设备接口(Audio Interface)分为三大类，即： 123456[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]static const char * const audio_interfaces[] = &#123; AUDIO_HARDWARE_MODULE_ID_PRIMARY, AUDIO_HARDWARE_MODULE_ID_A2DP, AUDIO_HARDWARE_MODULE_ID_USB,&#125;; 每种音频设备接口由一个对应的so库提供支持。那么AudioFlinger怎么会知道当前设备中支持上述的哪些接口，每种接口又支持哪些具体的音频设备呢？这是AudioPolicyService的责任之一，即根据用户配置来指导AudioFlinger加载设备接口。 当AudioPolicyManagerBase(AudioPolicyService中持有的Policy管理者，后面小节有详细介绍)构造时，它会读取厂商关于音频设备的描述文件(audio_policy.conf)，然后据此来打开以上三类音频接口(如果存在的话)。这一过程最终会调用loadHwModule@AudioFlinger，如下所示： 1.2.1、加载设备loadHwModule()12345678910111213[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]/*name就是前面audio_interfaces 数组成员中的字符串*/audio_module_handle_t AudioFlinger::loadHwModule(const char *name)&#123; if (name == NULL) &#123; return AUDIO_MODULE_HANDLE_NONE; &#125; if (!settingsAllowed()) &#123; return AUDIO_MODULE_HANDLE_NONE; &#125; Mutex::Autolock _l(mLock); return loadHwModule_l(name);&#125; 这个函数没有做实质性的工作，只是执行了加锁动作，然后接着调用下面的函数： 12345678910111213141516171819202122232425262728293031[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]// loadHwModule_l() must be called with AudioFlinger::mLock heldaudio_module_handle_t AudioFlinger::loadHwModule_l(const char *name)&#123; /* Step 1. 是否已经添加了这个interface ? */ for (size_t i = 0; i &lt; mAudioHwDevs.size(); i++) &#123; if (strncmp(mAudioHwDevs.valueAt(i)-&gt;moduleName(), name, strlen(name)) == 0) &#123; ALOGW(&quot;loadHwModule() module %s already loaded&quot;, name); return mAudioHwDevs.keyAt(i); &#125; &#125; audio_hw_device_t *dev; /* Step 2. 加载audio interface */ int rc = load_audio_interface(name, &amp;dev); ...... /* Step 3. 初始化 */ mHardwareStatus = AUDIO_HW_INIT; rc = dev-&gt;init_check(dev); mHardwareStatus = AUDIO_HW_IDLE; ...... /* Step 4. 添加到全局变量中 */ audio_module_handle_t handle = (audio_module_handle_t) nextUniqueId(AUDIO_UNIQUE_ID_USE_MODULE); mAudioHwDevs.add(handle, new AudioHwDevice(handle, name, dev, flags)); return handle;&#125; Step1@ loadHwModule_l. 首先查找mAudioHwDevs是否已经添加了变量name所指示的audio interface，如果是的话直接返回。第一次进入时mAudioHwDevs的size为0，所以还会继续往下执行。 Step2@ loadHwModule_l. 加载指定的audiointerface，比如“primary”、“a2dp”或者“usb”。函数load_audio_interface用来加载设备所需的库文件，然后打开设备并创建一个audio_hw_device_t实例。音频接口设备所对应的库文件名称是有一定格式的，比如a2dp的模块名可能是audio.a2dp.so或者audio.a2dp.default.so等等。查找路径主要有两个，即： 1234[-&gt;\\hardware\\libhardware\\hardware.c]#define HAL_LIBRARY_PATH1 \"/system/lib64/hw\"#define HAL_LIBRARY_PATH2 \"/vendor/lib64/hw\"#define HAL_LIBRARY_PATH3 \"/odm/lib64/hw\" 当然，因为Android是完全开源的，各开发商可以根据自己的需要来进行相应的修改，比如下面是Google pixel 设备的音频库： 123456789adb shell &amp;&amp; cd system/lib64/hw &amp;&amp; ls -l-rw-r--r-- 1 root root 30440 2009-01-01 00:00 audio.a2dp.default.so-rw-r--r-- 1 root root 18156 2009-01-01 00:00 audio.primary.default.so-rw-r--r-- 1 root root 275612 2009-01-01 00:00 audio.primary.msm8996.so-rw-r--r-- 1 root root 34540 2009-01-01 00:00 audio.r_submix.default.so-rw-r--r-- 1 root root 22248 2009-01-01 00:00 audio.usb.default.so-rw-r--r-- 1 root root 96096 2009-01-01 00:00 audio_policy.default.so-rw-r--r-- 1 root root 1637208 2009-01-01 00:00 bluetooth.default.so...... Step3@ loadHwModule_l，进行初始化操作。其中init_check是为了确定这个audio interface是否已经成功初始化，0是成功，其它值表示失败。接下来如果这个device支持主音量，我们还需要通过set_master_volume进行设置。在每次操作device前，都要先改变mHardwareStatus的状态值，操作结束后将其复原为AUDIO_HW_IDLE(根据源码中的注释，这样做是为了方便dump时正确输出内部状态，这里我们就不去深究了)。 Step4@ loadHwModule_l. 把加载后的设备添加入mAudioHwDevs键值对中，其中key的值是由nextUniqueId生成的，这样做保证了这个audiointerface拥有全局唯一的id号。 完成了audiointerface的模块加载只是万里长征的第一步。因为每一个interface包含的设备通常不止一个，Android系统目前支持的音频设备如下列表所示： Android系统支持的音频设备列表(输出) 1234567891011121314151617181920[-&gt;\\hardware\\libhardware_legacy\\audio\\audio_hw_hal.cpp]static uint32_t audio_device_conv_table[][HAL_API_REV_NUM] =&#123; /* output devices */ &#123; AudioSystem::DEVICE_OUT_EARPIECE, AUDIO_DEVICE_OUT_EARPIECE &#125;,// &#123; AudioSystem::DEVICE_OUT_SPEAKER, AUDIO_DEVICE_OUT_SPEAKER &#125;,//SPEAKER &#123; AudioSystem::DEVICE_OUT_WIRED_HEADSET, AUDIO_DEVICE_OUT_WIRED_HEADSET &#125;,//HEADSET &#123; AudioSystem::DEVICE_OUT_WIRED_HEADPHONE, AUDIO_DEVICE_OUT_WIRED_HEADPHONE &#125;,//HEADPHONE &#123; AudioSystem::DEVICE_OUT_BLUETOOTH_SCO, AUDIO_DEVICE_OUT_BLUETOOTH_SCO &#125;, &#123; AudioSystem::DEVICE_OUT_BLUETOOTH_SCO_HEADSET, AUDIO_DEVICE_OUT_BLUETOOTH_SCO_HEADSET &#125;, &#123; AudioSystem::DEVICE_OUT_BLUETOOTH_SCO_CARKIT, AUDIO_DEVICE_OUT_BLUETOOTH_SCO_CARKIT &#125;, &#123; AudioSystem::DEVICE_OUT_BLUETOOTH_A2DP, AUDIO_DEVICE_OUT_BLUETOOTH_A2DP &#125;, &#123; AudioSystem::DEVICE_OUT_BLUETOOTH_A2DP_HEADPHONES, AUDIO_DEVICE_OUT_BLUETOOTH_A2DP_HEADPHONES &#125;, &#123; AudioSystem::DEVICE_OUT_BLUETOOTH_A2DP_SPEAKER, AUDIO_DEVICE_OUT_BLUETOOTH_A2DP_SPEAKER &#125;, &#123; AudioSystem::DEVICE_OUT_AUX_DIGITAL, AUDIO_DEVICE_OUT_AUX_DIGITAL &#125;, &#123; AudioSystem::DEVICE_OUT_ANLG_DOCK_HEADSET, AUDIO_DEVICE_OUT_ANLG_DOCK_HEADSET &#125;, &#123; AudioSystem::DEVICE_OUT_DGTL_DOCK_HEADSET, AUDIO_DEVICE_OUT_DGTL_DOCK_HEADSET &#125;, &#123; AudioSystem::DEVICE_OUT_DEFAULT, AUDIO_DEVICE_OUT_DEFAULT &#125;,//默认设备 ......&#125; 大家可能会有疑问： Ø 这么多的输出设备，那么当我们回放音频流(录音也是类似的情况)时，该选择哪一种呢？ Ø 而且当前系统中audio interface也很可能不止一个，应该如何选择？ 显然这些决策工作将由AudioPolicyService来完成，我们会在下一小节做详细阐述。这里先给大家分析下，AudioFlinger是如何打开一个Output通道的(一个audiointerface可能包含若干个output)。 1.2.2、打开音频输出通道openOutput()打开音频输出通道(output)在AudioFlinger中对应的接口是openOutput()，即： 1234567891011121314151617181920212223242526272829303132333435[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]status_t AudioFlinger::openOutput(audio_module_handle_t module, audio_io_handle_t *output, audio_config_t *config, audio_devices_t *devices, const String8&amp; address, uint32_t *latencyMs, audio_output_flags_t flags)&#123; ...... Mutex::Autolock _l(mLock); sp&lt;PlaybackThread&gt; thread = openOutput_l(module, output, config, *devices, address, flags); if (thread != 0) &#123; *latencyMs = thread-&gt;latency(); // notify client processes of the new output creation thread-&gt;ioConfigChanged(AUDIO_OUTPUT_OPENED); // the first primary output opened designates the primary hw device if ((mPrimaryHardwareDev == NULL) &amp;&amp; (flags &amp; AUDIO_OUTPUT_FLAG_PRIMARY)) &#123; ALOGI(\"Using module %d has the primary audio interface\", module); mPrimaryHardwareDev = thread-&gt;getOutput()-&gt;audioHwDev; AutoMutex lock(mHardwareLock); mHardwareStatus = AUDIO_HW_SET_MODE; mPrimaryHardwareDev-&gt;hwDevice()-&gt;set_mode(mPrimaryHardwareDev-&gt;hwDevice(), mMode); mHardwareStatus = AUDIO_HW_IDLE; &#125; return NO_ERROR; &#125; return NO_INIT;&#125; 继续调用 openOutput_l()函数从处理。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]sp&lt;AudioFlinger::PlaybackThread&gt; AudioFlinger::openOutput_l(audio_module_handle_t module, audio_io_handle_t *output, audio_config_t *config, audio_devices_t devices, const String8&amp; address, audio_output_flags_t flags)&#123; /*Step 1. 查找相应的audio interface AudioHwDevice *outHwDev = findSuitableHwDev_l(module, devices); ...... mHardwareStatus = AUDIO_HW_OUTPUT_OPEN; ...... AudioStreamOut *outputStream = NULL; /*Step 2. 为设备打开一个输出流*/ status_t status = outHwDev-&gt;openOutputStream( &amp;outputStream, *output, devices, flags, config, address.string()); mHardwareStatus = AUDIO_HW_IDLE; /*Step 3.创建PlaybackThread*/ if (status == NO_ERROR) &#123; PlaybackThread *thread; if (flags &amp; AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD) &#123; thread = new OffloadThread(this, outputStream, *output, devices, mSystemReady); ALOGV(\"openOutput_l() created offload output: ID %d thread %p\", *output, thread); &#125; else if ((flags &amp; AUDIO_OUTPUT_FLAG_DIRECT) || !isValidPcmSinkFormat(config-&gt;format) || !isValidPcmSinkChannelMask(config-&gt;channel_mask)) &#123; thread = new DirectOutputThread(this, outputStream, *output, devices, mSystemReady); ALOGV(\"openOutput_l() created direct output: ID %d thread %p\", *output, thread); &#125; else &#123; thread = new MixerThread(this, outputStream, *output, devices, mSystemReady); ALOGV(\"openOutput_l() created mixer output: ID %d thread %p\", *output, thread); &#125; mPlaybackThreads.add(*output, thread); return thread; &#125; return 0;&#125; 上面这段代码中，颜色加深的部分是我们接下来分析的重点，主要还是围绕outHwDev这个变量所做的一系列操作，即： · 查找合适的音频接口设备( findSuitableHwDev_l() ) · 创建音频输出流( 通过openOutputStream()创建AudioStreamOut ) · 创建播放线程( PlaybackThread ) outHwDev用于记录一个打开的音频接口设备，它的数据类型是audio_hw_device_t，是由HAL规定的一个音频接口设备所应具有的属性集合，如下所示： 12345678910111213141516171819202122[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioHwDevice.h]class AudioHwDevice &#123;...... audio_module_handle_t handle() const &#123; return mHandle; &#125; const char *moduleName() const &#123; return mModuleName; &#125; audio_hw_device_t *hwDevice() const &#123; return mHwDevice; &#125; uint32_t version() const &#123; return mHwDevice-&gt;common.version; &#125; status_t openOutputStream( AudioStreamOut **ppStreamOut, audio_io_handle_t handle, audio_devices_t devices, audio_output_flags_t flags, struct audio_config *config, const char *address);private: const audio_module_handle_t mHandle; const char * const mModuleName; audio_hw_device_t * const mHwDevice; const Flags mFlags;&#125;;&#125; 其中common代表了HAL层所有设备的共有属性;set_master_volume、set_mode、open_output_stream分别为我们设置audio interface的主音量、设置音频模式类型(比如AUDIO_MODE_RINGTONE、AUDIO_MODE_IN_CALL等等)、打开输出数据流提供了接口。 接下来我们分步来阐述。 1.2.2.1、查找合适的音频接口设备findSuitableHwDev_l()Step1@ AudioFlinger::openOutput. 在openOutput中，设备outHwDev是通过查找当前系统来得到的，代码如下： 123456789101112131415161718192021222324252627282930[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]AudioHwDevice* AudioFlinger::findSuitableHwDev_l( audio_module_handle_t module, audio_devices_t devices)&#123; // if module is 0, the request comes from an old policy manager and we should load // well known modules if (module == 0) &#123; ALOGW(\"findSuitableHwDev_l() loading well know audio hw modules\"); for (size_t i = 0; i &lt; ARRAY_SIZE(audio_interfaces); i++) &#123; loadHwModule_l(audio_interfaces[i]); &#125; // then try to find a module supporting the requested device. for (size_t i = 0; i &lt; mAudioHwDevs.size(); i++) &#123; AudioHwDevice *audioHwDevice = mAudioHwDevs.valueAt(i); audio_hw_device_t *dev = audioHwDevice-&gt;hwDevice(); if ((dev-&gt;get_supported_devices != NULL) &amp;&amp; (dev-&gt;get_supported_devices(dev) &amp; devices) == devices) return audioHwDevice; &#125; &#125; else &#123; // check a match for the requested module handle AudioHwDevice *audioHwDevice = mAudioHwDevs.valueFor(module); if (audioHwDevice != NULL) &#123; return audioHwDevice; &#125; &#125; return NULL;&#125; 变量module值为0的情况，是为了兼容之前的Audio Policy而特别做的处理。当module等于0时，首先加载所有已知的音频接口设备，然后再根据devices来确定其中符合要求的。入参devices的值实际上来源于“ Android系统支持的音频设备列表(输出)”所示的设备。可以看到，enum中每个设备类型都对应一个特定的比特位，因而上述代码段中可以通过“与运算”来找到匹配的设备。 当modules为非0值时，说明Audio Policy指定了具体的设备id号，这时就通过查找全局的mAudioHwDevs变量来确认是否存在符合要求的设备。 12[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.h]DefaultKeyedVector&lt;audio_module_handle_t, AudioHwDevice*&gt; mAudioHwDevs; 变量mAudioHwDevs是一个Vector，以audio_module_handle_t为key，每一个handle值唯一确定了已经添加的音频设备。那么在什么时候添加设备呢？ 一种情况就是前面看到的modules为0时，会load所有潜在设备，另一种情况就是AudioPolicyManagerBase在构造时会预加载所有audio_policy.conf中所描述的output。不管是哪一种情况，最终都会调用loadHwModuleàloadHwModule_l，这个函数我们开头就分析过了。 如果modules为非0，且从mAudioHwDevs中也找不到符合要求的设备，程序并不会就此终结——它会退而求其次，遍历数组中的所有元素寻找支持devices的任何一个audio interface。 1.2.2.2、创建音频输出流openOutputStream()Step2@ AudioFlinger::openOutput，调用openOutputStream()函数打开一个AudioStreamOut 。源码实现如下： 12345678910111213141516171819[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioHwDevice.cpp]status_t AudioHwDevice::openOutputStream( AudioStreamOut **ppStreamOut, audio_io_handle_t handle, audio_devices_t devices, audio_output_flags_t flags, struct audio_config *config, const char *address)&#123; struct audio_config originalConfig = *config; AudioStreamOut *outputStream = new AudioStreamOut(this, flags); status_t status = outputStream-&gt;open(handle, devices, config, address); ...... *ppStreamOut = outputStream; return status;&#125; 生成AudioStreamOut对象并赋值给ppStreamOut ，进一步调用了AudioStreamOut-&gt;open()函数。 1234567891011121314151617181920[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioStreamOut.cpp]status_t AudioStreamOut::open( audio_io_handle_t handle, audio_devices_t devices, struct audio_config *config, const char *address)&#123; audio_stream_out_t *outStream; ....... int status = hwDev()-&gt;open_output_stream( hwDev(), handle, devices, customFlags, config, &amp;outStream, address); ...... return status;&#125; 即会通过audio_hw_device_t-&gt;-&gt;open_output_stream()创建音频输出流 1.2.2.2.1、audio_hw_device_t-&gt;-&gt;open_output_stream()我们先看一下HAL层代码 1234567891011121314151617181920212223242526272829303132[-&gt;\\hardware\\qcom\\audio\\hal\\audio_hw.c]static int adev_open(const hw_module_t *module, const char *name, hw_device_t **device)&#123; ...... adev = calloc(1, sizeof(struct audio_device)); pthread_mutex_init(&amp;adev-&gt;lock, (const pthread_mutexattr_t *) NULL); adev-&gt;device.common.tag = HARDWARE_DEVICE_TAG; adev-&gt;device.common.version = AUDIO_DEVICE_API_VERSION_2_0; adev-&gt;device.common.module = (struct hw_module_t *)module; adev-&gt;device.common.close = adev_close; adev-&gt;device.init_check = adev_init_check; adev-&gt;device.set_voice_volume = adev_set_voice_volume; adev-&gt;device.set_master_volume = adev_set_master_volume; adev-&gt;device.get_master_volume = adev_get_master_volume; adev-&gt;device.set_master_mute = adev_set_master_mute; adev-&gt;device.get_master_mute = adev_get_master_mute; adev-&gt;device.set_mode = adev_set_mode; adev-&gt;device.set_mic_mute = adev_set_mic_mute; adev-&gt;device.get_mic_mute = adev_get_mic_mute; adev-&gt;device.set_parameters = adev_set_parameters; adev-&gt;device.get_parameters = adev_get_parameters; adev-&gt;device.get_input_buffer_size = adev_get_input_buffer_size; adev-&gt;device.open_output_stream = adev_open_output_stream; adev-&gt;device.close_output_stream = adev_close_output_stream; adev-&gt;device.open_input_stream = adev_open_input_stream; adev-&gt;device.close_input_stream = adev_close_input_stream; ......&#125; 可以看到当调用open_output_stream 就会调用adev_open_output_stream。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[-&gt;\\hardware\\qcom\\audio\\hal\\audio_hw.c]static int adev_open_output_stream(struct audio_hw_device *dev, audio_io_handle_t handle, audio_devices_t devices, audio_output_flags_t flags, struct audio_config *config, struct audio_stream_out **stream_out, const char *address __unused)&#123; struct audio_device *adev = (struct audio_device *)dev; struct stream_out *out; int i, ret; *stream_out = NULL; out = (struct stream_out *)calloc(1, sizeof(struct stream_out)); ...... out-&gt;stream.common.get_sample_rate = out_get_sample_rate; out-&gt;stream.common.set_sample_rate = out_set_sample_rate; out-&gt;stream.common.get_buffer_size = out_get_buffer_size; out-&gt;stream.common.get_channels = out_get_channels; out-&gt;stream.common.get_format = out_get_format; out-&gt;stream.common.set_format = out_set_format; out-&gt;stream.common.standby = out_standby; out-&gt;stream.common.dump = out_dump; out-&gt;stream.common.set_parameters = out_set_parameters; out-&gt;stream.common.get_parameters = out_get_parameters; out-&gt;stream.common.add_audio_effect = out_add_audio_effect; out-&gt;stream.common.remove_audio_effect = out_remove_audio_effect; out-&gt;stream.get_latency = out_get_latency; out-&gt;stream.set_volume = out_set_volume;#ifdef NO_AUDIO_OUT out-&gt;stream.write = out_write_for_no_output;#else out-&gt;stream.write = out_write;#endif out-&gt;stream.get_render_position = out_get_render_position; out-&gt;stream.get_next_write_timestamp = out_get_next_write_timestamp; out-&gt;stream.get_presentation_position = out_get_presentation_position; out-&gt;af_period_multiplier = out-&gt;realtime ? af_period_multiplier : 1; out-&gt;standby = 1; ...... *stream_out = &amp;out-&gt;stream; ALOGV(&quot;%s: exit&quot;, __func__); return 0;&#125; 根据音频流的熟悉做一系列初始化操作。转了一大圈，继续看看 1.2.2.3、创建播放线程(PlaybackThread)Step3@ AudioFlinger::openOutput. 既然通道已经打开，那么由谁来往通道里放东西呢？这就是PlaybackThread。这里分三种不同的情况：· OffloadThread · DirectOutput 如果不需要混音 · Mixer 需要混音 这三种情况分别对应DirectOutputThread、OffloadThread和MixerThread两种线程。我们以后者为例来分析下PlaybackThread的工作模式，也会后面小节打下基础。回放线程（PlaybackThread 及其派生的子类）和录制线程（RecordThread）进行的，先简单看看回放线程和录制线程类关系： · ThreadBase：PlaybackThread 和 RecordThread 的基类· RecordThread：录制线程类，由 ThreadBase 派生· PlaybackThread：回放线程基类，同由 ThreadBase 派生· MixerThread：混音回放线程类，由 PlaybackThread 派生，负责处理标识为 AUDIO_OUTPUT_FLAG_PRIMARY、AUDIO_OUTPUT_FLAG_FAST、AUDIO_OUTPUT_FLAG_DEEP_BUFFER 的音频流，MixerThread 可以把多个音轨的数据混音后再输出· DirectOutputThread：直输回放线程类，由 PlaybackThread 派生，负责处理标识为 AUDIO_OUTPUT_FLAG_DIRECT 的音频流，这种音频流数据不需要软件混音，直接输出到音频设备即可· DuplicatingThread：复制回放线程类，由 MixerThread 派生，负责复制音频流数据到其他输出设备，使用场景如主声卡设备、蓝牙耳机设备、USB 声卡设备同时输出· OffloadThread：硬解回放线程类，由 DirectOutputThread 派生，负责处理标识为 AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD 的音频流，这种音频流未经软件解码的（一般是 MP3、AAC 等格式的数据），需要输出到硬件解码器，由硬件解码器解码成 PCM 数据 PlaybackThread 中有个极为重要的函数 threadLoop()，当 PlaybackThread 被强引用时，threadLoop() 会真正运行起来进入循环主体，处理音频流数据相关事务，threadLoop() 大致流程如下（以 MixerThread 为例）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889[-&gt;\\frameworks\\av\\services\\audioflinger\\Threads.cpp]bool AudioFlinger::PlaybackThread::threadLoop()&#123; // ...... while (!exitPending()) &#123; // ...... &#123; // scope for mLock Mutex::Autolock _l(mLock); processConfigEvents_l(); // ...... if ((!mActiveTracks.size() &amp;&amp; systemTime() &gt; mStandbyTimeNs) || isSuspended()) &#123; // put audio hardware into standby after short delay if (shouldStandby_l()) &#123; threadLoop_standby(); mStandby = true; &#125; // ...... &#125; // mMixerStatusIgnoringFastTracks is also updated internally mMixerStatus = prepareTracks_l(&amp;tracksToRemove); // ...... &#125; // mLock scope ends // ...... if (mBytesRemaining == 0) &#123; mCurrentWriteLength = 0; if (mMixerStatus == MIXER_TRACKS_READY) &#123; // threadLoop_mix() sets mCurrentWriteLength threadLoop_mix(); &#125; // ...... &#125; // ...... if (!waitingAsyncCallback()) &#123; // mSleepTimeUs == 0 means we must write to audio hardware if (mSleepTimeUs == 0) &#123; // ...... if (mBytesRemaining) &#123; // FIXME rewrite to reduce number of system calls ret = threadLoop_write(); lastWriteFinished = systemTime(); delta = lastWriteFinished - mLastWriteTime; if (ret &lt; 0) &#123; mBytesRemaining = 0; &#125; else &#123; mBytesWritten += ret; mBytesRemaining -= ret; mFramesWritten += ret / mFrameSize; &#125; &#125; // ...... &#125; // ...... &#125; // Finally let go of removed track(s), without the lock held // since we can&apos;t guarantee the destructors won&apos;t acquire that // same lock. This will also mutate and push a new fast mixer state. threadLoop_removeTracks(tracksToRemove); tracksToRemove.clear(); // ...... &#125; threadLoop_exit(); if (!mStandby) &#123; threadLoop_standby(); mStandby = true; &#125; // ...... return false;&#125; threadLoop() 循环的条件是 exitPending() 返回 false，如果想要 PlaybackThread 结束循环，则可以调用 requestExit() 来请求退出；processConfigEvents_l() ：处理配置事件；当有配置改变的事件发生时，需要调用 sendConfigEvent_l() 来通知 PlaybackThread，这样 PlaybackThread 才能及时处理配置事件；常见的配置事件是切换音频通路；检查此时此刻是否符合 standby 条件，比如当前并没有 ACTIVE 状态的 Track（mActiveTracks.size() = 0），那么调用 threadLoop_standby() 关闭音频硬件设备以节省能耗；prepareTracks_l()： 准备音频流和混音器，该函数非常复杂，这里不详细分析了，仅列一下流程要点：遍历 mActiveTracks，逐个处理 mActiveTracks 上的 Track，检查该 Track 是否为 ACTIVE 状态；如果 Track 设置是 ACTIVE 状态，则再检查该 Track 的数据是否准备就绪了；根据音频流的音量值、格式、声道数、音轨的采样率、硬件设备的采样率，配置好混音器参数；如果 Track 的状态是 PAUSED 或 STOPPED，则把该 Track 添加到 tracksToRemove 向量中；threadLoop_mix()：读取所有置了 ACTIVE 状态的音频流数据，混音器开始处理这些数据；threadLoop_write()： 把混音器处理后的数据写到输出流设备；threadLoop_removeTracks()： 把 tracksToRemove 上的所有 Track 从 mActiveTracks 中移除出来；这样下一次循环时就不会处理这些 Track 了。这里说说 PlaybackThread 与输出流设备的关系：PlaybackThread 实例与输出流设备是一一对应的，比方说 OffloadThread 只会将音频数据输出到 compress_offload 设备中，MixerThread(with FastMixer) 只会将音频数据输出到 low_latency 设备中。 从 Audio HAL 中，我们通常看到如下 4 种输出流设备，分别对应着不同的播放场景： primary_out：主输出流设备，用于铃声类声音输出，对应着标识为 AUDIO_OUTPUT_FLAG_PRIMARY 的音频流和一个 MixerThread 回放线程实例low_latency：低延迟输出流设备，用于按键音、游戏背景音等对时延要求高的声音输出，对应着标识为 AUDIO_OUTPUT_FLAG_FAST 的音频流和一个 MixerThread 回放线程实例deep_buffer：音乐音轨输出流设备，用于音乐等对时延要求不高的声音输出，对应着标识为 AUDIO_OUTPUT_FLAG_DEEP_BUFFER 的音频流和一个 MixerThread 回放线程实例compress_offload：硬解输出流设备，用于需要硬件解码的数据输出，对应着标识为 AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD 的音频流和一个 OffloadThread 回放线程实例其中 primary_out 设备是必须声明支持的，而且系统启动时就已经打开 primary_out 设备并创建好对应的 MixerThread 实例。其他类型的输出流设备并非必须声明支持的，主要是看硬件上有无这个能力。 可能有人产生这样的疑问：既然 primary_out 设备一直保持打开，那么能耗岂不是很大？这里阐释一个概念：输出流设备属于逻辑设备，并不是硬件设备。所以即使输出流设备一直保持打开，只要硬件设备不工作，那么就不会影响能耗。那么硬件设备什么时候才会打开呢？答案是 PlaybackThread 将音频数据写入到输出流设备时。 下图简单描述 AudioTrack、PlaybackThread、输出流设备三者的对应关系： 我们可以这么说：输出流设备决定了它对应的 PlaybackThread 是什么类型。怎么理解呢？意思是说：只有支持了该类型的输出流设备，那么该类型的 PlaybackThread 才有可能被创建。举个例子：只有硬件上具备硬件解码器，系统才建立 compress_offload 设备，然后播放 mp3 格式的音乐文件时，才会创建 OffloadThread 把数据输出到 compress_offload 设备上；反之，如果硬件上并不具备硬件解码器，系统则不应该建立 compress_offload 设备，那么播放 mp3 格式的音乐文件时，通过 MixerThread 把数据输出到其他输出流设备上。 那么有无可能出现这种情况：底层并不支持 compress_offload 设备，但偏偏有个标识为 AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD 的音频流送到 AudioFlinger 了呢？这是不可能的。系统启动时，会检查并保存输入输出流设备的支持信息；播放器在播放 mp3 文件时，首先看 compress_offload 设备是否支持了，如果支持，那么不进行软件解码，直接把数据标识为 AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD；如果不支持，那么先进行软件解码，然后把解码好的数据标识为 AUDIO_OUTPUT_FLAG_DEEP_BUFFER，前提是 deep_buffer 设备是支持了的；如果 deep_buffer 设备也不支持，那么把数据标识为 AUDIO_OUTPUT_FLAG_PRIMARY。 系统启动时，就已经打开 primary_out、low_latency、deep_buffer 这三种输出流设备，并创建对应的 MixerThread 了；而此时 DirectOutputThread 与 OffloadThread 不会被创建，直到标识为 AUDIO_OUTPUT_FLAG_DIRECT/AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD 的音频流需要输出时，才开始创建 DirectOutputThread/OffloadThread 和打开 direct_out/compress_offload 设备。这一点请参考如下代码，注释非常清晰： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[-&gt;\\frameworks\\av\\services\\audiopolicy\\managerdefault\\AudioPolicyManager.cpp]AudioPolicyManager::AudioPolicyManager(AudioPolicyClientInterface *clientInterface) // ......&#123; // ...... // mAvailableOutputDevices and mAvailableInputDevices now contain all attached devices // open all output streams needed to access attached devices // ...... for (size_t i = 0; i &lt; mHwModules.size(); i++) &#123; // ...... // open all output streams needed to access attached devices // except for direct output streams that are only opened when they are actually // required by an app. // This also validates mAvailableOutputDevices list for (size_t j = 0; j &lt; mHwModules[i]-&gt;mOutputProfiles.size(); j++) &#123; // ...... if ((outProfile-&gt;getFlags() &amp; AUDIO_OUTPUT_FLAG_DIRECT) != 0) &#123; continue; &#125; // ...... audio_io_handle_t output = AUDIO_IO_HANDLE_NONE; status_t status = mpClientInterface-&gt;openOutput(outProfile-&gt;getModuleHandle(), &amp;output, &amp;config, &amp;outputDesc-&gt;mDevice, address, &amp;outputDesc-&gt;mLatency, outputDesc-&gt;mFlags); // ...... &#125; // open input streams needed to access attached devices to validate // mAvailableInputDevices list for (size_t j = 0; j &lt; mHwModules[i]-&gt;mInputProfiles.size(); j++) &#123; // ...... status_t status = mpClientInterface-&gt;openInput(inProfile-&gt;getModuleHandle(), &amp;input, &amp;config, &amp;inputDesc-&gt;mDevice, address, AUDIO_SOURCE_MIC, AUDIO_INPUT_FLAG_NONE); // ...... &#125; &#125; // ...... updateDevicesAndOutputs();&#125; 其中 mpClientInterface-&gt;openOutput() 最终会调用到 AudioFlinger::openOutput()：打开输出流设备，并创建 PlaybackThread 对象： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374status_t AudioFlinger::openOutput(audio_module_handle_t module, audio_io_handle_t *output, audio_config_t *config, audio_devices_t *devices, const String8&amp; address, uint32_t *latencyMs, audio_output_flags_t flags)&#123; // ...... Mutex::Autolock _l(mLock); sp&lt;PlaybackThread&gt; thread = openOutput_l(module, output, config, *devices, address, flags); // ......&#125;sp&lt;AudioFlinger::PlaybackThread&gt; AudioFlinger::openOutput_l(audio_module_handle_t module, audio_io_handle_t *output, audio_config_t *config, audio_devices_t devices, const String8&amp; address, audio_output_flags_t flags)&#123; // ...... // 分配全局唯一的 audio_io_handle_t，可以理解它是回放线程的索引号 if (*output == AUDIO_IO_HANDLE_NONE) &#123; *output = nextUniqueId(); &#125; mHardwareStatus = AUDIO_HW_OUTPUT_OPEN; //...... // 打开音频输出流设备，HAL 层根据 flags 选择打开相关类型的输出流设备 AudioStreamOut *outputStream = NULL; status_t status = outHwDev-&gt;openOutputStream( &amp;outputStream, *output, devices, flags, config, address.string()); mHardwareStatus = AUDIO_HW_IDLE; if (status == NO_ERROR) &#123; PlaybackThread *thread; if (flags &amp; AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD) &#123; // AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD 音频流，创建 OffloadThread 实例 thread = new OffloadThread(this, outputStream, *output, devices, mSystemReady); ALOGV(&quot;openOutput_l() created offload output: ID %d thread %p&quot;, *output, thread); &#125; else if ((flags &amp; AUDIO_OUTPUT_FLAG_DIRECT) || !isValidPcmSinkFormat(config-&gt;format) || !isValidPcmSinkChannelMask(config-&gt;channel_mask)) &#123; // AUDIO_OUTPUT_FLAG_DIRECT 音频流，创建 DirectOutputThread 实例 thread = new DirectOutputThread(this, outputStream, *output, devices, mSystemReady); ALOGV(&quot;openOutput_l() created direct output: ID %d thread %p&quot;, *output, thread); &#125; else &#123; // 其他标识的音频流，创建 MixerThread 实例 thread = new MixerThread(this, outputStream, *output, devices, mSystemReady); ALOGV(&quot;openOutput_l() created mixer output: ID %d thread %p&quot;, *output, thread); &#125; // 把 audio_io_handle_t 和 PlaybackThread 添加到键值对向量 mPlaybackThreads 中 // 键值对向量 mPlaybackThreads 中，由于 audio_io_handle_t 和 PlaybackThread 是一 // 一对应的关系，所以拿到一个 audio_io_handle_t，就能找到它对应的 PlaybackThread // 所以可以理解 audio_io_handle_t 为 PlaybackThread 的索引号 mPlaybackThreads.add(*output, thread); return thread; &#125; return 0;&#125; 1.2.3、AudioFlinger 音频流管理AudioFlinger 音频流管理由 AudioFlinger::PlaybackThread::Track 实现，Track 与 AudioTrack 是一对一的关系，一个 AudioTrack 创建后，那么 AudioFlinger 会创建一个 Track 与之对应；PlaybackThread 与 AudioTrack/Track 是一对多的关系，一个 PlaybackThread 可以挂着多个 Track。 具体来说：AudioTrack 创建后，AudioPolicyManager 根据 AudioTrack 的输出标识和流类型，找到对应的输出流设备和 PlaybackThread（如果没有找到的话，则系统会打开对应的输出流设备并新建一个 PlaybackThread），然后创建一个 Track 并挂到这个 PlaybackThread 下面。 PlaybackThread 有两个私有成员向量与此强相关： · mTracks：该 PlaybackThread 创建的所有 Track 均添加保存到这个向量中· mActiveTracks：只有需要播放（设置了 ACTIVE 状态）的 Track 会添加到这个向量中；PlaybackThread 会从该向量上找到所有设置了 ACTIVE 状态的 Track，把这些 Track 数据混音后写到输出流设备音频流控制最常用的三个接口： AudioFlinger::PlaybackThread::Track::start：开始播放：把该 Track 置 ACTIVE 状态，然后添加到 mActiveTracks 向量中，最后调用 AudioFlinger::PlaybackThread::broadcast_l() 告知 PlaybackThread 情况有变· AudioFlinger::PlaybackThread::Track::stop：停止播放：把该 Track 置 STOPPED 状态，最后调用 AudioFlinger::PlaybackThread::broadcast_l() 告知 PlaybackThread 情况有变· AudioFlinger::PlaybackThread::Track::pause：暂停播放：把该 Track 置 PAUSING 状态，最后调用 AudioFlinger::PlaybackThread::broadcast_l() 告知 PlaybackThread 情况有变· AudioFlinger::PlaybackThread::threadLoop() 得悉情况有变后，调用 prepareTracks_l() 重新准备音频流和混音器：ACTIVE 状态的 Track 会添加到 mActiveTracks，此外的 Track 会从 mActiveTracks 上移除出来，然后重新准备 AudioMixer。 可见这三个音频流控制接口是非常简单的，主要是设置一下 Track 的状态，然后发个事件通知 PlaybackThread 就行，复杂的处理都在 AudioFlinger::PlaybackThread::threadLoop() 中了。 (二)、深入剖析Android音频之AudioPolicyServiceAudioPolicyService是策略的制定者，比如什么时候打开音频接口设备、某种Stream类型的音频对应什么设备等等。而AudioFlinger则是策略的执行者，例如具体如何与音频设备通信，如何维护现有系统中的音频设备，以及多个音频流的混音如何处理等等都得由它来完成。AudioPolicyService根据用户配置来指导AudioFlinger加载设备接口，起到路由功能 1234567891011121314151617181920212223[-&gt;\\frameworks\\av\\media\\audioserver\\main_audioserver.cpp]int main(int argc __unused, char **argv)&#123; ...... if (doLog &amp;&amp; (childPid = fork()) != 0) &#123; ...... &#125; else &#123; // all other services if (doLog) &#123; prctl(PR_SET_PDEATHSIG, SIGKILL); // if parent media.log dies before me, kill me also setpgid(0, 0); // but if I die first, don't kill my parent &#125; sp&lt;ProcessState&gt; proc(ProcessState::self()); sp&lt;IServiceManager&gt; sm = defaultServiceManager(); ALOGI(\"ServiceManager: %p\", sm.get()); AudioFlinger::instantiate(); AudioPolicyService::instantiate(); RadioService::instantiate(); SoundTriggerHwService::instantiate(); ProcessState::self()-&gt;startThreadPool(); IPCThreadState::self()-&gt;joinThreadPool(); &#125;&#125; AudioPolicyService继承了模板类BinderService，该类用于注册native service。BinderService是一个模板类，该类的publish函数就是完成向ServiceManager注册服务。 12[-&gt;\\frameworks\\av\\services\\audiopolicy\\service\\AudioPolicyService.h]static const char *getServiceName() ANDROID_API &#123; return \"media.audio_policy\"; &#125; AudioPolicyService注册名为”media.audio_policy”的服务。首先看看AudioPolicyService的onFirstRef()函数 1234567891011121314151617181920212223242526272829303132333435363738[-&gt;\\frameworks\\av\\services\\audiopolicy\\service\\AudioPolicyService.cpp]AudioPolicyService::AudioPolicyService() : BnAudioPolicyService(), mpAudioPolicyDev(NULL), mpAudioPolicy(NULL), mAudioPolicyManager(NULL), mAudioPolicyClient(NULL), mPhoneState(AUDIO_MODE_INVALID)&#123;&#125;void AudioPolicyService::onFirstRef()&#123; &#123; Mutex::Autolock _l(mLock); /* Step 1:创建AudioCommandThread线程 */ // start tone playback thread mTonePlaybackThread = new AudioCommandThread(String8(\"ApmTone\"), this); // start audio commands thread mAudioCommandThread = new AudioCommandThread(String8(\"ApmAudio\"), this); // start output activity command thread mOutputCommandThread = new AudioCommandThread(String8(\"ApmOutput\"), this);#ifdef USE_LEGACY_AUDIO_POLICY // 使用老版本的 audio policy 初始化方式 ....#else // 使用最新的 audio policy 初始化方式 ALOGI(\"AudioPolicyService CSTOR in new mode\"); /* Step 2:创建AudioPolicyClient、 AudioPolicyManager */ mAudioPolicyClient = new AudioPolicyClient(this); mAudioPolicyManager = createAudioPolicyManager(mAudioPolicyClient);#endif &#125; // load audio processing modules sp&lt;AudioPolicyEffects&gt;audioPolicyEffects = new AudioPolicyEffects(); &#123; Mutex::Autolock _l(mLock); mAudioPolicyEffects = audioPolicyEffects; &#125;&#125; 先看看总体时序图： 2.1、Step 1:创建AudioCommandThread线程在AudioPolicyService对象构造过程中，分别创建了ApmTone、ApmAudio、ApmOutput三个AudioCommandThread线程： 1、 ApmTone用于播放tone音； 2、 ApmAudio用于执行audio命令； 3、ApmOutput用于执行输出命令； 在第一次强引用AudioCommandThread线程对象时，AudioCommandThread的onFirstRef函数被回调，在此启动线程 12345[-&gt;\\frameworks\\av\\services\\audiopolicy\\service\\AudioPolicyService.cpp]void AudioPolicyService::AudioCommandThread::onFirstRef()&#123; run(mName.string(), ANDROID_PRIORITY_AUDIO);&#125; 这里采用异步方式来执行audio command，当需要执行上表中的命令时，首先将命令投递到AudioCommandThread的mAudioCommands命令向量表中，然后通过mWaitWorkCV.signal()唤醒AudioCommandThread线程，被唤醒的AudioCommandThread线程执行完command后，又通过mWaitWorkCV.waitRelative(mLock, waitTime)睡眠等待命令到来。 2.2、Step 2: 创建AudioPolicyClient、 AudioPolicyManager首先创建AudioPolicyClient 123456789101112131415161718192021222324252627282930[-&gt;\\frameworks\\av\\services\\audiopolicy\\service\\AudioPolicyService.h] class AudioPolicyClient : public AudioPolicyClientInterface &#123; public: AudioPolicyClient(AudioPolicyService *service) : mAudioPolicyService(service) &#123;&#125; virtual ~AudioPolicyClient() &#123;&#125; // loads a HW module. virtual audio_module_handle_t loadHwModule(const char *name); ...... virtual status_t openOutput(audio_module_handle_t module, audio_io_handle_t *output, audio_config_t *config, audio_devices_t *devices, const String8&amp; address, uint32_t *latencyMs, audio_output_flags_t flags); ...... // opens an audio input virtual audio_io_handle_t openInput(audio_module_handle_t module, audio_io_handle_t *input, audio_config_t *config, audio_devices_t *devices, const String8&amp; address, audio_source_t source, audio_input_flags_t flags); ...... private: AudioPolicyService *mAudioPolicyService; &#125;; createAudioPolicyManager() 函数的实现位于 frameworks/av/services/audiopolicy/manager/AudioPolicyFactory.cpp文件中。查看源码后我们会发现它实际上是直接调用了 AudioPolicyManager 的构造函数。代码如下： 123456[-&gt;\\frameworks\\av\\services\\audiopolicy\\manager\\AudioPolicyFactory.cpp]extern \"C\" AudioPolicyInterface* createAudioPolicyManager( AudioPolicyClientInterface *clientInterface)&#123; return new AudioPolicyManager(clientInterface);&#125; 2.3、创建AudioPolicyManager()总体流程图： AudioPolicyManager 的构造函数将解析音频策略配置文件，从而获取到设备所支持的音频设备信息（包括设备是否支持 Offload、Direct 模式输出，各输入输出 profile 所支持的采样率、通道数、数据格式等），加载全部 HwModule，为之创建所有非 direct 输出类型的 outputStream 和所有 inputStream，并创建相应的 playbackThread 或 recordThread 线程。需要注意的是，Android 7.0上的音频策略配置文件开始使用 XML 格式，其文件名为 audio_policy_configuration.xml， 而在之前的版本上音频策略配置文件为 audio_policy.conf。frameworks/av/services/audiopolicy/managerdefault/AudioPolicyManager.cpp 中 AudioPolicyManager 构造函数的关键代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138[-&gt;\\frameworks\\av\\services\\audiopolicy\\managerdefault\\AudioPolicyManager.cpp]AudioPolicyManager::AudioPolicyManager(AudioPolicyClientInterface *clientInterface) :#ifdef AUDIO_POLICY_TEST Thread(false),#endif //AUDIO_POLICY_TEST mLimitRingtoneVolume(false), mLastVoiceVolume(-1.0f), mA2dpSuspended(false), mAudioPortGeneration(1), mBeaconMuteRefCount(0), mBeaconPlayingRefCount(0), mBeaconMuted(false), mTtsOutputAvailable(false), mMasterMono(false)&#123; ....#ifdef USE_XML_AUDIO_POLICY_CONF // 设备使用的配置文件为 audio_policy_configuration.xml mVolumeCurves = new VolumeCurvesCollection(); AudioPolicyConfig config(mHwModules, mAvailableOutputDevices, mAvailableInputDevices, mDefaultOutputDevice, speakerDrcEnabled, static_cast&lt;VolumeCurvesCollection *&gt;(mVolumeCurves)); PolicySerializer serializer; // 解析 xml 配置文件，将设备支持的音频输出设备保存在 mAvailableOutputDevices 变量中， // 将设备支持的音频输入设备保存在 mAvailableInputDevices 变量中，将设备的默认音频输出 // 设备保存在 mDefaultOutputDevice 变量中。 if (serializer.deserialize(AUDIO_POLICY_XML_CONFIG_FILE, config) != NO_ERROR) &#123;#else // 设备使用的配置文件为 audio_policy.conf mVolumeCurves = new StreamDescriptorCollection(); AudioPolicyConfig config(mHwModules, mAvailableOutputDevices, mAvailableInputDevices, mDefaultOutputDevice, speakerDrcEnabled); // 优先解析 vendor 目录下的 conf 配置文件，然后解析 device 目录下的 conf 配置文件。 // 将设备支持的音频输出设备保存在 mAvailableOutputDevices 变量中， // 将设备支持的音频输入设备保存在 mAvailableInputDevices 变量中，将设备的默认音频输出 // 设备保存在 mDefaultOutputDevice 变量中。 if ((ConfigParsingUtils::loadConfig(AUDIO_POLICY_VENDOR_CONFIG_FILE, config) != NO_ERROR) &amp;&amp; (ConfigParsingUtils::loadConfig(AUDIO_POLICY_CONFIG_FILE, config) != NO_ERROR)) &#123;#endif ALOGE(\"could not load audio policy configuration file, setting defaults\"); config.setDefault(); &#125; // must be done after reading the policy (since conditionned by Speaker Drc Enabling) mVolumeCurves-&gt;initializeVolumeCurves(speakerDrcEnabled); // 设置音量调节曲线 .... // 依次加载 HwModule 并打开其所含 profile 的 outputStream 及 inputStream for (size_t i = 0; i &lt; mHwModules.size(); i++) &#123; mHwModules[i]-&gt;mHandle = mpClientInterface-&gt;loadHwModule(mHwModules[i]-&gt;getName()); if (mHwModules[i]-&gt;mHandle == 0) &#123; ALOGW(\"could not open HW module %s\", mHwModules[i]-&gt;getName()); continue; &#125; // open all output streams needed to access attached devices // except for direct output streams that are only opened when they are actually // required by an app. // This also validates mAvailableOutputDevices list // 打开当前 module 下所有非 direct 类型 profile 的 outputStream for (size_t j = 0; j &lt; mHwModules[i]-&gt;mOutputProfiles.size(); j++) &#123; const sp&lt;IOProfile&gt; outProfile = mHwModules[i]-&gt;mOutputProfiles[j]; .... // 如果当前操作的 module.profile 是 direct 类型，则不为其打开 outputStream if ((outProfile-&gt;getFlags() &amp; AUDIO_OUTPUT_FLAG_DIRECT) != 0) &#123; continue; &#125; .... // 获取采样率、通道数、数据格式等各音频参数 sp&lt;SwAudioOutputDescriptor&gt; outputDesc = new SwAudioOutputDescriptor(outProfile, mpClientInterface); const DeviceVector &amp;supportedDevices = outProfile-&gt;getSupportedDevices(); const DeviceVector &amp;devicesForType = supportedDevices.getDevicesFromType(profileType); String8 address = devicesForType.size() &gt; 0 ? devicesForType.itemAt(0)-&gt;mAddress : String8(\"\"); outputDesc-&gt;mDevice = profileType; audio_config_t config = AUDIO_CONFIG_INITIALIZER; config.sample_rate = outputDesc-&gt;mSamplingRate; config.channel_mask = outputDesc-&gt;mChannelMask; config.format = outputDesc-&gt;mFormat; audio_io_handle_t output = AUDIO_IO_HANDLE_NONE; // 为当前 module.profile 打开对应的 outputStream 并创建 playbackThread 线程 status_t status = mpClientInterface-&gt;openOutput(outProfile-&gt;getModuleHandle(), &amp;output, &amp;config, &amp;outputDesc-&gt;mDevice, address, &amp;outputDesc-&gt;mLatency, outputDesc-&gt;mFlags); .... &#125; // open input streams needed to access attached devices to validate // mAvailableInputDevices list // 打开当前 module 下所有 profile 的 inputStream for (size_t j = 0; j &lt; mHwModules[i]-&gt;mInputProfiles.size(); j++) &#123; const sp&lt;IOProfile&gt; inProfile = mHwModules[i]-&gt;mInputProfiles[j]; .... sp&lt;AudioInputDescriptor&gt; inputDesc = new AudioInputDescriptor(inProfile); inputDesc-&gt;mDevice = profileType; // 获取采样率、通道数、数据格式等各音频参数 // find the address DeviceVector inputDevices = mAvailableInputDevices.getDevicesFromType(profileType); // the inputs vector must be of size 1, but we don't want to crash here String8 address = inputDevices.size() &gt; 0 ? inputDevices.itemAt(0)-&gt;mAddress : String8(\"\"); ALOGV(\" for input device 0x%x using address %s\", profileType, address.string()); ALOGE_IF(inputDevices.size() == 0, \"Input device list is empty!\"); audio_config_t config = AUDIO_CONFIG_INITIALIZER; config.sample_rate = inputDesc-&gt;mSamplingRate; config.channel_mask = inputDesc-&gt;mChannelMask; config.format = inputDesc-&gt;mFormat; audio_io_handle_t input = AUDIO_IO_HANDLE_NONE; // 为当前 module.profile 打开对应的 inputStream 并创建 recordThread 线程 status_t status = mpClientInterface-&gt;openInput(inProfile-&gt;getModuleHandle(), &amp;input, &amp;config, &amp;inputDesc-&gt;mDevice, address, AUDIO_SOURCE_MIC, AUDIO_INPUT_FLAG_NONE); .... &#125; &#125; .... updateDevicesAndOutputs(); // 更新系统缓存的音频输出设备信息 ....&#125; AudioPolicyManager对象构造过程中主要完成以下几个步骤： 1、 加载audio_policy_configuration.xml或者audio_policy.conf配置文件 2、 初始化音量调节点initializeVolumeCurves(speakerDrcEnabled) 3、 加载audio policy硬件抽象库：mpClientInterface-&gt;loadHwModule(mHwModules[i]-&gt;mName) 4、 打开对应的outputStream和inputStream ： mpClientInterface-&gt;openOutput()、mpClientInterface-&gt;openInput 5、 更新系统缓存的音频输出设备信息updateDevicesAndOutputs() 2.3.1、加载audio_policy_configuration.xml或者audio_policy.conf配置文件audio_policy_configuration.xmlaudio_policy.conf同时定义了多个audio 接口，每一个audio 接口包含若干output和input，而每个output和input又同时支持多种输入输出模式，每种输入输出模式又支持若干种设备。 2.3.2、初始化音量调节点initializeVolumeCurves(speakerDrcEnabled)在AudioPolicyManagerBase中定义了音量调节对应的音频流描述符数组： 12345678910111213141516//audio_policy_volumes.xmlconst AudioPolicyManagerBase::VolumeCurvePoint *AudioPolicyManagerBase::sVolumeProfiles[AudioSystem::NUM_STREAM_TYPES] [AudioPolicyManagerBase::DEVICE_CATEGORY_CNT] = &#123; &#123; // AUDIO_STREAM_VOICE_CALL sDefaultVoiceVolumeCurve, // DEVICE_CATEGORY_HEADSET sSpeakerVoiceVolumeCurve, // DEVICE_CATEGORY_SPEAKER sDefaultVoiceVolumeCurve // DEVICE_CATEGORY_EARPIECE &#125;, &#123; // AUDIO_STREAM_SYSTEM sHeadsetSystemVolumeCurve, // DEVICE_CATEGORY_HEADSET sDefaultSystemVolumeCurve, // DEVICE_CATEGORY_SPEAKER sDefaultSystemVolumeCurve // DEVICE_CATEGORY_EARPIECE &#125;, ...... &#125; initializeVolumeCurves()函数就是初始化该数组元素： 12345678910111213141516171819202122[-&gt;]void AudioPolicyManagerBase::initializeVolumeCurves()&#123; for (int i = 0; i &lt; AudioSystem::NUM_STREAM_TYPES; i++) &#123; for (int j = 0; j &lt; DEVICE_CATEGORY_CNT; j++) &#123; mStreams[i].mVolumeCurve[j] = sVolumeProfiles[i][j]; &#125; &#125; // Check availability of DRC on speaker path: if available, override some of the speaker curves if (mSpeakerDrcEnabled) &#123; mStreams[AUDIO_STREAM_SYSTEM].mVolumeCurve[DEVICE_CATEGORY_SPEAKER] = sDefaultSystemVolumeCurveDrc; mStreams[AUDIO_STREAM_RING].mVolumeCurve[DEVICE_CATEGORY_SPEAKER] = sSpeakerSonificationVolumeCurveDrc; mStreams[AUDIO_STREAM_ALARM].mVolumeCurve[DEVICE_CATEGORY_SPEAKER] = sSpeakerSonificationVolumeCurveDrc; mStreams[AUDIO_STREAM_NOTIFICATION].mVolumeCurve[DEVICE_CATEGORY_SPEAKER] = sSpeakerSonificationVolumeCurveDrc; &#125;&#125; 2.3.3、加载audio policy硬件抽象库loadHwModule()我们直接分析AudioFlinger::loadHwModule_l()中的load_audio_interface()函数12345678910111213141516static int load_audio_interface(const char *if_name, audio_hw_device_t **dev)&#123; const hw_module_t *mod; int rc; //根据名字加载audio_module模块 rc = hw_get_module_by_class(AUDIO_HARDWARE_MODULE_ID, if_name, &amp;mod); ALOGE_IF(rc, &quot;%s couldn&apos;t load audio hw module %s.%s (%s)&quot;, __func__, AUDIO_HARDWARE_MODULE_ID, if_name, strerror(-rc)); //打开audio_device设备 rc = audio_hw_device_open(mod, dev); ALOGE_IF(rc, &quot;%s couldn&apos;t open audio hw device in %s.%s (%s)&quot;, __func__, AUDIO_HARDWARE_MODULE_ID, if_name, strerror(-rc)); return 0;&#125; [-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp] 123456static inline int audio_hw_device_open(const struct hw_module_t* module, struct audio_hw_device** device)&#123; return module-&gt;methods-&gt;open(module, AUDIO_HARDWARE_INTERFACE, (struct hw_device_t**)device);&#125; [-&gt;\\hardware\\libhardware_legacy\\audio\\audio_hw_hal.cpp] 1234567891011121314151617181920212223242526272829303132333435static int legacy_adev_open(const hw_module_t* module, const char* name, hw_device_t** device)&#123; struct legacy_audio_device *ladev; int ret; ladev = (struct legacy_audio_device *)calloc(1, sizeof(*ladev)); ladev-&gt;device.common.tag = HARDWARE_DEVICE_TAG; ladev-&gt;device.common.version = AUDIO_DEVICE_API_VERSION_2_0; ladev-&gt;device.common.module = const_cast&lt;hw_module_t*&gt;(module); ladev-&gt;device.common.close = legacy_adev_close; ladev-&gt;device.init_check = adev_init_check; ladev-&gt;device.set_voice_volume = adev_set_voice_volume; ladev-&gt;device.set_master_volume = adev_set_master_volume; ladev-&gt;device.get_master_volume = adev_get_master_volume; ladev-&gt;device.set_mode = adev_set_mode; ladev-&gt;device.set_mic_mute = adev_set_mic_mute; ladev-&gt;device.get_mic_mute = adev_get_mic_mute; ladev-&gt;device.set_parameters = adev_set_parameters; ladev-&gt;device.get_parameters = adev_get_parameters; ladev-&gt;device.get_input_buffer_size = adev_get_input_buffer_size; ladev-&gt;device.open_output_stream = adev_open_output_stream; ladev-&gt;device.close_output_stream = adev_close_output_stream; ladev-&gt;device.open_input_stream = adev_open_input_stream; ladev-&gt;device.close_input_stream = adev_close_input_stream; ladev-&gt;device.dump = adev_dump; ladev-&gt;hwif = createAudioHardware(); *device = &amp;ladev-&gt;device.common; return 0;&#125; 到此就加载完系统定义的所有音频接口，并生成相应的数据对象，如下图所示：’ 2.3.4、打开对应的outputStream和inputStream前面一小节已经分析过outputStream，这里不再分析了 打开音频输出后，在AudioFlinger与AudioPolicyService中的表现形式如下： 打开音频输入:打开音频输入后，在AudioFlinger与AudioPolicyService中的表现形式如下： 2.3.5、 更新系统缓存的音频输出设备信息updateDevicesAndOutputs()12345678[-&gt;\\frameworks\\av\\services\\audiopolicy\\managerdefault\\AudioPolicyManager.cpp]void AudioPolicyManager::updateDevicesAndOutputs()&#123; for (int i = 0; i &lt; NUM_STRATEGIES; i++) &#123; mDeviceForStrategy[i] = getDeviceForStrategy((routing_strategy)i, false /*fromCache*/); &#125; mPreviousOutputs = mOutputs;&#125; 2.4、总结-&gt;打开音频输出时创建一个audio_stream_out通道，并创建AudioStreamOut对象以及新建PlaybackThread播放线程。 -&gt; 打开音频输入时创建一个audio_stream_in通道，并创建AudioStreamIn对象以及创建RecordThread录音线程。 (三)、深入剖析Android音频之AudioTrack现在我们开始分析 AudioTrack 的创建过程，特别留意 AudioTrack 与 AudioFlinger 如何建立联系、用于 AudioTrack 与 AudioFlinger 交换数据的匿名共享内存如何分配。 3.1. AudioTrack &amp; AudioFlinger 相关类时序图： 首先看一下 AudioTrack &amp; AudioFlinger 的类图，理一下 AudioFlinger 的主要类及其关系、AudioTrack 与 AudioFlinger 之间的联系，后面将以该图为脉络展开分析。 ☯ AudioFlinger::PlaybackThread：回放线程基类，不同输出标识的音频流对应不同类型的 PlaybackThread 实例（分为四种：MixerThread、DirectOutputThread、DuplicatingThread、OffloadThread），具体见 3.4. AudioFlinger 回放录制线程 小节，所有的 PlaybackThread 实例都会添加到 AudioFlinger.mPlaybackThreads 向量中；这个向量的定义： DefaultKeyedVector&lt; audio_io_handle_t, sp &gt; mPlaybackThreads;，可见 audio_io_handle_t 是与 PlaybackThread 是一一对应的，由已知的 audio_io_handle_t 就能找到对应的 PlaybackThread；audio_io_handle_t 在创建 PlaybackThread 时由系统分配，这个值是全局唯一的☯ AudioFlinger::PlaybackThread::Track：音频流管理类，创建一块匿名共享内存用于 AudioTrack 与 AudioFlinger 之间的数据交换（方便起见，这块匿名共享内存，以后均简单称为 FIFO），同时实现 start()、stop()、pause() 等音频流常用控制手段；注意，多个 Track 对象可能都注册到同一个 PlaybackThread 中（尤其对于 MixerThread 而言，一个 MixerThread 往往挂着多个 Track 对象），这多个 Track 对象都会添加到 PlaybackThread.mTracks 向量中统一管理☯ AudioFlinger::TrackHandle：Track 对象只负责音频流管理业务，对外并没有提供跨进程的 Binder 调用接口，而应用进程又需要对音频流进行控制，所以需要一个对象来代理 Track 的跨进程通讯，这个角色就是 TrackHandle，AudioTrack 通过它与 Track 交互☯ AudioTrack：Android 音频系统对外提供的一个 API 类，负责音频流数据输出；每个音频流对应着一个 AudioTrack 实例，不同输出标识的 AudioTrack 会匹配到不同的 AudioFlinger::PlaybackThread；AudioTrack 与 AudioFlinger::PlaybackThread 之间通过 FIFO 来交换音频数据，AudioTrack 是 FIFO 生产者，AudioFlinger::PlaybackThread 是 FIFO 消费者☯ AudioTrack::AudioTrackThread：数据传输模式为 TRANSFER_CALLBACK 时，需要创建该线程，它通过调用 audioCallback 回调函数主动从用户进程处索取数据并填充到 FIFO 上；数据传输模式为 TRANSFER_SYNC 时，则不需要创建这个线程，因为用户进程会持续调用 AudioTrack.write() 填充数据到 FIFO；数据传输模式为 TRANSFER_SHARED 时，也不需要创建这个线程，因为用户进程会创建一块匿名共享内存，并把要播放的音频数据一次性拷贝到这块匿名共享内存上了☯ IAudioTrack：IAudioTrack 是链结 AudioTrack 与 AudioFlinger 的桥梁；它在 AudioTrack 端的对象是 BpAudioTrack，在 AudioFlinger 端的对象是 BnAudioTrack，从图中不难看出，AudioFlinger::TrackHandle 继承自 BnAudioTrack，而 AudioFlinger::TrackHandle 恰恰是AudioFlinger::PlaybackThread::Track 的代理对象，所以 AudioTrack 得到 IAudioTrack 实例后，就可以调用 IAudioTrack 的接口与 AudioFlinger::PlaybackThread::Track 交互 audio_io_handle_t： 这里再详细说明一下 audio_io_handle_t，它是 AudioTrack/AudioRecord/AudioSystem、AudioFlinger、AudioPolicyManager 之间一个重要的链结点。3.4. AudioFlinger 回放录制线程 小节在 AudioFlinger::openOutput_l() 注释中大致说明了它的来历及其作用，现在回顾下：当打开输出流设备及创建 PlaybackThread 时，系统会分配一个全局唯一的值作为 audio_io_handle_t，并把 audio_io_handle_t 和 PlaybackThread 添加到键值对向量 mPlaybackThreads 中，由于 audio_io_handle_t 和 PlaybackThread 是一一对应的关系，因此拿到一个 audio_io_handle_t，就能遍历键值对向量 mPlaybackThreads 找到它对应的 PlaybackThread，可以简单理解 audio_io_handle_t 为 PlaybackThread 的索引号或线程 id。由于 audio_io_handle_t 具有 PlaybackThread 索引特性，所以应用进程想获取 PlaybackThread 某些信息的话，只需要传入对应的 audio_io_handle_t 即可。例如 AudioFlinger::format(audio_io_handle_t output)，这是 AudioFlinger 的一个服务接口，用户进程可以通过该接口获取某个 PlaybackThread 配置的音频格式： 123456789101112131415161718[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]audio_format_t AudioFlinger::format(audio_io_handle_t output) const&#123; Mutex::Autolock _l(mLock); // checkPlaybackThread_l() 根据传入的 audio_io_handle_t，从键值对向量 // mPlaybackThreads 中找到它对应的 PlaybackThread PlaybackThread *thread = checkPlaybackThread_l(output); if (thread == NULL) &#123; ALOGW(\"format() unknown thread %d\", output); return AUDIO_FORMAT_INVALID; &#125; return thread-&gt;format();&#125;AudioFlinger::PlaybackThread *AudioFlinger::checkPlaybackThread_l(audio_io_handle_t output) const&#123; return mPlaybackThreads.valueFor(output).get();&#125; 3.2. AudioTrack 构造过程当我们构造一个 AudioTrack 实例时（以 MODE_STREAM/TRANSFER_SYNC 模式为例，这也是最常用的模式了，此时 sharedBuffer 为空），系统都发生了什么事？阐述下大致流程： 如果 cbf（audioCallback 回调函数）非空，那么创建 AudioTrackThread 线程处理 audioCallback 回调函数（MODE_STREAM 模式时，cbf 为空）；根据 streamType（流类型）、flags（输出标识）等参数调用 AudioSystem::getOutputForAttr()；经过一系列的调用，进入 AudioPolicyManager::getOutputForDevice()：如果输出标识置了 AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD 或 AUDIO_OUTPUT_FLAG_DIRECT，那么最终调用 AudioFlinger::openOutput() 打开输出标识对应的输出流设备并创建相应的 PlaybackThread，保存该 PlaybackThread 对应的 audio_io_handle_t 给 AudioTrack；如果输出标识是其他类型，那么根据策略选择一个输出流设备和 PlaybackThread，并保存该 PlaybackThread 对应的 audio_io_handle_t 给 AudioTrack；别忘了在 3.4. AudioFlinger 回放录制线程 小节中提到：系统启动时，就已经打开 primary_out、low_latency、deep_buffer 这三种输出流设备，并创建对应的 PlaybackThread 了；通过 Binder 机制调用 AudioFlinger::createTrack()（注意 step2 中 AudioTrack 已经拿到一个 audio_io_handle_t 了，此时把这个 audio_io_handle_t 传入给 createTrack()）：根据传入的 audio_io_handle_t 找到它对应的 PlaybackThread；PlaybackThread 新建一个音频流管理对象 Track；Track 构造时会分配一块匿名共享内存用于 AudioFlinger 与 AudioTrack 的数据交换缓冲区（FIFO）及其控制块（audio_track_cblk_t），并创建一个 AudioTrackServerProxy 对象（PlaybackThread 将使用它从 FIFO 上取得可读数据的位置）；最后新建一个 Track 的通讯代理 TrackHandle，并以 IAudioTrack 作为返回值给 AudioTrack（TrackHandle、BnAudioTrack、BpAudioTrack、IAudioTrack 的关系见上一个小节）；通过 IAudioTrack 接口，取得 AudioFlinger 中的 FIFO 控制块（audio_track_cblk_t），由此再计算得到 FIFO 的首地址；创建一个 AudioTrackClientProxy 对象（AudioTrack 将使用它从 FIFO 上取得可用空间的位置）；AudioTrack 由此建立了和 AudioFlinger 的全部联系工作： 通过 IAudioTrack 接口可以控制该音轨的状态，例如 start、stop、pause持续写入数据到 FIFO 上，实现音频连续播放通过 audio_io_handle_t，可以找到它对应的 PlaybackThread，从而查询该 PlaybackThread 的相关信息，如所设置的采样率、格式等等构造 1 个 AudioTrack 实例时，AudioFlinger 会有 1 个 PlaybackThread 实例、1 个 Track 实例、1 个 TrackHandle 实例、1 个 AudioTrackServerProxy 实例、1 块 FIFO 与之对应。 当同时构造 1 个 AudioTrack with AUDIO_OUTPUT_FLAG_PRIMARY、1 个 AudioTrack with AUDIO_OUTPUT_FLAG_FAST、3 个 AudioTrack with AUDIO_OUTPUT_FLAG_DEEP_BUFFER、1 个 AudioTrack with AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD、1 个 AudioTrack with AUDIO_OUTPUT_FLAG_DIRECT 时（事实上，Android 音频策略不允许出现这种情形的），AudioFlinger 拥有的 PlaybackThread、Track、TrackHandle 实例如下图所示： 最后附上相关代码的流程分析，我本意是不多贴代码的，但不上代码总觉得缺点什么，这里我尽量把代码精简，提取主干，忽略细节。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140[-&gt;\\frameworks\\av\\media\\libmedia\\AudioTrack.cpp]AudioTrack::AudioTrack( audio_stream_type_t streamType, // 音频流类型：如 Music、Voice-Call、DTMF、Alarm 等等 uint32_t sampleRate, // 采样率：如 16KHz、44.1KHz、48KHz 等等 audio_format_t format, // 音频格式：如 PCM、MP3、AAC 等等 audio_channel_mask_t channelMask, // 声道数：如 Mono（单声道）、Stereo（双声道） const sp&lt;IMemory&gt;&amp; sharedBuffer, // 共享内存缓冲区：数据模式是 MODE_STATIC 时使用，数据模式是 MODE_STREAM 时为空 audio_output_flags_t flags, // 输出标识位，详见 AUDIO_OUTPUT_FLAG 描述 callback_t cbf, // 回调函数 void* user, // 回调函数的参数 uint32_t notificationFrames, int sessionId, transfer_type transferType, // 数据传输类型 const audio_offload_info_t *offloadInfo, int uid, pid_t pid, const audio_attributes_t* pAttributes, bool doNotReconnect) : mStatus(NO_INIT), mIsTimed(false), mPreviousPriority(ANDROID_PRIORITY_NORMAL), mPreviousSchedulingGroup(SP_DEFAULT), mPausedPosition(0), mSelectedDeviceId(AUDIO_PORT_HANDLE_NONE)&#123; mStatus = set(streamType, sampleRate, format, channelMask, 0 /*frameCount*/, flags, cbf, user, notificationFrames, sharedBuffer, false /*threadCanCallJava*/, sessionId, transferType, offloadInfo, uid, pid, pAttributes, doNotReconnect);&#125;status_t AudioTrack::set( audio_stream_type_t streamType, uint32_t sampleRate, audio_format_t format, audio_channel_mask_t channelMask, size_t frameCount, audio_output_flags_t flags, callback_t cbf, void* user, uint32_t notificationFrames, const sp&lt;IMemory&gt;&amp; sharedBuffer, bool threadCanCallJava, int sessionId, transfer_type transferType, const audio_offload_info_t *offloadInfo, int uid, pid_t pid, const audio_attributes_t* pAttributes, bool doNotReconnect)&#123; // 参数格式合法性检查、音轨音量初始化 // 如果 cbf 非空，那么创建 AudioTrackThread 线程处理 audioCallback 回调函数 if (cbf != NULL) &#123; mAudioTrackThread = new AudioTrackThread(*this, threadCanCallJava); mAudioTrackThread-&gt;run(\"AudioTrack\", ANDROID_PRIORITY_AUDIO, 0 /*stack*/); // thread begins in paused state, and will not reference us until start() &#125; // create the IAudioTrack status_t status = createTrack_l(); //......&#125;status_t AudioTrack::createTrack_l()&#123; // 获取 IAudioFlinger，通过 binder 请求 AudioFlinger 服务 const sp&lt;IAudioFlinger&gt;&amp; audioFlinger = AudioSystem::get_audio_flinger(); if (audioFlinger == 0) &#123; ALOGE(\"Could not get audioflinger\"); return NO_INIT; &#125; //...... // AudioSystem::getOutputForAttr() 经过一系列的调用，进入 AudioPolicyManager::getOutputForDevice() // 如果输出标识置了 AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD 或 AUDIO_OUTPUT_FLAG_DIRECT， // 那么最终调用 AudioFlinger::openOutput() 打开输出标识对应的输出流设备并创建相关的 // PlaybackThread，保存该 PlaybackThread 对应的 audio_io_handle_t 给 AudioTrack； // 如果输出标识是其他类型，那么根据策略选择一个输出流设备和 PlaybackThread，并保存该 // PlaybackThread 对应的 audio_io_handle_t 给 AudioTrack audio_io_handle_t output; status = AudioSystem::getOutputForAttr(attr, &amp;output, (audio_session_t)mSessionId, &amp;streamType, mClientUid, mSampleRate, mFormat, mChannelMask, mFlags, mSelectedDeviceId, mOffloadInfo); //...... // 向 AudioFlinger 发出 createTrack 请求 sp&lt;IAudioTrack&gt; track = audioFlinger-&gt;createTrack(streamType, mSampleRate, mFormat, mChannelMask, &amp;temp, &amp;trackFlags, mSharedBuffer, output, tid, &amp;mSessionId, mClientUid, &amp;status); //...... // AudioFlinger 创建 Track 对象时会分配一个 FIFO，这里获取 FIFO 的控制块 sp&lt;IMemory&gt; iMem = track-&gt;getCblk(); if (iMem == 0) &#123; ALOGE(\"Could not get control block\"); return NO_INIT; &#125; // 匿名共享内存首地址 void *iMemPointer = iMem-&gt;pointer(); if (iMemPointer == NULL) &#123; ALOGE(\"Could not get control block pointer\"); return NO_INIT; &#125; mAudioTrack = track; // 保存 AudioFlinger::PlaybackThread::Track 的代理对象 IAudioTrack mCblkMemory = iMem; // 保存匿名共享内存首地址 // 控制块位于 AudioFlinger 分配的匿名共享内存的首部 audio_track_cblk_t* cblk = static_cast&lt;audio_track_cblk_t*&gt;(iMemPointer); mCblk = cblk; mOutput = output; // 保存返回的 audio_io_handle_t，用它可以找到对应的 PlaybackThread //...... // update proxy if (mSharedBuffer == 0) &#123; // 当 mSharedBuffer 为空，意味着音轨数据模式为 MODE_STREAM，那么创建 AudioTrackClientProxy 对象 mStaticProxy.clear(); mProxy = new AudioTrackClientProxy(cblk, buffers, frameCount, mFrameSize); &#125; else &#123; // 当 mSharedBuffer 非空，意味着音轨数据模式为 MODE_STATIC，那么创建 StaticAudioTrackClientProxy 对象 mStaticProxy = new StaticAudioTrackClientProxy(cblk, buffers, frameCount, mFrameSize); mProxy = mStaticProxy; &#125; //......&#125; AudioFlinger::createTrack()，顾名思义，创建一个 Track 对象，将用于音频流的控制： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]sp&lt;IAudioTrack&gt; AudioFlinger::createTrack( audio_stream_type_t streamType, uint32_t sampleRate, audio_format_t format, audio_channel_mask_t channelMask, size_t *frameCount, IAudioFlinger::track_flags_t *flags, const sp&lt;IMemory&gt;&amp; sharedBuffer, audio_io_handle_t output, pid_t tid, int *sessionId, int clientUid, status_t *status)&#123; sp&lt;PlaybackThread::Track&gt; track; sp&lt;TrackHandle&gt; trackHandle; sp&lt;Client&gt; client; status_t lStatus; int lSessionId; //...... &#123; Mutex::Autolock _l(mLock); // 根据传入来的 audio_io_handle_t，找到对应的 PlaybackThread PlaybackThread *thread = checkPlaybackThread_l(output); if (thread == NULL) &#123; ALOGE(&quot;no playback thread found for output handle %d&quot;, output); lStatus = BAD_VALUE; goto Exit; &#125; //...... // 在 PlaybackThread 上创建一个音频流管理对象 Track track = thread-&gt;createTrack_l(client, streamType, sampleRate, format, channelMask, frameCount, sharedBuffer, lSessionId, flags, tid, clientUid, &amp;lStatus); //...... setAudioHwSyncForSession_l(thread, (audio_session_t)lSessionId); &#125; //...... // 创建 Track 的通讯代理 TrackHandle 并返回它 trackHandle = new TrackHandle(track);Exit: *status = lStatus; return trackHandle;&#125;sp&lt;AudioFlinger::PlaybackThread::Track&gt; AudioFlinger::PlaybackThread::createTrack_l( const sp&lt;AudioFlinger::Client&gt;&amp; client, audio_stream_type_t streamType, uint32_t sampleRate, audio_format_t format, audio_channel_mask_t channelMask, size_t *pFrameCount, const sp&lt;IMemory&gt;&amp; sharedBuffer, int sessionId, IAudioFlinger::track_flags_t *flags, pid_t tid, int uid, status_t *status)&#123; size_t frameCount = *pFrameCount; sp&lt;Track&gt; track; status_t lStatus; bool isTimed = (*flags &amp; IAudioFlinger::TRACK_TIMED) != 0; // ...... &#123; // scope for mLock Mutex::Autolock _l(mLock); // ...... if (!isTimed) &#123; // 创建 Track，等会再看看 Track 构造函数干些啥 track = new Track(this, client, streamType, sampleRate, format, channelMask, frameCount, NULL, sharedBuffer, sessionId, uid, *flags, TrackBase::TYPE_DEFAULT); &#125; else &#123; // 创建 TimedTrack，带时间戳的 Track？这里不深究 track = TimedTrack::create(this, client, streamType, sampleRate, format, channelMask, frameCount, sharedBuffer, sessionId, uid); &#125; // ...... // 把创建的 Track 添加到 mTracks 向量中，方便 PlaybackThread 统一管理 mTracks.add(track); // ...... &#125; lStatus = NO_ERROR;Exit: *status = lStatus; return track;&#125;// ----------------------------------------------------------------------------// 如下是 TrackHandle 的相关代码，可以看到，TrackHandle 其实就是一个壳子，是 Track 的包装类// 所有 TrackHandle 接口都是调向 Track 的// Google 为什么要搞这么一则？Track 是 PlaybackThread 内部使用的，不适宜对外暴露，但应用进程// 又确实需要控制音频流的状态（start、stop、pause），所以就采取这么一种方式实现AudioFlinger::TrackHandle::TrackHandle(const sp&lt;AudioFlinger::PlaybackThread::Track&gt;&amp; track) : BnAudioTrack(), mTrack(track)&#123;&#125;AudioFlinger::TrackHandle::~TrackHandle() &#123; // just stop the track on deletion, associated resources // will be freed from the main thread once all pending buffers have // been played. Unless it&apos;s not in the active track list, in which // case we free everything now... mTrack-&gt;destroy();&#125;sp&lt;IMemory&gt; AudioFlinger::TrackHandle::getCblk() const &#123; return mTrack-&gt;getCblk();&#125;status_t AudioFlinger::TrackHandle::start() &#123; return mTrack-&gt;start();&#125;void AudioFlinger::TrackHandle::stop() &#123; mTrack-&gt;stop();&#125;void AudioFlinger::TrackHandle::flush() &#123; mTrack-&gt;flush();&#125;void AudioFlinger::TrackHandle::pause() &#123; mTrack-&gt;pause();&#125;// ---------------------------------------------------------------------------- 最后，我们看看 Track 的构造过程，主要分析数据 FIFO 及它的控制块是如何分配的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]AudioFlinger::PlaybackThread::Track::Track( PlaybackThread *thread, const sp&lt;Client&gt;&amp; client, audio_stream_type_t streamType, uint32_t sampleRate, audio_format_t format, audio_channel_mask_t channelMask, size_t frameCount, void *buffer, const sp&lt;IMemory&gt;&amp; sharedBuffer, int sessionId, int uid, IAudioFlinger::track_flags_t flags, track_type type) : TrackBase(thread, client, sampleRate, format, channelMask, frameCount, (sharedBuffer != 0) ? sharedBuffer-&gt;pointer() : buffer, sessionId, uid, flags, true /*isOut*/, (type == TYPE_PATCH) ? ( buffer == NULL ? ALLOC_LOCAL : ALLOC_NONE) : ALLOC_CBLK, type), mFillingUpStatus(FS_INVALID), // mRetryCount initialized later when needed mSharedBuffer(sharedBuffer), mStreamType(streamType), mName(-1), // see note below mMainBuffer(thread-&gt;mixBuffer()), mAuxBuffer(NULL), mAuxEffectId(0), mHasVolumeController(false), mPresentationCompleteFrames(0), mFastIndex(-1), mCachedVolume(1.0), mIsInvalid(false), mAudioTrackServerProxy(NULL), mResumeToStopping(false), mFlushHwPending(false)&#123; // client == 0 implies sharedBuffer == 0 ALOG_ASSERT(!(client == 0 &amp;&amp; sharedBuffer != 0)); ALOGV_IF(sharedBuffer != 0, \"sharedBuffer: %p, size: %d\", sharedBuffer-&gt;pointer(), sharedBuffer-&gt;size()); // 检查 FIFO 控制块（audio_track_cblk_t）是否分配好了，上面代码并未分配 audio_track_cblk_t // 因此只可能是构造 TrackBase 时分配的，等下再看看 TrackBase 的构造函数 if (mCblk == NULL) &#123; return; &#125; if (sharedBuffer == 0) &#123; // 数据传输模式为 MODE_STREAM 模式，创建一个 AudioTrackServerProxy 对象 // PlaybackThread 将持续使用它从 FIFO 上取得可读数据的位置 mAudioTrackServerProxy = new AudioTrackServerProxy(mCblk, mBuffer, frameCount, mFrameSize, !isExternalTrack(), sampleRate); &#125; else &#123; // 数据传输模式为 MODE_STATIC 模式，创建一个 StaticAudioTrackServerProxy 对象 mAudioTrackServerProxy = new StaticAudioTrackServerProxy(mCblk, mBuffer, frameCount, mFrameSize); &#125; mServerProxy = mAudioTrackServerProxy; // 为 Track 分配一个名称，AudioMixer 会根据 TrackName 找到对应的 Track mName = thread-&gt;getTrackName_l(channelMask, format, sessionId); if (mName &lt; 0) &#123; ALOGE(\"no more track names available\"); return; &#125; // ......&#125;AudioFlinger::ThreadBase::TrackBase::TrackBase( ThreadBase *thread, const sp&lt;Client&gt;&amp; client, uint32_t sampleRate, audio_format_t format, audio_channel_mask_t channelMask, size_t frameCount, void *buffer, int sessionId, int clientUid, IAudioFlinger::track_flags_t flags, bool isOut, alloc_type alloc, track_type type) : RefBase(), mThread(thread), mClient(client), mCblk(NULL), // mBuffer mState(IDLE), mSampleRate(sampleRate), mFormat(format), mChannelMask(channelMask), mChannelCount(isOut ? audio_channel_count_from_out_mask(channelMask) : audio_channel_count_from_in_mask(channelMask)), mFrameSize(audio_is_linear_pcm(format) ? mChannelCount * audio_bytes_per_sample(format) : sizeof(int8_t)), mFrameCount(frameCount), mSessionId(sessionId), mFlags(flags), mIsOut(isOut), mServerProxy(NULL), mId(android_atomic_inc(&amp;nextTrackId)), mTerminated(false), mType(type), mThreadIoHandle(thread-&gt;id())&#123; // ...... // ALOGD(\"Creating track with %d buffers @ %d bytes\", bufferCount, bufferSize); size_t size = sizeof(audio_track_cblk_t); size_t bufferSize = (buffer == NULL ? roundup(frameCount) : frameCount) * mFrameSize; if (buffer == NULL &amp;&amp; alloc == ALLOC_CBLK) &#123; // 这个 size 将是分配的匿名共享内存的大小 // 等于控制块的大小（sizeof(audio_track_cblk_t)加上数据 FIFO的大小（bufferSize） // 待会看到这块内存的结构，就明白这样分配的意义了 size += bufferSize; &#125; if (client != 0) &#123; // 分配一块匿名共享内存 mCblkMemory = client-&gt;heap()-&gt;allocate(size); if (mCblkMemory == 0 || (mCblk = static_cast&lt;audio_track_cblk_t *&gt;(mCblkMemory-&gt;pointer())) == NULL) &#123; ALOGE(\"not enough memory for AudioTrack size=%u\", size); client-&gt;heap()-&gt;dump(\"AudioTrack\"); mCblkMemory.clear(); return; &#125; &#125; else &#123; // this syntax avoids calling the audio_track_cblk_t constructor twice mCblk = (audio_track_cblk_t *) new uint8_t[size]; // assume mCblk != NULL &#125; // construct the shared structure in-place. if (mCblk != NULL) &#123; // 这是 C++ 的 placement new（定位创建对象）语法：new(@BUFFER) @CLASS(); // 可以在特定内存位置上构造一个对象 // 这里，在匿名共享内存首地址上构造了一个 audio_track_cblk_t 对象 // 这样 AudioTrack 与 AudioFlinger 都能访问这个 audio_track_cblk_t 对象了 new(mCblk) audio_track_cblk_t(); // 如下分配数据 FIFO，将用于 AudioTrack 与 AudioFlinger 的数据交换 switch (alloc) &#123; // ...... case ALLOC_CBLK: // clear all buffers if (buffer == NULL) &#123; // 数据传输模式为 MODE_STREAM/TRANSFER_SYNC 时，数据 FIFO 的分配 // 数据 FIFO 的首地址紧靠控制块（audio_track_cblk_t）之后 // | | // | -------------------&gt; mCblkMemory &lt;--------------------- | // | | // +--------------------+------------------------------------+ // | audio_track_cblk_t | Buffer | // +--------------------+------------------------------------+ // ^ ^ // | | // mCblk mBuffer mBuffer = (char*)mCblk + sizeof(audio_track_cblk_t); memset(mBuffer, 0, bufferSize); &#125; else &#123; // 数据传输模式为 MODE_STATIC/TRANSFER_SHARED 时，直接指向 sharedBuffer // sharedBuffer 是应用进程分配的匿名共享内存，应用进程已经一次性把数据 // 写到 sharedBuffer 来了，AudioFlinger 可以直接从这里读取 // +--------------------+ +-----------------------------------+ // | audio_track_cblk_t | | sharedBuffer | // +--------------------+ +-----------------------------------+ // ^ ^ // | | // mCblk mBuffer mBuffer = buffer; &#125; break; // ...... &#125; // ...... &#125;&#125; 3.3. AudioTrack 数据写入AudioTrack 实例构造后，应用程序接着可以写入音频数据了。如之前所描述：AudioTrack 与 AudioFlinger 是 生产者-消费者 的关系： ☯ AudioTrack：AudioTrack 在 FIFO 中找到一块可用空间，把用户传入的音频数据写入到这块可用空间上，然后更新写位置（对于 AudioFinger 来说，意味 FIFO 上有更多的可读数据了）；如果用户传入的数据量比可用空间要大，那么要把用户传入的数据拆分多次写入到 FIFO 中（AudioTrack 和 AudioFlinger 是不同的进程，AudioFlinger 同时也在不停地读取数据，所以 FIFO 可用空间是在不停变化的）☯ AudioFlinger：AudioFlinger 在 FIFO 中找到一块可读数据块，把可读数据拷贝到目的缓冲区上，然后更新读位置（对于 AudioTrack 来说，意味着 FIFO 上有更多的可用空间了）；如果FIFO 上可读数据量比预期的要小，那么要进行多次的读取，才能积累到预期的数据量（AudioTrack 和 AudioFlinger 是不同的进程，AudioTrack 同时也在不停地写入数据，所以 FIFO 可读的数据量是在不停变化的）上面的过程中，如果 AudioTrack 总能及时生产数据，并且 AudioFlinger 总能及时消耗掉这些数据，那么整个过程将是非常和谐的；但系统可能会发生异常，出现如下的状态： ☯ Block：AudioFlinger 长时间不读取 FIFO 上的可读数据，使得 AudioTrack 长时间获取不到可用空间，无法写入数据；这种情况的根本原因大多是底层驱动发生阻塞异常，导致 AudioFlinger 无法继续写数据到硬件设备中，AudioFlinger 本身并没有错☯ Underrun：AudioTrack 写入数据的速度跟不上 AudioFlinger 读取数据的速度，使得 AudioFlinger 不能及时获取到预期的数据量，反映到现实的后果就是声音断续；这种情况的根本原因大多是应用程序不能及时写入数据或者缓冲区分配过小，AudioTrack 本身并没有错；AudioFlinger 针对这点做了容错处理：当发现 underrun 时，先陷入短时间的睡眠，不急着读取数据，让应用程序准备更多的数据（如果某一天做应用的哥们意识到自己的错误原来由底层的兄弟默默埋单了，会不会感动得哭了^_^） 3.3. 1. AudioTrack 写数据流程我们看一下 AudioTrack 写数据的代码，流程很简单：obtainBuffer() 在 FIFO 中找到一块可用区间，memcpy() 把用户传入的音频数据拷贝到这个可用区间上，releaseBuffer() 更新写位置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[-&gt;\\frameworks\\av\\media\\libmedia\\AudioTrack.cpp]ssize_t AudioTrack::write(const void* buffer, size_t userSize, bool blocking)&#123; if (mTransfer != TRANSFER_SYNC) &#123; return INVALID_OPERATION; &#125; if (isDirect()) &#123; AutoMutex lock(mLock); int32_t flags = android_atomic_and( ~(CBLK_UNDERRUN | CBLK_LOOP_CYCLE | CBLK_LOOP_FINAL | CBLK_BUFFER_END), &amp;mCblk-&gt;mFlags); if (flags &amp; CBLK_INVALID) &#123; return DEAD_OBJECT; &#125; &#125; if (ssize_t(userSize) &lt; 0 || (buffer == NULL &amp;&amp; userSize != 0)) &#123; // Sanity-check: user is most-likely passing an error code, and it would // make the return value ambiguous (actualSize vs error). ALOGE(&quot;AudioTrack::write(buffer=%p, size=%zu (%zd)&quot;, buffer, userSize, userSize); return BAD_VALUE; &#125; size_t written = 0; Buffer audioBuffer; while (userSize &gt;= mFrameSize) &#123; // 单帧数据量 frameSize = channelCount * bytesPerSample // 对于双声道，16位采样的音频数据来说，frameSize = 2 * 2 = 4(bytes) // 用户传入的数据帧数 frameCount = userSize / frameSize audioBuffer.frameCount = userSize / mFrameSize; // obtainBuffer() 从 FIFO 上得到一块可用区间 status_t err = obtainBuffer(&amp;audioBuffer, blocking ? &amp;ClientProxy::kForever : &amp;ClientProxy::kNonBlocking); if (err &lt; 0) &#123; if (written &gt; 0) &#123; break; &#125; if (err == TIMED_OUT || err == -EINTR) &#123; err = WOULD_BLOCK; &#125; return ssize_t(err); &#125; // toWrite 是 FIFO 可用区间的大小，可能比 userSize（用户传入数据的大小）要小 // 因此用户传入的数据可能要拆分多次拷贝到 FIFO 上 // 注意：AudioTrack 和 AudioFlinger 是不同的进程，AudioFlinger 同时也在不停地 // 消耗数据，所以 FIFO 可用区间是在不停变化的 size_t toWrite = audioBuffer.size; memcpy(audioBuffer.i8, buffer, toWrite); // 把用户数据拷贝到 FIFO 可用区间 buffer = ((const char *) buffer) + toWrite; // 未拷贝数据的位置 userSize -= toWrite; // 未拷贝数据的大小 written += toWrite; // 已拷贝数据的大小 // releaseBuffer() 更新 FIFO 写位置 // 对于 AudioFinger 来说，意味 FIFO 上有更多的可读数据 releaseBuffer(&amp;audioBuffer); &#125; if (written &gt; 0) &#123; mFramesWritten += written / mFrameSize; &#125; return written;&#125; 3.3. 2. AudioFlinger 读数据流程AudioFlinger 消费数据的流程稍微复杂一点，3.4. AudioFlinger 回放录制线程 小节中描述了 AudioFlinger::PlaybackThread::threadLoop() 工作流程，这里不累述了，我们把焦点放在“如何从 FIFO 读取数据”节点上。 我们以 DirectOutputThread/OffloadThread 为例说明（MixerThread 读数据也是类似的过程，只不过是在 AudioMixer 中进行的）。 1234567891011121314151617181920212223242526272829303132333435[-&gt;\\frameworks\\av\\services\\audioflinger\\AudioFlinger.cpp]void AudioFlinger::DirectOutputThread::threadLoop_mix()&#123; // mFrameCount 是硬件设备（PCM 设备）处理单个数据块的帧数（周期大小） // 上层必须积累了足够多（mFrameCount）的数据，才写入到 PCM 设备 // 所以 mFrameCount 也就是 AudioFlinger 预期的数据量 size_t frameCount = mFrameCount; // mSinkBuffer 目的缓冲区，threadLoop_write() 会把 mSinkBuffer 上的数据写到 PCM 设备 int8_t *curBuf = (int8_t *)mSinkBuffer; // output audio to hardware // FIFO 上可读的数据量可能要比预期的要小，因此可能需要多次读取才能积累足够的数据量 // 注意：AudioTrack 和 AudioFlinger 是不同的进程，AudioTrack 同时也在不停地生产数据 // 所以 FIFO 可读的数据量是在不停变化的 while (frameCount) &#123; AudioBufferProvider::Buffer buffer; buffer.frameCount = frameCount; // getNextBuffer() 从 FIFO 上获取可读数据块 status_t status = mActiveTrack-&gt;getNextBuffer(&amp;buffer); if (status != NO_ERROR || buffer.raw == NULL) &#123; memset(curBuf, 0, frameCount * mFrameSize); break; &#125; // memcpy() 把 FIFO 可读数据拷贝到 mSinkBuffer 目的缓冲区 memcpy(curBuf, buffer.raw, buffer.frameCount * mFrameSize); frameCount -= buffer.frameCount; curBuf += buffer.frameCount * mFrameSize; // releaseBuffer() 更新 FIFO 读位置 // 对于 AudioTrack 来说，意味着 FIFO 上有更多的可用空间 mActiveTrack-&gt;releaseBuffer(&amp;buffer); &#125; mCurrentWriteLength = curBuf - (int8_t *)mSinkBuffer; mSleepTimeUs = 0; mStandbyTimeNs = systemTime() + mStandbyDelayNs; mActiveTrack.clear();&#125; 3.3. 3. 环形 FIFO 管理在上述过程中，不知大家有无意识到：整个过程中，最难的是如何协调生产者与消费者之间的步调。上文所说的 FIFO 是环形 FIFO，AudioTrack 写指针、AudioFlinger 读指针都是基于 FIFO 当前的读写位置来计算的。 ☯AudioTrack 与 AudioFlinger 不在同一个进程上，怎么保证读写指针的线程安全☯读写指针越过 FIFO 后，怎么处理☯AudioTrack 写数据完成后，需要同步状态给 AudioFlinger，让 AudioFlinger 知道当前有可读数据了，而 AudioFlinger 读数据完成后，也需要同步状态给 AudioTrack，让 AudioTrack 知道当前有可用空间了；这里采取什么同步机制我们回顾下创建 AudioTrack 对象时，FIFO 及其控制块的结构如下所示： 1234567891011121314MODE_STREAM 模式下的匿名共享内存结构： | | | -------------------&gt; mCblkMemory &lt;--------------------- | | | +--------------------+------------------------------------+ | audio_track_cblk_t | FIFO | +--------------------+------------------------------------+ ^ ^ | |mCblk mBuffermCblk = static_cast&lt;audio_track_cblk_t *&gt;(mCblkMemory-&gt;pointer());new(mCblk) audio_track_cblk_t();mBuffer = (char*)mCblk + sizeof(audio_track_cblk_t); ☯MODE_STATIC 模式下的匿名共享内存结构： 12345678910 +--------------------+ +-----------------------------------+ | audio_track_cblk_t | | FIFO (sharedBuffer) | +--------------------+ +-----------------------------------+ ^ ^ | |mCblk mBuffermCblk = (audio_track_cblk_t *) new uint8_t[size];new(mCblk) audio_track_cblk_t();mBuffer = sharedBuffer-&gt;pointer() FIFO 管理相关的类图： ☯AudioTrackClientProxy：MODE_STREAM 模式下，生产者 AudioTrack 使用它在 FIFO 中找到可用空间的位置☯AudioTrackServerProxy：MODE_STREAM 模式下，消费者 AudioFlinger::PlaybackThread 使用它在 FIFO 中找到可读数据的位置☯StaticAudioTrackClientProxy：MODE_STATIC 模式下，生产者 AudioTrack 使用它在 FIFO 中找到可用空间的位置☯StaticAudioTrackServerProxy：MODE_STATIC 模式下，消费者 AudioFlinger::PlaybackThread 使用它在 FIFO 中找到可读数据的位置☯AudioRecordClientProxy：消费者 AudioRecord 使用它在 FIFO 中找到可读数据的位置☯AudioTrackServerProxy：生产者 AudioFlinger::RecordThread 使用它在 FIFO 中找到可用空间的位置到这里，我决定结束本文了。环形 FIFO 管理是 Android 音频系统的精髓，一个小节并不足以描述其原理及实现细节；Android 环形 FIFO 的实现可说得上精妙绝伦，其他项目如果要用到环形 FIFO，不妨多借鉴它。 (四)、深入剖析MediaPlayer播放音频流程时序图： （五）、参考资料(特别感谢各位前辈的分析和图示)：Android音频模块启动流程分析Jhuster的专栏​ Android音频开发高通audio offload学习 | ThinkingDroidPhone的专栏 - CSDN博客alsa音频架构1-CSDN博客alsa音频架构2-ASoc - CSDN博客alsa音频架构3-Pcm - CSDN博客alsa音频架构4-声卡控制 - CSDN博客Linux ALSA 音频系统：逻辑设备篇 - CSDN博客Linux ALSA 音频系统：物理链路篇 - CSDN博客专栏：MultiMedia框架总结(基于6.0源码) - CSDN博客Android 音频系统：从 AudioTrack 到 AudioFlinger - CSDN博客AZURE - CSDN博客 - ALSA-Android AudioAZURE - CSDN博客 - ANDROID音频系统Audio驱动总结–ALSA | Winddoing’s Blogaudio HAL - 牧 天 - 博客园林学森的Android专栏 - CSDN博客深入剖析Android音频 - CSDN博客Yangwen123播放框架 - 标签 - Tocy - 博客园Android-7.0-Nuplayer概述 - CSDN博客Android-7.0-Nuplayer-启动流程 - CSDN博客Android Media Player 框架分析-Nuplayer（1） - CSDN博客Android Media Player 框架分析-AHandler AMessage ALooper - CSDN博客Android N Audio播放 start真面目- (六篇) CSDN博客深入理解Android音视频同步机制（五篇）NuPlayer的avsync逻辑 - CSDN博客wangyf的专栏 - CSDN博客-MT6737 Android N 平台 Audio系统学习Android 7.0 Audio: Mediaplayer - CSDN博客Android 7.0 Audio-相关类浅析- CSDN博客Android N Audio播放六：如何读取buffer - CSDN博客Fuchsia OS中的RPC机制-FIDL - CSDN博客高通Audio中ASOC的codec驱动 - yooooooo - 博客园","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Audio System（2）：Linux ALSA音频系统分析","slug":"Audio System（2）：Linux ALSA音频系统分析","date":"2018-05-14T16:00:00.000Z","updated":"2018-05-09T15:03:49.790Z","comments":true,"path":"2018/05/15/Audio System（2）：Linux ALSA音频系统分析/","link":"","permalink":"http://zhoujinjian.cc/2018/05/15/Audio System（2）：Linux ALSA音频系统分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】 【特别感谢 - 雲和山的彼端 - 音频系统分析】 Google Pixel、Pixel XL 内核代码（Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) 源码（主要源码路径）： User space audio code 源码： • /hardware/qcom/audio/hal/ – (Audio HAL 源码) • /external/tinyalsa/ – (tinymix, tinyplay, tinycap 源码) • /vendor/qcom/proprietary/mm-audio/ – (QTI OMX audio encoder and decoders 源码，未公开) • /frameworks/av/media/audioserver/ – (Audioserver 源码) • /frameworks/av/media/libstagefright/ – (Google Stagefright 多媒体框架源码) • /frameworks/av/services/audioflinger/ – (Audioflinger 相关源码) • /external/bluetooth/bluedroid/ – (A2DP audio HAL 相关源码)/ • /hardware/libhardware/modules/usbaudio/ – (USB HAL 源码)/ • /vendor/qcom/proprietary/wfd/mm/source/framework/src/ – (Wi-Fi Display (WFD)、 WFDMMSourceAudioSource.cpp，未公开) • /system/core/include/system/ – (audio.h)/ Kernel space audio code 源码： • /kernel/sound/soc/msm/ – msm8996.c machine driver 源码 • /kernel/sound/soc/msm/qdsp6v2 – platform drivers, front end (FE), and back-end (BE) DAI driver, Hexagon DSP drivers for AFE, ADM, and ASM, voice driver 相关源码 • kernel/sound/soc/soc-.c – All the SoC-.c ALSA SoCs framework 源码 • kernel/drivers/slimbus/ – SLIMbus driver 源码 • kernel/arch/arm/mach-msm/ – 包含比如 acpuclock-8996.c, board-8996-gpiomux.c, board-8996.c, and clock-8996.c related to the GPIO, clock, and board-specific information on the MSM8996 相关源码 • /kernel/arch/arm/mach-msm/qdsp6v2/ – Contains the drivers for DSP-based encoders and decoders, code for the aDSP loader, APR driver, Ion memory driver, and other utility files • /kernel/arch/arm/boot/dts – Contains MSM8996-.its and MSM8996-.Dtsi files that contain MSM8996-specific information; audio-related customization is available in files such as MSM8996.dtsi, msm8996-mtp.dtsi, and msm8996-cdp.dtsi • /kernel/sound/soc/codecs/ – Contains the source code for the codec driver for WCD9335; codec driver-related source files are wcd9335.c, wcd9xxx-mbhc.c, wcd9xxx-resmgr.c, wcd9xxx-common.c, and so on. • /kernel/drivers/mfd/ – Contains the source code for the codec driver; wcd9xxx-core.c, wcd9xxx-slimslave.c, and wcd9xxx-irq.c are the codec driverrelated files （一） Overview硬件平台及软件版本：☁ Kernel - 3.18☁ SoC - Qualcomm snapdragon☁ CODEC - WCD9335☁ Machine - msm8996☁ Userspace - tinyalsa Linux ALSA 音频系统架构大致如下： • Native ALSA Application：tinyplay/tinycap/tinymix，这些用户程序直接调用 alsa 用户库接口来实现放音、录音、控制• ALSA Library API：alsa 用户库接口，常见有 tinyalsa、alsa-lib• ALSA CORE：alsa 核心层，向上提供逻辑设备（PCM/CTL/MIDI/TIMER/…）系统调用，向下驱动硬件设备（Machine/I2S/DMA/CODEC）• ASoC CORE：asoc 是建立在标准 alsa core 基础上，为了更好支持嵌入式系统和应用于移动设备的音频 codec 的一套软件体系• Hardware Driver：音频硬件设备驱动，由三大部分组成，分别是 Machine、Platform、Codec Platform：指某款 SoC 平台的音频模块，如 exynos、omap、qcom 等等。Platform 又可细分两部分： • cpu dai：在嵌入式系统里面通常指 SoC 的 I2S、PCM 总线控制器，负责把音频数据从 I2S tx FIFO 搬运到 CODEC（这是回放的情形，录制则方向相反）。cpu_dai 通过 snd_soc_register_dai() 来注册。注：DAI 是 Digital Audio Interface 的简称，分为 cpu_dai 和 codec_dai，这两者通过 I2S/PCM 总线连接；AIF 是 Audio Interface 的简称，嵌入式系统中一般是 I2S 和 PCM 接口。 • pcm dma：负责把 dma buffer 中的音频数据搬运到 I2S tx FIFO。值得留意的是：某些情形下是不需要 dma 操作的，比如 Modem 和 CODEC 直连，因为 Modem 本身已经把数据送到 FIFO 了，这时只需启动 codec_dai 接收数据即可；该情形下，Machine 驱动 dai_link 中需要设定 .platform_name = “msm-pcm-xxx”。 Codec：对于回放来说，userspace 送过来的音频数据是经过采样量化的数字信号，在 codec 经过 DAC 转换成模拟信号然后输出到外放或耳机，这样我们就可以听到声音了。Codec 字面意思是编解码器，但芯片里面的功能部件很多，常见的有 AIF、DAC、ADC、Mixer、PGA、Line-in、Line-out，有些高端的 codec 芯片还有 EQ、DSP、SRC、DRC、AGC、Echo-Canceller、Noise-Suppression 等部件。 Machine：指某款机器，通过配置 dai_link 把 cpu_dai、codec_dai、modem_dai 各个音频接口给链结成一条条音频链路，然后注册 snd_soc_card。和上面两个不一样，Platform 和 CODEC 驱动一般是可以重用的，而 Machine 有它特定的硬件特性，几乎是不可重用的。所谓的硬件特性指：SoC Platform 与 Codec 的差异；DAIs 之间的链结方式；通过某个 GPIO 打开 Amplifier；通过某个 GPIO 检测耳机插拔；使用某个时钟如 MCLK/External-OSC 作为 I2S、CODEC 的时钟源等等。 从上面的描述来看，对于回放的情形，PCM 数据流向大致是： 12345 copy_from_user DMA I2S DAC ^ ^ ^ ^+---------+ | +----------+ | +-----------+ | +-----+ | +------+|userspace+--------&gt;DMA Buffer+-------&gt;I2S TX FIFO+-------&gt;CODEC+-------&gt;SPK/HP|+---------+ +----------+ +-----------+ +-----+ +------+ 几个音频物理链路的概念： dai_link：machine 驱动中定义的音频数据链路，它指定链路用到的 codec、codec_dai、cpu_dai、platform。比如对于 WCD9335 平台的 media 链路：.codec_dai_name = “snd-soc-dummy-dai”, .codec_name = “snd-soc-dummy”, .cpu_dai_name = “MultiMediaX”, .platform_name = “msm-pcm-dsp.0”，这四者就构成了一条音频数据链路用于多媒体声音的回放和录制。一个系统可能有多个音频数据链路，比如 media 和 voice，因此可以定义多个 dai_link 。代码如下： 123456789101112131415161718192021222324[-&gt;/sound/soc/msm/msm8996.c]/* Digital audio interface glue - connects codec &lt;---&gt; CPU */static struct snd_soc_dai_link msm8996_common_dai_links[] = &#123; /* FrontEnd DAI Links */ &#123; .name = \"MSM8996 Media1\", .stream_name = \"MultiMedia1\", .cpu_dai_name = \"MultiMedia1\", .platform_name = \"msm-pcm-dsp.0\", .dynamic = 1, .async_ops = ASYNC_DPCM_SND_SOC_PREPARE, .dpcm_playback = 1, .dpcm_capture = 1, .trigger = &#123;SND_SOC_DPCM_TRIGGER_POST, SND_SOC_DPCM_TRIGGER_POST&#125;, .codec_dai_name = \"snd-soc-dummy-dai\", .codec_name = \"snd-soc-dummy\", .ignore_suspend = 1, /* this dainlink has playback support */ .ignore_pmdown_time = 1, .be_id = MSM_FRONTEND_DAI_MULTIMEDIA1 &#125;, ......&#125; 高通平台因DSP而存在特殊性，如上图，Frontend 链接 “Platform”，经由 “Platform”-&gt;Backend链接到Codec。Front-end DAI： 1234567891011121314151617181920212223242526272829303132333435[-&gt;/sound/soc/msm/msm-dai-fe.c]static struct snd_soc_dai_driver msm_fe_dais[] = &#123; &#123; .playback = &#123; .stream_name = \"MultiMedia1 Playback\", .aif_name = \"MM_DL1\", .rates = (SNDRV_PCM_RATE_8000_192000| SNDRV_PCM_RATE_KNOT), .formats = (SNDRV_PCM_FMTBIT_S16_LE | SNDRV_PCM_FMTBIT_S24_LE | SNDRV_PCM_FMTBIT_S24_3LE), .channels_min = 1, .channels_max = 8, .rate_min = 8000, .rate_max = 192000, &#125;, .capture = &#123; .stream_name = \"MultiMedia1 Capture\", .aif_name = \"MM_UL1\", .rates = (SNDRV_PCM_RATE_8000_192000| SNDRV_PCM_RATE_KNOT), .formats = (SNDRV_PCM_FMTBIT_S16_LE | SNDRV_PCM_FMTBIT_S24_LE| SNDRV_PCM_FMTBIT_S24_3LE), .channels_min = 1, .channels_max = 4, .rate_min = 8000, .rate_max = 48000, &#125;, .ops = &amp;msm_fe_Multimedia_dai_ops, .name = \"MultiMedia1\", .probe = fe_dai_probe, &#125;, ......&#125; Back-end DAI： 1234567891011121314151617181920212223242526272829303132333435363738[-&gt;sound/soc/msm/msm8996.c]static struct snd_soc_dai_link msm8996_tasha_be_dai_links[] = &#123; /* Backend DAI Links */ &#123; .name = LPASS_BE_SLIMBUS_0_RX, .stream_name = \"Slimbus Playback\", .cpu_dai_name = \"msm-dai-q6-dev.16384\", .platform_name = \"msm-pcm-routing\", .codec_name = \"tasha_codec\", .codec_dai_name = \"tasha_mix_rx1\", .no_pcm = 1, .dpcm_playback = 1, .be_id = MSM_BACKEND_DAI_SLIMBUS_0_RX, .init = &amp;msm_audrx_init, .be_hw_params_fixup = msm_slim_0_rx_be_hw_params_fixup, /* this dainlink has playback support */ .ignore_pmdown_time = 1, .ignore_suspend = 1, .ops = &amp;msm8996_be_ops, &#125;, ......&#125;static struct snd_soc_dai_link msm8996_tasha_fe_dai_links[] = &#123; &#123; .name = LPASS_BE_SLIMBUS_4_TX, .stream_name = \"Slimbus4 Capture\", .cpu_dai_name = \"msm-dai-q6-dev.16393\", .platform_name = \"msm-pcm-hostless\", .codec_name = \"tasha_codec\", .codec_dai_name = \"tasha_vifeedback\", .be_id = MSM_BACKEND_DAI_SLIMBUS_4_TX, .be_hw_params_fixup = msm_slim_4_tx_be_hw_params_fixup, .ops = &amp;msm8996_be_ops, .no_host_mode = SND_SOC_DAI_LINK_NO_HOST, .ignore_suspend = 1, &#125;, ......&#125; hw constraints：指平台本身的硬件限制，如所能支持的通道数/采样率/数据格式、DMA 支持的数据周期大小（period size）、周期次数（period count）等，通过 snd_pcm_hardware 结构体描述：1234567891011121314151617181920212223[-&gt;sound/soc/msm/qdsp6v2/msm-pcm-q6-v2.c]static struct snd_pcm_hardware msm_pcm_hardware_capture = &#123; .info = (SNDRV_PCM_INFO_MMAP | SNDRV_PCM_INFO_BLOCK_TRANSFER | SNDRV_PCM_INFO_MMAP_VALID | SNDRV_PCM_INFO_INTERLEAVED | SNDRV_PCM_INFO_PAUSE | SNDRV_PCM_INFO_RESUME), .formats = (SNDRV_PCM_FMTBIT_S16_LE | SNDRV_PCM_FMTBIT_S24_LE | SNDRV_PCM_FMTBIT_S24_3LE), .rates = SNDRV_PCM_RATE_8000_48000, .rate_min = 8000, .rate_max = 48000, .channels_min = 1, .channels_max = 4, .buffer_bytes_max = CAPTURE_MAX_NUM_PERIODS * CAPTURE_MAX_PERIOD_SIZE, .period_bytes_min = CAPTURE_MIN_PERIOD_SIZE, .period_bytes_max = CAPTURE_MAX_PERIOD_SIZE, .periods_min = CAPTURE_MIN_NUM_PERIODS, .periods_max = CAPTURE_MAX_NUM_PERIODS, .fifo_size = 0,&#125;; hw params：用户层设置的硬件参数，如 channels、sample rate、pcm format、period size、period count；这些参数受 hw constraints 约束。 sw params：用户层设置的软件参数，如 start threshold、stop threshold、silence threshold。 （二）ASoC CoreASoC：ALSA System on Chip，是建立在标准 ALSA 驱动之上，为了更好支持嵌入式系统和应用于移动设备的音频 codec 的一套软件体系，它依赖于标准 ALSA 驱动框架。内核文档 Documentation/alsa/soc/overview.txt 中详细介绍了 ASoC 的设计初衷，这里不一一引用，简单陈述如下： • 独立的 codec 驱动，标准的 ALSA 驱动框架里面 codec 驱动往往与 SoC/CPU 耦合过于紧密，不利于在多样化的平台/机器上移植复用• 方便 codec 与 SoC 通过 PCM/I2S 总线建立链接• 动态音频电源管理 DAPM，使得 codec 任何时候都工作在最低功耗状态，同时负责音频路由的创建• POPs 和 click 音抑制弱化处理，在 ASoC 中通过正确的音频部件上下电次序来实现• Machine 驱动的特定控制，比如耳机、麦克风的插拔检测，外放功放的开关在概述中已经介绍了 ASoC 硬件设备驱动的三大构成：Codec、Platform 和 Machine，下面列举各驱动的功能构成： ASoC Codec Driver： • Codec DAI 和 PCM 的配置信息• Codec 的控制接口，如 I2C/SPI• Mixer 和其他音频控件• Codec 的音频接口函数，见 snd_soc_dai_ops 结构体定义• DAPM 描述信息• DAPM 事件处理句柄• DAC 数字静音控制 ASoC Platform Driver： 包括 dma 和 cpu_dai 两部分： • dma 驱动实现音频 dma 操作，具体见 snd_pcm_ops 结构体定义• cpu_dai 驱动实现音频数字接口控制器的描述和配置• ASoC Machine Driver： 作为链结 Platform 和 Codec 的载体，它必须配置 dai_link 为音频数据链路指定 Platform 和 Codec处理机器特有的音频控件和音频事件，例如回放时打开外放功放硬件设备驱动相关结构体： • snd_soc_codec_driver：音频编解码芯片描述及操作函数，如控件/微件/音频路由的描述信息、时钟配置、IO 控制等• snd_soc_dai_driver：音频数据接口描述及操作函数，根据 codec 端和 soc 端，分为 codec_dai 和 cpu_dai• snd_soc_platform_driver：音频 dma 设备描述及操作函数• snd_soc_dai_link：音频链路描述及板级操作函数 （三）Codec Driver基本是以内核文档 Documentation/sound/alsa/soc/codec.txt 中的内容为脉络来分析的。Codec 的作用，之前已有描述，本章主要罗列下 Codec driver 中重要的数据结构及注册流程。其中有着各种功能部件，包括但不限于 ： ADC 把麦克风拾取的模拟信号转换成数字信号DAC 把音频接口过来的数字信号转换成模拟信号MIXER 混音器，把多路输入信号混合成单路输出 3.1. Codec DAI and PCM configurationcodec_dai 和 pcm 配置信息通过结构体 snd_soc_dai_driver 描述，包括 dai 的能力描述和操作接口，snd_soc_dai_driver 最终会被注册到 soc-core 中。 12345678910111213141516171819202122232425262728293031323334[-&gt;include/sound/soc-dai.h]/* * Digital Audio Interface Driver. * * Describes the Digital Audio Interface in terms of its ALSA, DAI and AC97 * operations and capabilities. Codec and platform drivers will register this * structure for every DAI they have. * This structure covers the clocking, formating and ALSA operations for each * interface. */struct snd_soc_dai_driver &#123; /* DAI description */ const char *name; unsigned int id; int ac97_control; /* DAI driver callbacks */ int (*probe)(struct snd_soc_dai *dai); int (*remove)(struct snd_soc_dai *dai); int (*suspend)(struct snd_soc_dai *dai); int (*resume)(struct snd_soc_dai *dai); /* ops */ const struct snd_soc_dai_ops *ops; /* DAI capabilities */ struct snd_soc_pcm_stream capture; struct snd_soc_pcm_stream playback; unsigned int symmetric_rates:1; /* probe ordering - for components with runtime dependencies */ int probe_order; int remove_order;&#125;; name：codec_dai 的名称标识，dai_link 通过配置 codec_dai_name 来找到对应的 codec_dai；probe：codec_dai 的初始化函数，注册声卡时回调；playback：回放能力描述，如回放设备所支持的声道数、采样率、音频格式；capture：录制能力描述，如录制设备所支持声道数、采样率、音频格式；ops：codec_dai 的操作函数集，这些函数集非常重要，用于 dai 的时钟配置、格式配置、硬件参数配置。 codec_dai：1234567891011121314151617181920212223242526272829303132[-&gt;sound/soc/codecs/wcd9335.c]static struct snd_soc_dai_driver tasha_i2s_dai[] = &#123; &#123; .name = \"tasha_i2s_rx1\", .id = AIF1_PB, .playback = &#123; .stream_name = \"AIF1 Playback\", .rates = WCD9335_RATES_MASK, .formats = TASHA_FORMATS_S16_S24_LE, .rate_max = 192000, .rate_min = 8000, .channels_min = 1, .channels_max = 2, &#125;, .ops = &amp;tasha_dai_ops, &#125;, &#123; .name = \"tasha_i2s_tx1\", .id = AIF1_CAP, .capture = &#123; .stream_name = \"AIF1 Capture\", .rates = WCD9335_RATES_MASK, .formats = TASHA_FORMATS, .rate_max = 192000, .rate_min = 8000, .channels_min = 1, .channels_max = 4, &#125;, .ops = &amp;tasha_dai_ops, &#125;, ......&#125; 3.2. Codec control IO移动设备的音频 Codec，其控制接口一般是 I2C 或 SPI，控制接口用于读写 codec 的寄存器。在 snd_soc_codec_driver 结构体中，有如下字段描述 Codec 的控制接口： 12345678910111213141516171819202122[-&gt;include/sound/soc.h]/* codec driver */struct snd_soc_codec_driver &#123; ...... /* codec IO */ struct regmap *(*get_regmap)(struct device *); unsigned int (*read)(struct snd_soc_codec *, unsigned int); int (*write)(struct snd_soc_codec *, unsigned int, unsigned int); int (*display_register)(struct snd_soc_codec *, char *, size_t, unsigned int); int (*volatile_register)(struct snd_soc_codec *, unsigned int); int (*readable_register)(struct snd_soc_codec *, unsigned int); int (*writable_register)(struct snd_soc_codec *, unsigned int); unsigned int reg_cache_size; short reg_cache_step; short reg_word_size; const void *reg_cache_default; ......&#125;; • read：读寄存器；• write：写寄存器；• volatile_register：判断指定的寄存器是否是 volatile 属性；假如是，则读取寄存器时不是读 cache，而直接访问硬件；• readable_register：判断指定的寄存器是否可读；• reg_cache_default：寄存器的缺省值；• reg_cache_size：缺省的寄存器值数组大小；• reg_word_size：寄存器宽度。在 Linux-3.4.5 中，很多 codec 的控制接口都改用 regmap 了。soc-core 中判断是否用的是 regmap，如果是，则调用 regmap 接口。 3.3. Mixers and audio controls音频控件多用于部件开关和音量的设定，音频控件可通过 soc.h 中的宏来定义，例如单一型控件： 123456[-&gt;include/sound/soc.h]#define SOC_SINGLE(xname, reg, shift, max, invert) \\&#123; .iface = SNDRV_CTL_ELEM_IFACE_MIXER, .name = xname, \\ .info = snd_soc_info_volsw, .get = snd_soc_get_volsw,\\ .put = snd_soc_put_volsw, \\ .private_value = SOC_SINGLE_VALUE(reg, shift, max, invert) &#125; 这种控件只有一个设置量，一般用于部件开关。宏定义的参数说明： • xname：控件的名称标识；• reg：控件对应的寄存器地址；• shift：控件控制位在寄存器中的偏移；• max：控件设置值范围；• invert：设定值是否取反。其他类型控件类似，不一一介绍了。 上述只是宏定义，音频控件真正的结构是 snd_kcontrol_new： 123456789101112131415161718[-&gt;/include/sound/control.h]struct snd_kcontrol_new &#123; snd_ctl_elem_iface_t iface; /* interface identifier */ unsigned int device; /* device/client number */ unsigned int subdevice; /* subdevice (substream) number */ const unsigned char *name; /* ASCII name of item */ unsigned int index; /* index of item */ unsigned int access; /* access rights */ unsigned int count; /* count of same elements */ snd_kcontrol_info_t *info; snd_kcontrol_get_t *get; snd_kcontrol_put_t *put; union &#123; snd_kcontrol_tlv_rw_t *c; const unsigned int *p; &#125; tlv; unsigned long private_value;&#125;; Codec 初始化时，通过 snd_soc_add_codec_controls() 把所有定义好的音频控件注册到 alsa-core ，上层可以通过 tinymix、alsa_amixer 等工具查看修改这些控件的设定。 3.6. Codec audio operationsCodec 音频操作接口通过结构体 snd_soc_dai_ops 描述： 123456789101112131415161718192021222324252627282930313233[-&gt;include/sound/soc-dai.h]struct snd_soc_dai_ops &#123; /* * DAI clocking configuration, all optional. * Called by soc_card drivers, normally in their hw_params. */ int (*set_sysclk)(struct snd_soc_dai *dai, int clk_id, unsigned int freq, int dir); int (*set_pll)(struct snd_soc_dai *dai, int pll_id, int source, unsigned int freq_in, unsigned int freq_out); int (*set_clkdiv)(struct snd_soc_dai *dai, int div_id, int div); int (*set_bclk_ratio)(struct snd_soc_dai *dai, unsigned int ratio); /* * DAI format configuration * Called by soc_card drivers, normally in their hw_params. */ int (*set_fmt)(struct snd_soc_dai *dai, unsigned int fmt); int (*xlate_tdm_slot_mask)(unsigned int slots, unsigned int *tx_mask, unsigned int *rx_mask); int (*set_tdm_slot)(struct snd_soc_dai *dai, unsigned int tx_mask, unsigned int rx_mask, int slots, int slot_width); int (*set_channel_map)(struct snd_soc_dai *dai, unsigned int tx_num, unsigned int *tx_slot, unsigned int rx_num, unsigned int *rx_slot); int (*set_tristate)(struct snd_soc_dai *dai, int tristate); int (*get_channel_map)(struct snd_soc_dai *dai, unsigned int *tx_num, unsigned int *tx_slot, unsigned int *rx_num, unsigned int *rx_slot); ......&#125;; 注释比较详细的了，Codec 音频操作接口分为 5 大部分：时钟配置、格式配置、数字静音、PCM 音频接口、FIFO 延迟。着重说下时钟配置及格式配置接口： • set_sysclk：codec_dai 系统时钟设置，当上层打开 pcm 设备时，需要回调该接口设置 Codec 的系统时钟，Codec 才能正常工作；• set_pll：Codec FLL 设置，Codec 一般接了一个 MCLK 输入时钟，回调该接口基于 MCLK 来产生 Codec FLL 时钟，接着 codec_dai 的 sysclk、bclk、lrclk 均可从 FLL 分频出来（假设 Codec 作为 master）；• set_fmt：codec_dai 格式设置，具体见 soc-dai.h； • SND_SOC_DAIFMT_I2S：音频数据是 I2S 格式，常用于多媒体音频； • SND_SOC_DAIFMT_DSP_A：音频数据是 PCM 格式，常用于通话语音； • SND_SOC_DAIFMT_CBM_CFM：Codec 作为 master，BCLK 和 LRCLK 由 Codec 提供； • SND_SOC_DAIFMT_CBS_CFS：Codec 作为 slave，BCLK 和 LRCLK 由 SoC/CPU 提供；• hw_params：codec_dai 硬件参数设置，根据上层设定的声道数、采样率、数据格式，来配置 codec_dai 相关寄存器。 WCD9335的snd_soc_dai_ops ：1234567891011[-&gt;/sound/soc/codecs/wcd9335.c]static struct snd_soc_dai_ops tasha_dai_ops = &#123; .startup = tasha_startup, .shutdown = tasha_shutdown, .hw_params = tasha_hw_params, .prepare = tasha_prepare, .set_sysclk = tasha_set_dai_sysclk, .set_fmt = tasha_set_dai_fmt, .set_channel_map = tasha_set_channel_map, .get_channel_map = tasha_get_channel_map,&#125;; 3.6. Codec register当 platform_driver： 123456789101112[-&gt;/sound/soc/codecs/wcd9335.c]static struct platform_driver tasha_codec_driver = &#123; .probe = tasha_probe, .remove = tasha_remove, .driver = &#123; .name = \"tasha_codec\", .owner = THIS_MODULE,#ifdef CONFIG_PM .pm = &amp;tasha_pm_ops,#endif &#125;,&#125;; 与.name = “tasha_codec” 的 platform_device（该 platform_device 在 drivers/mfd/wcd9xxx-core.c 中注册wcd9xxx_device_init-&gt;wcd9xxx_check_codec_type-&gt;tasha_devs）匹配后， 123456[-&gt;drivers/mfd/wcd9xxx-core.c]static struct mfd_cell tasha_devs[] = &#123; &#123; .name = \"tasha_codec\", &#125;,&#125;; 立即回调 tasha_probe() 注册 Codec： 12345678910111213141516171819202122232425262728293031323334353637383940414243[-&gt;/sound/soc/codecs/wcd9335.c]static int tasha_probe(struct platform_device *pdev)&#123; int ret = 0; struct tasha_priv *tasha; struct clk *wcd_ext_clk, *wcd_native_clk; struct wcd9xxx_resmgr_v2 *resmgr; struct wcd9xxx_power_region *cdc_pwr; ...... tasha = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(struct tasha_priv), GFP_KERNEL); ...... tasha-&gt;resmgr = resmgr; tasha-&gt;swr_plat_data.handle = (void *) tasha; tasha-&gt;swr_plat_data.read = tasha_swrm_read; tasha-&gt;swr_plat_data.write = tasha_swrm_write; tasha-&gt;swr_plat_data.bulk_write = tasha_swrm_bulk_write; tasha-&gt;swr_plat_data.clk = tasha_swrm_clock; tasha-&gt;swr_plat_data.handle_irq = tasha_swrm_handle_irq; /* Register for Clock */ wcd_ext_clk = clk_get(tasha-&gt;wcd9xxx-&gt;dev, \"wcd_clk\"); if (IS_ERR(wcd_ext_clk)) &#123; dev_err(tasha-&gt;wcd9xxx-&gt;dev, \"%s: clk get %s failed\\n\", __func__, \"wcd_ext_clk\"); goto err_clk; &#125; tasha-&gt;wcd_ext_clk = wcd_ext_clk; tasha-&gt;sido_voltage = SIDO_VOLTAGE_NOMINAL_MV; set_bit(AUDIO_NOMINAL, &amp;tasha-&gt;status_mask); tasha-&gt;sido_ccl_cnt = 0; ...... if (wcd9xxx_get_intf_type() == WCD9XXX_INTERFACE_TYPE_SLIMBUS) ret = snd_soc_register_codec(&amp;pdev-&gt;dev, &amp;soc_codec_dev_tasha, tasha_dai, ARRAY_SIZE(tasha_dai)); else if (wcd9xxx_get_intf_type() == WCD9XXX_INTERFACE_TYPE_I2C) ret = snd_soc_register_codec(&amp;pdev-&gt;dev, &amp;soc_codec_dev_tasha, tasha_i2s_dai, ARRAY_SIZE(tasha_i2s_dai)); else ret = -EINVAL; ......&#125; snd_soc_register_codec：将 codec_driver 和 codec_dai_driver 注册到 soc-core。 12345678910[-&gt;]/** * snd_soc_register_codec - Register a codec with the ASoC core * * @codec: codec to register */int snd_soc_register_codec(struct device *dev, const struct snd_soc_codec_driver *codec_drv, struct snd_soc_dai_driver *dai_drv, int num_dai) 创建一个 snd_soc_codec 实例，包含 codec_drv（snd_soc_dai_driver）相关信息，封装给 soc-core 使用，相关代码段如下： 1234567891011121314151617181920212223242526272829303132[sound/soc/soc-core.c: snd_soc_register_codec] struct snd_soc_codec *codec; dev_dbg(dev, \"codec register %s\\n\", dev_name(dev)); codec = kzalloc(sizeof(struct snd_soc_codec), GFP_KERNEL); if (codec == NULL) return -ENOMEM; /* create CODEC component name */ codec-&gt;name = fmt_single_name(dev, &amp;codec-&gt;id); if (codec-&gt;name == NULL) &#123; kfree(codec); return -ENOMEM; &#125; // 初始化 Codec 的寄存器缓存配置及读写接口 codec-&gt;write = codec_drv-&gt;write; codec-&gt;read = codec_drv-&gt;read; codec-&gt;volatile_register = codec_drv-&gt;volatile_register; codec-&gt;readable_register = codec_drv-&gt;readable_register; codec-&gt;writable_register = codec_drv-&gt;writable_register; codec-&gt;ignore_pmdown_time = codec_drv-&gt;ignore_pmdown_time; codec-&gt;dapm.bias_level = SND_SOC_BIAS_OFF; codec-&gt;dapm.dev = dev; codec-&gt;dapm.codec = codec; codec-&gt;dapm.seq_notifier = codec_drv-&gt;seq_notifier; codec-&gt;dapm.stream_event = codec_drv-&gt;stream_event; codec-&gt;dev = dev; codec-&gt;driver = codec_drv; codec-&gt;num_dai = num_dai; mutex_init(&amp;codec-&gt;mutex); 把以上 codec 实例插入到 codec_list链表中（声卡注册时会遍历该链表，找到 dai_link 声明的 codec 并绑定）： 12[sound/soc/soc-core.c: snd_soc_register_codec]list_add(&amp;codec-&gt;list, &amp;codec_list); 把 codec_drv 中的 snd_soc_dai_driver（tasha_dai 或者tasha_i2s_dai ）注册到 soc-core： 12[sound/soc/soc-core.c: snd_soc_register_codec]snd_soc_register_dais(&amp;codec-&gt;component, dai_drv, num_dai, false); snd_soc_register_dais() 会把 dai 插入到 dai_list 链表中（声卡注册时会遍历该链表，找到 dai_link 声明的 codec_dai 并绑定）： 12[sound/soc/soc-core.c: snd_soc_register_codec]list_add(&amp;dai-&gt;list, &amp;dai_list); 最后顺便提下 codec 和 codec_dai 的区别：codec 指音频芯片共有的部分，包括 codec 初始化函数、控制接口、寄存器缓存、控件、dapm 部件、音频路由、偏置电压设置函数等描述信息；而 codec_dai 指 codec 上的音频接口驱动描述，包括时钟配置、格式配置、能力描述等等，各个接口的描述信息不一定都是一致的，所以每个音频接口都有着各自的驱动描述。 （四）Platform Driver概述中提到音频 Platform 驱动主要用于音频数据传输，这里又细分为两步： 启动 dma 设备，把音频数据从 dma buffer 搬运到 cpu_dai FIFO，这部分驱动用 snd_soc_platform_driver 描述，后面分析用 pcm_dma 指代它。启动数字音频接口控制器（I2S/PCM/AC97），把音频数据从 cpu_dai FIFO 传送到 codec_dai（高通平台会将数据传送到ADSP）这部分驱动用 snd_soc_dai_driver 描述，后面分析用 cpu_dai 指代它。 MSM8996 包含三个 Hexagon DSP ：application, modem, and sensor。Application DSP：不仅可以处理语音和音频，还可以处理计算机 视觉、视频、图像和Camera。 Sensor DSP：也叫做SLPI，所有的sensor都链接到SLPI上面，它管理所有的Sensor及相关算法。 对于 cpu_dai 驱动，从上面的类图我们可知，主要工作有： 实现 dai 操作函数，见 snd_soc_dai_ops 定义，用于配置和操作音频数字接口控制器，如时钟配置 set_sysclk()、格式配置 set_fmt()、硬件参数配置 hw_params()、启动/停止数据传输 trigger() 等；实现 probe 函数（初始化）、remove 函数（卸载）、suspend/resume 函数（电源管理）；初始化 snd_soc_dai_driver 实例，包括回放和录制的能力描述、dai 操作函数集、probe/remove 回调、电源管理相关的 suspend/resume 回调；通过 snd_soc_register_dai() 把初始化完成的 snd_soc_dai_driver 注册到 soc-core：首先创建一个 snd_soc_dai 实例，然后把该 snd_soc_dai 实例插入到 dai_list 链表（声卡注册时会遍历该链表，找到 dai_link 声明的 cpu_dai 并绑定）。 123456789101112131415161718192021222324252627282930313233343536373839404142[sound/soc/soc-core.c]static int snd_soc_register_dais(struct snd_soc_component *component, struct snd_soc_dai_driver *dai_drv, size_t count, bool legacy_dai_naming)&#123; struct device *dev = component-&gt;dev; struct snd_soc_dai *dai; unsigned int i; int ret; dev_dbg(dev, \"ASoC: dai register %s #%Zu\\n\", dev_name(dev), count); component-&gt;dai_drv = dai_drv; component-&gt;num_dai = count; for (i = 0; i &lt; count; i++) &#123; dai = kzalloc(sizeof(struct snd_soc_dai), GFP_KERNEL); ...... if (count == 1 &amp;&amp; legacy_dai_naming) &#123; dai-&gt;name = fmt_single_name(dev, &amp;dai-&gt;id); &#125; else &#123; dai-&gt;name = fmt_multiple_name(dev, &amp;dai_drv[i]); if (dai_drv[i].id) dai-&gt;id = dai_drv[i].id; else dai-&gt;id = i; &#125; ...... dai-&gt;component = component; dai-&gt;dev = dev; dai-&gt;driver = &amp;dai_drv[i]; if (!dai-&gt;driver-&gt;ops) dai-&gt;driver-&gt;ops = &amp;null_dai_ops; list_add(&amp;dai-&gt;list, &amp;component-&gt;dai_list); &#125; return 0; return ret;&#125; dai 操作函数的实现是 cpu_dai 驱动的主体，需要配置好相关寄存器让 I2S/PCM 总线控制器正常运转，snd_soc_dai_ops 字段的详细说明见 3.6. Codec audio operations 章节。 cpu_dai 驱动应该算是这个系列中最简单的一环，因此不多花费笔墨在这里了。倒是某些平台上，dma 设备信息（总线地址、通道号、传输单元大小）是在这里初始化的，这点要留意，这些 dma 设备信息在 pcm_dma 驱动中用到。 4.1. pcm operations操作函数的实现是本模块的主体，见 snd_pcm_ops 结构体描述： 12345678910111213141516171819202122232425262728[-&gt;include/sound/pcm.h]struct snd_pcm_ops &#123; int (*open)(struct snd_pcm_substream *substream); int (*close)(struct snd_pcm_substream *substream); int (*ioctl)(struct snd_pcm_substream * substream, unsigned int cmd, void *arg); int (*compat_ioctl)(struct snd_pcm_substream *substream, unsigned int cmd, void *arg); int (*hw_params)(struct snd_pcm_substream *substream, struct snd_pcm_hw_params *params); int (*hw_free)(struct snd_pcm_substream *substream); int (*prepare)(struct snd_pcm_substream *substream); int (*trigger)(struct snd_pcm_substream *substream, int cmd); snd_pcm_uframes_t (*pointer)(struct snd_pcm_substream *substream); int (*delay_blk)(struct snd_pcm_substream *substream); int (*wall_clock)(struct snd_pcm_substream *substream, struct timespec *audio_ts); int (*copy)(struct snd_pcm_substream *substream, int channel, snd_pcm_uframes_t pos, void __user *buf, snd_pcm_uframes_t count); int (*silence)(struct snd_pcm_substream *substream, int channel, snd_pcm_uframes_t pos, snd_pcm_uframes_t count); struct page *(*page)(struct snd_pcm_substream *substream, unsigned long offset); int (*mmap)(struct snd_pcm_substream *substream, struct vm_area_struct *vma); int (*ack)(struct snd_pcm_substream *substream); int (*restart)(struct snd_pcm_substream *substream);&#125;; 4.1. platform_driver 注册当 platform_driver： 12345678910[-&gt;sound/soc/msm/qdsp6v2/msm-pcm-q6-v2.c]static struct platform_driver msm_pcm_driver = &#123; .driver = &#123; .name = \"msm-pcm-dsp\", .owner = THIS_MODULE, .of_match_table = msm_pcm_dt_match, &#125;, .probe = msm_pcm_probe, .remove = msm_pcm_remove,&#125;; 与 .name = “msm-pcm-dsp” 的 platform_device 注册 匹配后，系统会回调 msm_pcm_probe() 注册 platform：1234567891011121314151617181920212223242526272829303132333435[-&gt;sound/soc/msm/qdsp6v2/msm-pcm-q6-v2.c]static int msm_pcm_probe(struct platform_device *pdev)&#123; int rc; int id; struct msm_plat_data *pdata; const char *latency_level; rc = of_property_read_u32(pdev-&gt;dev.of_node, \"qcom,msm-pcm-dsp-id\", &amp;id); ...... pdata = kzalloc(sizeof(struct msm_plat_data), GFP_KERNEL); ...... if (of_property_read_bool(pdev-&gt;dev.of_node, \"qcom,msm-pcm-low-latency\")) &#123; pdata-&gt;perf_mode = LOW_LATENCY_PCM_MODE; rc = of_property_read_string(pdev-&gt;dev.of_node, \"qcom,latency-level\", &amp;latency_level); if (!rc) &#123; if (!strcmp(latency_level, \"ultra\")) pdata-&gt;perf_mode = ULTRA_LOW_LATENCY_PCM_MODE; else if (!strcmp(latency_level, \"ull-pp\")) pdata-&gt;perf_mode = ULL_POST_PROCESSING_PCM_MODE; &#125; &#125; else pdata-&gt;perf_mode = LEGACY_PCM_MODE; dev_set_drvdata(&amp;pdev-&gt;dev, pdata); return snd_soc_register_platform(&amp;pdev-&gt;dev, &amp;msm_soc_platform);&#125; snd_soc_register_platform：将 platform_drv 注册到 soc-core。创建一个 snd_soc_platform 实例，包含 platform_drv（snd_soc_platform_driver）的相关信息，封装给 soc-core 使用；把以上创建的 platform 实例插入到 platform_list 链表上（声卡注册时会遍历该链表，找到 dai_link 声明的 platform 并绑定）。代码实现： 123456789101112int snd_soc_register_platform(struct device *dev, const struct snd_soc_platform_driver *platform_drv)&#123; struct snd_soc_platform *platform; int ret; platform = kzalloc(sizeof(struct snd_soc_platform), GFP_KERNEL); ret = snd_soc_add_platform(dev, platform, platform_drv); return ret;&#125; 至此，完成了 Platform 驱动的实现。回放情形下，pcm_dma 设备负责把 dma buffer 中的数据搬运到 I2S tx FIFO，I2S 总线控制器负责把 I2S tx FIFO 中的数据传送DSP，DSP经处理后传送到到 Codec。 （五） Machine Driver章节 3. Codec 和 4. Platform 介绍了 Codec、Platform 驱动，但仅有 Codec、Platform 驱动是不能工作的，需要一个角色把 codec、codec_dai、cpu_dai、platform 给链结起来才能构成一个完整的音频链路，这个角色就由 machine_drv 承担了。 snd_soc_dai_link 结构体： 12345678910111213141516171819202122232425262728293031323334353637[-&gt;/include/sound/soc.h]struct snd_soc_dai_link &#123; const char *name; /* Codec name */ const char *stream_name; /* Stream name */ const char *cpu_name; struct device_node *cpu_of_node; const char *cpu_dai_name; const char *codec_name; struct device_node *codec_of_node; const char *codec_dai_name; struct snd_soc_dai_link_component *codecs; unsigned int num_codecs; const char *platform_name; struct device_node *platform_of_node; int be_id; /* optional ID for machine driver BE identification */ const struct snd_soc_pcm_stream *params; unsigned int dai_fmt; /* format to set on init */ enum snd_soc_dpcm_trigger trigger[2]; /* trigger type for DPCM */ unsigned int ignore_suspend:1; unsigned int symmetric_rates:1; unsigned int symmetric_channels:1; unsigned int symmetric_samplebits:1; unsigned int no_pcm:1; unsigned int dynamic:1; unsigned int no_host_mode:2; unsigned int dpcm_capture:1; unsigned int dpcm_playback:1; unsigned int ignore_pmdown_time:1; int (*init)(struct snd_soc_pcm_runtime *rtd); int (*be_hw_params_fixup)(struct snd_soc_pcm_runtime *rtd, struct snd_pcm_hw_params *params); const struct snd_soc_ops *ops; const struct snd_soc_compr_ops *compr_ops; bool playback_only; bool capture_only; enum snd_soc_async_ops async_ops;&#125; 重点介绍如下几个字段： • codec_name：音频链路需要绑定的 codec 名称，声卡注册时会遍历 codec_list，找到同名的 codec 并绑定；• platform_name：音频链路需要绑定的 platform 名称，声卡注册时会遍历 platform_list，找到同名的 platform 并绑定；• cpu_dai_name：音频链路需要绑定的 cpu_dai 名称，声卡注册时会遍历 dai_list，找到同名的 dai 并绑定；• codec_dai_name：音频链路需要绑定的 codec_dai 名称，声卡注册时会遍历 dai_list，找到同名的 dai 并绑定；ops：重点留意 hw_params() 回调，一般来说这个回调是要实现的，用于配置 codec、codec_dai、cpu_dai 的数据格式和系统时钟。在 3.6. Codec audio operations 小节中有描述。/sound/soc/msm/msm8996.c 中的 dai_link 定义，两个音频链路分别用于 Media和 Voice： 123456789101112131415161718192021222324252627282930313233343536373839404142[-&gt;/sound/soc/msm/msm8996.c]/* Digital audio interface glue - connects codec &lt;---&gt; CPU */static struct snd_soc_dai_link msm8996_common_dai_links[] = &#123; /* FrontEnd DAI Links */ &#123; .name = \"MSM8996 Media1\", .stream_name = \"MultiMedia1\", .cpu_dai_name = \"MultiMedia1\", .platform_name = \"msm-pcm-dsp.0\", .dynamic = 1, .async_ops = ASYNC_DPCM_SND_SOC_PREPARE, .dpcm_playback = 1, .dpcm_capture = 1, .trigger = &#123;SND_SOC_DPCM_TRIGGER_POST, SND_SOC_DPCM_TRIGGER_POST&#125;, .codec_dai_name = \"snd-soc-dummy-dai\", .codec_name = \"snd-soc-dummy\", .ignore_suspend = 1, /* this dainlink has playback support */ .ignore_pmdown_time = 1, .be_id = MSM_FRONTEND_DAI_MULTIMEDIA1 &#125;, ...... &#123; .name = \"VoiceMMode1\", .stream_name = \"VoiceMMode1\", .cpu_dai_name = \"VoiceMMode1\", .platform_name = \"msm-pcm-voice\", .dynamic = 1, .dpcm_playback = 1, .dpcm_capture = 1, .trigger = &#123;SND_SOC_DPCM_TRIGGER_POST, SND_SOC_DPCM_TRIGGER_POST&#125;, .no_host_mode = SND_SOC_DAI_LINK_NO_HOST, .ignore_suspend = 1, .ignore_pmdown_time = 1, .codec_dai_name = \"snd-soc-dummy-dai\", .codec_name = \"snd-soc-dummy\", .be_id = MSM_FRONTEND_DAI_VOICEMMODE1, &#125;, &#125; 除了 dai_link，机器中一些特定的音频控件和音频事件也可以在 machine_drv 定义，如耳机插拔检测、外部功放打开关闭等。 我们再分析 machine_drv 初始化过程： 1234567891011[-&gt;/sound/soc/msm/msm8996.c]static struct platform_driver msm8996_asoc_machine_driver = &#123; .driver = &#123; .name = DRV_NAME, .owner = THIS_MODULE, .pm = &amp;snd_soc_pm_ops, .of_match_table = msm8996_asoc_machine_of_match, &#125;, .probe = msm8996_asoc_machine_probe, .remove = msm8996_asoc_machine_remove,&#125;; 1234567891011121314151617181920212223[-&gt;/sound/soc/msm/msm8996.c]static int msm8996_asoc_machine_probe(struct platform_device *pdev)&#123; struct snd_soc_card *card; struct msm8996_asoc_mach_data *pdata; const char *mbhc_audio_jack_type = NULL; char *mclk_freq_prop_name; const struct of_device_id *match; int ret; ...... pdata = devm_kzalloc(&amp;pdev-&gt;dev, sizeof(struct msm8996_asoc_mach_data), GFP_KERNEL); card = populate_snd_card_dailinks(&amp;pdev-&gt;dev); ...... match = of_match_node(msm8996_asoc_machine_of_match, pdev-&gt;dev.of_node); ret = msm8996_populate_dai_link_component_of_node(card); ...... ret = snd_soc_register_card(card); &#125; 设置dailinks后，继而调用 snd_soc_register_card() 注册声卡。由于该过程很冗长，这里不一一贴代码分析了，但整个流程是比较简单的，流程图如下： • 取出 platform_device 的私有数据，该私有数据就是 snd_soc_card ；• snd_soc_register_card() 为每个 dai_link 分配一个 snd_soc_pcm_runtime 实例，别忘了之前提过 snd_soc_pcm_runtime 是 ASoC 的桥梁，保存着 codec、codec_dai、cpu_dai、platform 等硬件设备实例。• 随后的工作都在 snd_soc_instantiate_card() 进行：• 遍历 dai_list、codec_list、platform_list 链表，为每个音频链路找到对应的 cpu_dai、codec_dai、codec、platform；找到的 cpu_dai、codec_dai、codec、platform 保存到 snd_soc_pcm_runtime ，完成音频链路的设备绑定；• 调用 snd_card_create() 创建声卡；• soc_probe_dai_link() 依次回调 cpu_dai、codec、platform、codec_dai 的 probe() 函数，完成各音频设备的初始化，随后调用• soc_new_pcm() 创建 pcm 逻辑设备（因为涉及到本系列的重点内容，后面具体分析这个函数）；最后调用 snd_card_register() 注册声卡。 [-&gt;sound/soc/soc-core.c] soc_new_pcm 源码分析： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151[-&gt;/sound/soc/soc-pcm.c]/* create a new pcm */int soc_new_pcm(struct snd_soc_pcm_runtime *rtd, int num)&#123; struct snd_soc_platform *platform = rtd-&gt;platform; struct snd_soc_dai *codec_dai; struct snd_soc_dai *cpu_dai = rtd-&gt;cpu_dai; struct snd_pcm *pcm; char new_name[64]; int ret = 0, playback = 0, capture = 0; int i; if (rtd-&gt;dai_link-&gt;dynamic || rtd-&gt;dai_link-&gt;no_pcm) &#123; playback = rtd-&gt;dai_link-&gt;dpcm_playback; capture = rtd-&gt;dai_link-&gt;dpcm_capture; &#125; else &#123; for (i = 0; i &lt; rtd-&gt;num_codecs; i++) &#123; codec_dai = rtd-&gt;codec_dais[i]; if (codec_dai-&gt;driver-&gt;playback.channels_min) playback = 1; if (codec_dai-&gt;driver-&gt;capture.channels_min) capture = 1; &#125; capture = capture &amp;&amp; cpu_dai-&gt;driver-&gt;capture.channels_min; playback = playback &amp;&amp; cpu_dai-&gt;driver-&gt;playback.channels_min; &#125; if (rtd-&gt;dai_link-&gt;playback_only) &#123; playback = 1; capture = 0; &#125; if (rtd-&gt;dai_link-&gt;capture_only) &#123; playback = 0; capture = 1; &#125; /* create the PCM */ if (rtd-&gt;dai_link-&gt;no_pcm) &#123; snprintf(new_name, sizeof(new_name), \"(%s)\", rtd-&gt;dai_link-&gt;stream_name); ret = snd_pcm_new_internal(rtd-&gt;card-&gt;snd_card, new_name, num, playback, capture, &amp;pcm); &#125; else &#123; if (rtd-&gt;dai_link-&gt;dynamic) snprintf(new_name, sizeof(new_name), \"%s (*)\", rtd-&gt;dai_link-&gt;stream_name); else snprintf(new_name, sizeof(new_name), \"%s %s-%d\", rtd-&gt;dai_link-&gt;stream_name, (rtd-&gt;num_codecs &gt; 1) ? \"multicodec\" : rtd-&gt;codec_dai-&gt;name, num); ret = snd_pcm_new(rtd-&gt;card-&gt;snd_card, new_name, num, playback, capture, &amp;pcm); &#125; if (ret &lt; 0) &#123; dev_err(rtd-&gt;card-&gt;dev, \"ASoC: can't create pcm for %s\\n\", rtd-&gt;dai_link-&gt;name); return ret; &#125; dev_dbg(rtd-&gt;card-&gt;dev, \"ASoC: registered pcm #%d %s\\n\",num, new_name); /* DAPM dai link stream work */ INIT_DELAYED_WORK(&amp;rtd-&gt;delayed_work, close_delayed_work); rtd-&gt;pcm = pcm; pcm-&gt;private_data = rtd; if (rtd-&gt;dai_link-&gt;no_pcm) &#123; if (playback) pcm-&gt;streams[SNDRV_PCM_STREAM_PLAYBACK].substream-&gt;private_data = rtd; if (capture) pcm-&gt;streams[SNDRV_PCM_STREAM_CAPTURE].substream-&gt;private_data = rtd; if (platform-&gt;driver-&gt;pcm_new) rtd-&gt;platform-&gt;driver-&gt;pcm_new(rtd); goto out; &#125; /* setup any hostless PCMs - i.e. no host IO is performed */ if (rtd-&gt;dai_link-&gt;no_host_mode) &#123; if (pcm-&gt;streams[SNDRV_PCM_STREAM_PLAYBACK].substream) &#123; pcm-&gt;streams[SNDRV_PCM_STREAM_PLAYBACK].substream-&gt;hw_no_buffer = 1; snd_soc_set_runtime_hwparams( pcm-&gt;streams[SNDRV_PCM_STREAM_PLAYBACK].substream, &amp;no_host_hardware); &#125; if (pcm-&gt;streams[SNDRV_PCM_STREAM_CAPTURE].substream) &#123; pcm-&gt;streams[SNDRV_PCM_STREAM_CAPTURE].substream-&gt;hw_no_buffer = 1; snd_soc_set_runtime_hwparams( pcm-&gt;streams[SNDRV_PCM_STREAM_CAPTURE].substream, &amp;no_host_hardware); &#125; &#125; /* ASoC PCM operations */ if (rtd-&gt;dai_link-&gt;dynamic) &#123; rtd-&gt;ops.open = dpcm_fe_dai_open; rtd-&gt;ops.hw_params = dpcm_fe_dai_hw_params; rtd-&gt;ops.prepare = dpcm_fe_dai_prepare; rtd-&gt;ops.trigger = dpcm_fe_dai_trigger; rtd-&gt;ops.hw_free = dpcm_fe_dai_hw_free; rtd-&gt;ops.close = dpcm_fe_dai_close; rtd-&gt;ops.pointer = soc_pcm_pointer; rtd-&gt;ops.delay_blk = soc_pcm_delay_blk; rtd-&gt;ops.ioctl = soc_pcm_ioctl; rtd-&gt;ops.compat_ioctl = soc_pcm_compat_ioctl; &#125; else &#123; rtd-&gt;ops.open = soc_pcm_open; rtd-&gt;ops.hw_params = soc_pcm_hw_params; rtd-&gt;ops.prepare = soc_pcm_prepare; rtd-&gt;ops.trigger = soc_pcm_trigger; rtd-&gt;ops.hw_free = soc_pcm_hw_free; rtd-&gt;ops.close = soc_pcm_close; rtd-&gt;ops.pointer = soc_pcm_pointer; rtd-&gt;ops.delay_blk = soc_pcm_delay_blk; rtd-&gt;ops.ioctl = soc_pcm_ioctl; rtd-&gt;ops.compat_ioctl = soc_pcm_compat_ioctl; &#125; if (platform-&gt;driver-&gt;ops) &#123; rtd-&gt;ops.ack = platform-&gt;driver-&gt;ops-&gt;ack; rtd-&gt;ops.copy = platform-&gt;driver-&gt;ops-&gt;copy; rtd-&gt;ops.silence = platform-&gt;driver-&gt;ops-&gt;silence; rtd-&gt;ops.page = platform-&gt;driver-&gt;ops-&gt;page; rtd-&gt;ops.mmap = platform-&gt;driver-&gt;ops-&gt;mmap; rtd-&gt;ops.restart = platform-&gt;driver-&gt;ops-&gt;restart; &#125; if (playback) snd_pcm_set_ops(pcm, SNDRV_PCM_STREAM_PLAYBACK, &amp;rtd-&gt;ops); if (capture) snd_pcm_set_ops(pcm, SNDRV_PCM_STREAM_CAPTURE, &amp;rtd-&gt;ops); if (platform-&gt;driver-&gt;pcm_new) &#123; ret = platform-&gt;driver-&gt;pcm_new(rtd); if (ret &lt; 0) &#123; dev_err(platform-&gt;dev, \"ASoC: pcm constructor failed: %d\\n\", ret); return ret; &#125; &#125; pcm-&gt;private_free = platform-&gt;driver-&gt;pcm_free; return ret;&#125; 可见 soc_new_pcm() 最主要的工作是创建 pcm 逻辑设备，创建回放子流和录制子流实例，并初始化回放子流和录制子流的 pcm 操作函数（数据搬运时，需要调用这些函数来驱动 codec、codec_dai、cpu_dai、dma 设备工作）。 （六）、声卡和 PCM 设备的建立过程前面几章分析了 Codec、Platform、Machine 驱动的组成部分及其注册过程，这三者都是物理设备相关的，大家应该对音频物理链路有了一定的认知。接着分析音频驱动的中间层，由于这些并不是真正的物理设备，故我们称之为逻辑设备。 PCM 逻辑设备，我们又习惯称之为 PCM 中间层或 pcm native，起着承上启下的作用：往上是与用户态接口的交互，实现音频数据在用户态和内核态之间的拷贝；往下是触发 codec、platform、machine 的操作函数，实现音频数据在 dma_buffer &lt;-&gt; cpu_dai &lt;-&gt; codec 之间的传输。后面章节将会详细分析这个过程，这里还是先从声卡的注册谈起。声卡驱动中，一般挂载着多个逻辑设备，看看我们计算机的声卡驱动有几个逻辑设备： 123456789101112adb shell cat /proc/asound/devices 2: [ 0] : control 3: [ 0- 0]: digital audio playback 4: [ 0- 0]: digital audio capture 5: [ 0- 1]: digital audio playback 6: [ 0- 1]: digital audio capture ...... 27: [ 0-16]: digital audio playback 28: [ 0-16]: digital audio capture 29: [ 0-17]: digital audio playback 30: [ 0-17]: digital audio capture 33: : timer digital audio playback 用于回放的 PCM 设备digital audio capture 用于录制的 PCM 设备control 用于声卡控制的 CTL 设备，如通路控制、音量调整等timer 定时器设备手机系统中，通常我们更关心 PCM 和 CTL 这两种设备。 设备节点如下： 123456789101112131415adb shell ls -l /dev/sndcrw-rw---- 1 system audio 116, 51 1970-06-19 02:07 comprC0D24crw-rw---- 1 system audio 116, 52 1970-06-19 02:07 comprC0D27crw-rw---- 1 system audio 116, 53 1970-06-19 02:07 comprC0D28......crw-rw---- 1 system audio 116, 2 1970-06-19 02:07 controlC0crw-rw---- 1 system audio 116, 59 1970-06-19 02:07 hwC0D1000crw-rw---- 1 system audio 116, 66 1970-06-19 02:07 hwC0D11crw-rw---- 1 system audio 116, 67 1970-06-19 02:07 hwC0D12crw-rw---- 1 system audio 116, 76 1970-06-19 02:07 hwC0D13......crw-rw---- 1 system audio 116, 13 1970-06-19 02:07 pcmC0D6ccrw-rw---- 1 system audio 116, 14 1970-06-19 02:07 pcmC0D7pcrw-rw---- 1 system audio 116, 15 1970-06-19 02:07 pcmC0D8ccrw-rw---- 1 system audio 116, 33 1970-06-19 02:07 timer 可以看到这些设备节点的 Major=116，Minor 则与 /proc/asound/devices 所列的对应起来，都是字符设备。上层可以通过 open/close/read/write/ioctl 等系统调用来操作声卡设备，这和其他字符设备类似，但一般情况下我们会使用已封装好的用户接口库如 tinyalsa、alsa-lib。 6.1. 声卡结构概述回顾下 ASoC 是如何注册声卡的，详细请参考章节 5. ASoC machine driver，这里仅简单陈述下： • Machine 驱动初始化时，.name = “soc-audio” 的 platform_device 与 platform_driver 匹配成功，触发 soc_probe() 调用；• 继而调用 snd_soc_register_card()： ﹋• 为每个音频物理链路找到对应的 codec、codec_dai、cpu_dai、platform 设备实例，完成 dai_link 的绑定； ﹋ • 调用 snd_card_create() 创建声卡； ﹋ • 依次回调 cpu_dai、codec、platform 的 probe() 函数，完成物理设备的初始化；• 随后调用 soc_new_pcm()： ﹋ • 设置 pcm native 中要使用的 pcm 操作函数，这些函数用于驱动音频物理设备，包括 machine、codec_dai、cpu_dai、platform； ﹋ • 调用 snd_pcm_new() 创建 pcm 逻辑设备，回放子流和录制子流都在这里创建； ﹋ • 回调 platform 驱动的 pcm_new()，完成音频 dma 设备初始化和 dma buffer 内存分配；• 最后调用 snd_card_register() 注册声卡。关于音频物理设备部分（Codec/Platform/Machine）不再累述，下面详细分析声卡和 PCM 逻辑设备的注册过程。 上面提到声卡驱动上挂着多个逻辑子设备，有 pcm 音频数据流、control 混音器、midi 迷笛、timer 定时器等。 123456789 +-----------+ | snd_card | +-----------+ | | | +-----------+ | +------------+ | | |+-----------+ +-----------+ +-----------+ | snd_pcm | |snd_control| | snd_timer | ... +-----------+ +-----------+ +-----------+ 这些与声音相关的逻辑设备都在结构体 snd_card 管理之下，可以说 snd_card 是 alsa 中最顶层的结构。我们再看看 alsa 声卡驱动的大致结构图（不是严格的 UML 类图，有结构体定义、模块关系、函数调用，方便标示结构模块的层次及关系）： snd_cards：记录着所注册的声卡实例，每个声卡实例有着各自的逻辑设备，如 PCM 设备、CTL 设备、MIDI 设备等，并一一记录到 snd_card 的 devices 链表上snd_minors：记录着所有逻辑设备的上下文信息，它是声卡逻辑设备与系统调用 API 之间的桥梁；每个 snd_minor 在逻辑设备注册时被填充，在逻辑设备使用时就可以从该结构体中得到相应的信息（主要是系统调用函数集 file_operations） 6.2. 声卡的创建snd_card_create()123456789101112131415161718192021[-&gt;sound/core/init.c]/** * snd_card_new - create and initialize a soundcard structure * @parent: the parent device object * @idx: card index (address) [0 ... (SNDRV_CARDS-1)] * @xid: card identification (ASCII string) * @module: top level module for locking * @extra_size: allocate this extra size after the main soundcard structure * @card_ret: the pointer to store the created card instance * * Creates and initializes a soundcard structure. * * The function allocates snd_card instance via kzalloc with the given * space for the driver to use freely. The allocated struct is stored * in the given card_ret pointer. * * Return: Zero if successful or a negative error code. */int snd_card_new(struct device *parent, int idx, const char *xid, struct module *module, int extra_size, struct snd_card **card_ret) 注释非常详细，简单说下：idx：声卡的编号，如为 -1，则由系统自动分配xid：声卡标识符，如为 NULL，则以 snd_card 的 shortname 或 longname 代替card_ret：返回所创建的声卡实例的指针如下是Google Pixel手机的声卡信息：1234adb shellsailfish:/ $ cat /proc/asound/cards 0 [msm8996tashamar]: msm8996-tasha-m - msm8996-tasha-marlin-snd-card msm8996-tasha-marlin-snd-card 6.3. 逻辑设备的创建当声卡实例建立后，接着可以创建声卡下面的各个逻辑设备了。每个逻辑设备创建时，都会调用 snd_device_new() 生成一个 snd_device 实例，并把该实例挂到声卡 snd_card 的 devices 链表上。alsa 驱动为各种逻辑设备提供了创建接口，如下： PCM snd_pcm_new()CONTROL snd_ctl_create()MIDI snd_rawmidi_new()TIMER snd_timer_new()SEQUENCER snd_seq_device_new()JACK snd_jack_new() 这些接口的一般过程如下： 1234567891011121314int snd_xxx_new()&#123; // 这些接口供逻辑设备注册时回调 static struct snd_device_ops ops = &#123; .dev_free = snd_xxx_dev_free, .dev_register = snd_xxx_dev_register, .dev_disconnect = snd_xxx_dev_disconnect, &#125;; // 逻辑设备实例初始化 // 新建一个设备实例 snd_device，挂到 snd_card 的 devices 链表上，把该逻辑设备纳入声卡的管理当中，SNDRV_DEV_xxx 是逻辑设备的类型 return snd_device_new(card, SNDRV_DEV_xxx, card, &amp;ops);&#125; 其中 snd_device_ops 是声卡逻辑设备的注册函数集，dev_register() 回调尤其重要，它在声卡注册时被调用，用于建立系统的设备节点，/dev/snd/ 目录的设备节点都是在这里创建的，通过这些设备节点可系统调用 open/release/read/write/ioctl… 访问操作该逻辑设备。 例如 snd_ctl_dev_register()： 123456789101112131415161718192021222324252627282930313233[-&gt;/sound/core/control.c]static const struct file_operations snd_ctl_f_ops =&#123; .owner = THIS_MODULE, .read = snd_ctl_read, .open = snd_ctl_open, .release = snd_ctl_release, .llseek = no_llseek, .poll = snd_ctl_poll, .unlocked_ioctl = snd_ctl_ioctl, .compat_ioctl = snd_ctl_ioctl_compat, .fasync = snd_ctl_fasync,&#125;;/* * registration of the control device */static int snd_ctl_dev_register(struct snd_device *device)&#123; struct snd_card *card = device-&gt;device_data; int err, cardnum; char name[16]; if (snd_BUG_ON(!card)) return -ENXIO; cardnum = card-&gt;number; if (snd_BUG_ON(cardnum &lt; 0 || cardnum &gt;= SNDRV_CARDS)) return -ENXIO; sprintf(name, \"controlC%i\", cardnum); if ((err = snd_register_device(SNDRV_DEVICE_TYPE_CONTROL, card, -1, &amp;snd_ctl_f_ops, card, name)) &lt; 0) return err; return 0;&#125; 事实是调用 snd_register_device_for_dev ()： 12345678910111213141516171819202122232425262728293031[-&gt;/sound/core/sound.c]int snd_register_device_for_dev(int type, struct snd_card *card, int dev, const struct file_operations *f_ops, void *private_data, const char *name, struct device *device)&#123; int minor; struct snd_minor *preg; preg = kmalloc(sizeof *preg, GFP_KERNEL); preg-&gt;type = type; preg-&gt;card = card ? card-&gt;number : -1; preg-&gt;device = dev; preg-&gt;f_ops = f_ops; preg-&gt;private_data = private_data; preg-&gt;card_ptr = card; mutex_lock(&amp;sound_mutex);#ifdef CONFIG_SND_DYNAMIC_MINORS minor = snd_find_free_minor(type);#else minor = snd_kernel_minor(type, card, dev);#endif ...... snd_minors[minor] = preg; preg-&gt;dev = device_create(sound_class, device, MKDEV(major, minor), private_data, \"%s\", name); ...... mutex_unlock(&amp;sound_mutex); return 0;&#125; 分配并初始化一个 snd_minor 实例；保存该 snd_minor 实例到 snd_minors 数组中；调用 device_create() 生成设备文件节点。 上面过程是声卡注册时才被回调的。 6.4. 声卡的注册当声卡下的所有逻辑设备都已经准备就绪后，就可以调用 snd_card_register() 注册声卡了： • 创建声卡的 sysfs 设备；• 调用 snd_device_register_all() 注册所有挂在该声卡下的逻辑设备；• 建立 proc 信息文件和 sysfs 属性文件。 （七）、DAPM分析7.1、DAPM简介 DAPM是Dynamic Audio Power Management的缩写，直译过来就是动态音频电源管理的意思，DAPM是为了使基于linux的移动设备上的音频子系统，在任何时候都工作在最小功耗状态下。DAPM对用户空间的应用程序来说是透明的，所有与电源相关的开关都在ASoc core中完成。用户空间的应用程序无需对代码做出修改，也无需重新编译，DAPM根据当前激活的音频流（playback/capture）和声卡中的mixer等的配置来决定那些音频控件的电源开关被打开或关闭。 DAPM是基于kcontrol改进过后的相应框架，增加了相应的电源管理机制，其电源管理机制其实就是按照相应的音频路径，完美的对各种部件的电源进行控制，而且按照某种顺序进行。 7.1、kcontrol通常，一个kcontrol代表着一个mixer（混音器），或者是一个mux（多路开关），又或者是一个音量控制器等等。 从上述文章中我们知道，定义一个kcontrol主要就是定义一个snd_kcontrol_new 结构， 123456789101112131415161718[-&gt;/include/sound/control.h]struct snd_kcontrol_new &#123; snd_ctl_elem_iface_t iface; /* interface identifier */ unsigned int device; /* device/client number */ unsigned int subdevice; /* subdevice (substream) number */ const unsigned char *name; /* ASCII name of item */ unsigned int index; /* index of item */ unsigned int access; /* access rights */ unsigned int count; /* count of same elements */ snd_kcontrol_info_t *info; snd_kcontrol_get_t *get; snd_kcontrol_put_t *put; union &#123; snd_kcontrol_tlv_rw_t *c; const unsigned int *p; &#125; tlv; unsigned long private_value;&#125;; 对于每个控件，我们需要定义一个和他对应的snd_kcontrol_new结构，这些snd_kcontrol_new结构会在声卡的初始化阶段，通过snd_soc_dapm_new_controls()函数注册到系统中，用户空间就可以通过tinymix查看和设定这些控件的状态。编译/external/tinyalsa/得到tinymix, tinyplay, tinycap，Push到手机执行tinymix可得到如下类似信息。 123456789101112131415......990 BOOL 1 QUAT_MI2S_RX Audio Mixer MultiMedia10 Off991 BOOL 1 QUAT_MI2S_RX Audio Mixer MultiMedia11 Off992 BOOL 1 QUAT_MI2S_RX Audio Mixer MultiMedia12 Off993 BOOL 1 QUAT_MI2S_RX Audio Mixer MultiMedia13 Off994 BOOL 1 QUAT_MI2S_RX Audio Mixer MultiMedia14 Off995 BOOL 1 QUAT_MI2S_RX Audio Mixer MultiMedia15 Off996 BOOL 1 QUAT_MI2S_RX Audio Mixer MultiMedia16 Off997 BOOL 1 MI2S_RX Audio Mixer MultiMedia1 Off998 BOOL 1 MI2S_RX Audio Mixer MultiMedia2 Off999 BOOL 1 MI2S_RX Audio Mixer MultiMedia3 Off1000 BOOL 1 MI2S_RX Audio Mixer MultiMedia4 Off1001 BOOL 1 MI2S_RX Audio Mixer MultiMedia5 Off1002 BOOL 1 MI2S_RX Audio Mixer MultiMedia6 Off...... snd_kcontrol_new结构中，几个主要的字段是get，put，private_value，get回调函数用于获取该控件当前的状态值，而put回调函数则用于设置控件的状态值，而private_value字段则根据不同的控件类型有不同的意义，比如对于普通的控件，private_value字段可以用来定义该控件所对应的寄存器的地址以及对应的控制位在寄存器中的位置信息。值得庆幸的是，ASoc系统已经为我们准备了大量的宏定义，用于定义常用的控件，这些宏定义位于include/sound/soc.h中。下面我们分别讨论一下如何用这些预设的宏定义来定义一些常用的控件。 7.1.1、简单型的控件SOC_SINGLE SOC_SINGLE应该算是最简单的控件了，这种控件只有一个控制量，比如一个开关，或者是一个数值变量（比如Codec中某个频率，FIFO大小等等）。我们看看这个宏是如何定义的：123456[-&gt;/include/sound/soc.h]#define SOC_SINGLE(xname, reg, shift, max, invert) \\&#123; .iface = SNDRV_CTL_ELEM_IFACE_MIXER, .name = xname, \\ .info = snd_soc_info_volsw, .get = snd_soc_get_volsw,\\ .put = snd_soc_put_volsw, \\ .private_value = SOC_SINGLE_VALUE(reg, shift, max, invert, 0) &#125; 宏定义的参数分别是：xname（该控件的名字），reg（该控件对应的寄存器的地址），shift（控制位在寄存器中的位移），max（控件可设置的最大值），invert（设定值是否逻辑取反）。这里又使用了一个宏来定义private_value字段：SOC_SINGLE_VALUE，我们看看它的定义： 12345678[-&gt;/include/sound/soc.h]#define SOC_DOUBLE_VALUE(xreg, shift_left, shift_right, xmax, xinvert, xautodisable) \\ ((unsigned long)&amp;(struct soc_mixer_control) \\ &#123;.reg = xreg, .rreg = xreg, .shift = shift_left, \\ .rshift = shift_right, .max = xmax, .platform_max = xmax, \\ .invert = xinvert, .autodisable = xautodisable&#125;)#define SOC_SINGLE_VALUE(xreg, xshift, xmax, xinvert, xautodisable) \\ SOC_DOUBLE_VALUE(xreg, xshift, xshift, xmax, xinvert, xautodisable) 这里实际上是定义了一个soc_mixer_control结构，然后把该结构的地址赋值给了private_value字段，soc_mixer_control结构是这样的： 12345678910[-&gt;/include/sound/soc.h]/* mixer control */struct soc_mixer_control &#123; int min, max, platform_max; int reg, rreg; unsigned int shift, rshift; unsigned int sign_bit; unsigned int invert:1; unsigned int autodisable:1;&#125;; 看来soc_mixer_control是控件特征的真正描述者，它确定了该控件对应寄存器的地址，位移值，最大值和是否逻辑取反等特性，控件的put回调函数和get回调函数需要借助该结构来访问实际的寄存器。SOC_SINGLE_TLV SOC_SINGLE_TLV是SOC_SINGLE的一种扩展，主要用于定义那些有增益控制的控件，例如音量控制器，EQ均衡器等等。 123456789[-&gt;/include/sound/soc.h]#define SOC_SINGLE_TLV(xname, reg, shift, max, invert, tlv_array) \\&#123; .iface = SNDRV_CTL_ELEM_IFACE_MIXER, .name = xname, \\ .access = SNDRV_CTL_ELEM_ACCESS_TLV_READ |\\ SNDRV_CTL_ELEM_ACCESS_READWRITE,\\ .tlv.p = (tlv_array), \\ .info = snd_soc_info_volsw, .get = snd_soc_get_volsw,\\ .put = snd_soc_put_volsw, \\ .private_value = SOC_SINGLE_VALUE(reg, shift, max, invert, 0) &#125; 从他的定义可以看出，用于设定寄存器信息的private_value字段的定义和SOC_SINGLE是一样的，甚至put、get回调函数也是使用同一套，唯一不同的是增加了一个tlv_array参数，并把它赋值给了tlv.p字段。用户空间可以通过对声卡的control设备发起以下两种ioctl来访问tlv字段所指向的数组： • SNDRV_CTL_IOCTL_TLV_READ • SNDRV_CTL_IOCTL_TLV_WRITE • SNDRV_CTL_IOCTL_TLV_COMMAND SOC_DOUBLE 与SOC_SINGLE相对应，区别是SOC_SINGLE只控制一个变量，而SOC_DOUBLE则可以同时在一个寄存器中控制两个相似的变量，最常用的就是用于一些立体声的控件，我们需要同时对左右声道进行控制，因为多了一个声道，参数也就相应地多了一个shift位移值 SOC_DOUBLE_R 与SOC_DOUBLE类似，对于左右声道的控制寄存器不一样的情况，使用SOC_DOUBLE_R来定义，参数中需要指定两个寄存器地址。SOC_DOUBLE_TLV 与SOC_SINGLE_TLV对应的立体声版本，通常用于立体声音量控件的定义。 SOC_DOUBLE_R_TLV 左右声道有独立寄存器控制的SOC_DOUBLE_TLV版本 7.1.2、Mixer控件Mixer控件用于音频通道的路由控制，由多个输入和一个输出组成，多个输入可以自由地混合在一起，形成混合后的输出： 对于Mixer控件，我们可以认为是多个简单控件的组合，通常，我们会为mixer的每个输入端都单独定义一个简单控件来控制该路输入的开启和关闭，反应在代码上，就是定义一个soc_kcontrol_new数组： 1234567[-&gt;/sound/soc/codecs/wcd9335.c]static const struct snd_kcontrol_new aif4_vi_mixer[] = &#123; SOC_SINGLE_EXT(\"SPKR_VI_1\", SND_SOC_NOPM, TASHA_TX14, 1, 0, tasha_vi_feed_mixer_get, tasha_vi_feed_mixer_put), SOC_SINGLE_EXT(\"SPKR_VI_2\", SND_SOC_NOPM, TASHA_TX15, 1, 0, tasha_vi_feed_mixer_get, tasha_vi_feed_mixer_put),&#125;; 7.1.3、Mux控件mux控件与mixer控件类似，也是多个输入端和一个输出端的组合控件，与mixer控件不同的是，mux控件的多个输入端同时只能有一个被选中。因此，mux控件所对应的寄存器，通常可以设定一段连续的数值，每个不同的数值对应不同的输入端被打开，与上述的mixer控件不同，ASoc用soc_enum结构来描述mux控件的寄存器信息： 1234567891011[-&gt;/include/sound/soc.h]/* enumerated kcontrol */struct soc_enum &#123; int reg; unsigned char shift_l; unsigned char shift_r; unsigned int items; unsigned int mask; const char * const *texts; const unsigned int *values;&#125;; 两个寄存器地址和位移字段：reg，reg2，shift_l，shift_r，用于描述左右声道的控制寄存器信息。字符串数组指针用于描述每个输入端对应的名字，value字段则指向一个数组，该数组定义了寄存器可以选择的值，每个值对应一个输入端，如果value是一组连续的值，通常我们可以忽略values参数。 7.2、widget、path、route前面一节中，我们介绍了音频驱动中对基本控制单元的封装：kcontrol。利用kcontrol，我们可以完成对音频系统中的mixer，mux，音量控制，音效控制，以及各种开关量的控制，通过对各种kcontrol的控制，使得音频硬件能够按照我们预想的结果进行工作。同时我们可以看到，kcontrol还是有以下几点不足：只能描述自身，无法描述各个kcontrol之间的连接关系；没有相应的电源管理机制；没有相应的时间处理机制来响应播放、停止、上电、下电等音频事件；为了防止pop-pop声，需要用户程序关注各个kcontrol上电和下电的顺序；当一个音频路径不再有效时，不能自动关闭该路径上的所有的kcontrol；为此，DAPM框架正是为了要解决以上这些问题而诞生的，DAPM目前已经是ASoc中的重要组成部分，让我们先从DAPM的数据结构开始，了解它的设计思想和工作原理。 7.2.1、DAPM的基本单元：widget文章的开头，我们说明了一下目前kcontrol的一些不足，而DAPM框架为了解决这些问题，引入了widget这一概念，所谓widget，其实可以理解为是kcontrol的进一步升级和封装，她同样是指音频系统中的某个部件，比如mixer，mux，输入输出引脚，电源供应器等等，甚至，我们可以定义虚拟的widget，例如playback stream widget。widget把kcontrol和动态电源管理进行了有机的结合，同时还具备音频路径的连结功能，一个widget可以与它相邻的widget有某种动态的连结关系。在DAPM框架中，widget用结构体snd_soc_dapm_widget来描述： 123456789101112131415161718192021222324252627282930313233343536[-&gt;/include/sound/soc-dapm.h]/* dapm widget */struct snd_soc_dapm_widget &#123; enum snd_soc_dapm_type id; const char *name; /* widget name */ const char *sname; /* stream name */ struct snd_soc_codec *codec; struct list_head list; struct snd_soc_dapm_context *dapm; void *priv; /* widget specific data */ struct regulator *regulator; /* attached regulator */ const struct snd_soc_pcm_stream *params; /* params for dai links */ /* dapm control */ int reg; /* negative reg = no direct dapm */ unsigned char shift; /* bits to shift */ unsigned int mask; /* non-shifted mask */ unsigned int on_val; /* on state value */ unsigned int off_val; /* off state value */ unsigned char power:1; /* block power status */ unsigned char active:1; /* active stream on DAC, ADC's */ unsigned char connected:1; /* connected codec pin */ unsigned char new:1; /* cnew complete */ unsigned char ext:1; /* has external widgets */ unsigned char force:1; /* force state */ unsigned char ignore_suspend:1; /* kept enabled over suspend */ unsigned char new_power:1; /* power from this run */ unsigned char power_checked:1; /* power checked this run */ int subseq; /* sort within widget type */ ...... /* widget input and outputs */ struct list_head sources; struct list_head sinks; ......&#125;; snd_soc_dapm_widget结构比较大，为了简洁一些，这里我没有列出该结构体的完整字段，不过不用担心，下面我会说明每个字段的意义：id 该widget的类型值，比如snd_soc_dapm_output，snd_soc_dapm_mixer等等。 *name 该widget的名字 *sname 代表该widget所在stream的名字，比如对于snd_soc_dapm_dai_in类型的widget，会使用该字段。 codec platform 指向该widget所属的codec和platform。 list 所有注册到系统中的widget都会通过该list，链接到代表声卡的snd_soc_card结构的widgets链表头字段中。 *dapm snd_soc_dapm_context结构指针，ASoc把系统划分为多个dapm域，每个widget属于某个dapm域，同一个域代表着同样的偏置电压供电策略，比如，同一个codec中的widget通常位于同一个dapm域，而平台上的widget可能又会位于另外一个platform域中。 *priv 有些widget可能需要一些专有的数据，可以使用该字段来保存，像snd_soc_dapm_dai_in类型的widget，会使用该字段来记住与之相关联的snd_soc_dai结构指针。 *regulator 对于snd_soc_dapm_regulator_supply类型的widget，该字段指向与之相关的regulator结构指针。 *params 目前对于snd_soc_dapm_dai_link类型的widget，指向该dai的配置信息的snd_soc_pcm_stream结构。 reg shift mask 这3个字段用来控制该widget的电源状态，分别对应控制信息所在的寄存器地址，位移值和屏蔽值。 value on_val off_val 电源状态的当前只，开启时和关闭时所对应的值。 power invert 用于指示该widget当前是否处于上电状态，invert则用于表明power字段是否需要逻辑反转。 active connected 分别表示该widget是否处于激活状态和连接状态，当和相邻的widget有连接关系时，connected位会被置1，否则置0。 new 我们定义好的widget（snd_soc_dapm_widget结构），在注册到声卡中时需要进行实例化，该字段用来表示该widget是否已经被实例化。 ext 表示该widget当前是否有外部连接，比如连接mic，耳机，喇叭等等。 force 该位被设置后，将会不管widget当前的状态，强制更新至新的电源状态。 ignore_suspend new_power power_checked 这些电源管理相关的字段。 subseq 该widget目前在上电或下电队列中的排序编号，为了防止在上下电的过程中出现pop-pop声，DAPM会给每个widget分配合理的上下电顺序。 *power_check 用于检查该widget是否应该上电或下电的回调函数指针。event_flags 该字段是一个位或字段，每个位代表该widget会关注某个DAPM事件通知。只有被关注的通知事件会被发送到widget的事件处理回调函数中。 *event DAPM事件处理回调函数指针。 num_kcontrols kcontrol_news *kcontrols 这3个字段用来描述与该widget所包含的kcontrol控件，例如一个mixer控件或者是一个mux控件。 sources sinks 两个链表字段，两个widget如果有连接关系，会通过一个snd_soc_dapm_path结构进行连接，sources链表用于链接所有的输入path，sinks链表用于链接所有的输出path。 power_list 每次更新整个dapm的电源状态时，会根据一定的算法扫描所有的widget，然后把需要变更电源状态的widget利用该字段链接到一个上电或下电的链表中，扫描完毕后，dapm系统会遍历这两个链表执行相应的上电或下电操作。 dirty 链表字段，widget的状态变更后，dapm系统会利用该字段，把该widget加入到一个dirty链表中，稍后会对dirty链表进行扫描，以执行整个路径的更新。 inputs 该widget的所有有效路径中，连接到输入端的路径数量。 outputs 该widget的所有有效路径中，连接到输出端的路径数量。 *clk 对于snd_soc_dapm_clock_supply类型的widget，指向相关联的clk结构指针。 以上我们对snd_soc_dapm_widget结构的各个字段所代表的意义一一做出了说明，这里只是让大家现有个概念 7.2.2、widget的种类在DAPM框架中，把各种不同的widget划分为不同的种类，snd_soc_dapm_widget结构中的id字段用来表示该widget的种类，可选的种类都定义在一个枚举中： 123456[-&gt;/include/sound/soc-dapm.h]/* dapm widget types */enum snd_soc_dapm_type &#123; snd_soc_dapm_input = 0, /* input pin */ snd_soc_dapm_output, /* output pin */ ...... 下面我们逐个解释一下这些widget的种类：snd_soc_dapm_input 该widget对应一个输入引脚。snd_soc_dapm_output 该widget对应一个输出引脚。snd_soc_dapm_mux 该widget对应一个mux控件。snd_soc_dapm_virt_mux 该widget对应一个虚拟的mux控件。snd_soc_dapm_value_mux 该widget对应一个value类型的mux控件。snd_soc_dapm_mixer 该widget对应一个mixer控件。snd_soc_dapm_mixer_named_ctl 该widget对应一个mixer控件，但是对应的kcontrol的名字不会加入widget的名字作为前缀。snd_soc_dapm_pga 该widget对应一个pga控件（可编程增益控件）。snd_soc_dapm_out_drv 该widget对应一个输出驱动控件snd_soc_dapm_adc 该widget对应一个ADCsnd_soc_dapm_dac 该widget对应一个DACsnd_soc_dapm_micbias 该widget对应一个麦克风偏置电压控件snd_soc_dapm_mic 该widget对应一个麦克风。snd_soc_dapm_hp 该widget对应一个耳机。snd_soc_dapm_spk 该widget对应一个扬声器。snd_soc_dapm_line 该widget对应一个线路输入。snd_soc_dapm_switch 该widget对应一个模拟开关。snd_soc_dapm_vmid 该widget对应一个codec的vmid偏置电压。snd_soc_dapm_pre machine级别的专用widget，会先于其它widget执行检查操作。snd_soc_dapm_post machine级别的专用widget，会后于其它widget执行检查操作。snd_soc_dapm_supply 对应一个电源或是时钟源。snd_soc_dapm_regulator_supply 对应一个外部regulator稳压器。snd_soc_dapm_clock_supply 对应一个外部时钟源。snd_soc_dapm_aif_in 对应一个数字音频输入接口，比如I2S接口的输入端。snd_soc_dapm_aif_out 对应一个数字音频输出接口，比如I2S接口的输出端。snd_soc_dapm_siggen 对应一个信号发生器。snd_soc_dapm_dai_in 对应一个platform或codec域的输入DAI结构。snd_soc_dapm_dai_out 对应一个platform或codec域的输出DAI结构。snd_soc_dapm_dai_link 用于链接一对输入/输出DAI结构。 7.2.3、widget之间的连接器：path之前已经提到，一个widget是有输入和输出的，而且widget之间是可以动态地进行连接的，那它们是用什么来连接两个widget的呢？DAPM为我们提出了path这一概念，path相当于电路中的一根跳线，它把一个widget的输出端和另一个widget的输入端连接在一起，path用snd_soc_dapm_path结构来描述： 1234567891011121314151617181920212223[-&gt;/include/sound/soc-dapm.h]/* dapm audio path between two widgets */struct snd_soc_dapm_path &#123; const char *name; /* source (input) and sink (output) widgets */ struct snd_soc_dapm_widget *source; struct snd_soc_dapm_widget *sink; /* status */ u32 connect:1; /* source and sink widgets are connected */ u32 walked:1; /* path has been walked */ u32 walking:1; /* path is in the process of being walked */ u32 weak:1; /* path ignored for power management */ int (*connected)(struct snd_soc_dapm_widget *source, struct snd_soc_dapm_widget *sink); struct list_head list_source; struct list_head list_sink; struct list_head list_kcontrol; struct list_head list;&#125;; 当widget之间发生连接关系时，snd_soc_dapm_path作为连接者，它的source字段会指向该连接的起始端widget，而它的sink字段会指向该连接的到达端widget，还记得前面snd_soc_dapm_widget结构中的两个链表头字段：sources和sinks么？widget的输入端和输出端可能连接着多个path，所有输入端的snd_soc_dapm_path结构通过list_sink字段挂在widget的souces链表中，同样，所有输出端的snd_soc_dapm_path结构通过list_source字段挂在widget的sinks链表中。这里可能大家会被搞得晕呼呼的，一会source，一会sink，不要紧，只要记住，连接的路径是这样的：起始端widget的输出–&gt;path的输入–&gt;path的输出–&gt;到达端widget输入。另外，snd_soc_dapm_path结构的list字段用于把所有的path注册到声卡中，其实就是挂在snd_soc_card结构的paths链表头字段中。如果你要自己定义方法来检查path的当前连接状态，你可以提供自己的connected回调函数指针。 connect，walked，walking，weak是几个辅助字段，用于帮助所有path的遍历。 7.2.4、widget的连接关系：route通过上一节的内容，我们知道，一个路径的连接至少包含以下几个元素：起始端widget，跳线path，到达端widget，在DAPM中，用snd_soc_dapm_route结构来描述这样一个连接关系： 12345678910[-&gt;/include/sound/soc-dapm.h]struct snd_soc_dapm_route &#123; const char *sink; const char *control; const char *source; /* Note: currently only supported for links where source is a supply */ int (*connected)(struct snd_soc_dapm_widget *source, struct snd_soc_dapm_widget *sink);&#125;; sink指向到达端widget的名字字符串，source指向起始端widget的名字字符串，control指向负责控制该连接所对应的kcontrol名字字符串，connected回调则定义了上一节所提到的自定义连接检查回调函数。该结构的意义很明显就是：source通过一个kcontrol，和sink连接在一起，现在是否处于连接状态，请调用connected回调函数检查。这里直接使用名字字符串来描述连接关系，所有定义好的route，最后都要注册到dapm系统中，dapm会根据这些名字找出相应的widget，并动态地生成所需要的snd_soc_dapm_path结构，正确地处理各个链表和指针的关系，实现两个widget之间的连接 7.3、建立widget之间的连接关系前面我们主要着重于codec、platform、machine驱动程序中如何使用和建立dapm所需要的widget，route，这些是音频驱动开发人员必须要了解的内容，经过前几章的介绍，我们应该知道如何在alsa音频驱动的3大部分（codec、platform、machine）中，按照所使用的音频硬件结构，定义出相应的widget，kcontrol，以及必要的音频路径，而在本节中，我们将会深入dapm的核心部分，看看各个widget之间是如何建立连接关系，形成一条完整的音频路径。 前面我们已经简单地介绍过，驱动程序需要使用以下api函数创建widget： • snd_soc_dapm_new_controls()实际上，这个函数只是创建widget的第一步，它为每个widget分配内存，初始化必要的字段，然后把这些widget挂在代表声卡的snd_soc_card的widgets链表字段中。要使widget之间具备连接能力，我们还需要第二个函数：• snd_soc_dapm_new_widgets()这个函数会根据widget的信息，创建widget所需要的dapm kcontrol，这些dapm kcontol的状态变化，代表着音频路径的变化，从而影响着各个widget的电源状态。看到函数的名称可能会迷惑一下，实际上，snd_soc_dapm_new_controls的作用更多地是创建widget，而snd_soc_dapm_new_widget的作用则更多地是创建widget所包含的kcontrol，所以在我看来，这两个函数名称应该换过来叫更好！下面我们分别介绍一下这两个函数是如何工作的。 7.3.1、创建widgetsnd_soc_dapm_new_controls()函数完成widget的创建工作，并把这些创建好的widget注册在声卡的widgets链表中，我们看看他的定义： 123456789101112131415161718192021222324[-&gt;/sound/soc/soc-dapm.c]int snd_soc_dapm_new_controls(struct snd_soc_dapm_context *dapm, const struct snd_soc_dapm_widget *widget, int num)&#123; struct snd_soc_dapm_widget *w; int i; int ret = 0; mutex_lock_nested(&amp;dapm-&gt;card-&gt;dapm_mutex, SND_SOC_DAPM_CLASS_INIT); for (i = 0; i &lt; num; i++) &#123; w = snd_soc_dapm_new_control(dapm, widget); if (!w) &#123; dev_err(dapm-&gt;dev, \"ASoC: Failed to create DAPM control %s\\n\", widget-&gt;name); ret = -ENOMEM; break; &#125; widget++; &#125; mutex_unlock(&amp;dapm-&gt;card-&gt;dapm_mutex); return ret;&#125; 该函数只是简单的一个循环，为传入的widget模板数组依次调用snd_soc_dapm_new_control函数，实际的工作由snd_soc_dapm_new_control完成，继续进入该函数，看看它做了那些工作。我们之前已经说过，驱动中定义的snd_soc_dapm_widget数组，只是作为一个模板，所以，snd_soc_dapm_new_control所做的第一件事，就是为该widget重新分配内存，并把模板的内容拷贝过来： 1234567891011121314151617181920212223242526272829[-&gt;/sound/soc/soc-dapm.c]static struct snd_soc_dapm_widget *snd_soc_dapm_new_control(struct snd_soc_dapm_context *dapm, const struct snd_soc_dapm_widget *widget)&#123; struct snd_soc_dapm_widget *w; const char *prefix; int ret; if ((w = dapm_cnew_widget(widget)) == NULL) return NULL; //由dapm_cnew_widget完成内存申请和拷贝模板的动作。接下来，根据widget的类型做不同的处理： switch (w-&gt;id) &#123; case snd_soc_dapm_regulator_supply: ...... &#125; prefix = soc_dapm_prefix(dapm); //对于snd_soc_dapm_regulator_supply类型的widget，根据widget的名称获取对应的regulator结构，对于snd_soc_dapm_clock_supply类型的widget，根据widget的名称，获取对应的clock结构。接下来，根据需要，在widget的名称前加入必要的前缀： if (prefix) &#123; w-&gt;name = kasprintf(GFP_KERNEL, \"%s %s\", prefix, widget-&gt;name); if (widget-&gt;sname) w-&gt;sname = kasprintf(GFP_KERNEL, \"%s %s\", prefix, widget-&gt;sname); &#125; else &#123; w-&gt;name = kasprintf(GFP_KERNEL, \"%s\", widget-&gt;name); if (widget-&gt;sname) w-&gt;sname = kasprintf(GFP_KERNEL, \"%s\", widget-&gt;sname); &#125; ...... 当音频路径发生变化时，power_check回调会被调用，用于检查该widget的电源状态是否需要更新。power_check设置完成后，需要设置widget所属的codec、platform和dapm context，几个用于音频路径的链表也需要初始化，然后，把该widget加入到声卡的widgets链表中： 123456789[-&gt;/sound/soc/soc-dapm.c：snd_soc_dapm_new_control()w-&gt;dapm = dapm; w-&gt;codec = dapm-&gt;codec; w-&gt;platform = dapm-&gt;platform; INIT_LIST_HEAD(&amp;w-&gt;sources); INIT_LIST_HEAD(&amp;w-&gt;sinks); INIT_LIST_HEAD(&amp;w-&gt;list); INIT_LIST_HEAD(&amp;w-&gt;dirty); list_add(&amp;w-&gt;list, &amp;dapm-&gt;card-&gt;widgets); 几个链表的作用如下：sources 用于链接所有连接到该widget输入端的snd_soc_path结构sinks 用于链接所有连接到该widget输出端的snd_soc_path结构list 用于链接到声卡的widgets链表dirty 用于链接到声卡的dapm_dirty链表最后，把widget设置为connect状态： 1234[-&gt;/sound/soc/soc-dapm.c：snd_soc_dapm_new_control()/* machine layer set ups unconnected pins and insertions */ w-&gt;connected = 1; return w; connected字段代表着引脚的连接状态，目前，只有以下这些widget使用connected字段：snd_soc_dapm_outputsnd_soc_dapm_inputsnd_soc_dapm_hpsnd_soc_dapm_spksnd_soc_dapm_linesnd_soc_dapm_vmidsnd_soc_dapm_micsnd_soc_dapm_siggen驱动程序可以使用以下这些api来设置引脚的连接状态：snd_soc_dapm_enable_pinsnd_soc_dapm_force_enable_pinsnd_soc_dapm_disable_pinsnd_soc_dapm_nc_pin到此，widget已经被正确地创建并初始化，而且被挂在声卡的widgets链表中，以后我们就可以通过声卡的widgets链表来遍历所有的widget，再次强调一下snd_soc_dapm_new_controls函数所完成的主要功能：为widget分配内存，并拷贝参数中传入的在驱动中定义好的模板设置power_check回调函数把widget挂在声卡的widgets链表中 7.3.2、为widget建立dapm kcontrol定义一个widget，我们需要指定两个很重要的内容：一个是用于控制widget的电源状态的reg/shift等寄存器信息，另一个是用于控制音频路径切换的dapm kcontrol信息，这些dapm kcontrol有它们自己的reg/shift寄存器信息用于切换widget的路径连接方式。前一节的内容中，我们只是创建了widget的实例，并把它们注册到声卡的widgts链表中，但是到目前为止，包含在widget中的dapm kcontrol并没有建立起来，dapm框架在声卡的初始化阶段，等所有的widget（包括machine、platform、codec）都创建好之后，通过snd_soc_dapm_new_widgets函数，创建widget内包含的dapm kcontrol，并初始化widget的初始电源状态和音频路径的初始连接状态。我们看看声卡的初始化函数，都有那些初始化与dapm有关： 1234567891011121314151617181920212223242526272829303132333435363738394041[-&gt;/sound/soc/soc-dapm.c]static int snd_soc_instantiate_card(struct snd_soc_card *card) &#123; ...... /* card bind complete so register a sound card */ ret = snd_card_create(SNDRV_DEFAULT_IDX1, SNDRV_DEFAULT_STR1, card-&gt;owner, 0, &amp;card-&gt;snd_card); ...... card-&gt;dapm.bias_level = SND_SOC_BIAS_OFF; card-&gt;dapm.dev = card-&gt;dev; card-&gt;dapm.card = card; list_add(&amp;card-&gt;dapm.list, &amp;card-&gt;dapm_list); ...... if (card-&gt;dapm_widgets) /* 创建machine级别的widget */ snd_soc_dapm_new_controls(&amp;card-&gt;dapm, card-&gt;dapm_widgets, card-&gt;num_dapm_widgets); ...... snd_soc_dapm_link_dai_widgets(card); /* 连接dai widget */ if (card-&gt;controls) /* 建立machine级别的普通kcontrol控件 */ snd_soc_add_card_controls(card, card-&gt;controls, card-&gt;num_controls); if (card-&gt;dapm_routes) /* 注册machine级别的路径连接信息 */ snd_soc_dapm_add_routes(&amp;card-&gt;dapm, card-&gt;dapm_routes, card-&gt;num_dapm_routes); ...... if (card-&gt;fully_routed) /* 如果该标志被置位，自动把codec中没有路径连接信息的引脚设置为无用widget */ list_for_each_entry(codec, &amp;card-&gt;codec_dev_list, card_list) snd_soc_dapm_auto_nc_codec_pins(codec); snd_soc_dapm_new_widgets(card); /*初始化widget包含的dapm kcontrol、电源状态和连接状态*/ ret = snd_card_register(card-&gt;snd_card); ...... card-&gt;instantiated = 1; snd_soc_dapm_sync(&amp;card-&gt;dapm); ...... return 0; &#125; 正如我添加的注释中所示，在完成machine级别的widget和route处理之后，调用的snd_soc_dapm_new_widgets函数，来为所有已经注册的widget初始化他们所包含的dapm kcontrol，并初始化widget的电源状态和路径连接状态。下面我们看看snd_soc_dapm_new_widgets函数的工作过程。 7.3.2.1、snd_soc_dapm_new_widgets函数该函数通过声卡的widgets链表，遍历所有已经注册了的widget，其中的new字段用于判断该widget是否已经执行过snd_soc_dapm_new_widgets函数，如果num_kcontrols字段有数值，表明该widget包含有若干个dapm kcontrol，那么就需要为这些kcontrol分配一个指针数组，并把数组的首地址赋值给widget的kcontrols字段，该数组存放着指向这些kcontrol的指针，当然现在这些都是空指针，因为实际的kcontrol现在还没有被创建： 1234567891011121314[-&gt;/sound/soc/soc-dapm.c]int snd_soc_dapm_new_widgets(struct snd_soc_card *card) &#123; ...... list_for_each_entry(w, &amp;card-&gt;widgets, list) &#123; if (w-&gt;new) continue; if (w-&gt;num_kcontrols) &#123; w-&gt;kcontrols = kzalloc(w-&gt;num_kcontrols * sizeof(struct snd_kcontrol *), GFP_KERNEL); ...... &#125; 接着，对几种能影响音频路径的widget，创建并初始化它们所包含的dapm kcontrol： 12345678910111213141516171819[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_new_widgets()]switch(w-&gt;id) &#123; case snd_soc_dapm_switch: case snd_soc_dapm_mixer: case snd_soc_dapm_mixer_named_ctl: dapm_new_mixer(w); break; case snd_soc_dapm_mux: case snd_soc_dapm_virt_mux: case snd_soc_dapm_value_mux: dapm_new_mux(w); break; case snd_soc_dapm_pga: case snd_soc_dapm_out_drv: dapm_new_pga(w); break; default: break; &#125; 需要用到的创建函数分别是：dapm_new_mixer() 对于mixer类型，用该函数创建dapm kcontrol；dapm_new_mux() 对于mux类型，用该函数创建dapm kcontrol；dapm_new_pga() 对于pga类型，用该函数创建dapm kcontrol；然后，根据widget寄存器的当前值，初始化widget的电源状态，并设置到power字段中： 12345678[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_new_widgets()]/* Read the initial power state from the device */ if (w-&gt;reg &gt;= 0) &#123; val = soc_widget_read(w, w-&gt;reg) &gt;&gt; w-&gt;shift; val &amp;= w-&gt;mask; if (val == w-&gt;on_val) w-&gt;power = 1; &#125; 接着，设置new字段，表明该widget已经初始化完成，我们还要吧该widget加入到声卡的dapm_dirty链表中，表明该widget的状态发生了变化，稍后在合适的时刻，dapm框架会扫描dapm_dirty链表，统一处理所有已经变化的widget。为什么要统一处理？因为dapm要控制各种widget的上下电顺序，同时也是为了减少寄存器的读写次数（多个widget可能使用同一个寄存器）： 12345[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_new_widgets()]w-&gt;new = 1; dapm_mark_dirty(w, \"new widget\"); dapm_debugfs_add_widget(w); 最后，通过dapm_power_widgets函数，统一处理所有位于dapm_dirty链表上的widget的状态改变： 1234[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_new_widgets()]dapm_power_widgets(card, SND_SOC_DAPM_STREAM_NOP); ...... return 0; 7.3.2.2、dapm mixer kcontrol上一节中，我们提到，对于mixer类型的dapm kcontrol，我们会使用dapm_new_mixer来完成具体的创建工作，先看代码后分析： 1234567891011121314151617181920212223242526272829[-&gt;/sound/soc/soc-dapm.c]static int dapm_new_mixer(struct snd_soc_dapm_widget *w) &#123; int i, ret; struct snd_soc_dapm_path *path; /* add kcontrol */ （1） for (i = 0; i &lt; w-&gt;num_kcontrols; i++) &#123; /* match name */ （2） list_for_each_entry(path, &amp;w-&gt;sources, list_sink) &#123; /* mixer/mux paths name must match control name */ （3） if (path-&gt;name != (char *)w-&gt;kcontrol_news[i].name) continue; （4） if (w-&gt;kcontrols[i]) &#123; dapm_kcontrol_add_path(w-&gt;kcontrols[i], path); continue; &#125; （5） ret = dapm_create_or_share_mixmux_kcontrol(w, i); if (ret &lt; 0) return ret; （6） dapm_kcontrol_add_path(w-&gt;kcontrols[i], path); &#125; &#125; return 0; &#125; （1） 因为一个mixer是由多个kcontrol组成的，每个kcontrol控制着mixer的一个输入端的开启和关闭，所以，该函数会根据kcontrol的数量做循环，逐个建立对应的kcontrol。（2）（3） 之前多次提到，widget之间使用snd_soc_path进行连接，widget的sources链表保存着所有和输入端连接的snd_soc_path结构，所以我们可以用kcontrol模板中指定的名字来匹配对应的snd_soc_path结构。（4） 因为一个输入脚可能会连接多个输入源，所以可能在上一个输入源的path关联时已经创建了这个kcontrol，所以这里判断kcontrols指针数组中对应索引中的指针值，如果已经赋值，说明kcontrol已经在之前创建好了，所以我们只要简单地把连接该输入端的path加入到kcontrol的path_list链表中，并且增加一个虚拟的影子widget，该影子widget连接和输入端对应的源widget，因为使用了kcontrol本身的reg/shift等寄存器信息，所以实际上控制的是该kcontrol的开和关，这个影子widget只有在kcontrol的autodisable字段被设置的情况下才会被创建，该特性使得source的关闭时，与之连接的mixer的输入端也可以自动关闭，这个特性通过dapm_kcontrol_add_path来实现这一点： 1234567891011121314[-&gt;/sound/soc/soc-dapm.c]static void dapm_kcontrol_add_path(const struct snd_kcontrol *kcontrol, struct snd_soc_dapm_path *path) &#123; struct dapm_kcontrol_data *data = snd_kcontrol_chip(kcontrol); /* 把kcontrol连接的path加入到paths链表中 */ /* paths链表所在的dapm_kcontrol_data结构会保存在kcontrol的private_data字段中 */ list_add_tail(&amp;path-&gt;list_kcontrol, &amp;data-&gt;paths); if (data-&gt;widget) &#123; snd_soc_dapm_add_path(data-&gt;widget-&gt;dapm, data-&gt;widget, path-&gt;source, NULL, NULL); &#125; &#125; （5） 如果kcontrol之前没有被创建，则通过dapm_create_or_share_mixmux_kcontrol创建这个输入端的kcontrol，同理，kcontrol对应的影子widget也会通过dapm_kcontrol_add_path判断是否需要创建。 7.3.2.3、dapm mux kcontrol因为一个widget最多只会包含一个mux类型的damp kcontrol，所以他的创建方法稍有不同，dapm框架使用dapm_new_mux函数来创建mux类型的dapm kcontrol： 1234567891011121314151617181920212223242526[-&gt;/sound/soc/soc-dapm.c]static int dapm_new_mux(struct snd_soc_dapm_widget *w) &#123; struct snd_soc_dapm_context *dapm = w-&gt;dapm; struct snd_soc_dapm_path *path; int ret; (1) if (w-&gt;num_kcontrols != 1) &#123; dev_err(dapm-&gt;dev, \"ASoC: mux %s has incorrect number of controls\\n\", w-&gt;name); return -EINVAL; &#125; if (list_empty(&amp;w-&gt;sources)) &#123; dev_err(dapm-&gt;dev, \"ASoC: mux %s has no paths\\n\", w-&gt;name); return -EINVAL; &#125; (2) ret = dapm_create_or_share_mixmux_kcontrol(w, 0); if (ret &lt; 0) return ret; (3) list_for_each_entry(path, &amp;w-&gt;sources, list_sink) dapm_kcontrol_add_path(w-&gt;kcontrols[0], path); return 0; &#125; （1） 对于mux类型的widget，因为只会有一个kcontrol，所以在这里做一下判断。（2） 同样地，和mixer类型一样，也使用dapm_create_or_share_mixmux_kcontrol来创建这个kcontrol。（3） 对每个输入端所连接的path都加入dapm_kcontrol_data结构的paths链表中，并且创建一个影子widget，用于支持autodisable特性。 7.3.2.4、dapm pga kcontrol目前对于pga类型的widget，kcontrol的创建函数是个空函数，所以我们不用太关注它： 123456789[-&gt;/sound/soc/soc-dapm.c]static int dapm_new_pga(struct snd_soc_dapm_widget *w)&#123; if (w-&gt;num_kcontrols) dev_err(w-&gt;dapm-&gt;dev, \"ASoC: PGA controls not supported: '%s'\\n\", w-&gt;name); return 0;&#125; dapm_create_or_share_mixmux_kcontrol函数上面所说的mixer类型和mux类型的widget，在创建他们所包含的dapm kcontrol时，最后其实都是使用了dapm_create_or_share_mixmux_kcontrol函数来完成创建工作的，所以在这里我们有必要分析一下这个函数的工作原理。这个函数中有很大一部分代码实在处理kcontrol的名字是否要加入codec的前缀，我们会忽略这部分的代码，感兴趣的读者可以自己查看内核的代码，路径在：sound/soc/soc-dapm.c中，简化后的代码如下： 12345678910111213141516171819202122[-&gt;/sound/soc/soc-dapm.c]static int dapm_create_or_share_mixmux_kcontrol(struct snd_soc_dapm_widget *w, int kci) &#123; ...... (1) shared = dapm_is_shared_kcontrol(dapm, w, &amp;w-&gt;kcontrol_news[kci], &amp;kcontrol); (2) if (!kcontrol) &#123; (3) kcontrol = snd_soc_cnew(&amp;w-&gt;kcontrol_news[kci], NULL, name,prefix）; ...... kcontrol-&gt;private_free = dapm_kcontrol_free; (4) ret = dapm_kcontrol_data_alloc(w, kcontrol); ...... (5) ret = snd_ctl_add(card, kcontrol); ...... &#125; (6) ret = dapm_kcontrol_add_widget(kcontrol, w); ...... (7) w-&gt;kcontrols[kci] = kcontrol; return 0; &#125; （1） 为了节省内存，通过kcontrol名字的匹配查找，如果这个kcontrol已经在其他widget中已经创建好了，那我们不再创建，dapm_is_shared_kcontrol的参数kcontrol会返回已经创建好的kcontrol的指针。（2） 如果kcontrol指针被赋值，说明在（1）中查找到了其他widget中同名的kcontrol，我们不用再次创建，只要共享该kcontrol即可。（3） 标准的kcontrol创建函数，（4） 如果widget支持autodisable特性，创建与该kcontrol所对应的影子widget，该影子widget的类型是：snd_soc_dapm_kcontrol。（5） 标准的kcontrol创建函数，（6） 把所有共享该kcontrol的影子widget（snd_soc_dapm_kcontrol），加入到kcontrol的private_data字段所指向的dapm_kcontrol_data结构中。（7） 把创建好的kcontrol指针赋值到widget的kcontrols数组中。需要注意的是，如果kcontol支持autodisable特性，一旦kcontrol由于source的关闭而被自动关闭，则用户空间只能操作该kcontrol的cache值，只有该kcontrol再次打开时，该cache值才会被真正地更新到寄存器中。现在。我们总结一下，创建一个widget所包含的kcontrol所做的工作：• 循环每一个输入端，为每个输入端依次执行下面的一系列操作• 为每个输入端创建一个kcontrol，能共享的则直接使用创建好的kcontrol• kcontrol的private_data字段保存着这些共享widget的信息• 如果支持autodisable特性，每个输入端还要额外地创建一个虚拟的snd_soc_dapm_kcontrol类型的影子widget，该影子widget也记录在private_data字段中• 创建好的kcontrol会依次存放在widget的kcontrols数组中，供路径的控制和匹配之用。 7.3.2.5、为widget建立连接关系如果widget之间没有连接关系，dapm就无法实现动态的电源管理工作，正是widget之间有了连结关系，这些连接关系形成了一条所谓的完成的音频路径，dapm可以顺着这条路径，统一控制路径上所有widget的电源状态，前面我们已经知道，widget之间是使用snd_soc_path结构进行连接的，驱动要做的是定义一个snd_soc_route结构数组，该数组的每个条目描述了目的widget的和源widget的名称，以及控制这个连接的kcontrol的名称，最终，驱动程序使用api函数snd_soc_dapm_add_routes来注册这些连接信息，接下来我们就是要分析该函数的具体实现方式： 12345678910111213141516[-&gt;/sound/soc/soc-dapm.c]int snd_soc_dapm_add_routes(struct snd_soc_dapm_context *dapm, const struct snd_soc_dapm_route *route, int num) &#123; int i, r, ret = 0; mutex_lock_nested(&amp;dapm-&gt;card-&gt;dapm_mutex, SND_SOC_DAPM_CLASS_INIT); for (i = 0; i &lt; num; i++) &#123; r = snd_soc_dapm_add_route(dapm, route); ...... route++; &#125; mutex_unlock(&amp;dapm-&gt;card-&gt;dapm_mutex); return ret; &#125; 该函数只是一个循环，依次对参数传入的数组调用snd_soc_dapm_add_route，主要的工作由snd_soc_dapm_add_route完成。我们进入snd_soc_dapm_add_route函数看看： 12345678910111213141516171819202122[-&gt;/sound/soc/soc-dapm.c]static int snd_soc_dapm_add_route(struct snd_soc_dapm_context *dapm, const struct snd_soc_dapm_route *route) &#123; struct snd_soc_dapm_widget *wsource = NULL, *wsink = NULL, *w; struct snd_soc_dapm_widget *wtsource = NULL, *wtsink = NULL; const char *sink; const char *source; ...... list_for_each_entry(w, &amp;dapm-&gt;card-&gt;widgets, list) &#123; if (!wsink &amp;&amp; !(strcmp(w-&gt;name, sink))) &#123; wtsink = w; if (w-&gt;dapm == dapm) wsink = w; continue; &#125; if (!wsource &amp;&amp; !(strcmp(w-&gt;name, source))) &#123; wtsource = w; if (w-&gt;dapm == dapm) wsource = w; &#125; &#125; 上面的代码我再次省略了关于名称前缀的处理部分。我们可以看到，用widget的名字来比较，遍历声卡的widgets链表，找出源widget和目的widget的指针，这段代码虽然正确，但我总感觉少了一个判断退出循环的条件，如果链表的开头就找到了两个widget，还是要遍历整个链表才结束循环，好浪费时间。下面，如果在本dapm context中没有找到，则使用别的dapm context中找到的widget： 12345[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_add_route()]if (!wsink) wsink = wtsink; if (!wsource) wsource = wtsource; 最后，使用来增加一条连接信息： 1234567[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_add_route()] ret = snd_soc_dapm_add_path(dapm, wsource, wsink, route-&gt;control, route-&gt;connected); ...... return 0; &#125; snd_soc_dapm_add_path函数是整个调用链条中的关键，我们来分析一下： 123456789101112131415161718192021[-&gt;/sound/soc/soc-dapm.c]static int snd_soc_dapm_add_path(struct snd_soc_dapm_context *dapm, struct snd_soc_dapm_widget *wsource, struct snd_soc_dapm_widget *wsink, const char *control, int (*connected)(struct snd_soc_dapm_widget *source, struct snd_soc_dapm_widget *sink)) &#123; struct snd_soc_dapm_path *path; int ret; path = kzalloc(sizeof(struct snd_soc_dapm_path), GFP_KERNEL); if (!path) return -ENOMEM; path-&gt;source = wsource; path-&gt;sink = wsink; path-&gt;connected = connected; INIT_LIST_HEAD(&amp;path-&gt;list); INIT_LIST_HEAD(&amp;path-&gt;list_kcontrol); INIT_LIST_HEAD(&amp;path-&gt;list_source); INIT_LIST_HEAD(&amp;path-&gt;list_sink); 函数的一开始，首先为这个连接分配了一个snd_soc_path结构，path的source和sink字段分别指向源widget和目的widget，connected字段保存connected回调函数，初始化几个snd_soc_path结构中的几个链表。 12345678910111213141516[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_add_path()]/* check for external widgets */ if (wsink-&gt;id == snd_soc_dapm_input) &#123; if (wsource-&gt;id == snd_soc_dapm_micbias || wsource-&gt;id == snd_soc_dapm_mic || wsource-&gt;id == snd_soc_dapm_line || wsource-&gt;id == snd_soc_dapm_output) wsink-&gt;ext = 1; &#125; if (wsource-&gt;id == snd_soc_dapm_output) &#123; if (wsink-&gt;id == snd_soc_dapm_spk || wsink-&gt;id == snd_soc_dapm_hp || wsink-&gt;id == snd_soc_dapm_line || wsink-&gt;id == snd_soc_dapm_input) wsource-&gt;ext = 1; &#125; 这段代码用于判断是否有外部连接关系，如果有，置位widget的ext字段。判断方法从代码中可以方便地看出：目的widget是一个输入脚，如果源widget是mic、line、micbias或output，则认为目的widget具有外部连接关系。源widget是一个输出脚，如果目的widget是spk、hp、line或input，则认为源widget具有外部连接关系。 123456789101112[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_add_path()]dapm_mark_dirty(wsource, \"Route added\"); dapm_mark_dirty(wsink, \"Route added\"); /* connect static paths */ if (control == NULL) &#123; list_add(&amp;path-&gt;list, &amp;dapm-&gt;card-&gt;paths); list_add(&amp;path-&gt;list_sink, &amp;wsink-&gt;sources); list_add(&amp;path-&gt;list_source, &amp;wsource-&gt;sinks); path-&gt;connect = 1; return 0; &#125; 因为增加了连结关系，所以把源widget和目的widget加入到dapm_dirty链表中。如果没有kcontrol来控制该连接关系，则这是一个静态连接，直接用path把它们连接在一起。在接着往下看： 12345678910111213141516171819202122232425262728[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_add_path()]/* connect dynamic paths */ switch (wsink-&gt;id) &#123; case snd_soc_dapm_adc: case snd_soc_dapm_dac: case snd_soc_dapm_pga: case snd_soc_dapm_out_drv: case snd_soc_dapm_input: case snd_soc_dapm_output: case snd_soc_dapm_siggen: case snd_soc_dapm_micbias: case snd_soc_dapm_vmid: case snd_soc_dapm_pre: case snd_soc_dapm_post: case snd_soc_dapm_supply: case snd_soc_dapm_regulator_supply: case snd_soc_dapm_clock_supply: case snd_soc_dapm_aif_in: case snd_soc_dapm_aif_out: case snd_soc_dapm_dai_in: case snd_soc_dapm_dai_out: case snd_soc_dapm_dai_link: case snd_soc_dapm_kcontrol: list_add(&amp;path-&gt;list, &amp;dapm-&gt;card-&gt;paths); list_add(&amp;path-&gt;list_sink, &amp;wsink-&gt;sources); list_add(&amp;path-&gt;list_source, &amp;wsource-&gt;sinks); path-&gt;connect = 1; return 0; 按照目的widget来判断，如果属于以上这些类型，直接把它们连接在一起即可，这段感觉有点多余，因为通常以上这些类型的widget本来也没有kcontrol，直接用上一段代码就可以了，也许是dapm的作者们想着以后可能会有所扩展吧。 12345678910111213141516[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_add_path()]case snd_soc_dapm_mux: case snd_soc_dapm_virt_mux: case snd_soc_dapm_value_mux: ret = dapm_connect_mux(dapm, wsource, wsink, path, control, &amp;wsink-&gt;kcontrol_news[0]); if (ret != 0) goto err; break; case snd_soc_dapm_switch: case snd_soc_dapm_mixer: case snd_soc_dapm_mixer_named_ctl: ret = dapm_connect_mixer(dapm, wsource, wsink, path, control); if (ret != 0) goto err; break; 目的widget如果是mixer和mux类型，分别用dapm_connect_mixer和dapm_connect_mux函数完成连接工作，这两个函数我们后面再讲。 1234567891011121314151617[-&gt;/sound/soc/soc-dapm.c:snd_soc_dapm_add_path()] case snd_soc_dapm_hp: case snd_soc_dapm_mic: case snd_soc_dapm_line: case snd_soc_dapm_spk: list_add(&amp;path-&gt;list, &amp;dapm-&gt;card-&gt;paths); list_add(&amp;path-&gt;list_sink, &amp;wsink-&gt;sources); list_add(&amp;path-&gt;list_source, &amp;wsource-&gt;sinks); path-&gt;connect = 0; return 0; &#125; return 0; err: kfree(path); return ret; &#125; hp、mic、line和spk这几种widget属于外部器件，也只是简单地连接在一起，不过connect字段默认为是未连接状态。现在，我们回过头来看看目的widget是mixer和mux这两种类型时的连接方式：dapm_connect_mixer 用该函数连接一个目的widget为mixer类型的所有输入端： 1234567891011121314151617181920[-&gt;/sound/soc/soc-dapm.c]static int dapm_connect_mixer(struct snd_soc_dapm_context *dapm, struct snd_soc_dapm_widget *src, struct snd_soc_dapm_widget *dest, struct snd_soc_dapm_path *path, const char *control_name) &#123; int i; /* search for mixer kcontrol */ for (i = 0; i &lt; dest-&gt;num_kcontrols; i++) &#123; if (!strcmp(control_name, dest-&gt;kcontrol_news[i].name)) &#123; list_add(&amp;path-&gt;list, &amp;dapm-&gt;card-&gt;paths); list_add(&amp;path-&gt;list_sink, &amp;dest-&gt;sources); list_add(&amp;path-&gt;list_source, &amp;src-&gt;sinks); path-&gt;name = dest-&gt;kcontrol_news[i].name; dapm_set_path_status(dest, path, i); return 0; &#125; &#125; return -ENODEV; &#125; 用需要用来连接的kcontrol的名字，和目的widget中的kcontrol模板数组中的名字相比较，找出该kcontrol在widget中的编号，path的名字设置为该kcontrol的名字，然后用dapm_set_path_status函数来初始化该输入端的连接状态。连接两个widget的链表操作和其他widget是一样的。 dapm_connect_mux 用该函数连接一个目的widget是mux类型的所有输入端： 12345678910111213141516171819202122[-&gt;/sound/soc/soc-dapm.c]static int dapm_connect_mux(struct snd_soc_dapm_context *dapm, struct snd_soc_dapm_widget *src, struct snd_soc_dapm_widget *dest, struct snd_soc_dapm_path *path, const char *control_name, const struct snd_kcontrol_new *kcontrol) &#123; struct soc_enum *e = (struct soc_enum *)kcontrol-&gt;private_value; int i; for (i = 0; i &lt; e-&gt;max; i++) &#123; if (!(strcmp(control_name, e-&gt;texts[i]))) &#123; list_add(&amp;path-&gt;list, &amp;dapm-&gt;card-&gt;paths); list_add(&amp;path-&gt;list_sink, &amp;dest-&gt;sources); list_add(&amp;path-&gt;list_source, &amp;src-&gt;sinks); path-&gt;name = (char*)e-&gt;texts[i]; dapm_set_path_status(dest, path, 0); return 0; &#125; &#125; return -ENODEV; &#125; 和mixer类型一样用名字进行匹配，只不过mux类型的kcontrol只需一个，所以要通过private_value字段所指向的soc_enum结构找出匹配的输入脚编号，最后也是通过dapm_set_path_status函数来初始化该输入端的连接状态，因为只有一个kcontrol，所以第三个参数是0。连接两个widget的链表操作和其他widget也是一样的。dapm_set_path_status 该函数根据传入widget中的kcontrol编号，读取实际寄存器的值，根据寄存器的值来初始化这个path是否处于连接状态，详细的代码这里就不贴了。当widget之间通过path进行连接之后，他们之间的关系就如下图所示： 到这里为止，我们为声卡创建并初始化好了所需的widget，各个widget也通过path连接在了一起，接下来，dapm等待用户的指令，一旦某个dapm kcontrol被用户空间改变，利用这些连接关系，dapm会重新创建音频路径，脱离音频路径的widget会被下电，加入音频路径的widget会被上电，所有的上下电动作都会自动完成，用户空间的应用程序无需关注这些变化，它只管按需要改变某个dapm kcontrol即可。 （八）、tinyplay playback、capture8.1、tinyplay playback 有时序图可知：主要涉及pcm_open()、pcm_write()、pcm_prepare()、pcm_start()。 8.1.1、使用耳机播放 启动音频播放 启用 Rx codec 路径tinymix ‘RX1 MIX1 INP1’ ‘RX1’tinymix ‘RX2 MIX1 INP1’ ‘RX2’tinymix ‘RDAC2 MUX’ ‘RX2’tinymix ‘HPHL’ ‘Switch’tinymix ‘HPHR’ ‘Switch’tinymix ‘MI2S_RX Channels’ ‘Two 启用用于通过 MI2S 接口进行播放的 DSP AFEtinymix ‘PRI_MI2S_RX Audio Mixer MultiMedia1’ 1 播放 PCM 音频tinyplay 停止音频播放 禁用接收 Rx codec 路径tinymix ‘RX1 MIX1 INP1’ ‘ZERO’tinymix ‘RX2 MIX1 INP1’ ‘ZERO’tinymix ‘RDAC2 MUX’ ‘ZERO’tinymix ‘HPHL’ ‘ZERO’tinymix ‘HPHR’ ‘ZERO’tinymix ‘MI2S_RX Channels’ ‘One’ 禁用用于通过 I2S 接口进行音频播放的 DSP AFEtinymix ‘PRI_MI2S_RX Audio Mixer MultiMedia1’ 0 8.2、tinyplay capture 有时序图可知：主要涉及pcm_open()、pcm_read()、pcm_start()。 8.2.1、使用音频录制 输入以下命令：//Enable DSP AFE for Audio Recording over I2Stinymix ‘MultiMedia1 Mixer TERT_MI2S_TX’ 1//Enable Codec TX Pathtinymix ‘DEC1 MUX’ ‘ADC2’tinymix ‘ADC2 MUX’ ‘INP2’ 启动录音功能：tinycap /data/rec.wav 禁用 HeadsetX 设备 (AMIC2)：tinymix ‘MultiMedia1 Mixer TERT_MI2S_TX’ 0tinymix ‘DEC1 MUX’ ‘ZERO’tinymix ‘ADC2 MUX’ ‘ZERO’ （九）、参考资料(特别感谢各位前辈的分析和图示)：Android音频模块启动流程分析Jhuster的专栏​ Android音频开发高通audio offload学习 | ThinkingDroidPhone的专栏 - CSDN博客alsa音频架构1-CSDN博客alsa音频架构2-ASoc - CSDN博客alsa音频架构3-Pcm - CSDN博客alsa音频架构4-声卡控制 - CSDN博客Linux ALSA 音频系统：逻辑设备篇 - CSDN博客Linux ALSA 音频系统：物理链路篇 - CSDN博客专栏：MultiMedia框架总结(基于6.0源码) - CSDN博客Android 音频系统：从 AudioTrack 到 AudioFlinger - CSDN博客AZURE - CSDN博客 - ALSA-Android AudioAZURE - CSDN博客 - ANDROID音频系统Audio驱动总结–ALSA | Winddoing’s Blogaudio HAL - 牧 天 - 博客园林学森的Android专栏 - CSDN博客深入剖析Android音频 - CSDN博客Yangwen123播放框架 - 标签 - Tocy - 博客园Android-7.0-Nuplayer概述 - CSDN博客Android-7.0-Nuplayer-启动流程 - CSDN博客Android Media Player 框架分析-Nuplayer（1） - CSDN博客Android Media Player 框架分析-AHandler AMessage ALooper - CSDN博客Android N Audio播放 start真面目- (六篇) CSDN博客深入理解Android音视频同步机制（五篇）NuPlayer的avsync逻辑 - CSDN博客wangyf的专栏 - CSDN博客-MT6737 Android N 平台 Audio系统学习Android 7.0 Audio: Mediaplayer - CSDN博客Android 7.0 Audio-相关类浅析- CSDN博客Android N Audio播放六：如何读取buffer - CSDN博客Fuchsia OS中的RPC机制-FIDL - CSDN博客高通Audio中ASOC的codec驱动 - yooooooo - 博客园","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Audio System（1）：Linux && Android Audio 系统框架分析","slug":"Audio System（1）：Linux && Android Audio 系统框架分析","date":"2018-04-30T16:00:00.000Z","updated":"2018-05-09T15:03:51.779Z","comments":true,"path":"2018/05/01/Audio System（1）：Linux && Android Audio 系统框架分析/","link":"","permalink":"http://zhoujinjian.cc/2018/05/01/Audio System（1）：Linux && Android Audio 系统框架分析/","excerpt":"","text":"注：文章都是通过阅读各位前辈总结的资料、Android 7.1.2 &amp;&amp; Linux（kernel 3.18）Qualcomm平台源码、加上自己的思考分析总结出来的，其中难免有理解不对的地方，欢迎大家批评指正。文章为个人学习、研究、欣赏之用，图文内容整理自互联网，如有侵权，请联系删除，禁止转载（©Qualcomm Technologies, Inc. 版权所有），谢谢。 【博客原图链接】 Google Pixel、Pixel XL 内核代码（Kernel-3.18）： Kernel source for Pixel and Pixel XL - GitHub AOSP 源码（Android 7.1.2）： Android 系统全套源代码分享 (更新到 8.1.0_r1) （一）、音频基础知识理解音频的一些基础知识，对于我们分析整个音频系统是大有裨益的。它可以让我们从实现的层面去思考，音频系统的目的是什么，然后才是怎么样去完成这个目的 #####（1）声音有哪些重要属性呢？ 1.1、响度(Loudness)响度就是人类可以感知到的各种声音的大小，也就是音量。响度与声波的振幅有直接关系。 1.2、音调(Pitch)音调与声音的频率有关系，当声音的频率越大时，人耳所感知到的音调就越高，否则就越低。 1.3、音色(Quality)同一种乐器，使用不同的材质来制作，所表现出来的音色效果是不一样的，这是由物体本身的结构特性所决定的。 如何将各种媒体源数字化呢？ 将声波波形信号通过ADC转换成计算机支持的二进制的过程叫做音频采样(Audio Sampling)。采样(Sampling)的核心是把连续的模拟信号转换成离散的数字信号。 1.4、样本(Sample)这是我们进行采样的初始资料，比如一段连续的声音波形。 1.5、采样器(Sampler)采样器是将样本转换成终态信号的关键。它可以是一个子系统，也可以指一个操作过程，甚至是一个算法，取决于不同的信号处理场景。理想的采样器要求尽可能不产生信号失真。 1.6、量化(Quantization)采样后的值还需要通过量化，也就是将连续值近似为某个范围内有限多个离散值的处理过程。因为原始数据是模拟的连续信号，而数字信号则是离散的，它的表达范围是有限的，所以量化是必不可少的一个步骤。 1.7、编码(Coding)计算机的世界里，所有数值都是用二进制表示的，因而我们还需要把量化值进行二进制编码。这一步通常与量化同时进行。 1.8、采样率（samplerate）采样就是把模拟信号数字化的过程，不仅仅是音频需要采样，所有的模拟信号都需要通过采样转换为可以用0101来表示的数字信号，示意图如下所示： 蓝色代表模拟音频信号，红色的点代表采样得到的量化数值。 采样频率越高，红色的间隔就越密集，记录这一段音频信号所用的数据量就越大，同时音频质量也就越高。 根据奈奎斯特理论，采样频率只要不低于音频信号最高频率的两倍，就可以无损失地还原原始的声音。 通常人耳能听到频率范围大约在20Hz～20kHz之间的声音，为了保证声音不失真，采样频率应在40kHz以上。常用的音频采样频率有：8kHz、11.025kHz、22.05kHz、16kHz、37.8kHz、44.1kHz、48kHz、96kHz、192kHz等。 1.9、量化精度（位宽）上图（1.8）中，每一个红色的采样点，都需要用一个数值来表示大小，这个数值的数据类型大小可以是：4bit、8bit、16bit、32bit等等，位数越多，表示得就越精细，声音质量自然就越好，当然，数据量也会成倍增大。 常见的位宽是：8bit 或者 16bit 1.10、 声道数（channels）由于音频的采集和播放是可以叠加的，因此，可以同时从多个音频源采集声音，并分别输出到不同的扬声器，故声道数一般表示声音录制时的音源数量或回放时相应的扬声器数量。 单声道（Mono）和双声道（Stereo）比较常见，顾名思义，前者的声道数为1，后者为2 1.11、音频帧（frame）这个概念在应用开发中非常重要，网上很多文章都没有专门介绍这个概念。 音频跟视频很不一样，视频每一帧就是一张图像，而从上面的正玄波可以看出，音频数据是流式的，本身没有明确的一帧帧的概念，在实际的应用中，为了音频算法处理/传输的方便，一般约定俗成取2.5ms~60ms为单位的数据量为一帧音频。 这个时间被称之为“采样时间”，其长度没有特别的标准，它是根据编解码器和具体应用的需求来决定的，我们可以计算一下一帧音频帧的大小： 假设某音频信号是采样率为8kHz、双通道、位宽为16bit，20ms一帧，则一帧音频数据的大小为： int size = 8000 x 2 x 16bit x 0.02s = 5120 bit = 640 byte 1.12、常见的音频编码方式有哪些？上面提到过，模拟的音频信号转换为数字信号需要经过采样和量化，量化的过程被称之为编码，根据不同的量化策略，产生了许多不同的编码方式，常见的编码方式有：PCM 和 ADPCM，这些数据代表着无损的原始数字音频信号，添加一些文件头信息，就可以存储为WAV文件了，它是一种由微软和IBM联合开发的用于音频数字存储的标准，可以很容易地被解析和播放。 我们在音频开发过程中，会经常涉及到WAV文件的读写，以验证采集、传输、接收的音频数据的正确性。 1.13、常见的音频压缩格式有哪些？首先简单介绍一下音频数据压缩的最基本的原理：因为有冗余信息，所以可以压缩。 （1） 频谱掩蔽效应： 人耳所能察觉的声音信号的频率范围为20Hz～20KHz，在这个频率范围以外的音频信号属于冗余信号。 （2） 时域掩蔽效应： 当强音信号和弱音信号同时出现时，弱信号会听不到，因此，弱音信号也属于冗余信号。 下面简单列出常见的音频压缩格式： MP3，AAC，OGG，WMA，Opus，FLAC，APE，M4A，AMR，等等 1.14、奈奎斯特采样理论“当对被采样的模拟信号进行还原时，其最高频率只有采样频率的一半”。换句话说，如果我们要完整重构原始的模拟信号，则采样频率就必须是它的两倍以上。比如人的声音范围是2~ 20kHZ,那么选择的采样频率就应该在40kHZ左右，数值太小则声音将产生失真现象，而数值太大也无法明显提升人耳所能感知的音质。 1.15、总结（音频处理和播放过程）： （二）、Audio 系统框架总体Audio框架图 2.1、APP音乐播放器软件等等。 2.2、FrameworkAndroid也提供了另两个相似功能的类，即AudioTrack和AudioRecorder，MediaPlayerService内部的实现就是通过它们来完成的,只不过MediaPlayer/MediaRecorder提供了更强大的控制功能，相比前者也更易于使用。除此以外，Android系统还为我们控制音频系统提供了AudioManager、AudioService及AudioSystem类。这些都是framework为便利上层应用开发所设计的。 2.3、Librariesframework只是向应用程序提供访问Android库的桥梁，具体功能实现放在库中完成。比如上面的AudioTrack、AudioRecorder、MediaPlayer和MediaRecorder等等在库中都能找到相对应的类。 1、frameworks/av/media/libmedia【libmedia.so】2、frameworks/av/services/audioflinger【libaudioflinger.so】3、frameworks/av/media/libmediaplayerservice【libmediaplayerservice.so】 2.4、HAL从设计上来看，硬件抽象层是AudioFlinger直接访问的对象。这说明了两个问题，一方面AudioFlinger并不直接调用底层的驱动程序;另一方面，AudioFlinger上层模块只需要与它进行交互就可以实现音频相关的功能了。因而我们可以认为AudioFlinger是Android音频系统中真正的“隔离板”，无论下面如何变化，上层的实现都可以保持兼容。 音频方面的硬件抽象层主要分为两部分，即AudioFlinger和AudioPolicyService。实际上后者并不是一个真实的设备，只是采用虚拟设备的方式来让厂商可以方便地定制出自己的策略。抽象层的任务是将AudioFlinger/AudioPolicyService真正地与硬件设备关联起来，但又必须提供灵活的结构来应对变化——特别是对于Android这个更新相当频繁的系统。比如以前Android系统中的Audio系统依赖于ALSA-lib，但后期就变为了tinyalsa，这样的转变不应该对上层造成破坏。因而Audio HAL提供了统一的接口来定义它与AudioFlinger/AudioPolicyService之间的通信方式，这就是audio_hw_device、audio_stream_in及audio_stream_out等等存在的目的，这些Struct数据类型内部大多只是函数指针的定义，是一些“壳”。当AudioFlinger/AudioPolicyService初始化时，它们会去寻找系统中最匹配的实现(这些实现驻留在以audio.primary.,audio.a2dp.为名的各种库中)来填充这些“壳”。根据产品的不同，音频设备存在很大差异，在Android的音频架构中，这些问题都是由HAL层的audio.primary等等库来解决的，而不需要大规模地修改上层实现。换句话说，厂商在定制时的重点就是如何提供这部分库的高效实现了。 2.5、Tinyalsa源码在external/tinyalsa目录下Tinyalsa：tinyplay/tinycap/tinymix，这些用户程序直接调用 alsa 用户库接口来实现放音、录音、控制 2.6、Kernel部分2.6.1、ALSA 和 ASoCNative ALSA Application：tinyplay/tinycap/tinymix，这些用户程序直接调用 alsa 用户库接口来实现放音、录音、控制ALSA Library API：alsa 用户库接口，常见有 tinyalsa、alsa-libALSA CORE：alsa 核心层，向上提供逻辑设备（PCM/CTL/MIDI/TIMER/…）系统调用，向下驱动硬件设备（Machine/I2S/DMA/CODEC）ASoC CORE：asoc 是建立在标准 alsa core 基础上，为了更好支持嵌入式系统和应用于移动设备的音频 codec 的一套软件体系Hardware Driver：音频硬件设备驱动，由三大部分组成，分别是 Machine、Platform、Codec 2.6.2、ASoCASoC被分为Machine、Platform和Codec三大部分。其中的Machine驱动负责Platform和Codec之间的耦合和设备或板子特定的代码。Platform驱动的主要作用是完成音频数据的管理，最终通过CPU的数字音频接口（DAI）把音频数据传送给Codec进行处理，最终由Codec输出驱动耳机或者是喇叭的音信信号。 2.6.2.1、Machine用于描述设备组件信息和特定的控制如耳机/外放等。 是指某一款机器，可以是某款设备，某款开发板，又或者是某款智能手机，由此可以看出Machine几乎是不可重用的，每个Machine上的硬件实现可能都不一样，CPU不一样，Codec不一样，音频的输入、输出设备也不一样，Machine为CPU、Codec、输入输出设备提供了一个载体。 这一部分将平台驱动和Codec驱动绑定在一起，描述了板级的硬件特征。主要负责Platform和Codec之间的耦合以及部分和设备或板子特定的代码。Machine驱动负责处理机器特有的一些控件和音频事件（例如，当播放音频时，需要先行打开一个放大器）；单独的Platform和Codec驱动是不能工作的，它必须由Machine驱动把它们结合在一起才能完成整个设备的音频处理工作。ASoC的一切都从Machine驱动开始，包括声卡的注册，绑定Platform和Codec驱动等等 2.6.2.2、Platform用于实现平台相关的DMA驱动和音频接口等。 一般是指某一个SoC平台，比如pxaxxx,s3cxxxx,omapxxx等等，与音频相关的通常包含该SoC中的时钟、DMA、I2S、PCM等等，只要指定了SoC，那么我们可以认为它会有一个对应的Platform，它只与SoC相关，与Machine无关，这样我们就可以把Platform抽象出来，使得同一款SoC不用做任何的改动，就可以用在不同的Machine中。实际上，把Platform认为是某个SoC更好理解。 这一部分只关心CPU本身，不关心Codec。主要处理两个问题：DMA引擎和SoC集成的PCM、I2S或AC ‘97数字接口控制。主要作用是完成音频数据的管理，最终通过CPU的数字音频接口（DAI）把音频数据传送给Codec进行处理，最终由Codec输出驱动耳机或者是喇叭的音信信号。在具体实现上，ASoC有把Platform驱动分为两个部分：snd_soc_platform_driver和snd_soc_dai_driver。其中，platform_driver负责管理音频数据，把音频数据通过dma或其他操作传送至cpu dai中，dai_driver则主要完成cpu一侧的dai的参数配置，同时也会通过一定的途径把必要的dma等参数与snd_soc_platform_driver进行交互。 2.6.2.3、Codec用于实现平台无关的功能，如寄存器读写接口，音频接口，各widgets的控制接口和DAPM的实现等 字面上的意思就是编解码器，Codec里面包含了I2S接口、D/A、A/D、Mixer、PA（功放），通常包含多种输入（Mic、Line-in、I2S、PCM）和多个输出（耳机、喇叭、听筒，Line-out），Codec和Platform一样，是可重用的部件，同一个Codec可以被不同的Machine使用。嵌入式Codec通常通过I2C对内部的寄存器进行控制。 这一部分只关心Codec本身，与CPU平台相关的特性不由此部分操作。在移动设备中，Codec的作用可以归结为4种，分别是： 1、对PCM等信号进行D/A转换，把数字的音频信号转换为模拟信号。2、对Mic、Linein或者其他输入源的模拟信号进行A/D转换，把模拟的声音信号转变CPU能够处理的数字信号。3、对音频通路进行控制，比如播放音乐，收听调频收音机，又或者接听电话时，音频信号在codec内的流通路线是不一样的。4、对音频信号做出相应的处理，例如音量控制，功率放大，EQ控制等等。 ASoC对Codec的这些功能都定义好了一些列相应的接口，以方便地对Codec进行控制。ASoC对Codec驱动的一个基本要求是：驱动程序的代码必须要做到平台无关性，以方便同一个Codec的代码不经修改即可用在不同的平台上。 ASoC对于Alsa来说，就是分别注册PCM/CONTROL类型的snd_device设备，并实现相应的操作方法集。图中DAI是数字音频接口，用于配置音频数据格式等。 ☁ Codec驱动向ASoC注册snd_soc_codec和snd_soc_dai设备。☁ Platform驱动向ASoC注册snd_soc_platform和snd_soc_dai设备。☁ Machine驱动通过snd_soc_dai_link绑定codec/dai/platform。 Widget是各个组件内部的小单元。处在活动通路上电，不在活动通路下电。ASoC的DAPM正是通过控制这些Widget的上下电达到动态电源管理的效果。 ☁ path描述与其它widget的连接关系。☁ event用于通知该widget的上下电状态。☁ power指示当前的上电状态。☁ control实现空间用户接口用于控制widget的音量/通路切换等。 对驱动开者来说，就可以很好的解耦了： ☁ codec驱动的开发者，实现codec的IO读写方法，描述DAI支持的数据格式/操作方法和Widget的连接关系就可以了;☁ soc芯片的驱动开发者，Platform实现snd_pcm的操作方法集和DAI的配置如操作 DMA，I2S/AC97/PCM的设定等;☁ 板级的开发者，描述Machine上codec与platform之间的总线连接， earphone/Speaker的布线情况就可以了。 2.6.3、DAPM DAPM是Dynamic Audio Power Management的缩写，直译过来就是动态音频电源管理的意思，DAPM是为了使基于linux的移动设备上的音频子系统，在任何时候都工作在最小功耗状态下。DAPM对用户空间的应用程序来说是透明的，所有与电源相关的开关都在ASoc core中完成。用户空间的应用程序无需对代码做出修改，也无需重新编译，DAPM根据当前激活的音频流（playback/capture）和声卡中的mixer等的配置来决定那些音频控件的电源开关被打开或关闭。 2.6.4、DPCMDynamic PCM 2.7、Audio devices具体的Audio硬件设备。 （三）、Qualcomm平台 - Audio系统框架由于接下来的一系列Android &amp;&amp; kernel 源码分析都是基于Qualcomm 平台的，十分有必要介绍Qualcomm 平台的Audio 系统框架。硬件平台及软件版本：☁ Kernel - 3.18☁ SoC - Qualcomm snapdragon☁ CODEC - WCD9335☁ Machine - msm8996☁ Userspace - tinyalsa 3.1、Qualcomm Audio系统总体框架图 3.2、ASoC driverALSA 片上系统 (ASoC) 驱动程序将音频系统分为四个组成部分Machine driver、Platform driver、CPU driver、Codec driver。 3.2.1、Machine driver将平台、CPU 和编解码驱动程序整合在一起kernel/sound/soc/msm/.c定义Frontend (FE) and Backend (BE), Digital Audio Interface (DAI) links 3.2.2、Platform driver包含用于流数据传输与路由的平台特定的控件（control）， 细分为 FE 和 BE 平台驱动程序FE Audio – 实例化 PCM 播放和录制会话；借助 ASM 接口，将 PCM 数据从用户空间传输到 DSP 进行播放以及从 DSP 传输到用户空间进行录制 – 在 kernel/sound/soc/msm-pcm-q6-v2.c 中实现 Voice – 初始化/取消初始化语音呼叫设置 – 在 kernel/sound/soc/msm-pcm-voice-v2.c 中实现 VoIP – 初始化/取消初始化 MVS 接口以传输自/至 DSP 的 PCM 数据 – 在kernel/sound/soc/msm-pcmvoip-v2.c中实现 Compressed offload – 支持将压缩数据发送到 DSP 进行压缩分流播放 – 在 kernel/sound/soc/msm-compress-q6-v2.c 中实现BE 路由 – 执行音频路由任务 – 在 /kernel/sound/soc/msm-pcm-routing-v2.c 中实现 3.2.3、CPU driverFE 向 ASoC 框架提供关于 FE PCM 设备的信息 ASoC 框架与平台驱动程序提供的路由表共同将 PCM 播放/捕获从 FE 传递至 BE 没有针对播放和录制的内置逻辑 定义 FE CPU DAI – 在 kernel/sound/soc/msm/msm-dai-fe.c 中实现BE 要在初始化 PCM 播放/捕获时激活所需音频硬件端口，则配置 DSP AFE 模块 定义 BE CPU DAI – 在 kernel/sound/soc/msm/qdsp6v2/msm-dai-q6-v2.c 中实现 3.2.4、Codec driver与平台无关，其中包含音频控制、音频接口功能、编解码器 DAPM 定义以及编解码器输入输出功能 此外，实现 MBHC 状态机，用于检测有线耳机插入/拔出、附件类型、连接器类型和多按钮检测 3.3、DSP driver ASM（Audio Stream Manager） 用于与 DSP ASM 模块通信的接口 提供将 PCM 数据路由至 DSP 的机制，支持按数据流进行后期处理/预处理ADM（Audio Device Manager） 允许在 DSP 中使用 ADM 服务 配置 COPP 和路由矩阵 与音频校准数据库 (ACDB) 进行通信，使用正确的校准数据配置 COPP 将 ASM 会话 ID 路由至 ADM 会话AFE（Audio Front-End） 允许在 DSP 中使用 AFE 服务 激活/禁用音频硬件端口 子系统管理器 – 发生 MDSP 复位事件时，通知音频和语音驱动程序关闭待处理会话、执行清理操作并等待一个指示 MDSP 已启动的事件APR（Asynchronous Packet Router） 为处理器间通信提供异步框架 用于与 Hexagon 和调制解调器处理器进行通信 Image loader PIL – 载入 MDSP 图像 3.4、User Space Audio Hardware Abstraction Layer (AHAL) – 通过 tinyALSA 将 AudioFlinger 调用映射至ASoC 驱动程序的硬件抽象层。 ACDB loader – 检索特定设备的校准信息，并写入 PMEM。ACDB 驱动程序在启动过程中分配该 PMEM。在设备切换时，此校准将被发送到 DSP。 tinyALSA – 连接至内核 ASoC 驱动程序的接口，供音频 HAL 使用。提供用于音频流和设备管理的基本 PCM 和混音控件 API。 Audio route – 此模块会从一个 .xml 文件读取 ALSA 混音控件，并根据音频HAL 所选的设备设置混音控件。 Concurrency Manager - 在MSM8x10中，视频解码和编码在DSP中完成; 因此，有对可支持的并发性有一些限制。MSM8x10中引入的并发管理器管理并发性可以支持涉及语音和音频的不同用例 Multimedia framework – Stagefright 支持标准音频格式的播放/录制 与解码器/编码器库以及 OpenMAX IL 组件通信，以便进行解码和编码 Audio service 由系统服务器启动并由服务管理器管理的运行时服务之一 意图注册；当从各种应用程序（HDMI、蓝牙等）接收到这些意图时，通知音频系统 AudioFlinger 通过 libaudio 接口、蓝牙 A2DP 接口管理所有音频输出/输入设备 将多个音频流处理为单一的 PCM 音频；混合后的输出被传送到输出设备 播放音乐流时的音量 Audio Policy Manager (APM) 定义多个音频用例之间的并发规则 用例示例 – 电话通话、音乐播放、系统声音和通知 定义播放的音频（例如：语音、播放、铃声）以及播放的设备（蓝牙、扬声器和耳机）APM 用途： 管理各种输入输出设备接口 管理各种输入输出设备，例如：麦克风、扬声器、耳机、听筒、A2DP、蓝牙 SCO 基于音频流、模式和方法选择和定义适当的路由策略 管理每个音频流的音量/静音设置（在它们激活或禁用时） （四）、参考资料(特别感谢各位前辈的分析和图示)：Android音频模块启动流程分析Jhuster的专栏​ Android音频开发高通audio offload学习 | ThinkingDroidPhone的专栏 - CSDN博客alsa音频架构1-CSDN博客alsa音频架构2-ASoc - CSDN博客alsa音频架构3-Pcm - CSDN博客alsa音频架构4-声卡控制 - CSDN博客Linux ALSA 音频系统：逻辑设备篇 - CSDN博客Linux ALSA 音频系统：物理链路篇 - CSDN博客专栏：MultiMedia框架总结(基于6.0源码) - CSDN博客Android 音频系统：从 AudioTrack 到 AudioFlinger - CSDN博客AZURE - CSDN博客 - ALSA-Android AudioAZURE - CSDN博客 - ANDROID音频系统Audio驱动总结–ALSA | Winddoing’s Blogaudio HAL - 牧 天 - 博客园林学森的Android专栏 - CSDN博客深入剖析Android音频 - CSDN博客Yangwen123播放框架 - 标签 - Tocy - 博客园Android-7.0-Nuplayer概述 - CSDN博客Android-7.0-Nuplayer-启动流程 - CSDN博客Android Media Player 框架分析-Nuplayer（1） - CSDN博客Android Media Player 框架分析-AHandler AMessage ALooper - CSDN博客Android N Audio播放 start真面目- (六篇) CSDN博客深入理解Android音视频同步机制（五篇）NuPlayer的avsync逻辑 - CSDN博客wangyf的专栏 - CSDN博客-MT6737 Android N 平台 Audio系统学习Android 7.0 Audio: Mediaplayer - CSDN博客Android 7.0 Audio-相关类浅析- CSDN博客Android N Audio播放六：如何读取buffer - CSDN博客Fuchsia OS中的RPC机制-FIDL - CSDN博客高通Audio中ASOC的codec驱动 - yooooooo - 博客园","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Google Pixel Oreo 8.1 OPM2.171019.029 Root 亲测成功 [i.wonder~]","slug":"Google Pixel Oreo 8.1 OPM2.171019.029 Root 亲测成功","date":"2018-04-19T16:00:00.000Z","updated":"2018-04-21T06:06:04.591Z","comments":true,"path":"2018/04/20/Google Pixel Oreo 8.1 OPM2.171019.029 Root 亲测成功/","link":"","permalink":"http://zhoujinjian.cc/2018/04/20/Google Pixel Oreo 8.1 OPM2.171019.029 Root 亲测成功/","excerpt":"","text":"Root教程原文 Root Google Pixel Oreo 8.1 OPM2.171019.029 Install TWRP 注意：✢ 电池电量高于80％。✢ 数据会完全丢失不可恢复，请提前备份您的数据。✢ 解锁你的Bootloader。✢ 仅限Google Pixel Oreo 8.1。✢ 仅限奥利奥版本。✢ 笔者手机如图： 一、解锁Bootloader在Google Pixel Oreo 8.1 OPM2.171019.029中解锁引导加载程序bootloader 1.1、OEM解锁解锁BootLoader教程 Unlocking Bootloader in Google Pixel and Pixel XL✢ 下载（Windows）/下载（Mac）相应的ADB、Fastboot、驱动程序并将其安装到您的系统中。✢ 进入 设置-&gt;关于手机 连续点击5次版本号，直到提示”你已处于开发者模式，无需进行此操作”✢ 点击返回进入 “开发者模式” 打开 “OEM解锁” 和 “USB调试”✢ 连接手机，若弹出提示框请选择是/允许，命令行执行：adb devices，成功会有以下类似提示 1234List of devices attached* daemon not running; starting now at tcp:5037* daemon started successfullyFA7240301112 device ✢ 命令行执行：adb reboot bootloader✢ 执行：fastboot oem unlock✢ 您需要通过增大音量按钮来确认手机中的操作。您将成功解除成功消息。当您收到此消息时，请按音量键并导航开始，然后按电源按钮。手机将重新启动。启动第一次启动需要一些时间。✢ 解锁成功在开机界面会有一个打开的小锁图标 1.2、将系统升级到Oreo 8.1 OPM2.171019.029稳定版Root Google Pixel Oreo 8.1 OPM2.171019.029稳定的版本于2018年4月发布。1、Sailfish-OPM2.171019.029下载地址2、解压运行update-all.bat（Windows）/ update-all.bat（Mac） 1.3、Root Pixel Oreo 8.1 OPM2.171019.029有四个步骤✢ Install TWRP Recovery Officially in Google Pixel Oreo 8.1 OPM2.171019.029✢ Install Root Files in Google Pixel Oreo 8.1 OPM2.171019.029 Using TWRP Recovery 3.2.0✢ Install SuperSu in Google Pixel Oreo 8.1 OPM2.171019.029 Using TWRP Recovery 3.2.0✢ Install SuperSu in Google Pixel Oreo 8.1 OPM2.171019.029 Using TWRP Recovery 3.2.0 二、Install TWRP Recovery Officially in Google Pixel Oreo 8.1 OPM2.171019.029步骤：1、确保 系统OEM解锁成功、手机版本为Oreo 8.1 OPM2.171019.029，然后执行接下来的操作2、下载 twrp-3.2.1-2-sailfish.img Google Pixel Oreo 8.1 OPM2.171019.029的TWRP恢复镜像3、通过USB连接您的手机，执行：adb reboot bootloader进入BootLoader模式（或者关机状态 Power键+音量下键进入）4、确保您的手机已被系统检测到，执行：fastboot devices 会看到设备号5、fastboot boot twrp-3.2.1-2-sailfish.img6、重启进入下一步操作 三、Install Root Files in Google Pixel Oreo 8.1 OPM2.171019.029 Using TWRP Recovery 3.2.0步骤：1、下载 twrp-pixel-installer-sailfish-3.1.1-0.zip，将手机USB使用方式切换为传输文件模式，并将其复制到手机存储根目录2、关掉你的手机3、Power键+音量下键进入BootLoader模式模式 ，然后音量键选择 recovery mode，按power键进入Recovery模式4、Select Wipe-&gt;Advance Wipe-&gt;Select Data5、返回到TWRP主界面：select Install6、Select twrp-pixel-installer-sailfish-3.1.1-0.zip安装后重新启动您的手机。 四、Install SuperSu in Google Pixel Oreo 8.1 OPM2.171019.029 Using TWRP Recovery 3.2.0步骤：1、下载 SR5-SuperSU-v2.82-SR5-20171001224502.zip 将手机USB使用方式切换为传输文件模式，并将其复制到手机存储根目录2、关掉你的手机3、Power键+音量下键进入BootLoader模式模式 ，然后音量键选择 recovery mode，按power键进入Recovery模式4、Select Wipe-&gt;Advance Wipe-&gt;Select Data5、返回到TWRP主界面：select Install6、Select SR5-SuperSU-v2.82-SR5-20171001224502.zip安装后重新启动您的手机。 五、Install Magisk in Google Pixel Oreo 8.1 OPM2.171019.029 Using TWRP Recovery 3.2.0步骤：1、下载 Magisk-v16.1(1610).zip 将手机USB使用方式切换为传输文件模式，并将其复制到手机存储根目录2、关掉你的手机3、Power键+音量下键进入BootLoader模式模式 ，然后音量键选择 recovery mode，按power键进入Recovery模式4、Select Wipe-&gt;Advance Wipe-&gt;Select Data5、返回到TWRP主界面：select Install6、Select Magisk-v16.1(1610).zip安装后重新启动您的手机。 六、出现的问题（一）问题：adb 出现 device offline，更新adb版本到1.0.39（二）问题：笔者Pixel手机执行上述步骤后，无法开机，执行以下步骤就可以开机了：1、关掉你的手机2、Power键+音量下键进入BootLoader模式模式，然后音量键选择 recovery mode，按power键进入Recovery模式3、Select Wipe-&gt;Advance Wipe-&gt;Select Data4、滑动清除Data Root成功，(o゜▽゜)o☆[BINGO!] 七、参考资料(特别感谢各位前辈的辛苦奉献)：解锁BootLoader教程 Unlocking Bootloader in Google Pixel and Pixel XLRoot教程原文 Root Google Pixel Oreo 8.1 OPM2.171019.029 Install TWRP","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"个人网站(分享一个有趣的的Loading gif) [i.wonder~]","slug":"个人网站(分享一个有趣的的Loading gif)","date":"2018-04-02T16:00:00.000Z","updated":"2018-04-19T14:30:15.322Z","comments":true,"path":"2018/04/03/个人网站(分享一个有趣的的Loading gif)/","link":"","permalink":"http://zhoujinjian.cc/2018/04/03/个人网站(分享一个有趣的的Loading gif)/","excerpt":"","text":"（一）、 Loading gif：茶不思饭不想、不眠不夜折腾近两周，总算把个人网站搭建好了(๑乛◡乛๑)。 （二）、个人网站（zhoujinjian.cc）闷骚的主题，我想基本也不会有人来浏览我的个人网站，闷骚就闷骚点吧(๑乛◡乛๑)： （三）、总结个人网站先暂时告一段落了，接下来还是继续老本行分析Android 源代码，之前分析虽已大致打通 App层 -&gt; Framework层 -&gt; Native层 -&gt; Kernel层，冒似有一定经验了然并卵，路漫漫其修远兮，生命不息，学无止境（其实嘛就是 -&gt; 人丑就要多读书๑乛◡乛๑）。好想读读书去看看外面的世界啊。 where you want to go-&gt;(Castelluccio di Norcia卡斯特鲁奇奥公园,意大利): Or-&gt;(Lofoten, Reinebringen(雷訥),挪威): PS：哇哦，好美，算了我就想想（ಡωಡ）。 （四）、参考资料(特别感谢各位前辈的辛苦奉献)：Bing 壁纸 APIApi-bing-wallpaperMrminfive - Hexo-theme-skappMolunerfinn - Hexo-theme-melodyStkevintan - canoe-blog(404) The page you were looking for doesn’t exist","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://zhoujinjian.cc/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://zhoujinjian.cc/tags/Hexo/"}]},{"title":"Linux内核（Kernel-3.18） - Linux Input 子系统分析 [i.wonder~]","slug":"Linux内核（Kernel-3-18）-Input-子系统分析-i-wonder","date":"2018-03-31T16:00:00.000Z","updated":"2018-04-19T14:30:10.486Z","comments":true,"path":"2018/04/01/Linux内核（Kernel-3-18）-Input-子系统分析-i-wonder/","link":"","permalink":"http://zhoujinjian.cc/2018/04/01/Linux内核（Kernel-3-18）-Input-子系统分析-i-wonder/","excerpt":"","text":"【博客原图链接】 源码（部分）： kernel/msm-3.18/include/linux Input.h evdev.h kernel/msm-3.18/drivers/input Input.c evdev.c gpio_keys.c kernel/msm-3.18/drivers/input/touchscreen/synaptics_dsx_htc_2.6 Makefile Kconfig synaptics_dsx_core.c 【博客原图链接】 Google Pixel、Pixel XL 内核代码（Kernel-3.18）： Kernel source for Pixel and Pixel XL - Google Kernel source for Pixel and Pixel XL - GitHub （一）、Linux Input 子系统框架输入(Input)子系统是分层架构的，总共分为5 层，从上到下分别是：用户空间层（User Space）事件处理层(Event Handler)、输入子系统核心层(Input Core)、硬件驱动层(Input Driver) 、硬件设备层（Hardware）。 驱动根据CORE提供的接口，向上报告发生的按键动作。然后CORE根据驱动的类型，分派这个报告给对应的事件处理层进行处理。事件处理层把数据变化反应到设备模型的文件中（事件缓冲区）。并通知在这些设备模型文件上等待的进程。 input子系统框架： (1) “硬件驱动层”负责操作具体的硬件设备，这层的代码是针对具体的驱动程序的，比如你的设备是触摸输入设备，还是鼠标输入设备，还是键盘输入设备，这些不同的设备，自然有不同的硬件操作，驱动工程师往往只需要完成这层的代码编写。 (2) “输入子系统核心层”是链接其他两层之间的纽带与桥梁，向下提供硬件驱动层的接口，向上提供事件处理层的接口。 (3) “事件处理层” 负责与用户程序打交道，将硬件驱动层传来的事件报告给用户程序。 各层之间通信的基本单位就是事件，任何一个输入设备的动作都可以抽象成一种事件，如键盘的按下，触摸屏的按下，鼠标的移动等。事件有三种属性：类型（type），编码(code)，值(value)， Input 子系统支持的所有事件都定义在 input.h中，包括所有支持的类型，所属类型支持的编码等。事件传送的方向是 硬件驱动层–&gt;子系统核心–&gt;事件处理层–&gt;用户空间。 （二）、Input 主要通用数据结构2.1、input_dev输入设备 input_dev，这是input设备基本的设备结构，每个input驱动程序中都必须分配初始化这样一个结构 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[-&gt;input.h]struct input_dev &#123; const char *name; //输入设备的名称 const char *phys; //输入设备节点名称 const char *uniq; //指定唯一的ID号，就像MAC地址一样 struct input_id id; //输入设备标识ID，用于和事件处理层进行匹配 unsigned long propbit[BITS_TO_LONGS(INPUT_PROP_CNT)]; unsigned long evbit[BITS_TO_LONGS(EV_CNT)]; //位图，记录设备支持的事件类型 unsigned long keybit[BITS_TO_LONGS(KEY_CNT)]; //位图，记录设备支持的按键类型 unsigned long relbit[BITS_TO_LONGS(REL_CNT)]; //位图，记录设备支持的相对坐标 unsigned long absbit[BITS_TO_LONGS(ABS_CNT)]; //位图，记录设备支持的绝对坐标 unsigned long mscbit[BITS_TO_LONGS(MSC_CNT)]; //位图，记录设备支持的其他功能 unsigned long ledbit[BITS_TO_LONGS(LED_CNT)]; //位图，记录设备支持的指示灯 unsigned long sndbit[BITS_TO_LONGS(SND_CNT)]; //位图，记录设备支持的声音或警报 unsigned long ffbit[BITS_TO_LONGS(FF_CNT)]; //位图，记录设备支持的作用力功能 unsigned long swbit[BITS_TO_LONGS(SW_CNT)]; //位图，记录设备支持的开关功能 unsigned int hint_events_per_packet; unsigned int keycodemax; //设备支持的最大按键值个数 unsigned int keycodesize; //每个按键的字节大小 void *keycode; //指向按键池，即指向按键值数组首地址 int (*setkeycode)(struct input_dev *dev, const struct input_keymap_entry *ke, unsigned int *old_keycode); //修改按键值 int (*getkeycode)(struct input_dev *dev, struct input_keymap_entry *ke); //获取按键值 struct ff_device *ff; //用于强制更新输入设备的部分内容 unsigned int repeat_key; //重复按键的键值 struct timer_list timer; //设置当有连击时的延时定时器 int rep[REP_CNT]; struct input_mt *mt; struct input_absinfo *absinfo; unsigned long key[BITS_TO_LONGS(KEY_CNT)]; //位图，按键的状态 unsigned long led[BITS_TO_LONGS(LED_CNT)]; //位图，led的状态 unsigned long snd[BITS_TO_LONGS(SND_CNT)]; //位图，声音的状态 unsigned long sw[BITS_TO_LONGS(SW_CNT)]; //位图，开关的状态 int (*open)(struct input_dev *dev); void (*close)(struct input_dev *dev); int (*flush)(struct input_dev *dev, struct file *file); int (*event)(struct input_dev *dev, unsigned int type, unsigned int code, int value); struct input_handle __rcu *grab; //类似私有指针，可以直接访问到事件处理接口event spinlock_t event_lock; struct mutex mutex; unsigned int users; bool going_away; struct device dev; struct list_head h_list; //该链表头用于链接此设备所关联的input_handle struct list_head node; //用于将此设备链接到input_dev_list(链接了所有注册到内核的事件处理器) unsigned int num_vals; unsigned int max_vals; struct input_value *vals; bool devres_managed;&#125;; 2.1、input_handlerinput_handler 这是事件处理器的数据结构，代表一个事件处理器 12345678910111213141516171819202122232425262728293031[-&gt;input.h]struct input_handler &#123; void *private; /* 当事件处理器接收到来自Input设备传来的事件时调用的处理函数, event、events用于处理事件 */ void (*event)(struct input_handle *handle, unsigned int type, unsigned int code, int value); void (*events)(struct input_handle *handle, const struct input_value *vals, unsigned int count); bool (*filter)(struct input_handle *handle, unsigned int type, unsigned int code, int value); /* 比较 device's id with handler's id_table ，匹配device and handler*/ bool (*match)(struct input_handler *handler, struct input_dev *dev); /* connect用于建立intput_handler和input_dev的联系, 当一个Input设备注册到内核的时候被调用,将输入设备与事件处理器联结起来 */ int (*connect)(struct input_handler *handler, struct input_dev *dev, const struct input_device_id *id); /* disconnect用于解除handler和device的联系 */ void (*disconnect)(struct input_handle *handle); void (*start)(struct input_handle *handle); bool legacy_minors; int minor; //次设备号 const char *name; //次设备号 const struct input_device_id *id_table; //用于和device匹配 ,这个是事件处理器所支持的input设备 //这个链表用来链接他所支持的input_handle结构,input_dev与input_handler配对之后就会生成一个input_handle结构 struct list_head h_list; //链接到input_handler_list，这个链表链接了所有注册到内核的事件处理器 struct list_head node;&#125;; 2.3、input_handle123456789101112131415161718192021222324[-&gt;input.h]struct input_handle &#123; /* 每个配对的事件处理器都会分配一个对应的设备结构，如evdev事件处理器的evdev结构， 注意这个结构与设备驱动层的input_dev不同，初始化handle时，保存到这里。 */ void *private; /* 打开标志，每个input_handle 打开后才能操作， 这个一般通过事件处理器的open方法间接设置 */ int open; const char *name; /* 指向Input_dev结构实体 */ struct input_dev *dev; /* 指向Input_Hander结构实体 */ struct input_handler *handler; /* input_handle通过d_node连接到了input_dev上的h_list链表上 */ struct list_head d_node; /* input_handle通过h_node连接到了input_handler的h_list链表上 */ struct list_head h_node;&#125;; 2.4、三个数据结构之间的关系 input_dev: 是硬件驱动层，代表一个input设备。 input_handler: 是事件处理层，代表一个事件处理器。 input_handle: 属于核心层，代表一个配对的input_dev与input_handler input_dev 通过全局的input_dev_list链接在一起。设备注册的时候实现这个操作。注：（稍后详细分析） 1234567891011121314151617181920[-&gt;input.c]static LIST_HEAD(input_dev_list);static LIST_HEAD(input_handler_list);int input_register_device(struct input_dev *dev)&#123; struct input_devres *devres = NULL; struct input_handler *handler; unsigned int packet_size; const char *path; int error; ...... list_add_tail(&amp;dev-&gt;node, &amp;input_dev_list); list_for_each_entry(handler, &amp;input_handler_list, node) input_attach_handler(dev, handler); ......&#125; input_handler 通过全局的input_handler_list链接在一起。事件处理器注册的时候实现这个操作（事件处理器一般内核自带，一般不需要我们来写）注：（稍后详细分析） 12345678910111213141516[-&gt;input.c]static LIST_HEAD(input_dev_list);static LIST_HEAD(input_handler_list);int input_register_handler(struct input_handler *handler)&#123; struct input_dev *dev; int error; ...... list_add_tail(&amp;handler-&gt;node, &amp;input_handler_list); list_for_each_entry(dev, &amp;input_dev_list, node) input_attach_handler(dev, handler); ...... return 0;&#125; input_hande 没有一个全局的链表，它注册的时候将自己分别挂在了input_dev 和 input_handler 的h_list上了。通过input_dev 和input_handler就可以找到input_handle在设备注册和事件处理器，注册的时候都要进行配对工作(input_match_device)，配对后就会实现链接(handler-&gt;connect)通过input_handle也可以找到input_dev和input_handler。注：（稍后详细分析） 123456789101112[-&gt;input.c]static int input_attach_handler(struct input_dev *dev, struct input_handler *handler)&#123; const struct input_device_id *id; int error; id = input_match_device(handler, dev); ...... error = handler-&gt;connect(handler, dev, id); ...... return error;&#125; 我们可以看到，input_device和input_handler中都有一个h_list,而input_handle拥有指向input_dev和input_handler的指针，也就是说input_handle是用来关联input_dev和input_handler的。 那么为什么一个input_device和input_handler中拥有的是h_list而不是一个handle呢？因为一个device可能对应多个handler,而一个handler也不能只处理一个device,比如说一个鼠标，它可以对应even handler，也可以对应mouse handler,因此当其注册时与系统中的handler进行匹配，就有可能产生两个实例，一个是evdev,另一个是mousedev,而任何一个实例中都只有一个handle。至于以何种方式来传递事件，就由用户程序打开哪个实例来决定。后面一个情况很容易理解，一个事件驱动不能只为一个甚至一种设备服务，系统中可能有多种设备都能使用这类handler,比如event handler就可以匹配所有的设备。在input子系统中，有8种事件驱动，每种事件驱动最多可以对应32个设备，因此dev实例总数最多可以达到256个。 （三）、Input 核心层（Input.c）这一节主要介绍核心层的初始化，input_device、input_handle、input_handler之间的关系(稍后回头看更佳)。 总体概览图： 3.1、输入核心层：初始化首先从驱动”入口函数”开始查看 1234567891011121314151617[-&gt;input.c]static int __init input_init(void)&#123; int err; //注册input类，可在/sys/class下看到对应节点文件 err = class_register(&amp;input_class); ...... err = input_proc_init();/*创建/proc中的项，查看/proc/bus/input */ ...... /*注册设备/dev/input，主设备号为INPUT_MAJOR，就是13，后面注册的输入设备都使用该主设备号*/ err = register_chrdev_region(MKDEV(INPUT_MAJOR, 0), INPUT_MAX_CHAR_DEVICES, \"input\"); ...... return 0; ...... return err;&#125; 在入口函数里面创建了一个input_class类，其实就在/sys/class下创建了一个目录input.当然对于一个新设备，可以注册进一个class也可以不注册进去，如果存在对应class的话注册进去更好，另外在/proc创建了入口项,这样就可以/proc目录看到input的信息，然后就注册设备，可以看出输入子系统的主设备号是13，在这里并没有生成设备文件。只是在/dev/目录下创建了input目录，以后所有注册进系统的输入设备文件都放在这个目录下。 相应的对应关系可以使用adb 命令进入文件系统之后，cat /proc/bus/input/devices ，查看各个设备对应的event多少，比如Google Pixel 手机： 12345678910I: Bus=0000 Vendor=0000 Product=0003 Version=2066N: Name=\"synaptics_dsxv26\"P: Phys=synaptics_dsx/touch_inputS: Sysfs=/devices/soc/7577000.i2c/i2c-3/3-0020/input/input3U: Uniq=H: Handlers=mdss_fb kgsl event3B: PROP=2B: EV=bB: KEY=8000 0 0B: ABS=663800000000000 event3 就是事件序号， 我们在调试的时候直接 adb shell getevent /dev/input/event3，来实时捕捉 event3 中储存的数据。 那么接下来看看怎么注册input设备的.我们需要在设备驱动层中完成输入设备的注册，通过调用input_register_device()函数来完成，该函数的一个重要任务就是完成设备与事件驱动的匹配 3.2、输入核心层：注册设备input_dev123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384[-&gt;input.c]int input_register_device(struct input_dev *dev)&#123; struct input_devres *devres = NULL; struct input_handler *handler; unsigned int packet_size; const char *path; int error; if (dev-&gt;devres_managed) &#123; devres = devres_alloc(devm_input_device_unregister, sizeof(struct input_devres), GFP_KERNEL); ...... devres-&gt;input = dev; &#125; //EN_SYN这个是设备都要支持的事件类型，所以要设置 /* Every input device generates EV_SYN/SYN_REPORT events. */ __set_bit(EV_SYN, dev-&gt;evbit); /* KEY_RESERVED is not supposed to be transmitted to userspace. */ __clear_bit(KEY_RESERVED, dev-&gt;keybit); /* Make sure that bitmasks not mentioned in dev-&gt;evbit are clean. */ input_cleanse_bitmasks(dev); packet_size = input_estimate_events_per_packet(dev); if (dev-&gt;hint_events_per_packet &lt; packet_size) dev-&gt;hint_events_per_packet = packet_size; dev-&gt;max_vals = dev-&gt;hint_events_per_packet + 2; dev-&gt;vals = kcalloc(dev-&gt;max_vals, sizeof(*dev-&gt;vals), GFP_KERNEL); ...... /* * If delay and period are pre-set by the driver, then autorepeating * is handled by the driver itself and we don't do it in input.c. */ // 这个定时器是为了重复按键而设置的 if (!dev-&gt;rep[REP_DELAY] &amp;&amp; !dev-&gt;rep[REP_PERIOD]) &#123; dev-&gt;timer.data = (long) dev; dev-&gt;timer.function = input_repeat_key; dev-&gt;rep[REP_DELAY] = 250; dev-&gt;rep[REP_PERIOD] = 33; &#125; /* 如果设备驱动没有设置自己的获取键值的函数，系统默认 */ if (!dev-&gt;getkeycode) dev-&gt;getkeycode = input_default_getkeycode; /* 如果设备驱动没有指定按键重置函数，系统默认 */ if (!dev-&gt;setkeycode) dev-&gt;setkeycode = input_default_setkeycode; error = device_add(&amp;dev-&gt;dev); ...... path = kobject_get_path(&amp;dev-&gt;dev.kobj, GFP_KERNEL); pr_info(\"%s as %s\\n\", dev-&gt;name ? dev-&gt;name : \"Unspecified device\", path ? path : \"N/A\"); kfree(path); error = mutex_lock_interruptible(&amp;input_mutex); ...... // 将新分配的input设备连接到input_dev_list链表上 list_add_tail(&amp;dev-&gt;node, &amp;input_dev_list); /* 核心重点，input设备在增加到input_dev_list链表上之后，会查找 * input_handler_list事件处理链表上的handler进行匹配，这里的匹配 * 方式与设备模型的device和driver匹配过程很相似，所有的input * 都挂在input_dev_list上，所有类型的事件都挂在input_handler_list * 上，进行“匹配相亲”，list_for_each_entry就是个for循环，跳出条件遍历了一遍，又回到链表头 */ list_for_each_entry(handler, &amp;input_handler_list, node) input_attach_handler(dev, handler); input_wakeup_procfs_readers(); mutex_unlock(&amp;input_mutex); if (dev-&gt;devres_managed) &#123; dev_dbg(dev-&gt;dev.parent, \"%s: registering %s with devres.\\n\", __func__, dev_name(&amp;dev-&gt;dev)); devres_add(dev-&gt;dev.parent, devres); &#125; return 0;......&#125; 上面的代码主要的功能有以下几个功能，也是设备驱动注册为输入设备委托内核做的事情： 1、进一步初始化输入设备，例如连击事件 2、注册输入设备到input类中，把输入设备挂到输入设备链表input_dev_list中 3、查找并匹配输入设备对应的事件处理层，通过input_handler_list链表 我们需要再分析下这个匹配的过程，input_attach_handler()匹配过程如下： 12345678910111213[-&gt;input.c]static int input_attach_handler(struct input_dev *dev, struct input_handler *handler)&#123; const struct input_device_id *id; int error; /* input_dev 和 input_handler 进行匹配,返回匹配的id，类型是struct input_device_id */ id = input_match_device(handler, dev); ...... /* 匹配成功，调用handler里面的connect函数,这个函数在事件处理器中定义，主要生成一个input_handle结构，并初始化，还生成一个事件处理器相关的设备结构 */ error = handler-&gt;connect(handler, dev, id); ...... return error;&#125; 我们先来看下input_match_device（）函数，看一下这个匹配的条件是什么，如何匹配的过程是怎样的，匹配的结果会是什么 12345678910111213141516171819202122232425262728293031323334353637[-&gt;input.c]static const struct input_device_id *input_match_device(struct input_handler *handler, struct input_dev *dev)&#123; const struct input_device_id *id; for (id = handler-&gt;id_table; id-&gt;flags || id-&gt;driver_info; id++) &#123; if (id-&gt;flags &amp; INPUT_DEVICE_ID_MATCH_BUS) if (id-&gt;bustype != dev-&gt;id.bustype) //匹配总线id continue; if (id-&gt;flags &amp; INPUT_DEVICE_ID_MATCH_VENDOR) if (id-&gt;vendor != dev-&gt;id.vendor) //匹配生产商id continue; if (id-&gt;flags &amp; INPUT_DEVICE_ID_MATCH_PRODUCT) if (id-&gt;product != dev-&gt;id.product) //匹配产品id continue; if (id-&gt;flags &amp; INPUT_DEVICE_ID_MATCH_VERSION) if (id-&gt;version != dev-&gt;id.version) //匹配版本 continue; //匹配id的evbit和input_dev中evbit的各个位，如果不匹配则continue，数组中下一个设备 if (!bitmap_subset(id-&gt;evbit, dev-&gt;evbit, EV_MAX)) continue; ...... if (!bitmap_subset(id-&gt;swbit, dev-&gt;swbit, SW_MAX)) continue; if (!handler-&gt;match || handler-&gt;match(handler, dev)) return id;//匹配成功,返回id &#125; return NULL;&#125; input_match_device() 到最合适的事件处理层驱动时，便执行handler-&gt;connect() 函数进行连接了，看下面这部分代码（以evdev类型驱动为例，在input/evdev.c中） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[-&gt;evdev.c]static int evdev_connect(struct input_handler *handler, struct input_dev *dev, const struct input_device_id *id)&#123; struct evdev *evdev; int minor; int dev_no; int error; /* EVDEV_MINORS为32，代表共能容纳32个evdev事件层设备，下面代码在找到空的地方，用于保存evdev事件层的数据，即上面定义的evdev */ minor = input_get_new_minor(EVDEV_MINOR_BASE, EVDEV_MINORS, true); ...... /* 开始给evdev事件层驱动分配空间了 */ evdev = kzalloc(sizeof(struct evdev), GFP_KERNEL); ...... /* 初始化client_list列表和evdev_wait队列，后面介绍 */ INIT_LIST_HEAD(&amp;evdev-&gt;client_list); spin_lock_init(&amp;evdev-&gt;client_lock); mutex_init(&amp;evdev-&gt;mutex); init_waitqueue_head(&amp;evdev-&gt;wait); evdev-&gt;exist = true; /* 初始化evdev结构体，其中handle为输入设备和事件处理的关联接口 */ dev_no = minor; /* Normalize device number if it falls into legacy range */ if (dev_no &lt; EVDEV_MINOR_BASE + EVDEV_MINORS) dev_no -= EVDEV_MINOR_BASE; dev_set_name(&amp;evdev-&gt;dev, \"event%d\", dev_no); /*这里就将handle的dev指针指向了input_dev*/ evdev-&gt;handle.dev = input_get_device(dev); evdev-&gt;handle.name = dev_name(&amp;evdev-&gt;dev); evdev-&gt;handle.handler = handler;/*这里将handle的handler指向了当前的input_handler.注意本函数evdev_connect,可能是在在输入设备注册的时候38 在input_register_device函数中调用input_attach_handler的时候调用;也可能是在输入设备的处理方法input_handler时在input_register_handler39 函数中也会用到input_attach_handler函数,就会调用本函数.这里就很明显了,本函数就将input_handler和input_dev都放在input_handle中统一管理*/ evdev-&gt;handle.private = evdev; /*初始化evdev中的内嵌device*/ evdev-&gt;dev.devt = MKDEV(INPUT_MAJOR, minor); evdev-&gt;dev.class = &amp;input_class; evdev-&gt;dev.parent = &amp;dev-&gt;dev; evdev-&gt;dev.release = evdev_free; device_initialize(&amp;evdev-&gt;dev); /* input_dev设备驱动层和input_handler事件处理层的关联，由input_handle完成(不要和handler搞混淆了，这不是一个概念～) */ error = input_register_handle(&amp;evdev-&gt;handle); ...... cdev_init(&amp;evdev-&gt;cdev, &amp;evdev_fops); evdev-&gt;cdev.kobj.parent = &amp;evdev-&gt;dev.kobj; error = cdev_add(&amp;evdev-&gt;cdev, evdev-&gt;dev.devt, 1); ...... error = device_add(&amp;evdev-&gt;dev); ...... return 0; ......&#125; 3.3、输入核心层：注册input_handler为了逻辑更清新，我们稍后再来看input_register_handle() 程，先来了解input_handler的注册过程。 要了解input_handler的注册过程，又需要先了解evdev初始化过程（以evdev为例）： /kernel/drivers/input下众多事件处理器handler其中的一个，可以看下源码/kernel/drivers/input/evdev.c中的模块init 12345[-&gt;edev.c]static int __init evdev_init(void)&#123; return input_register_handler(&amp;evdev_handler);&#125; 这个初始化就是往input核心中注册一个input_handler类型的evdev_handler，调用的是input.c提供的接口，input_handler结构前面有介绍，看下evdev_handler的赋值： 1234567891011[-&gt;edev.c]static struct input_handler evdev_handler = &#123; .event = evdev_event, .events = evdev_events, .connect = evdev_connect, .disconnect = evdev_disconnect, .legacy_minors = true, .minor = EVDEV_MINOR_BASE, .name = \"evdev\", .id_table = evdev_ids,&#125;; 可以注意的是evdev是匹配所有设备的，因为： 12345[-&gt;edev.c]static const struct input_device_id evdev_ids[] = &#123; &#123; .driver_info = 1 &#125;, /* Matches all devices */ &#123; &#125;, /* Terminating zero entry */&#125;; 123456789101112131415161718192021[-&gt;input.c]int input_register_handler(struct input_handler *handler)&#123; struct input_dev *dev; int error; error = mutex_lock_interruptible(&amp;input_mutex); ...... INIT_LIST_HEAD(&amp;handler-&gt;h_list); //添加进input_handler_list全局链表 list_add_tail(&amp;handler-&gt;node, &amp;input_handler_list); //同样遍历input_dev这个链表，依次调用下面的input_attach_handler去匹配input_dev,这个跟input_dev注册的时候的情形类似 list_for_each_entry(dev, &amp;input_dev_list, node) input_attach_handler(dev, handler); input_wakeup_procfs_readers(); mutex_unlock(&amp;input_mutex); return 0;&#125; 3.4、输入核心层：注册input_handle（不要和handler搞混淆了哦，这不是一个概念～）input_handle关联匹配input_dev和input_handler 继续分析input_dev和input_handler 是如何关联上的 123456789101112131415161718192021222324252627282930313233[-&gt;input.c]int input_register_handle(struct input_handle *handle)&#123; struct input_handler *handler = handle-&gt;handler; struct input_dev *dev = handle-&gt;dev; int error; ...... error = mutex_lock_interruptible(&amp;dev-&gt;mutex); ...... /* 将d_node链接到输入设备的h_list，h_node链接到事件层的h_list链表上 * 因此，在handle中是输入设备和事件层的关联结构体，通过输入设备可以 * 找到对应的事件处理层接口，通过事件处理层也可找到匹配的输入设备 */ //把这个handle的d_node 加到对应input_dev的h_list链表里面 if (handler-&gt;filter) list_add_rcu(&amp;handle-&gt;d_node, &amp;dev-&gt;h_list); else list_add_tail_rcu(&amp;handle-&gt;d_node, &amp;dev-&gt;h_list); mutex_unlock(&amp;dev-&gt;mutex); ...... //把这个handle的h_node 加到对应input_handler的h_list链表里面 list_add_tail_rcu(&amp;handle-&gt;h_node, &amp;handler-&gt;h_list); if (handler-&gt;start) handler-&gt;start(handle); return 0;&#125; 这个注册是把handle 本身的链表加入到它自己的input_dev 以及 input_handler的h_list链表中，这样以后就可以通过h_list遍历到这个handle，这样就实现了三者的绑定联系。 以上是输入设备驱动注册的全过程，纵观整个过程，输入设备驱动最终的目的就是能够与事件处理层的事件驱动相互匹配，但是在drivers/input目录下有evdev.c事件驱动、mousedev.c事件驱动、joydev.c事件驱动等等，我们的输入设备产生的事件应该最终上报给谁，然后让事件被谁去处理呢？知道了这么个原因再看上面代码就会明白，其实evdev.c、mousedev.c等根据硬件输入设备的处理方式的不同抽象出了不同的事件处理接口帮助上层去调用，而我们写的设备驱动程序只不过是完成了硬件寄存器中数据的读写，但提交给用户的事件必须是经过事件处理层的封装和同步才能够完成的，事件处理层提供给用户一个统一的界面来操作。 整个关联注册的过程： （四）、Input 事件处理层 Event handler （以evdev事件处理器为例） 4.1、主要数据结构（1） evdev设备结构 12345678910111213[evdev.h]struct evdev &#123; int exist; int open; //打开标志 int minor; //次设备号 struct input_handle handle; //关联的input_handle wait_queue_head_t wait; //等待队列，当进程读取设备，而没有事件产生的时候，进程就会睡在其上面 struct evdev_client *grab; //强制绑定的evdev_client结构，这个结构后面再分析 struct list_head client_list; //evdev_client 链表，这说明一个evdev设备可以处理多个evdev_client，可以有多个进程访问evdev设备 spinlock_t client_lock; /* protects client_list */ struct mutex mutex; struct device dev; //device结构，说明这是一个设备结构 &#125;; evdev结构体在配对成功的时候生成，由handler-&gt;connect生成，对应设备文件为/class/input/event(n)，如触摸屏驱动的event3，这个设备是用户空间要访问的设备，可以理解它是一个虚拟设备，因为没有对应的硬件，但是通过handle-&gt;dev 就可以找到input_dev结构，而它对应着触摸屏，设备文件为/class/input/input3。这个设备结构生成之后保存在evdev_table中，索引值是minor。 （2）evdev用户端结构 123456789101112131415[evdev.h]struct evdev_client &#123; unsigned int head; //buffer数组的索引头 unsigned int tail; //buffer数组的索引尾 unsigned int packet_head; /* [future] position of the first element of next packet */ spinlock_t buffer_lock; /* protects access to buffer, head and tail */ struct wake_lock wake_lock; bool use_wake_lock; char name[28]; struct fasync_struct *fasync; //异步通知函数 struct evdev *evdev; //包含一个evdev变量 struct list_head node; //链表 unsigned int bufsize; struct input_event buffer[]; //input_event数据结构的数组，input_event代表一个事件，基本成员：类型（type），编码（code），值（value） &#125;; 这个结构在进程打开event3设备的时候调用evdev的open方法，在open中创建这个结构，并初始化。在关闭设备文件的时候释放这个结构。 （3）input_event结构 1234567[input.h]struct input_event &#123; struct timeval time; //事件发生的时间 __u16 type; //事件类型 __u16 code; //子事件 __s32 value; //事件的value &#125;; 4.2、事件处理层evdev事件处理层与用户程序和输入子系统核心打交道，是他们两层的桥梁。一般内核有好几个事件处理器，像evdev mousedev jotdev。evdev事件处理器可以处理所有的事件，触摸屏驱动就是用的这个，所以下面分析这个事件处理器的实现。它也是作为模块注册到内核中的,前面已经分析过它的模块初始化函数 12345678910111213141516[-&gt;evdev.c]static const struct file_operations evdev_fops = &#123; .owner = THIS_MODULE, .read = evdev_read, .write = evdev_write, .poll = evdev_poll, .open = evdev_open, .release = evdev_release, .unlocked_ioctl = evdev_ioctl,#ifdef CONFIG_COMPAT .compat_ioctl = evdev_ioctl_compat,#endif .fasync = evdev_fasync, .flush = evdev_flush, .llseek = no_llseek,&#125;; 如果匹配上了就会创建一个evdev，它里边封装了一个handle，会把input_dev和input_handler关联到一起。关系如下： 4.3、evdev设备结点的open()操作我们知道.对主设备号为INPUT_MAJOR的设备节点进行操作，会将操作集转换成handler的操作集。在evdev中，这个操作集就是evdev_fops。对应的open函数如下示： 首先来看打开event(x)设备文件，evdev_open函数. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[-&gt;evdev.c]static int evdev_open(struct inode *inode, struct file *file)&#123; struct evdev *evdev = container_of(inode-&gt;i_cdev, struct evdev, cdev); //evdev_client的buffer大小 unsigned int bufsize = evdev_compute_buffer_size(evdev-&gt;handle.dev); unsigned int size = sizeof(struct evdev_client) + bufsize * sizeof(struct input_event); struct evdev_client *client; int error; //打开的时候创建一个evdev_client client = kzalloc(size, GFP_KERNEL | __GFP_NOWARN); ...... client-&gt;bufsize = bufsize; spin_lock_init(&amp;client-&gt;buffer_lock); snprintf(client-&gt;name, sizeof(client-&gt;name), \"%s-%d\", dev_name(&amp;evdev-&gt;dev), task_tgid_vnr(current)); client-&gt;evdev = evdev; evdev_attach_client(evdev, client); //调用打开真正的底层设备函数 error = evdev_open_device(evdev); ...... file-&gt;private_data = client; nonseekable_open(inode, file); return 0; ......&#125;static int evdev_open_device(struct evdev *evdev)&#123; int retval; retval = mutex_lock_interruptible(&amp;evdev-&gt;mutex); if (retval)/*如果设备不存在，返回错误*/ return retval; if (!evdev-&gt;exist) retval = -ENODEV; else if (!evdev-&gt;open++) &#123;//递增打开计数 retval = input_open_device(&amp;evdev-&gt;handle);//如果是被第一次打开，则调用input_open_device if (retval) evdev-&gt;open--; &#125; mutex_unlock(&amp;evdev-&gt;mutex); return retval;&#125;int input_open_device(struct input_handle *handle)&#123; struct input_dev *dev = handle-&gt;dev;//根据input_handle找到对应的input_dev设备 int retval; retval = mutex_lock_interruptible(&amp;dev-&gt;mutex); ...... handle-&gt;open++;//递增handle的打开计数 if (!dev-&gt;users++ &amp;&amp; dev-&gt;open) retval = dev-&gt;open(dev);//如果是第一次打开.则调用input device的open()函数 if (retval) &#123; dev-&gt;users--; if (!--handle-&gt;open) &#123; synchronize_rcu(); &#125; &#125; out: mutex_unlock(&amp;dev-&gt;mutex); return retval;&#125; 4.4、用户进程读取event的底层实现至于具体的如何初始化input_dev，这个是具体的输入设备去实现的，稍后具体实例再分析，现在来看看，对于一个event(x)设备文件的，应用程序来读，最终会导致”handler”里面的的”读函数”被调用。 evdev_fops 结 构 体 是 一 个 file_operations 的 类 型 。 当 用 户 层 调 用 类 似 代 码open(“/dev/input/event3” , O_RDONLY) 函 数 打 开 设 备 结 点 时 , 会 调 用 evdev_fops 中 的evdev_read()函数,该函数的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586[-&gt;evdev.c]static ssize_t evdev_read(struct file *file, char __user *buffer, size_t count, loff_t *ppos)&#123; struct evdev_client *client = file-&gt;private_data;//就是刚才在open函数中保存的evdev_client struct evdev *evdev = client-&gt;evdev; struct input_event event; size_t read = 0; int error; for (;;) &#123; ...... //如果获得了数据则取出来，调用evdev_fetch_next_event while (read + input_event_size() &lt;= count &amp;&amp; evdev_fetch_next_event(client, &amp;event)) &#123; //input_event_to_user调用copy_to_user传入用户程序中，这样读取完成 if (input_event_to_user(buffer + read, &amp;event)) return -EFAULT; read += input_event_size(); &#125; ...... /*如果是可阻塞状态的话,则等待在wait队列上.直到有数据要被处理,当前进程才被唤醒.这很好理解,既然是 输入设备,读的话比如读按键,那么必须要有硬件设备有按键按下才会返回按键值,这里还是处于事件处理层,应用程序在这里休眠,那么谁来唤醒? 当然是有按键按下才去唤醒,因此这个工作就交给了设备驱动层,那么找到这个唤醒呢,直接去找不好找,那么可以直接搜索evdev-&gt;wait,搜索结果 可知evdev-&gt;wait在evdev_event()函数中被唤醒*/ if (!(file-&gt;f_flags &amp; O_NONBLOCK)) &#123; error = wait_event_interruptible(evdev-&gt;wait, client-&gt;packet_head != client-&gt;tail || !evdev-&gt;exist || client-&gt;revoked); if (error) return error; &#125; &#125; return read;&#125;static int evdev_fetch_next_event(struct evdev_client *client, struct input_event *event)&#123; int have_event; spin_lock_irq(&amp;client-&gt;buffer_lock); /*先判断一下是否有数据*/ have_event = client-&gt;packet_head != client-&gt;tail; /*如果有就从环形缓冲区的取出来，记得是从head存储，tail取出*/ if (have_event) &#123; *event = client-&gt;buffer[client-&gt;tail++]; client-&gt;tail &amp;= client-&gt;bufsize - 1; if (client-&gt;use_wake_lock &amp;&amp; client-&gt;packet_head == client-&gt;tail) wake_unlock(&amp;client-&gt;wake_lock); &#125; spin_unlock_irq(&amp;client-&gt;buffer_lock); return have_event;&#125;int input_event_to_user(char __user *buffer, const struct input_event *event)&#123; /*如果设置了标志INPUT_COMPAT_TEST就将事件event包装成结构体compat_event*/ if (INPUT_COMPAT_TEST &amp;&amp; !COMPAT_USE_64BIT_TIME) &#123; struct input_event_compat compat_event; compat_event.time.tv_sec = event-&gt;time.tv_sec; compat_event.time.tv_usec = event-&gt;time.tv_usec; compat_event.type = event-&gt;type; compat_event.code = event-&gt;code; compat_event.value = event-&gt;value; /*将包装成的compat_event拷贝到用户空间*/ if (copy_to_user(buffer, &amp;compat_event, sizeof(struct input_event_compat))) return -EFAULT; &#125; else &#123; /*否则，将event拷贝到用户空间*/ if (copy_to_user(buffer, event, sizeof(struct input_event))) return -EFAULT; &#125; return 0;&#125; 如果是可阻塞状态的话，则等待在wait队列上。直到有数据要被处理，当前进程才被唤醒。这很好理解，既然是输入设备，读的话比如读按键，那么必须要有硬件设备有按键按下才会返回按键值，这里还是处于事件处理层，应用程序在这里休眠，那么谁来唤醒? 当然是有按键按下才去唤醒，因此这个工作就交给了设备驱动层。那么找到这个唤醒呢，直接去找不好找。那么可以直接搜索evdev-&gt;wait，搜索结果可知evdev-&gt;wait在evdev_event()函数中被唤醒 注释中说的很清楚，evdev_event()会唤醒此处的读按键进程。那么evdev_event()又是被谁调用?显然是设备驱动层，现在看一个设备层例子，内核中有个按键的例子，gpiokey.c，这只是个例子不针对任何设备，在gpio_keys.c终端处理函数里面 1234567891011121314[-&gt;gpio_keys.c]static irqreturn_t gpio_keys_irq_isr(int irq, void *dev_id)&#123; ...... if (!bdata-&gt;key_pressed) &#123; ...... input_event(input, EV_KEY, button-&gt;code, 1); input_sync(input); ...... &#125; ......&#125; 如此可以看出 在设备的中断服务程序里面，确定事件是什么，然后调用相应的input_handler的event处理函数 实际上这就是我们的核心 input_event()是用来上报事件的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[-&gt;input.c]void input_event(struct input_dev *dev, unsigned int type, unsigned int code, int value)&#123; unsigned long flags; if (is_event_supported(type, dev-&gt;evbit, EV_MAX)) &#123; spin_lock_irqsave(&amp;dev-&gt;event_lock, flags); input_handle_event(dev, type, code, value); spin_unlock_irqrestore(&amp;dev-&gt;event_lock, flags); &#125;&#125;static void input_handle_event(struct input_dev *dev, unsigned int type, unsigned int code, int value)&#123; ...... if (disposition &amp; INPUT_FLUSH) &#123; if (dev-&gt;num_vals &gt;= 2) input_pass_values(dev, dev-&gt;vals, dev-&gt;num_vals); dev-&gt;num_vals = 0; &#125; else if (dev-&gt;num_vals &gt;= dev-&gt;max_vals - 2) &#123; dev-&gt;vals[dev-&gt;num_vals++] = input_value_sync; input_pass_values(dev, dev-&gt;vals, dev-&gt;num_vals); dev-&gt;num_vals = 0; &#125;&#125;static void input_pass_values(struct input_dev *dev, struct input_value *vals, unsigned int count)&#123; struct input_handle *handle; struct input_value *v; ...... handle = rcu_dereference(dev-&gt;grab); if (handle) &#123; count = input_to_handler(handle, vals, count); &#125; else &#123; list_for_each_entry_rcu(handle, &amp;dev-&gt;h_list, d_node) if (handle-&gt;open) count = input_to_handler(handle, vals, count); &#125; ......&#125;static unsigned int input_to_handler(struct input_handle *handle, struct input_value *vals, unsigned int count)&#123; struct input_handler *handler = handle-&gt;handler; struct input_value *end = vals; struct input_value *v; ...... if (handler-&gt;events) handler-&gt;events(handle, vals, count); else if (handler-&gt;event) for (v = vals; v != end; v++) handler-&gt;event(handle, v-&gt;type, v-&gt;code, v-&gt;value); return count;&#125; 可以看到最终调用handler-&gt;event()来处理，此处handler即对应evdev。 12[-&gt;input.c]handler-&gt;event(handle, v-&gt;type, v-&gt;code, v-&gt;value) 所以会调用evdev_event()函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[-&gt;evdev.c]static void evdev_pass_values(struct evdev_client *client, const struct input_value *vals, unsigned int count, ktime_t mono, ktime_t real)&#123; struct evdev *evdev = client-&gt;evdev; const struct input_value *v; struct input_event event; bool wakeup = false; if (client-&gt;revoked) return; event.time = ktime_to_timeval(client-&gt;clkid == CLOCK_MONOTONIC ? mono : real); /* Interrupts are disabled, just acquire the lock. */ spin_lock(&amp;client-&gt;buffer_lock); for (v = vals; v != vals + count; v++) &#123; event.type = v-&gt;type; event.code = v-&gt;code; event.value = v-&gt;value; __pass_event(client, &amp;event); if (v-&gt;type == EV_SYN &amp;&amp; v-&gt;code == SYN_REPORT) wakeup = true; &#125; spin_unlock(&amp;client-&gt;buffer_lock); if (wakeup) wake_up_interruptible(&amp;evdev-&gt;wait);&#125;static void evdev_events(struct input_handle *handle, const struct input_value *vals, unsigned int count)&#123; struct evdev *evdev = handle-&gt;private; struct evdev_client *client; ...... if (client) evdev_pass_values(client, vals, count, time_mono, time_real); else list_for_each_entry_rcu(client, &amp;evdev-&gt;client_list, node) evdev_pass_values(client, vals, count, time_mono, time_real); rcu_read_unlock();&#125;static void evdev_event(struct input_handle *handle, unsigned int type, unsigned int code, int value)&#123; struct input_value vals[] = &#123; &#123; type, code, value &#125; &#125;; evdev_events(handle, vals, 1);&#125; 最终唤醒evdev_read()将数据拷贝到用户空间。 （五）、Input 事件上报过程5.1、Input 事件产生当按下触摸屏时，进入触摸屏按下中断，开始ad转换，ad转换完成进入ad完成中断，在这个终端中将事件发送出去，会调用以下函数上报事件: 12345678910 input_report_key(input_dev, BTN_TOUCH, 1); input_report_abs(input_dev, ABS_POSITION_X, x); input_report_abs(input_dev, ABS_POSITION_Y, y);input_sync(input_dev); 这两个函数调用了 input_event(dev, EV_ABS, code, value) 所有的事件报告函数都调用这个函数。 123456789101112131415[-&gt;input.h]static inline void input_report_key(struct input_dev *dev, unsigned int code, int value)&#123; input_event(dev, EV_KEY, code, !!value);&#125;static inline void input_report_abs(struct input_dev *dev, unsigned int code, int value)&#123; input_event(dev, EV_ABS, code, value);&#125;static inline void input_sync(struct input_dev *dev)&#123; input_event(dev, EV_SYN, SYN_REPORT, 0);&#125; 5.2、Input 事件报告input_event 函数前面已经分析过，这里不再分析。 123[-&gt;input.c:input_pass_values]for (v = vals; v != end; v++) handler-&gt;event(handle, v-&gt;type, v-&gt;code, v-&gt;value); 最终会调用handler-&gt;event(handle, v-&gt;type, v-&gt;code, v-&gt;value) 来将数据 传递给用户空间等待读取数据的进程 1copy_to_user(buffer, event, sizeof(struct input_event)) （六）、Android Input子系统输入子系统的系统架构如下图所示： 详细分析请参考：Android 7.1.2 (Android N) Android 输入子系统-Input System 分析 （七）、Input 设备驱动层实例（Synaptics）触摸屏也是用上面这一套框架来操作的。右边需要一个”evdev.c”文件。左边要分配一个”input_dev”结构。接着就看上图的硬件设备左边的过程：分配一个”input_dev”结构体 –&gt; 设置这个”input_dev”结构体 –&gt; 注册这个”input_dev”结构体 –&gt; 硬件相关的操作。 编写Input驱动一般框架: Google Pixel、Pixel XL 触控驱动模块型号为Synaptics（ClearPad S3708），源码：Synaptics 触摸屏驱动源码 Makefile： 123456789101112[-&gt;drivers/input/touchscreen/synaptics_dsx_htc_2.6/Makefile]## Makefile for the Synaptics DSX touchscreen driver.## Each configuration option enables a list of files.obj-$(CONFIG_TOUCHSCREEN_SYNAPTICS_DSX_I2C_HTC_v26) += synaptics_dsx_i2c.oobj-$(CONFIG_TOUCHSCREEN_SYNAPTICS_DSX_SPI_HTC_v26) += synaptics_dsx_spi.oobj-$(CONFIG_TOUCHSCREEN_SYNAPTICS_DSX_CORE_HTC_v26) += synaptics_dsx_core.oobj-$(CONFIG_TOUCHSCREEN_SYNAPTICS_DSX_RMI_DEV_HTC_v26) += synaptics_dsx_rmi_dev.o...... 抓取kernel log：可知input 驱动名为synaptics_dsxv26，全局搜索可知synaptics_rmi4_f12_init在[-&gt;synaptics_dsx_core.c]中。 123456[ 1.362728] c3 1 [TP]:synaptics_rmi4_f12_init: Function 12 max x = 1079 max y = 1919 Rx: 16 Tx: 28[ 1.363344] c3 1 [TP]synaptics_rmi4_f12_init:Wakeup Gesture range (0,0) -&gt; (1079,1919)[ 1.363623] c3 1 [TP]:synaptics_rmi4_f12_init report data init done[ 1.371945] c3 1 [TP]:synaptics_rmi4_query_device: chip_id:3708, firmware_id:2433782[ 1.372865] c3 1 [TP]:synaptics_rmi4_query_device: config_version: 5331763200190000000000000000000000000000000000000000000000000000[ 1.373249] c3 1 input: synaptics_dsxv26 as /devices/soc/7577000.i2c/i2c-3/3-0020/input/input3 查看input设备：adb shell cat /proc/bus/input/devices 123456789101112I: Bus=0000 Vendor=0000 Product=0003 Version=2066N: Name=&quot;synaptics_dsxv26&quot;P: Phys=synaptics_dsx/touch_inputS: Sysfs=/devices/soc/7577000.i2c/i2c-3/3-0020/input/input3U: Uniq=H: Handlers=mdss_fb kgsl event3B: PROP=2B: EV=bB: KEY=8000 0 0B: ABS=663800000000000对应：/dev/input/event3 7.1、分配Input_dev结构体7.1.1、synaptics_rmi4_f12_init()首先看一下初始化过程： 123456789101112131415161718192021222324[-&gt;synaptics_dsx_core.c]static struct platform_driver synaptics_rmi4_driver = &#123; .driver = &#123; .name = PLATFORM_DRIVER_NAME, .owner = THIS_MODULE,#ifdef CONFIG_PM .pm = &amp;synaptics_rmi4_dev_pm_ops,#endif &#125;, .probe = synaptics_rmi4_probe, .remove = synaptics_rmi4_remove,&#125;;static int __init synaptics_rmi4_init(void)&#123; int retval; retval = synaptics_rmi4_bus_init(); if (retval) return retval; return platform_driver_register(&amp;synaptics_rmi4_driver);&#125;module_init(synaptics_rmi4_init); 首先注册平台驱动，当驱动和设备匹配成功，继续看一下synaptics_rmi4_probe()函数 7.1.2、synaptics_rmi4_probe()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394[-&gt;synaptics_dsx_core.c]static int synaptics_rmi4_probe(struct platform_device *pdev)&#123; int retval, len; unsigned char attr_count; struct synaptics_rmi4_data *rmi4_data; const struct synaptics_dsx_hw_interface *hw_if; const struct synaptics_dsx_board_data *bdata; struct dentry *temp; //初始化platform_data、board_data、rmi4_data hw_if = pdev-&gt;dev.platform_data; bdata = hw_if-&gt;board_data; rmi4_data = kzalloc(sizeof(*rmi4_data), GFP_KERNEL); rmi4_data-&gt;pdev = pdev; rmi4_data-&gt;current_page = MASK_8BIT; rmi4_data-&gt;hw_if = hw_if; rmi4_data-&gt;touch_stopped = false; rmi4_data-&gt;sensor_sleep = false; rmi4_data-&gt;irq_enabled = false; rmi4_data-&gt;fw_updating = false; rmi4_data-&gt;fingers_on_2d = false; rmi4_data-&gt;update_coords = true; rmi4_data-&gt;write_buf = devm_kzalloc(&amp;pdev-&gt;dev, I2C_WRITE_BUF_MAX_LEN, GFP_KERNEL); rmi4_data-&gt;write_buf_len = I2C_WRITE_BUF_MAX_LEN; rmi4_data-&gt;irq_enable = synaptics_rmi4_irq_enable; rmi4_data-&gt;reset_device = synaptics_rmi4_reset_device; mutex_init(&amp;(rmi4_data-&gt;rmi4_io_ctrl_mutex)); mutex_init(&amp;(rmi4_data-&gt;rmi4_reset_mutex)); retval = synaptics_dsx_regulator_configure(rmi4_data); retval = synaptics_dsx_regulator_enable(rmi4_data, true); platform_set_drvdata(pdev, rmi4_data); if (bdata-&gt;gpio_config) &#123; retval = synaptics_rmi4_set_gpio(rmi4_data); &#125; else &#123; retval = synaptics_dsx_pinctrl_init(rmi4_data); if (!retval &amp;&amp; rmi4_data-&gt;ts_pinctrl) &#123; retval = pinctrl_select_state(rmi4_data-&gt;ts_pinctrl, rmi4_data-&gt;pinctrl_state_active); &#125; retval = synaptics_dsx_gpio_configure(rmi4_data, true); &#125; if (bdata-&gt;fw_name) &#123; len = strlen(bdata-&gt;fw_name); strlcpy(rmi4_data-&gt;fw_name, bdata-&gt;fw_name, len + 1); &#125; //分配Input_dev结构体，设置，注册 retval = synaptics_rmi4_set_input_dev(rmi4_data); ...... rmi4_data-&gt;irq = gpio_to_irq(bdata-&gt;irq_gpio); //请求中断，并设置中断处理函数synaptics_rmi4_irq retval = synaptics_rmi4_irq_enable(rmi4_data, true); if (!exp_data.initialized) &#123; mutex_init(&amp;exp_data.mutex); INIT_LIST_HEAD(&amp;exp_data.list); exp_data.initialized = true; &#125; exp_data.workqueue = create_singlethread_workqueue(\"dsx_exp_workqueue\"); INIT_DELAYED_WORK(&amp;exp_data.work, synaptics_rmi4_exp_fn_work); exp_data.rmi4_data = rmi4_data; exp_data.queue_work = true; queue_delayed_work(exp_data.workqueue, &amp;exp_data.work, msecs_to_jiffies(EXP_FN_WORK_DELAY_MS)); rmi4_data-&gt;dir = debugfs_create_dir(DEBUGFS_DIR_NAME, NULL); ...... for (attr_count = 0; attr_count &lt; ARRAY_SIZE(attrs); attr_count++) &#123; retval = sysfs_create_file(&amp;rmi4_data-&gt;input_dev-&gt;dev.kobj, &amp;attrs[attr_count].attr); ...... &#125; synaptics_secure_touch_init(rmi4_data); synaptics_secure_touch_stop(rmi4_data, 1); return retval; .......&#125; 7.1.3、分配Input_dev结构体12345678910[-&gt;synaptics_dsx_core.c]static int synaptics_rmi4_set_input_dev(struct synaptics_rmi4_data *rmi4_data)&#123; int retval; const struct synaptics_dsx_board_data *bdata = rmi4_data-&gt;hw_if-&gt;board_data; rmi4_data-&gt;input_dev = input_allocate_device(); ......&#125; 7.2、设置支持事件类型 set_bit(EV_SYN, evbit)、set_bit(EV_KEY, evbit)、set_bit(EV_ABS,evbit) ……1234567891011121314151617181920212223242526272829[-&gt;synaptics_dsx_core.c]static int synaptics_rmi4_set_input_dev(struct synaptics_rmi4_data *rmi4_data)&#123; int retval; const struct synaptics_dsx_board_data *bdata = rmi4_data-&gt;hw_if-&gt;board_data; rmi4_data-&gt;input_dev = input_allocate_device(); ...... retval = synaptics_rmi4_query_device(rmi4_data); .... //#define PLATFORM_DRIVER_NAME \"synaptics_dsxv26\"(synaptics_dsx_v2_6.h) rmi4_data-&gt;input_dev-&gt;name = PLATFORM_DRIVER_NAME; rmi4_data-&gt;input_dev-&gt;phys = INPUT_PHYS_NAME; rmi4_data-&gt;input_dev-&gt;id.product = SYNAPTICS_DSX_DRIVER_PRODUCT; rmi4_data-&gt;input_dev-&gt;id.version = SYNAPTICS_DSX_DRIVER_VERSION; rmi4_data-&gt;input_dev-&gt;dev.parent = rmi4_data-&gt;pdev-&gt;dev.parent; input_set_drvdata(rmi4_data-&gt;input_dev, rmi4_data); set_bit(EV_SYN, rmi4_data-&gt;input_dev-&gt;evbit); set_bit(EV_KEY, rmi4_data-&gt;input_dev-&gt;evbit); set_bit(EV_ABS, rmi4_data-&gt;input_dev-&gt;evbit); set_bit(BTN_TOUCH, rmi4_data-&gt;input_dev-&gt;keybit); set_bit(BTN_TOOL_FINGER, rmi4_data-&gt;input_dev-&gt;keybit); synaptics_rmi4_set_params(rmi4_data); ......&#125; 7.3、注册设备input_register_device()此处即与前面kernel log呼应：注册名为 synaptics_dsxv26 的输入设备 123456789101112131415161718192021222324252627282930[-&gt;synaptics_dsx_core.c]static int synaptics_rmi4_set_input_dev(struct synaptics_rmi4_data *rmi4_data)&#123; int retval; const struct synaptics_dsx_board_data *bdata = rmi4_data-&gt;hw_if-&gt;board_data; rmi4_data-&gt;input_dev = input_allocate_device(); ...... retval = synaptics_rmi4_query_device(rmi4_data); .... //#define PLATFORM_DRIVER_NAME \"synaptics_dsxv26\"(synaptics_dsx_v2_6.h) rmi4_data-&gt;input_dev-&gt;name = PLATFORM_DRIVER_NAME; rmi4_data-&gt;input_dev-&gt;phys = INPUT_PHYS_NAME; rmi4_data-&gt;input_dev-&gt;id.product = SYNAPTICS_DSX_DRIVER_PRODUCT; rmi4_data-&gt;input_dev-&gt;id.version = SYNAPTICS_DSX_DRIVER_VERSION; rmi4_data-&gt;input_dev-&gt;dev.parent = rmi4_data-&gt;pdev-&gt;dev.parent; input_set_drvdata(rmi4_data-&gt;input_dev, rmi4_data); set_bit(EV_SYN, rmi4_data-&gt;input_dev-&gt;evbit); set_bit(EV_KEY, rmi4_data-&gt;input_dev-&gt;evbit); set_bit(EV_ABS, rmi4_data-&gt;input_dev-&gt;evbit); set_bit(BTN_TOUCH, rmi4_data-&gt;input_dev-&gt;keybit); set_bit(BTN_TOOL_FINGER, rmi4_data-&gt;input_dev-&gt;keybit); synaptics_rmi4_set_params(rmi4_data); ...... retval = input_register_device(rmi4_data-&gt;input_dev);&#125; 7.4、硬件相关操作当触摸屏按下，会产生中断，进而调用中断处理函数synaptics_rmi4_irq(): 123456789101112131415161718[-&gt;synaptics_dsx_core.c]static irqreturn_t synaptics_rmi4_irq(int irq, void *data)&#123; struct synaptics_rmi4_data *rmi4_data = data; const struct synaptics_dsx_board_data *bdata = rmi4_data-&gt;hw_if-&gt;board_data; if (IRQ_HANDLED == synaptics_filter_interrupt(data)) return IRQ_HANDLED; if (gpio_get_value(bdata-&gt;irq_gpio) != bdata-&gt;irq_on_state) goto exit; synaptics_rmi4_sensor_report(rmi4_data, true);exit: return IRQ_HANDLED;&#125; 进一步调用synaptics_rmi4_sensor_report(rmi4_data, true)处理数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[-&gt;synaptics_dsx_core.c]static void synaptics_rmi4_sensor_report(struct synaptics_rmi4_data *rmi4_data, bool report)&#123; int retval; unsigned char data[MAX_INTR_REGISTERS + 1]; unsigned char *intr = &amp;data[1]; bool was_in_bl_mode; struct synaptics_rmi4_f01_device_status status; struct synaptics_rmi4_fn *fhandler; struct synaptics_rmi4_exp_fhandler *exp_fhandler; struct synaptics_rmi4_device_info *rmi; rmi = &amp;(rmi4_data-&gt;rmi4_mod_info); .... retval = synaptics_rmi4_reg_read(rmi4_data, rmi4_data-&gt;f01_data_base_addr, data, rmi4_data-&gt;num_of_intr_regs + 1); ...... //读取寄存器数据 status.data[0] = data[0]; if (status.status_code == STATUS_CRC_IN_PROGRESS) &#123; retval = synaptics_rmi4_check_status(rmi4_data, &amp;was_in_bl_mode); .... retval = synaptics_rmi4_reg_read(rmi4_data, rmi4_data-&gt;f01_data_base_addr, status.data, sizeof(status.data)); ...... &#125; if (status.unconfigured &amp;&amp; !status.flash_prog) &#123; pr_notice(\"%s: spontaneous reset detected\\n\", __func__); &#125; //synaptics_rmi4_report_touch()上报数据 if (!list_empty(&amp;rmi-&gt;support_fn_list)) &#123; list_for_each_entry(fhandler, &amp;rmi-&gt;support_fn_list, link) &#123; if (fhandler-&gt;num_of_data_sources) &#123; if (fhandler-&gt;intr_mask &amp; intr[fhandler-&gt;intr_reg_num]) &#123; synaptics_rmi4_report_touch(rmi4_data, fhandler); &#125; &#125; &#125; &#125; mutex_lock(&amp;exp_data.mutex); if (!list_empty(&amp;exp_data.list)) &#123; list_for_each_entry(exp_fhandler, &amp;exp_data.list, link) &#123; if (!exp_fhandler-&gt;insert &amp;&amp; !exp_fhandler-&gt;remove &amp;&amp; (exp_fhandler-&gt;exp_fn-&gt;attn != NULL)) exp_fhandler-&gt;exp_fn-&gt;attn(rmi4_data, intr[0]); &#125; &#125; mutex_unlock(&amp;exp_data.mutex); return;&#125; 7.4.1、Input数据上报：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112[-&gt;synaptics_dsx_core.c]static void synaptics_rmi4_report_touch(struct synaptics_rmi4_data *rmi4_data, struct synaptics_rmi4_fn *fhandler)&#123; ...... switch (fhandler-&gt;fn_number) &#123; ...... case SYNAPTICS_RMI4_F12: touch_count_2d = synaptics_rmi4_f12_abs_report(rmi4_data, fhandler); if (touch_count_2d) rmi4_data-&gt;fingers_on_2d = true; else rmi4_data-&gt;fingers_on_2d = false; break; ...... default: break; &#125; return;&#125;static int synaptics_rmi4_f12_abs_report(struct synaptics_rmi4_data *rmi4_data, struct synaptics_rmi4_fn *fhandler)&#123; int retval; unsigned char touch_count = 0; /* number of touch points */ unsigned char index; unsigned char finger; unsigned char fingers_to_process; unsigned char finger_status; unsigned char size_of_2d_data; unsigned char gesture_type; unsigned short data_addr; int x; int y; int wx; int wy; int temp; struct synaptics_rmi4_f12_extra_data *extra_data; struct synaptics_rmi4_f12_finger_data *data; struct synaptics_rmi4_f12_finger_data *finger_data; static unsigned char finger_presence; static unsigned char stylus_presence; fingers_to_process = fhandler-&gt;num_of_data_points; data_addr = fhandler-&gt;full_addr.data_base; extra_data = (struct synaptics_rmi4_f12_extra_data *)fhandler-&gt;extra; size_of_2d_data = sizeof(struct synaptics_rmi4_f12_finger_data); ...... retval = synaptics_rmi4_reg_read(rmi4_data, data_addr + extra_data-&gt;data1_offset, (unsigned char *)fhandler-&gt;data, fingers_to_process * size_of_2d_data); if (retval &lt; 0) return 0; data = (struct synaptics_rmi4_f12_finger_data *)fhandler-&gt;data; mutex_lock(&amp;(rmi4_data-&gt;rmi4_report_mutex)); //根据触摸点数量循环上报input数据 for (finger = 0; finger &lt; fingers_to_process; finger++) &#123; finger_data = data + finger; finger_status = finger_data-&gt;object_type_and_status; x = (finger_data-&gt;x_msb &lt;&lt; 8) | (finger_data-&gt;x_lsb); y = (finger_data-&gt;y_msb &lt;&lt; 8) | (finger_data-&gt;y_lsb); if (rmi4_data-&gt;hw_if-&gt;board_data-&gt;swap_axes) &#123; temp = x; x = y; y = temp; temp = wx; wx = wy; wy = temp; &#125; if (rmi4_data-&gt;hw_if-&gt;board_data-&gt;x_flip) x = rmi4_data-&gt;sensor_max_x - x; if (rmi4_data-&gt;hw_if-&gt;board_data-&gt;y_flip) y = rmi4_data-&gt;sensor_max_y - y; switch (finger_status) &#123; case F12_FINGER_STATUS: case F12_GLOVED_FINGER_STATUS: input_report_key(rmi4_data-&gt;input_dev, BTN_TOUCH, 1); input_report_key(rmi4_data-&gt;input_dev, BTN_TOOL_FINGER, 1); input_report_abs(rmi4_data-&gt;input_dev, ABS_MT_POSITION_X, x); input_report_abs(rmi4_data-&gt;input_dev, ABS_MT_POSITION_Y, y); ...... &#125; ...... input_sync(rmi4_data-&gt;input_dev); mutex_unlock(&amp;(rmi4_data-&gt;rmi4_report_mutex)); return touch_count;&#125; 调用input_report_key()、input_report_abs()、input_sync() 上报、同步数据。 （八）、参考文档(特别感谢各位前辈的分析和图示)：Linux/Android—-Input系统Android Input子系统浅谈Android(Linux) 输入子系统解析input子系统分析之三:驱动模块Linux驱动框架之—-Input子系统input子系统事件处理层(evdev)的环形缓冲区linux input输入子系统分析《四》：input子系统整体流程全面分析Linux input子系统分析之二：深入剖析input_handler、input_core、input_device","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android 7.1.2 (Android N) Android WindowManagerService 窗口管理服务 分析 [i.wonder~]","slug":"Android-7-1-2-Android-N-Android-WindowManagerService-窗口管理服务分析-i-wonder","date":"2018-02-28T16:00:00.000Z","updated":"2018-04-19T14:30:05.646Z","comments":true,"path":"2018/03/01/Android-7-1-2-Android-N-Android-WindowManagerService-窗口管理服务分析-i-wonder/","link":"","permalink":"http://zhoujinjian.cc/2018/03/01/Android-7-1-2-Android-N-Android-WindowManagerService-窗口管理服务分析-i-wonder/","excerpt":"窗口管理系统WMS是Android中的主要子系统之一，它涉及到App中组件的管理，系统和应用窗口的管理和绘制等工作。由于其涉及模块众多，且与用户体验密切相关，所以它也是Android当中最为复杂的子系统之一。一个App从启动到主窗口显示出来，需要App，ActivityManagerService（AMS），WindowManagerService（WMS），SurfaceFlinger（SF）等几个模块相互合作。App负责业务逻辑，绘制自己的视图；AMS管理组件、进程信息和Activity的堆栈及状态等等；WMS管理Activity对应的窗口及子窗口，还有系统窗口等；SF用于管理图形缓冲区，将App绘制的东西合成渲染在屏幕上。","text":"窗口管理系统WMS是Android中的主要子系统之一，它涉及到App中组件的管理，系统和应用窗口的管理和绘制等工作。由于其涉及模块众多，且与用户体验密切相关，所以它也是Android当中最为复杂的子系统之一。一个App从启动到主窗口显示出来，需要App，ActivityManagerService（AMS），WindowManagerService（WMS），SurfaceFlinger（SF）等几个模块相互合作。App负责业务逻辑，绘制自己的视图；AMS管理组件、进程信息和Activity的堆栈及状态等等；WMS管理Activity对应的窗口及子窗口，还有系统窗口等；SF用于管理图形缓冲区，将App绘制的东西合成渲染在屏幕上。 【博客原图链接】源码（部分）：/frameworks/base/services/core/java/com/android/server/am/ ActivityStack.java ActivityManagerService.java ActivityStackSupervisor.java ActivityStarter.java ActivityRecord.java /frameworks/base/core/java/android/view/ WindowManagerImpl.java ViewManager.java WindowManagerGlobal.java ViewRootImpl.java Choreographer.java IWindowSession.aidl DisplayEventReceiver.java SurfaceControl.java Surface.java SurfaceSession.java /frameworks/base/services/core/java/com/android/server/wm/ WindowManagerService.java AppWindowAnimator.java AppTransition.java AppWindowToken.java Session.java WindowState.java WindowAnimator.java WindowStateAnimator.java WindowSurfacePlacer.java WindowSurfaceController.java 【博客原图链接】我们先看一下窗口启动、退出过程动态图，之后再详细分析： （一）、Window 组织方式ActivityManagerService（AMS），WindowManagerService（WMS），SurfaceFlinger（SF）等几个模块相互合作。App负责业务逻辑，绘制自己的视图；AMS管理组件、进程信息和Activity的堆栈及状态等等；WMS管理Activity对应的窗口及子窗口，还有系统窗口等；SF用于管理图形缓冲区，将App绘制的东西合成渲染在屏幕上。 窗口管理系统主要框架： 主要对象功能介绍： WindowManagerService负责完成窗口的管理工作 WindowState和应用端窗口一一对应，应用调用WMS添加窗口时，最终会在WindowManagerService.addWindow()创建一个WindowState与之一一对应 WindowToken是一个句柄，保存了所有具有同一个token的WindowState。应用请求WindowManagerService添加窗口的时候，提供了一个token，该token标识了被添加窗口的归属，WindowManagerService为该token生成一个WindowToken对象，所有token相同的WindowState被关联到同一个WindowToken，如输入法添加窗口时，会传递一个IBinder mCurToken，墙纸服务添加窗口时，会传递一个WallpaperConnection::final Binder mToken。 AppWindowToken继承于WindowToken，专门用于标识一个Activity。AppWindowToken里的token实际上就是指向了一个Activity。ActivityManagerService通知应用启动的时候，在服务端生成一个token用于标识该Activity，并且把该token传递到应用客户端，客户端的Activity在申请添加窗口时，以该token作为标识传递到WindowManagerService。同一个Activity中的主窗口、对话框窗口、菜单窗口都关联到同一个AppWindowToken。 Session表示一个客户端和服务端的交互会话。一般来说不同的应用通过不同的会话来和WindowManagerService交互，但是处于同一个进程的不同应用通过同一个Session来交互。 1.1、Android Token介绍Token是ActivityRecord的内部静态类，我们先来看下Token的继承关系，Token extends IApplicationToken.Stub，从IApplicationToken.Stub类进行继承，根据Binder的机制可以知道Token是一个匿名Binder实体类，这个匿名Binder实体会传递给其他进程，其他进程会拿到Token的代理端。 我们知道匿名Binder有两个比较重要的用途，一个是拿到Binder代理端后可跨Binder调用实体端的函数接口，另一个作用便是在多个进程中标识同一个对象。往往这两个作用是同时存在的，比如我们这里研究的Token就同时存在这两个作用，但最重要的便是后者，Token标识了一个ActivityRecord对象，即间接标识了一个Activity。 Token梳理： 分析源码，我们发现，大多数 token 的对象，都表示一个 IBinder 对象。提到 IBinder，大家一点也不陌生，就是 Android 的 IPC 通信机制。在创建窗口过程中，涉及到的 IPC 通信，无非包含两方面，一个是 WmS 用来跟应用所在的进程进行通信的 ViewRootImpl.W 类的对象，另一个是指向一个 ActivityRecord 的对象，自然应该是WMS用来跟 AMS进行通信的了。我们梳理了一下，token 以下几处的定义，分别来讲讲这里的 token 代表什么。 分析一下 View 的 AttachInfo 的赋值。ViewRootImpl 在构建方法里，会初始化一个 AttachInfo 实例，把它的 Session，以及 W类对象赋值给 AttachInfo。分析可以看到，AttachInfo 中的 mWindowToken，与mWindow 都是指向 ViewRootImpl 中的 mWindow(W类实例)。当一个 View attach 到窗口后，ViewRootImpl会执行performTraversals，如果发现是首次调用会，会把自己的 mAttachInfo 传递给根 View（通过dispatchAttachedToWindow），告诉 View 树现在已经 attch to Window 了，马上可以显示了。根 View（一般是 ViewGroup）会把这个信息，遍历地传递给 View 树中的每一个子 View，这样每个 View 的 mAttachInfo 都被赋值为 ViewRootImp 的 mAttachInfo了。 1.1.1、Token对象的创建下面这个图是Token的传递，首先会传递到WMS中，接着会传递到应用进程ActivityThread中，下面来具体分析这个传递流程。 总体流程图： 我们之前分析：【Android 7.1.2 (Android N) Activity启动流程分析】 在启动Activity过程中会调用ActivityStarter.startActivityLocked() 123456789101112131415161718192021222324[-&gt;ActivityStarter.java] final int startActivityLocked(IApplicationThread caller, Intent intent, Intent ephemeralIntent, String resolvedType, ActivityInfo aInfo, ResolveInfo rInfo, IVoiceInteractionSession voiceSession, IVoiceInteractor voiceInteractor, IBinder resultTo, String resultWho, int requestCode, int callingPid, int callingUid, String callingPackage, int realCallingPid, int realCallingUid, int startFlags, ActivityOptions options, boolean ignoreTargetSecurity, boolean componentSpecified, ActivityRecord[] outActivity, ActivityStackSupervisor.ActivityContainer container, TaskRecord inTask) &#123; ...... ActivityRecord sourceRecord = null; ActivityRecord resultRecord = null; ...... ActivityRecord r = new ActivityRecord(mService, callerApp, callingUid, callingPackage, intent, resolvedType, aInfo, mService.mConfiguration, resultRecord, resultWho, requestCode, componentSpecified, voiceSession != null, mSupervisor, container, options, sourceRecord); ...... try &#123; err = startActivityUnchecked(r, sourceRecord, voiceSession, voiceInteractor, startFlags, true, options, inTask); &#125; return err;&#125; 可以看到在startActivityLocked()中创建了一个ActivityRecord对象 1234567891011121314[-&gt;ActivityRecord.java]final IApplicationToken.Stub appToken; // window manager tokenActivityRecord(ActivityManagerService _service, ProcessRecord _caller, int _launchedFromUid, String _launchedFromPackage, Intent _intent, String _resolvedType, ActivityInfo aInfo, Configuration _configuration, ActivityRecord _resultTo, String _resultWho, int _reqCode, boolean _componentSpecified, boolean _rootVoiceInteraction, ActivityStackSupervisor supervisor, ActivityContainer container, ActivityOptions options, ActivityRecord sourceRecord) &#123; service = _service; appToken = new Token(this, service); ...... &#125; 在ActivityRecord的构造函数中创建，标识着当前这个ActivityRecord，即间接代表着一个Activity。 1.1.2、AMS调用WMS的addAPPToken()接口在启动一个Activity时，会调用startActivityLocked()来在WMS中添加一个AppWindowToken对象 startActivityLocked()创建ActivityRecord对象后会继续调用startActivityUnchecked()方法。 12345678[-&gt;ActivityStarter.java]private int startActivityUnchecked(final ActivityRecord r, ActivityRecord sourceRecord, IVoiceInteractionSession voiceSession, IVoiceInteractor voiceInteractor, int startFlags, boolean doResume, ActivityOptions options, TaskRecord inTask) &#123; ...... mTargetStack.startActivityLocked(mStartActivity, newTask, mKeepCurTransition, mOptions); ...... &#125; 12345678910111213141516171819[-&gt;ActivityStack.java]final void startActivityLocked(ActivityRecord r, boolean newTask, boolean keepCurTransition, ActivityOptions options) &#123; ...... addConfigOverride(r, task); ...... &#125; void addConfigOverride(ActivityRecord r, TaskRecord task) &#123; final Rect bounds = task.updateOverrideConfigurationFromLaunchBounds(); // 跳转到WMS mWindowManager.addAppToken(task.mActivities.indexOf(r), r.appToken, r.task.taskId, mStackId, r.info.screenOrientation, r.fullscreen, (r.info.flags &amp; FLAG_SHOW_FOR_ALL_USERS) != 0, r.userId, r.info.configChanges, task.voiceSession != null, r.mLaunchTaskBehind, bounds, task.mOverrideConfig, task.mResizeMode, r.isAlwaysFocusable(), task.isHomeTask(), r.appInfo.targetSdkVersion, r.mRotationAnimationHint); r.taskConfigOverride = task.mOverrideConfig; &#125; 我们继续看下WindowManager.addAppToken()方法 123456789101112131415161718192021222324252627282930313233343536373839404142[-&gt;WindowManagerService.java] @Override public void addAppToken(int addPos, IApplicationToken token, int taskId, int stackId, int requestedOrientation, boolean fullscreen, boolean showForAllUsers, int userId, int configChanges, boolean voiceInteraction, boolean launchTaskBehind, Rect taskBounds, Configuration config, int taskResizeMode, boolean alwaysFocusable, boolean homeTask, int targetSdkVersion, int rotationAnimationHint) &#123; ...... synchronized(mWindowMap) &#123; AppWindowToken atoken = findAppWindowToken(token.asBinder()); if (atoken != null) &#123; Slog.w(TAG_WM, \"Attempted to add existing app token: \" + token); return; &#125; //根据ActivityRecord中IApplicationToken.Stub的代理，创建AppWindowToken atoken = new AppWindowToken(this, token, voiceInteraction); atoken.inputDispatchingTimeoutNanos = inputDispatchingTimeoutNanos; atoken.appFullscreen = fullscreen; atoken.showForAllUsers = showForAllUsers; atoken.targetSdk = targetSdkVersion; atoken.requestedOrientation = requestedOrientation; atoken.layoutConfigChanges = (configChanges &amp; (ActivityInfo.CONFIG_SCREEN_SIZE | ActivityInfo.CONFIG_ORIENTATION)) != 0; atoken.mLaunchTaskBehind = launchTaskBehind; atoken.mAlwaysFocusable = alwaysFocusable; atoken.mRotationAnimationHint = rotationAnimationHint; Task task = mTaskIdToTask.get(taskId); if (task == null) &#123; task = createTaskLocked(taskId, stackId, userId, atoken, taskBounds, config); &#125; task.addAppToken(addPos, atoken, taskResizeMode, homeTask); //将atoken放入到mTokenMap中，等应用程序addWindow时，进行身份验证 //其中token.asBinder()是IApplicationToken.Stub的代理，atoken就是根据代理，得到对应AppWindowToken mTokenMap.put(token.asBinder(), atoken); // Application tokens start out hidden. atoken.hidden = true; atoken.hiddenRequested = true; &#125; &#125; 1.1.3、AMS跨Binder调用应用进程的scheduleLaunchActivity()将Token传递给上层应用进程当框架通过ApplicationThread的代理回调到ActivityThread的时候，将对应的步骤一种生成的token代理传入。 ActivityStackSupervisor.realStartActivityLocked() 1234567891011121314 [-&gt;ActivityStackSupervisor.java] final boolean realStartActivityLocked(ActivityRecord r, ProcessRecord app, boolean andResume, boolean checkConfig) throws RemoteException &#123; ...... app.thread.scheduleLaunchActivity(new Intent(r.intent), r.appToken, System.identityHashCode(r), r.info, new Configuration(mService.mConfiguration), new Configuration(task.mOverrideConfig), r.compat, r.launchedFromPackage, task.voiceInteractor, app.repProcState, r.icicle, r.persistentState, results, newIntents, !andResume, mService.isNextTransitionForward(), profilerInfo); ...... return true;&#125; 这里通过调用IApplicationThread的方法实现的，这里调用的是scheduleLaunchActivity()方法，所以真正执行的是ActivityThread中的scheduleLaunchActivity()。 12345678910111213141516171819[-&gt; ActivityThread.java :ApplicationThread] @Override public final void scheduleLaunchActivity(Intent intent, IBinder token, int ident, ActivityInfo info, Configuration curConfig, Configuration overrideConfig, CompatibilityInfo compatInfo, String referrer, IVoiceInteractor voiceInteractor, int procState, Bundle state, PersistableBundle persistentState, List&lt;ResultInfo&gt; pendingResults, List&lt;ReferrerIntent&gt; pendingNewIntents, boolean notResumed, boolean isForward, ProfilerInfo profilerInfo) &#123; updateProcessState(procState, false); ActivityClientRecord r = new ActivityClientRecord(); //传递给了ActivityThread的token，这个token就是IApplicationToken.Stub的代理 r.token = token; ...... updatePendingConfiguration(curConfig); sendMessage(H.LAUNCH_ACTIVITY, r); &#125; 1.1.4、Activity窗口添加过程详细过程请查看：【Android 7.1.2 (Android N) Activity-Window加载显示流程分析】 1234567891011121314151617181920212223242526272829303132333435[-&gt;ActivityThread.java] final void handleResumeActivity(IBinder token, boolean clearHide, boolean isForward, boolean reallyResume, int seq, String reason) &#123; ActivityClientRecord r = mActivities.get(token); ...... r = performResumeActivity(token, clearHide, reason); ...... if (r.window == null &amp;&amp; !a.mFinished &amp;&amp; willBeVisible) &#123; //获得为当前Activity创建的窗口PhoneWindow对象 r.window = r.activity.getWindow(); //获取为窗口创建的视图DecorView对象 View decor = r.window.getDecorView(); decor.setVisibility(View.INVISIBLE); //在attach函数中就为当前Activity创建了WindowManager对象 ViewManager wm = a.getWindowManager(); //得到该视图对象的布局参数 WindowManager.LayoutParams l = r.window.getAttributes(); //将视图对象保存到Activity的成员变量mDecor中 a.mDecor = decor; l.type = WindowManager.LayoutParams.TYPE_BASE_APPLICATION; l.softInputMode |= forwardBit; ...... if (a.mVisibleFromClient &amp;&amp; !a.mWindowAdded) &#123; a.mWindowAdded = true; //将创建的视图对象DecorView添加到Activity的窗口管理器中 wm.addView(decor, l); &#125; ...... if (r.activity.mVisibleFromClient) &#123; r.activity.makeVisible(); &#125; &#125; ...... &#125;&#125; 进而层层调用到：ViewRootImpl.setView() 12[-&gt;ViewRootImpl.java]WindowManager.LayoutParams l = r.window.getAttributes(); ViewRootImpl.setView()函数中添加Activity窗口时在参数mWindowAttributes中携带Token代理对象 123456789101112131415[-&gt;ViewManager.java]public int addWindow(Session session, IWindow client, int seq, WindowManager.LayoutParams attrs, int viewVisibility, int displayId, Rect outContentInsets, Rect outStableInsets, InputChannel outInputChannel) &#123; ...... boolean addToken = false; //attrs这个是应用程序ActivityClientRecord中传递过来的参数，其中的attrs.token就是步骤三种的r.token WindowToken token = mTokenMap.get(attrs.token); ...... win = new WindowState(this, session, client, token, attachedWindow, appOp[0], seq, attrs, viewVisibility, displayContent); mWindowMap.put(client.asBinder(), win); ...... &#125; 根据Binder机制可以知道从上层应用传递过来的Token代理对象会转换成SystemServer进程中的Token本地对象，后者与第2步中从Token对象是同一个对象，所以上面调用mTokenMap.get(attrs.token)时便能返回正确返回一个WindowToken（这个WindowToken其实是一个APPWindowToken），这样添加的窗口也就跟Activity关联上了。 1.2、WMS组织方式Activity管理服务ActivityManagerService中每一个ActivityRecord对象在Window管理服务WindowManagerService中都对应有一个AppWindowToken对象。 此外，在输入法管理服务InputMethodManagerService中，每一个输入法窗口都对应有一个Binder对象，这个Binder对象在Window管理服务WindowManagerService又对应有一个WindowToken对象。 与输入法窗口类似，在壁纸管理服务WallpaperManagerService中，每一个壁纸窗口都对应有一个Binder对象，这个Binder对象在Window管理服务WindowManagerService也对应有一个WindowToken对象。 在Window管理服务WindowManagerService中，无论是AppWindowToken对象，还是WindowToken对象，它们都是用来描述一组有着相同令牌的窗口的，每一个窗口都是通过一个WindowState对象来描述的。例如，一个Activity组件窗口可能有一个启动窗口（Starting Window），还有若干个子窗口，那么这些窗口就会组成一组，并且都是以Activity组件在Window管理服务WindowManagerService中所对应的AppWindowToken对象为令牌的。从抽象的角度来看，就是在Window管理服务WindowManagerService中，每一个令牌（AppWindowToken或者WindowToken）都是用来描述一组窗口（WindowState）的，并且每一个窗口的子窗口也是与它同属于一个组，即都有着相同的令牌。 其中，Activity Stack是在ActivityManagerService服务中创建的，Token List和Window Stack是在WindowManagerService中创建的，而Binder for IM和Binder for WP分别是在InputMethodManagerService服务和WallpaperManagerService服务中创建的，用来描述一个输入法窗口和一个壁纸窗口。 1.3、WMS窗口类型添加一个窗口是通过 WindowManagerGlobal.addView()来完成的，分析 addView 方法的参数，有三个参数是必不可少的，view，params，以及 display。而 display 一般直接取 WindowMnagerImpl 中的 mDisplay，表示要输出的显示设备。view 自然表示要显示的 View，而 params 是 WindowManager.LayoutParams，用来描述这个 view 的些窗口属性，其中一个重要的参数 type，用来描述窗口的类型。 12345678910111213[-&gt;WindowManagerGlobal] public void addView(View view, ViewGroup.LayoutParams params, Display display, Window parentWindow) &#123; if (view == null) &#123; throw new IllegalArgumentException(\"view must not be null\"); &#125; if (display == null) &#123; throw new IllegalArgumentException(\"display must not be null\"); &#125; if (!(params instanceof WindowManager.LayoutParams)) &#123; throw new IllegalArgumentException(\"Params must be WindowManager.LayoutParams\"); &#125; ``` } 1234567打开WindowManager类，看到静态内部类。``` java[-&gt;WindowManager]public static class LayoutParams extends ViewGroup.LayoutParams implements Parcelable &#123;......&#125; 可以看到在LayoutParams中，有2个比较重要的参数: flags,type。 我们简要的分析一下flags,该参数表示Window的属性，它有很多选项，通过这些选项可以控制Window的显示特性，这里主要介绍几个比较常用的选项。 FLAG_NOT_FOCUSABLE 表示Window不需要获取焦点，也不需要接收各种输入事件，此标记会同时启用FLAG_NOT_TOUCH_MODAL，最终事件会直接传递给下层具有焦点的Window。 FLAG_NOT_TOUCH_MODAL 系统会将当前Window区域以外的单击事件传递给底层的Window，当前Window区域以内的单击事件则自己处理，这个标记很重要，一般来说都需要开启此标记，否则其他Window将无法接收到单击事件。 FLAG_SHOW_WHEN_LOCKED 开启此模式可以让Window显示在锁屏的界面上。 Type参数表示Window的类型，Window有三种类型，分别是应用Window、子Window、系统Window。应用类Window对应着一个Activity。子Window不能单独存在，它需要附属在特定的父Window之中，比如常见的PopupWindow就是一个子Window。有些系统Window是需要声明权限才能创建的Window，比如Toast和系统状态栏这些都是系统Window。 1.3.1、应用窗口Activity 对应的窗口类型是应用窗口， 所有 Activity 默认的窗口类型是 TYPE_BASE_APPLICATION。 WindowManager 的 LayoutParams 的默认类型是 TYPE_APPLICATION。 Dialog 并没有设置type，所以也是默认的窗口类型即 TYPE_APPLICATION。 1234567[-&gt;WindowManager.LayoutParams]public LayoutParams() &#123; super(LayoutParams.MATCH_PARENT, LayoutParams.MATCH_PARENT); type = TYPE_APPLICATION; format = PixelFormat.OPAQUE;&#125; type层级 类型 FIRST_APPLICATION_WINDOW=1 开始应用程序窗口，第一个普通应用窗口 TYPE_BASE_APPLICATION=1 所有程序窗口的base窗口，其他应用程序窗口都显示在它上面 TYPE_APPLICATION=2 普通应用程序窗口，token必须设置为Activity的token来指定窗口属于谁 TYPE_APPLICATION_STARTING=3 应用程序启动时先显示此窗口，当真正的窗口配置完成后，关闭此窗口 LAST_APPLICATION_WINDOW=99 最后一个应用窗口 1.3.2、子窗口子窗口不能单独存在，它需要附属在特定的父Window之中，例如开篇第一张图，绿色框框即为popupWindow，它就是子窗口，类型一般为TYPE_APPLICATION_PANEL。之所以称为子窗口，即它的父窗口显示时，子窗口才显示。父窗口不显示，它也不显示。追随父窗口。 type层级 类型 FIRST_SUB_WINDOW=1000 第一个子窗口 TYPE_APPLICATION_PANEL=1000 应用窗口的子窗口,popupWindow的默认类型 TYPE_APPLICATION_MEDIA=1001 媒体窗口 TYPE_APPLICATION_SUB_PANEL=1002 TYPE_APPLICATION_PANE的子窗口 TYPE_APPLICATION_ATTACHED_DIALOG=1003 对话框，类似于面板窗口(OptionMenu,ContextMenu) TYPE_APPLICATION_MEDIA_OVERLAY=1004 媒体信息，显示在媒体层和程序窗口之间，需要实现半透明效果 LAST_SUB_WINDOW=1999 最后一个子窗口 1.3.3、系统窗口系统窗口跟应用窗口不同，不需要对应 Activity。跟子窗口不同，不需要有父窗口。一般来讲，系统窗口应该由系统来创建的，例如发生异常，ANR时的提示框，又如系统状态栏，屏保等。但是，Framework 还是定义了一些，可以被应用所创建的系统窗口，如 TYPE _TOAST，TYPE INPUT METHOD，TYPE _WALLPAPTER 等等。 type层级 类型 FIRST_SYSTEM_WINDOW=2000 第一个系统窗口 TYPE_STATUS_BAR=2000 状态栏，只能有一个状态栏，位于屏幕顶端 TYPE_SEARCH_BAR =2001 搜索栏 TYPE_PHONE=2002 电话窗口，它用于电话交互 TYPE_SYSTEM_ALERT=2003 系统警告，出现在应用程序窗口之上 TYPE_KEYGUARD=2004 锁屏窗口 TYPE_TOAST=2005 信息窗口，用于显示Toast TYPE_SYSTEM_OVERLAY=2006 系统顶层窗口，显示在其他内容之上，此窗口不能获得输入焦点，否则影响锁屏 TYPE_PRIORITY_PHONE=2007 当锁屏时显示的来电显示窗口 TYPE_SYSTEM_DIALOG=2008 系统对话框 TYPE_KEYGUARD_DIALOG=2009 锁屏时显示的对话框 TYPE_SYSTEM_ERROR=2010 系统内部错误提示 TYPE_INPUT_METHOD=2011 输入法窗口，显示于普通应用/子窗口之上 TYPE_INPUT_METHOD_DIALOG=2012 输入法中备选框对应的窗口 TYPE_WALLPAPER=2013 墙纸窗口 TYPE_STATUS_BAR_PANEL=2014 滑动状态条后出现的窗口 TYPE_SECURE_SYSTEM_OVERLAY=2015 安全系统覆盖窗口 …… …… LAST_SYSTEM_WINDOW=2999 最后一个系统窗口 那么，这个type层级到底有什么作用呢？ Window是分层的，每个Window都有对应的z-ordered，（z轴，从1层层叠加到2999，你可以将屏幕想成三维坐标模式）层级大的会覆盖在层级小的Window上面。 在三类Window中，应用Window的层级范围是1~99。子Window的层级范围是1000~1999，系统Window的层级范围是2000~2999，这些层级范围对应着WindowManager.LayoutParams的type参数。如果想要Window位于所有Window的最顶层，那么采用较大的层级即可。另外有些系统层级的使用是需要声明权限的。 （二）、Window Size（大小）和 Window Position（位置） 计算过程之前在【Android 7.1.2 (Android N) Android Graphics 系统分析 [i.wonder~]】分析过，当Vsync事件到来时，就会通过Choreographer的postCallback()，接着执行mTraversalRunnable对象的run()方法。 mTraversalRunnable对象的类型为TraversalRunnable，该类实现了Runnable接口，在其run()函数中调用了doTraversal()函数来完成窗口布局。为了分析的连贯性，这里重新贴一下源码： 1234567891011121314151617[-&gt;ViewRootImpl.java]final class TraversalRunnable implements Runnable &#123; @Override public void run() &#123; doTraversal(); &#125;&#125;final TraversalRunnable mTraversalRunnable = new TraversalRunnable();void doTraversal() &#123; if (mTraversalScheduled) &#123; mTraversalScheduled = false; mHandler.getLooper().getQueue().removeSyncBarrier(mTraversalBarrier); ...... performTraversals(); ...... &#125;&#125; performTraversals()函数相当复杂，其主要实现以下几个重要步骤： 1.执行窗口测量； 2.执行窗口注册； 3.执行窗口布局； 4.执行窗口绘图； 12345678910111213141516171819202122232425262728293031[-&gt;ViewRootImpl.java] private void performTraversals() &#123; ...... /****************执行窗口测量******************/ if (layoutRequested) &#123; windowSizeMayChange |= measureHierarchy(host, lp, res, desiredWindowWidth, desiredWindowHeight); &#125; ...... /****************向WMS服务添加窗口******************/ if (mFirst || windowShouldResize || insetsChanged || viewVisibilityChanged || params != null || mForceNextWindowRelayout) &#123; ...... try &#123; ...... relayoutResult = relayoutWindow(params, viewVisibility, insetsPending); ...... &#125; ...... &#125; /****************执行窗口布局******************/ if (didLayout) &#123; performLayout(lp, mWidth, mHeight); ...... &#125; /****************执行窗口绘制******************/ if (!cancelDraw &amp;&amp; !newSurface) &#123; ...... performDraw(); &#125; ......&#125; 2.1、 Android 屏幕区域介绍首先来看relayoutWindow()。relayoutWindow() 是Window Manager Service 重要工作之一，它的流程如下图所示： 每个View将期望窗口尺寸交给WMS（WindowManager Service). WMS 将所有的窗口大小以及当前的Overscan区域传给WPM （WindowManager Policy). WPM根据用户配置确定每个Window在最终Display输出上的位置以及需要分配的Surface大小。 返回这些信息给每个View，他们将在给会的区域空间里绘图。 Android里定义了很多区域,如下图所示 Overscan: Overscan 是电视特有的概念，上图中黄色部分就是Overscan区域，指的是电视机屏幕四周某些不可见的区域（因为电视特性，这部分区域的buffer内容显示时被丢弃），也意味着如果窗口的某些内容画在这个区域里，它在某些电视上就会看不到。为了避免这种情况发生，通常要求UI不要画在屏幕的边角上，而是预留一定的空间。因为Overscan的区域大小随着电视不 同而不同，它一般由终端用户通过UI指定，（比如说GoogleTV里就有确定Overscan大小的应用）。 OverscanScreen, Screen: OverscanScreen 是包含Overscan区域的屏幕大小,而Screen则为去除Overscan区域后的屏幕区域, OverscanScreen &gt; Screen. Restricted and Unrestricted: 某些区域是被系统保留的，比如说手机屏幕上方的状态栏(如图纸绿色区域）和下方的导航栏，根据是否包括这些预留的区域，Android把区域分为Unrestricted Area 和 Resctrited Aread, 前者包括这部分预留区域，后者则不包含, Unrestricted area &gt; Rectricted area。 mFrame, mDisplayFrame, mContainingFrame Frame指的是一片内存区域, 对应于屏幕上的一块矩形区域. mFrame的大小就是Surface的大小, 如上上图中的蓝色区域. mDisplayFrame 和 mContainingFrame 一般和mFrame 大小一致. mXXX 是Window(ViewRootImpl, Windowstate) 里面定义的成员变量. mContentFrame, mVisibleFrame 一个Surface的所有内容不一定在屏幕上都得到显示, 与Overscan重叠的部分会被截掉, 系统的其他窗口也会遮挡掉部分区域 (比如短信窗口，ContentFrame是800x600(没有Status Bar), 但当输入法窗口弹出是，变成了800x352), 剩下的区域称为Visible Frame, UI内容只有画在这个区域里才能确保可见. 所以也称为Content Frame. mXXX也是Window(ViewRootImpl, WindowState) 里面定义的成员变量. Insets insets的定义如上图所示, 用了表示某个Frame的边缘大小. 2.2、 Window 大小位置计算过程在Android系统中，Activity窗口的大小是由WindowManagerService服务来计算的。WindowManagerService服务会根据屏幕及其装饰区的大小来决定Activity窗口的大小。一个Activity窗口只有知道自己的大小之后，才能对它里面的UI元素进行测量、布局以及绘制。 一般来说，Activity窗口的大小等于整个屏幕的大小，但是它并不占据着整块屏幕。为了理解这一点，我们首先分析一下Activity窗口的区域是如何划分的。 我们知道，Activity窗口的上方一般会有一个状态栏，用来显示3G信号、电量使用等图标，如图 从Activity窗口剔除掉状态栏所占用的区域之后，所得到的区域就称为内容区域（Content Region）。顾名思义，内容区域就是用来显示Activity窗口的内容的。我们再抽象一下，假设Activity窗口的四周都有一块类似状态栏的区域，那么将这些区域剔除之后，得到中间的那一块区域就称为内容区域，而被剔除出来的区域所组成的区域就称为内容边衬区域（Content Insets）。Activity窗口的内容边衬区域可以用一个四元组（content-left, content-top, content-right, content-bottom）来描述，其中，content-left、content-right、content-top、content-bottom分别用来描述内容区域与窗口区域的左右上下边界距离。 我们还知道，Activity窗口有时候需要显示输入法窗口，如图。 这时候Activity窗口的内容区域的大小有可能没有发生变化，这取决于它的Soft Input Mode。我们假设Activity窗口的内容区域没有发生变化，但是它在底部的一些区域被输入法窗口遮挡了，即它在底部的一些内容是不可见的。从Activity窗口剔除掉状态栏和输入法窗口所占用的区域之后，所得到的区域就称为可见区域（Visible Region）。同样，我们再抽象一下，假设Activity窗口的四周都有一块类似状态栏和输入法窗口的区域，那么将这些区域剔除之后，得到中间的那一块区域就称为可见区域，而被剔除出来的区域所组成的区域就称为可见边衬区域（Visible Insets）。Activity窗口的可见边衬区域可以用一个四元组（visible-left, visible-top, visible-right, visible-bottom）来描述，其中，visible-left、visible-right、visible-top、visible-bottom分别用来描述可见区域与窗口区域的左右上下边界距离。 在大多数情况下，Activity窗口的内容区域和可见区域的大小是一致的，而状态栏和输入法窗口所占用的区域又称为屏幕装饰区。理解了这些概念之后，我们就可以推断，WindowManagerService服务实际上就是需要根据屏幕以及可能出现的状态栏和输入法窗口的大小来计算出Activity窗口的整体大小及其内容区域边衬和可见区域边衬的大小。有了这三个数据之后，Activity窗口就可以对它里面的UI元素进行测量、布局以及绘制等操作了。 总体流程图： 这个过程可以分为13个步骤，接下来我们就详细分析每一个步骤。 2.2.1、ViewRootImpl.performTraversals()123456789101112131415161718192021222324252627282930313233343536373839[-&gt;ViewRootImpl.java]private void performTraversals() &#123; // cache mView since it is used so much below... final View host = mView; ...... WindowManager.LayoutParams lp = mWindowAttributes; int desiredWindowWidth; int desiredWindowHeight; ...... Rect frame = mWinFrame; if (mFirst) &#123; mFullRedrawNeeded = true; mLayoutRequested = true; //第一次被请求执行测量、布局和绘制操作，desiredWindowWidth和desiredWindowHeight等于Display Size，否则mWinFrame保存的宽度和高度值。 if (shouldUseDisplaySize(lp)) &#123; Point size = new Point(); mDisplay.getRealSize(size); desiredWindowWidth = size.x; desiredWindowHeight = size.y; &#125; else &#123; Configuration config = mContext.getResources().getConfiguration(); desiredWindowWidth = dipToPx(config.screenWidthDp); desiredWindowHeight = dipToPx(config.screenHeightDp); &#125; ...... host.dispatchAttachedToWindow(mAttachInfo, 0); mAttachInfo.mTreeObserver.dispatchOnWindowAttachedChange(true); dispatchApplyInsets(host); &#125; else &#123; //不是第一次请求，当desiredWindowWidth != mWidth || desiredWindowHeight != mHeight，说明Activity窗口的大小发生了变化，这时候windowSizeMayChange = true，以便接下来对Activity窗口大小变化进行处理 desiredWindowWidth = frame.width(); desiredWindowHeight = frame.height(); if (desiredWindowWidth != mWidth || desiredWindowHeight != mHeight) &#123; mFullRedrawNeeded = true; mLayoutRequested = true; windowSizeMayChange = true; &#125; &#125; 这段代码用来获得Activity窗口的当前宽度desiredWindowWidth和当前高度desiredWindowHeight。 继续阅读代码： 12345678910111213141516171819202122232425262728293031323334353637[-&gt;ViewRootImpl.java::performTraversals()]boolean layoutRequested = mLayoutRequested &amp;&amp; (!mStopped || mReportNextDraw);if (layoutRequested) &#123; final Resources res = mView.getContext().getResources(); if (mFirst) &#123; ...... &#125; else &#123; //AttachInfo对象用来描述Activity窗口的属性,mContentInsets和mVisibleInsets分别用来描述Activity窗口的当前内容边衬大小和可见边衬大小。 //判断Activity窗口的OverscanInsets、ContentInsets、StableInsets、VisibleInsets大小是否发生了变化 if (!mPendingOverscanInsets.equals(mAttachInfo.mOverscanInsets)) &#123; insetsChanged = true; &#125; ...... //WRAP_CONTENT表明Activity窗口的大小要等于内容区域的大小，同时等于Display size if (lp.width == ViewGroup.LayoutParams.WRAP_CONTENT || lp.height == ViewGroup.LayoutParams.WRAP_CONTENT) &#123; windowSizeMayChange = true; if (shouldUseDisplaySize(lp)) &#123; Point size = new Point(); mDisplay.getRealSize(size); desiredWindowWidth = size.x; desiredWindowHeight = size.y; &#125; else &#123; Configuration config = res.getConfiguration(); desiredWindowWidth = dipToPx(config.screenWidthDp); desiredWindowHeight = dipToPx(config.screenHeightDp); &#125; &#125; &#125; // Ask host how big it wants to be //知道了顶层Activity窗口大小从而计算Activity内各个子View的大小 windowSizeMayChange |= measureHierarchy(host, lp, res, desiredWindowWidth, desiredWindowHeight);&#125; 这段代码用来在Activity窗口主动请求WindowManagerService服务计算大小之前，对它的顶层视图进行一次测量操作。 继续阅读代码： 12345678910111213141516171819202122232425262728[-&gt;ViewRootImpl.java::performTraversals()] if (mFirst || windowShouldResize || insetsChanged || viewVisibilityChanged || params != null || mForceNextWindowRelayout) &#123; ...... try &#123; ...... //relayoutWindow来请求WMS计算Activity窗口的大小以及xxxInsets大小，并保存在PendingxxxInsets中 relayoutResult = relayoutWindow(params, viewVisibility, insetsPending); ...... if (contentInsetsChanged) &#123; mAttachInfo.mContentInsets.set(mPendingContentInsets); &#125; ...... if (visibleInsetsChanged) &#123; mAttachInfo.mVisibleInsets.set(mPendingVisibleInsets); &#125; ...... &#125; catch (RemoteException e) &#123; &#125; ...... //将计算得到的Activity窗口的左上角坐标保存在变量attachInfo所指向的一个AttachInfo对象的成员变量mWindowLeft和mWindowTop mAttachInfo.mWindowLeft = frame.left; mAttachInfo.mWindowTop = frame.top; //将计算得到的Activity窗口的宽度和高度保存在ViewRootImpl类的成员变量mWidth和mHeight中 if (mWidth != frame.width() || mHeight != frame.height()) &#123; mWidth = frame.width(); mHeight = frame.height(); &#125; 这段代码主要调用relayoutWindow()来请求WMS计算Activity窗口的大小以及边忖xxxInsets大小。计算完毕之后，分别保存在mPendingXXXInsets中。 继续阅读代码： 12345678910111213141516171819202122232425262728293031323334353637383940[-&gt;ViewRootImpl.java::performTraversals()] if (!mStopped || mReportNextDraw) &#123; boolean focusChangedDueToTouchMode = ensureTouchModeLocally( (relayoutResult&amp;WindowManagerGlobal.RELAYOUT_RES_IN_TOUCH_MODE) != 0); if (focusChangedDueToTouchMode || mWidth != host.getMeasuredWidth() || mHeight != host.getMeasuredHeight() || contentInsetsChanged || updatedConfiguration) &#123; int childWidthMeasureSpec = getRootMeasureSpec(mWidth, lp.width); int childHeightMeasureSpec = getRootMeasureSpec(mHeight, lp.height); ...... // Ask host how big it wants to be performMeasure(childWidthMeasureSpec, childHeightMeasureSpec); ...... int width = host.getMeasuredWidth(); int height = host.getMeasuredHeight(); boolean measureAgain = false; if (lp.horizontalWeight &gt; 0.0f) &#123; width += (int) ((mWidth - width) * lp.horizontalWeight); childWidthMeasureSpec = MeasureSpec.makeMeasureSpec(width, MeasureSpec.EXACTLY); measureAgain = true; &#125; if (lp.verticalWeight &gt; 0.0f) &#123; height += (int) ((mHeight - height) * lp.verticalWeight); childHeightMeasureSpec = MeasureSpec.makeMeasureSpec(height, MeasureSpec.EXACTLY); measureAgain = true; &#125; if (measureAgain) &#123; performMeasure(childWidthMeasureSpec, childHeightMeasureSpec); &#125; layoutRequested = true; &#125; &#125; &#125; else &#123; &#125; 这段代码用来检查是否需要重新测量Activity窗口的大小。 经过前面漫长的操作后，Activity窗口的大小测量工作终于尘埃落定，这时候就可以对Activity窗口的内容进行布局 performLayout(lp, mWidth, mHeight)和进行绘画了，performDraw()，由于主要关注Activity窗口大小计算过程，在此不做继续分析。 2.2.2、ViewRootImpl.relayoutWindow()通过调用这个Session对象的成员函数relayout()来请求WindowManagerService服务计算Activity窗口的大小。 123456789101112131415[-&gt;ViewRootImpl.java]private int relayoutWindow(WindowManager.LayoutParams params, int viewVisibility, boolean insetsPending) throws RemoteException &#123; ..... int relayoutResult = mWindowSession.relayout( mWindow, mSeq, params, (int) (mView.getMeasuredWidth() * appScale + 0.5f), (int) (mView.getMeasuredHeight() * appScale + 0.5f), viewVisibility, insetsPending ? WindowManagerGlobal.RELAYOUT_INSETS_PENDING : 0, mWinFrame, mPendingOverscanInsets, mPendingContentInsets, mPendingVisibleInsets, mPendingStableInsets, mPendingOutsets, mPendingBackDropFrame, mPendingConfiguration, mSurface); ...... return relayoutResult;&#125; 参数说明： 1、mWindow 用来标志要计算的是哪一个Activity窗口的大小p 2、Activity窗口的顶层视图经过测量后得到的宽度和高度 3、Activity窗口的可见状态，即viewVisibility 4、Activity窗口是否有额外的内容区域边衬和可见区域边衬等待告诉给WindowManagerService服务，即参数insetsPending 5、mWinFrame，这是一个输出参数，用来保存WindowManagerService服务计算后得到的Activity窗口的大小 6、mPendingOverscanInsets用来保存Overscan边衬，mPendingContentInsets用来保存内容区域边衬，mPendingVisibleInsets用来保存可见区域边衬，mPendingStableInsets用来保存可能被系统UI元素部分或完全遮蔽的全屏窗口区域 7、mPendingConfiguration，这是一个输出参数，用来保存WindowManagerService服务返回来的Activity窗口的配置信息 8、mSurface，这是一个输出参数，用来保存WindowManagerService服务返回来的Activity窗口的绘图表面 2.2.3、Session.relayout()123456789101112[-&gt;Session.java]public int relayout(IWindow window, int seq, WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewFlags, int flags, Rect outFrame, Rect outOverscanInsets, Rect outContentInsets, Rect outVisibleInsets, Rect outStableInsets, Rect outsets, Rect outBackdropFrame, Configuration outConfig, Surface outSurface) &#123; int res = mService.relayoutWindow(this, window, seq, attrs, requestedWidth, requestedHeight, viewFlags, flags, outFrame, outOverscanInsets, outContentInsets, outVisibleInsets, outStableInsets, outsets, outBackdropFrame, outConfig, outSurface); return res;&#125; 只是调用了WindowManagerService类的成员函数relayoutWindow来进一步计算参数window所描述的一个Activity窗品的大小 2.2.4、WindowManagerService.relayoutWindow()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[-&gt;WindowManagerService.java]public int relayoutWindow(Session session, IWindow client, int seq, WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewVisibility, int flags, Rect outFrame, Rect outOverscanInsets, Rect outContentInsets, Rect outVisibleInsets, Rect outStableInsets, Rect outOutsets, Rect outBackdropFrame, Configuration outConfig, Surface outSurface) &#123; int result = 0; ...... synchronized(mWindowMap) &#123; WindowState win = windowForClientLocked(session, client, false); WindowStateAnimator winAnimator = win.mWinAnimator; if (viewVisibility != View.GONE) &#123; win.setRequestedSize(requestedWidth, requestedHeight); &#125; ..... if (attrs != null) &#123; mPolicy.adjustWindowParamsLw(attrs); ...... &#125; win.setWindowScale(win.mRequestedWidth, win.mRequestedHeight); ...... if (viewVisibility == View.VISIBLE &amp;&amp; (win.mAppToken == null || !win.mAppToken.clientHidden)) &#123; result = relayoutVisibleWindow(outConfig, result, win, winAnimator, attrChanges, oldVisibility); try &#123; result = createSurfaceControl(outSurface, result, win, winAnimator); &#125; catch (Exception e) &#123; ...... &#125; ...... win.adjustStartingWindowFlags(); &#125; else &#123; ...... &#125; ...... mWindowPlacerLocked.performSurfacePlacement(); ...... outFrame.set(win.mCompatFrame); outOverscanInsets.set(win.mOverscanInsets); outContentInsets.set(win.mContentInsets); outVisibleInsets.set(win.mVisibleInsets); outStableInsets.set(win.mStableInsets); outOutsets.set(win.mOutsets); outBackdropFrame.set(win.getBackdropFrame(win.mFrame)); ...... return result;&#125; 只关注relayoutWindow中与窗口大小计算有关的逻辑，计算过程如下所示： 1、参数requestedWidth和requestedHeight描述的是应用程序进程请求设置Activity窗口中的宽度和高度，它们会被记录在WindowState对象win的成员变量mRequestedWidth和mRequestedHeight中 2、WindowState对象win的成员变量mAttrs，它指向的是一个WindowManager.LayoutParams对象，用来描述Activity窗口的布局参数 3、调用WindowSurfacePlacer.performSurfacePlacement()来计算Activity窗口的大小。计算完成之后，参数client所描述的Activity窗口的大小、内容区域边衬大小和可见区域边边衬大小就会分别保存在WindowState对象win的成员变量mCompatFrame、mOverscanInsets、mContentInsets、mVisibleInsets、mStableInsets、mOutsets中 4、 将WindowState对象win的成员变量mCompatFrame、mOverscanInsets、mContentInsets、mVisibleInsets、mStableInsets、mOutsets拷贝赋值对应变量中，以便可以返回给应用程序进程 经过上述4个操作后，Activity窗口的大小计算过程就完成了，接下来我们继续分析WindowSurfacePlacer.performSurfacePlacement()的实现，以便可以详细了解Activity窗口的大小计算过程 2.2.5、WindowSurfacePlacer.performSurfacePlacement()1234567891011121314[-&gt;WindowSurfacePlacer.java]final void performSurfacePlacement() &#123; if (mDeferDepth &gt; 0) &#123; return; &#125; int loopCount = 6; do &#123; mTraversalScheduled = false; performSurfacePlacementLoop(); mService.mH.removeMessages(DO_TRAVERSAL); loopCount--; &#125; while (mTraversalScheduled &amp;&amp; loopCount &gt; 0); mWallpaperActionPending = false;&#125; 2.2.6.WindowSurfacePlacer.performSurfacePlacementLoop()12345678910111213141516171819202122232425262728293031323334353637[-&gt;WindowSurfacePlacer.java]private void performSurfacePlacementLoop() &#123; mInLayout = true; boolean recoveringMemory = false; if (!mService.mForceRemoves.isEmpty()) &#123; recoveringMemory = true; while (!mService.mForceRemoves.isEmpty()) &#123; WindowState ws = mService.mForceRemoves.remove(0); mService.removeWindowInnerLocked(ws); &#125; ...... &#125; ...... try &#123; performSurfacePlacementInner(recoveringMemory); mInLayout = false; if (mService.needsLayout()) &#123; if (++mLayoutRepeatCount &lt; 6) &#123; requestTraversal(); &#125; else &#123; Slog.e(TAG, \"Performed 6 layouts in a row. Skipping\"); mLayoutRepeatCount = 0; &#125; &#125; else &#123; mLayoutRepeatCount = 0; &#125; if (mService.mWindowsChanged &amp;&amp; !mService.mWindowChangeListeners.isEmpty()) &#123; mService.mH.removeMessages(REPORT_WINDOWS_CHANGE); mService.mH.sendEmptyMessage(REPORT_WINDOWS_CHANGE); &#125; &#125; catch (RuntimeException e) &#123; mInLayout = false; &#125;&#125; 在调用成员函数performSurfacePlacementInner()刷新系统UI的前后 1、检查系统中是否存在强制删除的窗口 2、检查系统中是否有窗口需要移除 2.2.7、WindowSurfacePlacer.performSurfacePlacementInner()继续分析的performSurfacePlacementInner()实现，以便可以了解Activity窗口的大小计算过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[-&gt;WindowSurfacePlacer.java]private void performSurfacePlacementInner(boolean recoveringMemory) &#123; if (DEBUG_WINDOW_TRACE) Slog.v(TAG, \"performSurfacePlacementInner: entry. Called by \" + Debug.getCallers(3)); ...... final DisplayContent defaultDisplay = mService.getDefaultDisplayContentLocked(); final DisplayInfo defaultInfo = defaultDisplay.getDisplayInfo(); final int defaultDw = defaultInfo.logicalWidth; final int defaultDh = defaultInfo.logicalHeight; SurfaceControl.openTransaction(); try &#123; applySurfaceChangesTransaction(recoveringMemory, numDisplays, defaultDw, defaultDh); &#125; catch (RuntimeException e) &#123; Slog.wtf(TAG, \"Unhandled exception in Window Manager\", e); &#125; finally &#123; SurfaceControl.closeTransaction(); &#125; final WindowList defaultWindows = defaultDisplay.getWindowList(); ...... final int N = mService.mPendingRemove.size(); if (N &gt; 0) &#123; if (mService.mPendingRemoveTmp.length &lt; N) &#123; mService.mPendingRemoveTmp = new WindowState[N+10]; &#125; mService.mPendingRemove.toArray(mService.mPendingRemoveTmp); mService.mPendingRemove.clear(); DisplayContentList displayList = new DisplayContentList(); for (i = 0; i &lt; N; i++) &#123; WindowState w = mService.mPendingRemoveTmp[i]; mService.removeWindowInnerLocked(w); final DisplayContent displayContent = w.getDisplayContent(); if (displayContent != null &amp;&amp; !displayList.contains(displayContent)) &#123; displayList.add(displayContent); &#125; &#125; for (DisplayContent displayContent : displayList) &#123; mService.mLayersController.assignLayersLocked(displayContent.getWindowList()); displayContent.layoutNeeded = true; &#125; &#125; ...... mService.scheduleAnimationLocked(); ......&#125; 可以看到进一步调用applySurfaceChangesTransaction()方法进行进一步计算 2.2.8、WindowSurfacePlacer.applySurfaceChangesTransaction()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[-&gt;WindowSurfacePlacer.java]private void applySurfaceChangesTransaction(boolean recoveringMemory, int numDisplays, int defaultDw, int defaultDh) &#123; ...... boolean focusDisplayed = false; for (int displayNdx = 0; displayNdx &lt; numDisplays; ++displayNdx) &#123; final DisplayContent displayContent = mService.mDisplayContents.valueAt(displayNdx); boolean updateAllDrawn = false; WindowList windows = displayContent.getWindowList(); DisplayInfo displayInfo = displayContent.getDisplayInfo(); final int displayId = displayContent.getDisplayId(); final int dw = displayInfo.logicalWidth; final int dh = displayInfo.logicalHeight; final int innerDw = displayInfo.appWidth; final int innerDh = displayInfo.appHeight; final boolean isDefaultDisplay = (displayId == Display.DEFAULT_DISPLAY); // Reset for each display. mDisplayHasContent = false; mPreferredRefreshRate = 0; mPreferredModeId = 0; int repeats = 0; do &#123; repeats++; if (repeats &gt; 6) &#123;//最多执行7次的while循环 displayContent.layoutNeeded = false; break; &#125; //通知SurfaceFlinger服务了，也就是让SurfaceFlinger服务更新它里面的各个Layer的属性值，以便可以对这些Layer执行可见性计算、合成等操作，最后渲染到硬件帧缓冲区中去 if ((displayContent.pendingLayoutChanges &amp; FINISH_LAYOUT_REDO_WALLPAPER) != 0 &amp;&amp; mWallpaperControllerLocked.adjustWallpaperWindows()) &#123; mService.mLayersController.assignLayersLocked(windows); displayContent.layoutNeeded = true; &#125; ...... if ((displayContent.pendingLayoutChanges &amp; FINISH_LAYOUT_REDO_LAYOUT) != 0) &#123; displayContent.layoutNeeded = true; &#125; // FIRST LOOP: Perform a layout, if needed. //计算各个窗品的大小 if (repeats &lt; LAYOUT_REPEAT_THRESHOLD) &#123; performLayoutLockedInner(displayContent, repeats == 1, false /* updateInputWindows */); &#125; else &#123; &#125; // FIRST AND ONE HALF LOOP: Make WindowManagerPolicy think // it is animating. displayContent.pendingLayoutChanges = 0; if (isDefaultDisplay) &#123; mService.mPolicy.beginPostLayoutPolicyLw(dw, dh); for (int i = windows.size() - 1; i &gt;= 0; i--) &#123; WindowState w = windows.get(i); if (w.mHasSurface) &#123; mService.mPolicy.applyPostLayoutPolicyLw(w, w.mAttrs, w.mAttachedWindow); &#125; &#125; displayContent.pendingLayoutChanges |= mService.mPolicy.finishPostLayoutPolicyLw(); &#125; &#125; while (displayContent.pendingLayoutChanges != 0); ......&#125; 2.2.9、WindowSurfacePlacer.applySurfaceChangesTransaction()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586[-&gt;WindowSurfacePlacer.java]final void performLayoutLockedInner(final DisplayContent displayContent, boolean initial, boolean updateInputWindows) &#123; displayContent.layoutNeeded = false; WindowList windows = displayContent.getWindowList(); boolean isDefaultDisplay = displayContent.isDefaultDisplay; DisplayInfo displayInfo = displayContent.getDisplayInfo(); final int dw = displayInfo.logicalWidth; final int dh = displayInfo.logicalHeight; ...... final int N = windows.size(); int i; /// mService.mPolicy.beginLayoutLw(isDefaultDisplay, dw, dh, mService.mRotation, mService.mCurConfiguration.uiMode); ...... displayContent.resize(mTmpContentRect); int seq = mService.mLayoutSeq+1; if (seq &lt; 0) seq = 0; mService.mLayoutSeq = seq; boolean behindDream = false; int topAttached = -1; for (i = N-1; i &gt;= 0; i--) &#123; final WindowState win = windows.get(i); ...... final boolean gone = (behindDream &amp;&amp; mService.mPolicy.canBeForceHidden(win, win.mAttrs)) || win.isGoneForLayoutLw(); ...... if (!gone || !win.mHaveFrame || win.mLayoutNeeded || ((win.isConfigChanged() || win.setReportResizeHints()) &amp;&amp; !win.isGoneForLayoutLw() &amp;&amp; ((win.mAttrs.privateFlags &amp; PRIVATE_FLAG_KEYGUARD) != 0 || (win.mHasSurface &amp;&amp; win.mAppToken != null &amp;&amp; win.mAppToken.layoutConfigChanges)))) &#123; if (!win.mLayoutAttached) &#123; ...... win.mLayoutNeeded = false; win.prelayout(); mService.mPolicy.layoutWindowLw(win, null); win.mLayoutSeq = seq; // Window frames may have changed. Update dim layer with the new bounds. final Task task = win.getTask(); if (task != null) &#123; displayContent.mDimLayerController.updateDimLayer(task); &#125; ...... &#125; else &#123; &#125; &#125; &#125; boolean attachedBehindDream = false; ...... for (i = topAttached; i &gt;= 0; i--) &#123; final WindowState win = windows.get(i); if (win.mLayoutAttached) &#123; ...... if ((win.mViewVisibility != View.GONE &amp;&amp; win.mRelayoutCalled) || !win.mHaveFrame || win.mLayoutNeeded) &#123; ...... win.mLayoutNeeded = false; win.prelayout(); mService.mPolicy.layoutWindowLw(win, win.mAttachedWindow); win.mLayoutSeq = seq; &#125; &#125; else if (win.mAttrs.type == TYPE_DREAM) &#123; attachedBehindDream = behindDream; &#125; &#125; // Window frames may have changed. Tell the input dispatcher about it. mService.mInputMonitor.setUpdateInputWindowsNeededLw(); if (updateInputWindows) &#123; mService.mInputMonitor.updateInputWindowsLw(false /*force*/); &#125; mService.mPolicy.finishLayoutLw(); mService.mH.sendEmptyMessage(UPDATE_DOCKED_STACK_DIVIDER);&#125; 1、mPolicy指向的是一个窗口管理策略类，即PhoneWindowManager对象，主要是用来制定窗口的大小计算策略 2、准备阶段：调用PhoneWindowManager.beginLayoutLw()来设置屏幕的大小。包括NavigationBar、StatusBar大小计算 3、计算阶段：调用PhoneWindowManager.layoutWindowLw()来计算各个窗口的大小、内容区域边衬大小以及可见区域边衬大小。 4、结束阶段：调用PhoneWindowManager类的成员函数finishLayoutLw()来执行一些清理工作。 2.2.10、PhoneWindowManager.beginLayoutLw()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[-&gt;PhoneWindowManager.java]public void beginLayoutLw(boolean isDefaultDisplay, int displayWidth, int displayHeight, int displayRotation, int uiMode) &#123; mDisplayRotation = displayRotation; final int overscanLeft, overscanTop, overscanRight, overscanBottom; if (isDefaultDisplay) &#123; switch (displayRotation) &#123; case Surface.ROTATION_90: overscanLeft = mOverscanTop; overscanTop = mOverscanRight; overscanRight = mOverscanBottom; overscanBottom = mOverscanLeft; break; ...... &#125; &#125; else &#123; overscanLeft = 0; overscanTop = 0; overscanRight = 0; overscanBottom = 0; &#125; mOverscanScreenLeft = mRestrictedOverscanScreenLeft = 0; mOverscanScreenTop = mRestrictedOverscanScreenTop = 0; mOverscanScreenWidth = mRestrictedOverscanScreenWidth = displayWidth; mOverscanScreenHeight = mRestrictedOverscanScreenHeight = displayHeight; mSystemLeft = 0; mSystemTop = 0; mSystemRight = displayWidth; mSystemBottom = displayHeight; mUnrestrictedScreenLeft = overscanLeft; mUnrestrictedScreenTop = overscanTop; mUnrestrictedScreenWidth = displayWidth - overscanLeft - overscanRight; mUnrestrictedScreenHeight = displayHeight - overscanTop - overscanBottom; mRestrictedScreenLeft = mUnrestrictedScreenLeft; mRestrictedScreenTop = mUnrestrictedScreenTop; mRestrictedScreenWidth = mSystemGestures.screenWidth = mUnrestrictedScreenWidth; mRestrictedScreenHeight = mSystemGestures.screenHeight = mUnrestrictedScreenHeight; mDockLeft = mContentLeft = mVoiceContentLeft = mStableLeft = mStableFullscreenLeft = mCurLeft = mUnrestrictedScreenLeft; mDockTop = mContentTop = mVoiceContentTop = mStableTop = mStableFullscreenTop = mCurTop = mUnrestrictedScreenTop; mDockRight = mContentRight = mVoiceContentRight = mStableRight = mStableFullscreenRight = mCurRight = displayWidth - overscanRight; mDockBottom = mContentBottom = mVoiceContentBottom = mStableBottom = mStableFullscreenBottom = mCurBottom = displayHeight - overscanBottom; mDockLayer = 0x10000000; mStatusBarLayer = -1; // start with the current dock rect, which will be (0,0,displayWidth,displayHeight) final Rect pf = mTmpParentFrame; final Rect df = mTmpDisplayFrame; final Rect of = mTmpOverscanFrame; final Rect vf = mTmpVisibleFrame; final Rect dcf = mTmpDecorFrame; pf.left = df.left = of.left = vf.left = mDockLeft; pf.top = df.top = of.top = vf.top = mDockTop; pf.right = df.right = of.right = vf.right = mDockRight; pf.bottom = df.bottom = of.bottom = vf.bottom = mDockBottom; dcf.setEmpty(); // Decor frame N/A for system bars. ....... if (isDefaultDisplay) &#123; navVisible |= !canHideNavigationBar(); boolean updateSysUiVisibility = layoutNavigationBar(displayWidth, displayHeight, displayRotation, uiMode, overscanLeft, overscanRight, overscanBottom, dcf, navVisible, navTranslucent,navAllowedHidden, statusBarExpandedNotKeyguard); updateSysUiVisibility |= layoutStatusBar(pf, df, of, vf, dcf, sysui, isKeyguardShowing); ...... if (updateSysUiVisibility) &#123; updateSystemUiVisibilityLw(); &#125; &#125;&#125; 1、初始化Overscan、UnrestrictedScreen、RestrictedScreen等屏幕区域变量 2、计算NavigationBar和StatusBar大小 2.2.11、PhoneWindowManager.layoutWindowLw()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[-&gt;PhoneWindowManager.java]Overridepublic void layoutWindowLw(WindowState win, WindowState attached) &#123; ...... final int fl = PolicyControl.getWindowFlags(win, attrs); final int pfl = attrs.privateFlags; final int sim = attrs.softInputMode; final int sysUiFl = PolicyControl.getSystemUiVisibility(win, null); final Rect pf = mTmpParentFrame; final Rect df = mTmpDisplayFrame; final Rect of = mTmpOverscanFrame; final Rect cf = mTmpContentFrame; final Rect vf = mTmpVisibleFrame; final Rect dcf = mTmpDecorFrame; final Rect sf = mTmpStableFrame; Rect osf = null; dcf.setEmpty(); final boolean hasNavBar = (isDefaultDisplay &amp;&amp; mHasNavigationBar &amp;&amp; mNavigationBar != null &amp;&amp; mNavigationBar.isVisibleLw()); final int adjust = sim &amp; SOFT_INPUT_MASK_ADJUST; if (isDefaultDisplay) &#123; sf.set(mStableLeft, mStableTop, mStableRight, mStableBottom); &#125; else &#123; sf.set(mOverscanLeft, mOverscanTop, mOverscanRight, mOverscanBottom); &#125; if (!isDefaultDisplay) &#123; if (attached != null) &#123; // If this window is attached to another, our display // frame is the same as the one we are attached to. setAttachedWindowFrames(win, fl, adjust, attached, true, pf, df, of, cf, vf); &#125; else &#123; // Give the window full screen. pf.left = df.left = of.left = cf.left = mOverscanScreenLeft; pf.top = df.top = of.top = cf.top = mOverscanScreenTop; pf.right = df.right = of.right = cf.right = mOverscanScreenLeft + mOverscanScreenWidth; pf.bottom = df.bottom = of.bottom = cf.bottom = mOverscanScreenTop + mOverscanScreenHeight; &#125; &#125; ...... win.computeFrameLw(pf, df, of, cf, vf, dcf, sf, osf); ......&#125; 然后调用WindowState.computeFrameLw来计算窗口win的具体大小了。计算的结果便得到了窗口win的大小，以及它的内容区域边衬大小和可见区域边衬大小。 2.2.12、WindowState.computeFrameLw()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115[-&gt;WindowState.java]public void computeFrameLw(Rect pf, Rect df, Rect of, Rect cf, Rect vf, Rect dcf, Rect sf, Rect osf) &#123; ...... final Task task = getTask(); final boolean fullscreenTask = !isInMultiWindowMode(); final boolean windowsAreFloating = task != null &amp;&amp; task.isFloating(); ...... final Rect layoutContainingFrame; final Rect layoutDisplayFrame; // The offset from the layout containing frame to the actual containing frame. final int layoutXDiff; final int layoutYDiff; if (fullscreenTask || layoutInParentFrame()) &#123; // We use the parent frame as the containing frame for fullscreen and child windows mContainingFrame.set(pf); mDisplayFrame.set(df); layoutDisplayFrame = df; layoutContainingFrame = pf; layoutXDiff = 0; layoutYDiff = 0; &#125; else &#123; task.getBounds(mContainingFrame); ...... mDisplayFrame.set(mContainingFrame); layoutXDiff = !mInsetFrame.isEmpty() ? mInsetFrame.left - mContainingFrame.left : 0; layoutYDiff = !mInsetFrame.isEmpty() ? mInsetFrame.top - mContainingFrame.top : 0; layoutContainingFrame = !mInsetFrame.isEmpty() ? mInsetFrame : mContainingFrame; mTmpRect.set(0, 0, mDisplayContent.getDisplayInfo().logicalWidth, mDisplayContent.getDisplayInfo().logicalHeight); subtractInsets(mDisplayFrame, layoutContainingFrame, df, mTmpRect); ...... layoutDisplayFrame = df; layoutDisplayFrame.intersect(layoutContainingFrame); &#125; final int pw = mContainingFrame.width(); final int ph = mContainingFrame.height(); ...... mOverscanFrame.set(of); mContentFrame.set(cf); mVisibleFrame.set(vf); mDecorFrame.set(dcf); mStableFrame.set(sf); final boolean hasOutsets = osf != null; final int fw = mFrame.width(); final int fh = mFrame.height(); ...... if (windowsAreFloating &amp;&amp; !mFrame.isEmpty()) &#123; final int height = Math.min(mFrame.height(), mContentFrame.height()); final int width = Math.min(mContentFrame.width(), mFrame.width()); final DisplayMetrics displayMetrics = getDisplayContent().getDisplayMetrics(); final int minVisibleHeight = Math.min(height, WindowManagerService.dipToPixel( MINIMUM_VISIBLE_HEIGHT_IN_DP, displayMetrics)); final int minVisibleWidth = Math.min(width, WindowManagerService.dipToPixel( MINIMUM_VISIBLE_WIDTH_IN_DP, displayMetrics)); final int top = Math.max(mContentFrame.top, Math.min(mFrame.top, mContentFrame.bottom - minVisibleHeight)); final int left = Math.max(mContentFrame.left + minVisibleWidth - width, Math.min(mFrame.left, mContentFrame.right - minVisibleWidth)); mFrame.set(left, top, left + width, top + height); mContentFrame.set(mFrame); mVisibleFrame.set(mContentFrame); mStableFrame.set(mContentFrame); &#125; else if (mAttrs.type == TYPE_DOCK_DIVIDER) &#123; mDisplayContent.getDockedDividerController().positionDockedStackedDivider(mFrame); mContentFrame.set(mFrame); &#125; else &#123; mContentFrame.set(......); mVisibleFrame.set(......); mStableFrame.set(......); &#125; if (fullscreenTask &amp;&amp; !windowsAreFloating) &#123; // Windows that are not fullscreen can be positioned outside of the display frame, mOverscanInsets.set(......); &#125; if (mAttrs.type == TYPE_DOCK_DIVIDER) &#123; mStableInsets.set(......); mContentInsets.setEmpty(); mVisibleInsets.setEmpty(); &#125; else &#123; getDisplayContent().getLogicalDisplayRect(mTmpRect); boolean overrideRightInset = !fullscreenTask &amp;&amp; mFrame.right &gt; mTmpRect.right; boolean overrideBottomInset = !fullscreenTask &amp;&amp; mFrame.bottom &gt; mTmpRect.bottom; mContentInsets.set(......); mVisibleInsets.set(......); mStableInsets.set(......); &#125; // Offset the actual frame by the amount layout frame is off. mFrame.offset(-layoutXDiff, -layoutYDiff); mCompatFrame.offset(-layoutXDiff, -layoutYDiff); mContentFrame.offset(-layoutXDiff, -layoutYDiff); mVisibleFrame.offset(-layoutXDiff, -layoutYDiff); mStableFrame.offset(-layoutXDiff, -layoutYDiff); mCompatFrame.set(mFrame); if (mEnforceSizeCompat) &#123; mOverscanInsets.scale(mInvGlobalScale); mContentInsets.scale(mInvGlobalScale); mVisibleInsets.scale(mInvGlobalScale); mStableInsets.scale(mInvGlobalScale); mOutsets.scale(mInvGlobalScale); mCompatFrame.scale(mInvGlobalScale); &#125; ......&#125; 整个窗口大小保存在WindowState类的成员变量mFrame中，而窗品的内容区域边衬大小和可见区域边衬大小分别保在WindowState类的成员变量mContentInsets和mVisibleInsets中 2.2.13、PhoneWindowManager.finishLayoutLw()12345[WindowState.java]@Overridepublic void finishLayoutLw() &#123; return;&#125; （三）、Window Z-Order 计算和调整过程口的UI最终是需要通过SurfaceFlinger服务来统一渲染的，而SurfaceFlinger服务在渲染窗口的UI之前，需要计算基于各个窗口的Z轴位置来计算它们的可见区域。因此，WindowManagerService服务计算好每一个窗口的Z轴位置之后，还需要将它们设置到SurfaceFlinger服务中去，以便SurfaceFlinger服务可以正确地渲染每一个窗口的UI。 3.1、需要重新计算窗口Z轴位置的情景在【Android 7.1.2 (Android N) Activity-Window加载显示流程分析】中已经详细介绍Window添加过程，这里直接从 WMS.addWindow开始分析。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[-&gt;WindowManagerService.java]public int addWindow(Session session, IWindow client, int seq, WindowManager.LayoutParams attrs, int viewVisibility, int displayId, Rect outContentInsets, Rect outStableInsets, Rect outOutsets, InputChannel outInputChannel) &#123; ....... synchronized(mWindowMap) &#123; ...... boolean addToken = false; WindowToken token = mTokenMap.get(attrs.token); AppWindowToken atoken = null; boolean addToastWindowRequiresToken = false; ...... // WindowState win = new WindowState(this, session, client, token, attachedWindow, appOp[0], seq, attrs, viewVisibility, displayContent); ...... origId = Binder.clearCallingIdentity(); if (addToken) &#123; mTokenMap.put(attrs.token, token); &#125; win.attach(); mWindowMap.put(client.asBinder(), win); ...... boolean imMayMove = true; if (type == TYPE_INPUT_METHOD) &#123; win.mGivenInsetsPending = true; mInputMethodWindow = win; addInputMethodWindowToListLocked(win); imMayMove = false; &#125; else if (type == TYPE_INPUT_METHOD_DIALOG) &#123; mInputMethodDialogs.add(win); addWindowToListInOrderLocked(win, true); moveInputMethodDialogsLocked(findDesiredInputMethodWindowIndexLocked(true)); imMayMove = false; &#125; else &#123; addWindowToListInOrderLocked(win, true); ...... &#125; ...... //重新计算系统中所有窗口的Z轴位置 mLayersController.assignLayersLocked(displayContent.getWindowList()); ...... &#125; ...... return res;&#125; WMS.relayoutWindow()也会调用WindowLayersController.assignLayersLocked()重新计算、调整系统中所有窗口的Z轴位置，由于原理类似这里不做解释。 3.2、计算系统中所有窗口的Z轴位置接下来我们就通过WindowState类的构造函数来分析一个窗口的BaseLayer值是如何确定 12345678910111213141516[WindowState.java]WindowState(WindowManagerService service, Session s, IWindow c, WindowToken token, WindowState attachedWindow, int appOp, int seq, WindowManager.LayoutParams a, int viewVisibility, final DisplayContent displayContent) &#123; ...... if ((mAttrs.type &gt;= FIRST_SUB_WINDOW &amp;&amp; mAttrs.type &lt;= LAST_SUB_WINDOW)) &#123; ...... //windowTypeToLayerLw的返回值并且不是一个窗口的最终的BaseLayer值，而是要将它的返回值乘以一个常量TYPE_LAYER_MULTIPLIER，再加上另外一个常量TYPE_LAYER_OFFSET之后，才得到最终的BaseLayer值 mBaseLayer = mPolicy.windowTypeToLayerLw( attachedWindow.mAttrs.type) * WindowManagerService.TYPE_LAYER_MULTIPLIER + WindowManagerService.TYPE_LAYER_OFFSET; mSubLayer = mPolicy.subWindowTypeToLayerLw(a.type); ......&#125; 一个窗口除了有一个BaseLayer值之外，还有一个SubLayer值，分别保存在一个对应的WindowState对象的成员变量mBaseLayer和mSubLayer。SubLayer值是用来描述一个窗口是否是另外一个窗口的子窗口的。 在继续分析WindowLayersController.assignLayersLocked()之前，我们首先分析PhoneWindowManager.windowTypeToLayerLw()和subWindowTypeToLayerLw()的实现，以便可以了解一个窗口的BaseLayer值和SubLayer值是如何确定的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[-&gt;PhoneWindowManager.java]@Overridepublic int windowTypeToLayerLw(int type) &#123; if (type &gt;= FIRST_APPLICATION_WINDOW &amp;&amp; type &lt;= LAST_APPLICATION_WINDOW) &#123; return 2; &#125; switch (type) &#123; ...... case TYPE_SYSTEM_DIALOG: return 7; case TYPE_TOAST: // toasts and the plugged-in battery thing return 8; ...... case TYPE_SYSTEM_ALERT: // like the ANR / app crashed dialogs return 11; ...... case TYPE_STATUS_BAR_SUB_PANEL: return 15; case TYPE_STATUS_BAR: return 16; case TYPE_STATUS_BAR_PANEL: return 17; case TYPE_KEYGUARD_DIALOG: return 18; ...... case TYPE_NAVIGATION_BAR: // the navigation bar, if available, shows atop most things return 21; case TYPE_NAVIGATION_BAR_PANEL: // some panels (e.g. search) need to show on top of the navigation bar return 22; ...... &#125; return 2;&#125;/** &#123;@inheritDoc&#125; */@Overridepublic int subWindowTypeToLayerLw(int type) &#123; switch (type) &#123; case TYPE_APPLICATION_PANEL: case TYPE_APPLICATION_ATTACHED_DIALOG: return APPLICATION_PANEL_SUBLAYER; ...... case TYPE_APPLICATION_ABOVE_SUB_PANEL: return APPLICATION_ABOVE_SUB_PANEL_SUBLAYER; &#125; return 0;&#125; 主要根据不同的Window Type返回不一样的数值。 123[-&gt;WindowManagerService.java]static final int TYPE_LAYER_MULTIPLIER = 10000;static final int TYPE_LAYER_OFFSET = 1000; 我们可以用adb shell dumpsys window -a 命令查看一下Layer的数值，可以看到StatusBar的数值计算： mBaseLayer = 16 _WindowManagerService.TYPE_LAYER_MULTIPLIER+ WindowManagerService.TYPE_LAYEROFFSET StatusBar.mBaseLayer = 16 10000 + 1000 = 161000 。 12345678910Window #4 Window&#123;3c1f1fb u0 StatusBar&#125;: mBaseLayer=161000 mSubLayer=0 mAnimLayer=161000+0=161000 mLastLayer=161000Window #3 Window&#123;fe4aae2 u0 KeyguardScrim&#125;: mBaseLayer=141000 mSubLayer=0 mAnimLayer=141000+0=141000 mLastLayer=141000Window #2 Window&#123;173ee76 u0 DockedStackDivider&#125;: mBaseLayer=21000 mSubLayer=0 mAnimLayer=21010+0=21010 mLastLayer=0Window #1 Window&#123;3a83a4a u0 com.android.launcher&#125;: mBaseLayer=21000 mSubLayer=0 mAnimLayer=21005+0=21005 mLastLayer=21005Window #0 Window&#123;aefb7bc u0 com.android.systemui.ImageWallpaper&#125;: mBaseLayer=21000 mSubLayer=0 mAnimLayer=21000+0=21000 mLastLayer=21000 理解了窗口的BaseLayer值和SubLayer值的计算过程之外，接下来我们就可以分析WindowManagerService类的成员函数assignLayersLocked()的实现了 3.2.1、WindowLayersController.assignLayersLocked()123456789101112131415161718192021222324252627282930313233343536373839404142[-&gt;WindowLayersController.java]final void assignLayersLocked(WindowList windows) &#123; if (DEBUG_LAYERS) Slog.v(TAG_WM, \"Assigning layers based on windows=\" + windows, new RuntimeException(\"here\").fillInStackTrace()); clear(); int curBaseLayer = 0; int curLayer = 0; boolean anyLayerChanged = false; for (int i = 0, windowCount = windows.size(); i &lt; windowCount; i++) &#123; final WindowState w = windows.get(i); boolean layerChanged = false; int oldLayer = w.mLayer; if (w.mBaseLayer == curBaseLayer || w.mIsImWindow || (i &gt; 0 &amp;&amp; w.mIsWallpaper)) &#123; curLayer += WINDOW_LAYER_MULTIPLIER; &#125; else &#123; curBaseLayer = curLayer = w.mBaseLayer; &#125; assignAnimLayer(w, curLayer); // TODO: Preserved old behavior of code here but not sure comparing // oldLayer to mAnimLayer and mLayer makes sense...though the // worst case would be unintentionalp layer reassignment. if (w.mLayer != oldLayer || w.mWinAnimator.mAnimLayer != oldLayer) &#123; layerChanged = true; anyLayerChanged = true; &#125; if (w.mAppToken != null) &#123; mHighestApplicationLayer = Math.max(mHighestApplicationLayer, w.mWinAnimator.mAnimLayer); &#125; collectSpecialWindows(w); if (layerChanged) &#123; w.scheduleAnimationIfDimming(); &#125; &#125; adjustSpecialWindows(); ......&#125; 调用 assignAnimLayer() 进行Layer调整： 12345678910[-&gt;WindowLayersController.java] private void assignAnimLayer(WindowState w, int layer) &#123; w.mLayer = layer; w.mWinAnimator.mAnimLayer = w.mLayer + w.getAnimLayerAdjustment() + getSpecialWindowAnimLayerAdjustment(w); if (w.mAppToken != null &amp;&amp; w.mAppToken.mAppAnimator.thumbnailForceAboveLayer &gt; 0 &amp;&amp; w.mWinAnimator.mAnimLayer &gt; w.mAppToken.mAppAnimator.thumbnailForceAboveLayer) &#123; w.mAppToken.mAppAnimator.thumbnailForceAboveLayer = w.mWinAnimator.mAnimLayer; &#125; &#125; 3.3、设置窗口的Z轴位置到SurfaceFlinger服务中去WindowManagerService服务在刷新系统的UI的时候，就会将系统中已经计算好了的窗口Z轴位置设置到SurfaceFlinger服务中去，以便SurfaceFlinger服务可以对系统中的窗口进行可见性计算以及合成和渲染等操作 首先看一下堆栈信息： 123456789101112131415161718192021WindowSurfaceController.showSurface()添加打印LogSlog.i(\"zhoujinjian\", \"zhoujinjian\",new RuntimeException(\"here\").fillInStackTrace());03-01 19:06:45.229: I/zhoujinjian(1424): zhoujinjian03-01 19:06:45.229: I/zhoujinjian(1424): java.lang.RuntimeException: here03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.wm.WindowSurfaceController.showSurface(WindowSurfaceController.java:414)03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.wm.WindowSurfaceController.updateVisibility(WindowSurfaceController.java:402)03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.wm.WindowSurfaceController.showRobustlyInTransaction(WindowSurfaceController.java:391)03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.wm.WindowStateAnimator.showSurfaceRobustlyLocked(WindowStateAnimator.java:1814)03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.wm.WindowStateAnimator.prepareSurfaceLocked(WindowStateAnimator.java:1609)03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.wm.WindowAnimator.animateLocked(WindowAnimator.java:791)03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.wm.WindowAnimator.-wrap0(WindowAnimator.java)03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.wm.WindowAnimator$1.doFrame(WindowAnimator.java:166)03-01 19:06:45.229: I/zhoujinjian(1424): at android.view.Choreographer$CallbackRecord.run(Choreographer.java:879)03-01 19:06:45.229: I/zhoujinjian(1424): at android.view.Choreographer.doCallbacks(Choreographer.java:693)03-01 19:06:45.229: I/zhoujinjian(1424): at android.view.Choreographer.doFrame(Choreographer.java:625)03-01 19:06:45.229: I/zhoujinjian(1424): at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:867)03-01 19:06:45.229: I/zhoujinjian(1424): at android.os.Handler.handleCallback(Handler.java:751)03-01 19:06:45.229: I/zhoujinjian(1424): at android.os.Handler.dispatchMessage(Handler.java:95)03-01 19:06:45.229: I/zhoujinjian(1424): at android.os.Looper.loop(Looper.java:154)03-01 19:06:45.229: I/zhoujinjian(1424): at android.os.HandlerThread.run(HandlerThread.java:61)03-01 19:06:45.229: I/zhoujinjian(1424): at com.android.server.ServiceThread.run(ServiceThread.java:46) 为了了解WMS是如何将Z轴位置设置到SurfaceFlinger服务中去，首先看一下WMS构造方法中关键对象WindowAnimator的创建 3.3.1、Vsync刷新UI回调过程开机启动时会初始化WMS。 1234567[-&gt;WindowManagerService.java]private WindowManagerService(Context context, InputManagerService inputManager, boolean haveInputMethods, boolean showBootMsgs, boolean onlyCore) &#123; ...... mAnimator = new WindowAnimator(this); ...... &#125; 调用WindowAnimator构造方法 12345678910111213141516[-&gt;WindowAnimator.java]WindowAnimator(final WindowManagerService service) &#123; mService = service; mContext = service.mContext; mPolicy = service.mPolicy; mWindowPlacerLocked = service.mWindowPlacerLocked; mAnimationFrameCallback = new Choreographer.FrameCallback() &#123; public void doFrame(long frameTimeNs) &#123; synchronized (mService.mWindowMap) &#123; mService.mAnimationScheduled = false; animateLocked(frameTimeNs); &#125; &#125; &#125;;&#125; 可以看到创建了Choreographer.FrameCallback()，前面在【Android-7-1-2-Android-N-Activity-Window加载显示流程】分析过，FrameDisplayEventReceiver（在Choreographer构造方法中初始化）对象用于请求并接收Vsync信号，当Vsync信号到来时，系统会自动调用其onVsync()函数，后面会回调到FrameDisplayEventReceiver.run()方法，再回调函数中执行doFrame()实现屏幕刷新，doFrame()会顺序执行CALLBACK_INPUT、CALLBACK_ANIMATION、CALLBACK_TRAVERSAL 和CALLBACK_COMMIT 对应CallbackQueue队列中注册的回调，从而会执行CallbackRecord.run()，在执行其回调函数时，就需要区别这两种对象类型，如果注册的是Runnable对象，则调用其run()函数，如果注册的是FrameCallback对象，则调用它的doFrame()函数。 123456789101112131415[-&gt;Choreographer.java]private static final class CallbackRecord &#123; public CallbackRecord next; public long dueTime; public Object action; // Runnable or FrameCallback public Object token; public void run(long frameTimeNanos) &#123; if (token == FRAME_CALLBACK_TOKEN) &#123; ((FrameCallback)action).doFrame(frameTimeNanos); &#125; else &#123; ((Runnable)action).run(); &#125; &#125;&#125; 在此种情况下会执行FrameCallback对象的doFrame()函数（原因稍后再分析动画时详细分析），由WindowAnimator构造函数中可知接着就会执行WindowAnimator.animateLocked() 3.3.2、准备刷新UI123456789101112131415161718192021222324252627282930313233[-&gt;WindowAnimator.java] private void animateLocked(long frameTimeNs) &#123; ...... if (SHOW_TRANSACTIONS) Slog.i( TAG, \"&gt;&gt;&gt; OPEN TRANSACTION animateLocked\"); SurfaceControl.openTransaction(); SurfaceControl.setAnimationTransaction(); try &#123; final int numDisplays = mDisplayContentsAnimators.size(); for (int i = 0; i &lt; numDisplays; i++) &#123; final int displayId = mDisplayContentsAnimators.keyAt(i); updateAppWindowsLocked(displayId); DisplayContentsAnimator displayAnimator = mDisplayContentsAnimators.valueAt(i); ...... updateWindowsLocked(displayId); updateWallpaperLocked(displayId); final WindowList windows = mService.getWindowListLocked(displayId); final int N = windows.size(); //通过一个for循环来遍历保存在窗口堆栈的每一个WindowState对象，以便可以对系统中的每一个窗口的绘图表面进行更新 for (int j = 0; j &lt; N; j++) &#123; windows.get(j).mWinAnimator.prepareSurfaceLocked(true); &#125; &#125; ...... catch (RuntimeException e) &#123; ...... &#125; finally &#123; SurfaceControl.closeTransaction(); if (SHOW_TRANSACTIONS) Slog.i( TAG, \"&lt;&lt;&lt; CLOSE TRANSACTION animateLocked\"); &#125;&#125; 首先获取windows列表，然后循环调用windows.get(j).mWinAnimator.prepareSurfaceLocked(true)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[-&gt;WindowStateAnimator.java]void prepareSurfaceLocked(final boolean recoveringMemory) &#123; final WindowState w = mWin; ...... boolean displayed = false; //确定该窗口实际要显示的大小、位置、Alpha通道和变换矩阵等信息 computeShownFrameLocked(); setSurfaceBoundariesLocked(recoveringMemory); if (mIsWallpaper &amp;&amp; !mWin.mWallpaperVisible) &#123; ...... &#125; else if (w.mAttachedHidden || !w.isOnScreen()) &#123; ...... &#125; else if (mLastLayer != mAnimLayer || mLastAlpha != mShownAlpha || mLastDsDx != mDsDx || mLastDtDx != mDtDx || mLastDsDy != mDsDy || mLastDtDy != mDtDy || w.mLastHScale != w.mHScale || w.mLastVScale != w.mVScale || mLastHidden) &#123; displayed = true; mLastAlpha = mShownAlpha; mLastLayer = mAnimLayer; mLastDsDx = mDsDx; mLastDtDx = mDtDx; mLastDsDy = mDsDy; mLastDtDy = mDtDy; w.mLastHScale = w.mHScale; w.mLastVScale = w.mVScale; ...... boolean prepared = mSurfaceController.prepareToShowInTransaction(mShownAlpha, mAnimLayer, mDsDx * w.mHScale * mExtraHScale, mDtDx * w.mVScale * mExtraVScale, mDsDy * w.mHScale * mExtraHScale, mDtDy * w.mVScale * mExtraVScale, recoveringMemory); if (prepared &amp;&amp; mLastHidden &amp;&amp; mDrawState == HAS_DRAWN) &#123; if (showSurfaceRobustlyLocked()) &#123; markPreservedSurfaceForDestroy(); mAnimator.requestRemovalOfReplacedWindows(w); mLastHidden = false; if (mIsWallpaper) &#123; mWallpaperControllerLocked.dispatchWallpaperVisibility(w, true); &#125; ...... mAnimator.setPendingLayoutChanges(w.getDisplayId(), WindowManagerPolicy.FINISH_LAYOUT_REDO_ANIM); &#125; else &#123; w.mOrientationChanging = false; &#125; &#125; if (hasSurface()) &#123; w.mToken.hasVisible = true; &#125; &#125; else &#123; displayed = true; &#125; ......&#125; 调用prepareToShowInTransaction()将alph、alayer、setMatrix设置到mSurfaceControl中。 1234567891011121314151617boolean prepareToShowInTransaction(float alpha, int layer, float dsdx, float dtdx, float dsdy, float dtdy, boolean recoveringMemory) &#123; if (mSurfaceControl != null) &#123; try &#123; mSurfaceAlpha = alpha; mSurfaceControl.setAlpha(alpha); mSurfaceLayer = layer; mSurfaceControl.setLayer(layer); mSurfaceControl.setMatrix( dsdx, dtdx, dsdy, dtdy); &#125; catch (RuntimeException e) &#123; ....... &#125; &#125; return true; &#125; setSurfaceBoundariesLocked()方法中会调用SurfaceControl.setPosition()等等方法将计算好的数值设置到mSurfaceControl中。 说明：一个窗口的显示和隐藏，以及大小、X轴和Y轴位置、Z轴位置、Alpha通道和变换矩阵设置，是通过调用Java层的SurfaceControl类的成员函数show、hide、setSize、setPosition、setLayer、setAlpha和setMatrix来实现的，它们最终都是通过调用JNI方法实现的 1234567891011121314151617181920212223242526272829303132SurfaceControl.java......public void setAlpha(float alpha) &#123; checkNotReleased(); nativeSetAlpha(mNativeObject, alpha);&#125;public void setMatrix(float dsdx, float dtdx, float dsdy, float dtdy) &#123; checkNotReleased(); nativeSetMatrix(mNativeObject, dsdx, dtdx, dsdy, dtdy);&#125;public void setWindowCrop(Rect crop) &#123; checkNotReleased(); if (crop != null) &#123; nativeSetWindowCrop(mNativeObject, crop.left, crop.top, crop.right, crop.bottom); &#125; else &#123; nativeSetWindowCrop(mNativeObject, 0, 0, 0, 0); &#125;&#125;public void setFinalCrop(Rect crop) &#123; checkNotReleased(); if (crop != null) &#123; nativeSetFinalCrop(mNativeObject, crop.left, crop.top, crop.right, crop.bottom); &#125; else &#123; nativeSetFinalCrop(mNativeObject, 0, 0, 0, 0); &#125;&#125;.... 3.3.3、告知SurfaceFlinger显示UI如果WindowState对象w所描述的窗口满足条件：prepared &amp;&amp; mLastHidden &amp;&amp; mDrawState == HAS_DRAWN 那么就说明现在是时候要将WindowState对象w所描述的窗口显示出来了，通过调用showSurfaceRobustlyLocked实现 123456789101112131415[-&gt;WindowStateAnimator.java]private boolean showSurfaceRobustlyLocked() &#123; final Task task = mWin.getTask(); if (task != null &amp;&amp; StackId.windowsAreScaleable(task.mStack.mStackId)) &#123; mSurfaceController.forceScaleableInTransaction(true); &#125; boolean shown = mSurfaceController.showRobustlyInTransaction(); ...... if (mWin.mTurnOnScreen) &#123; ...... mWin.mTurnOnScreen = false; mAnimator.mBulkUpdateParams |= SET_TURN_ON_SCREEN; &#125; return true;&#125; 直接调用WindowSurfaceController.showRobustlyInTransaction() –&gt; updateVisibility()–&gt;showSurface()-&gt;SurfaceControl.show() 123456789101112131415161718192021222324252627282930[-&gt;WindowSurfaceController.java] boolean showRobustlyInTransaction() &#123; ...... mHiddenForOtherReasons = false; return updateVisibility(); &#125; private boolean updateVisibility() &#123; if (mHiddenForCrop || mHiddenForOtherReasons) &#123; if (mSurfaceShown) &#123;hideSurface();&#125; return false; &#125; else &#123; if (!mSurfaceShown) &#123;return showSurface(); &#125; else &#123; return true; &#125; &#125; &#125; private boolean showSurface() &#123; try &#123; mSurfaceShown = true; mSurfaceControl.show(); Slog.i(\"zhoujinjian\", \"zhoujinjian\",new RuntimeException(\"here\").fillInStackTrace()); return true; &#125; catch (RuntimeException e) &#123; ...... &#125; //出现异常,回收系统内存资源 mAnimator.reclaimSomeSurfaceMemory(\"show\", true); return false; &#125; 12345[-&gt;SurfaceControl.java]public void show() &#123; checkNotReleased(); nativeSetFlags(mNativeObject, 0, SURFACE_HIDDEN);&#125; 通过JNI调用android_view_SurfaceControl.cpp的nativeSetFlags函数，可以看到flags == 0； 12345678[-&gt;android_view_SurfaceControl.cpp]static void nativeSetFlags(JNIEnv* env, jclass clazz, jlong nativeObject, jint flags, jint mask) &#123; SurfaceControl* const ctrl = reinterpret_cast&lt;SurfaceControl *&gt;(nativeObject); status_t err = ctrl-&gt;setFlags(flags, mask); if (err &lt; 0 &amp;&amp; err != NO_INIT) &#123; doThrowIAE(env); &#125;&#125; 调用SurfaceControl.cpp的setFlags()函数 123456[-&gt;SurfaceControl.cpp]status_t SurfaceControl::setFlags(uint32_t flags, uint32_t mask) &#123; status_t err = validate(); if (err &lt; 0) return err; return mClient-&gt;setFlags(mHandle, flags, mask);&#125; 进一步通过Binder IPC机制，SurfaceComposerClient.cpp-&gt;Composer::setFlags() 1234567891011121314151617[-&gt;SurfaceComposerClient.cpp]status_t Composer::setFlags(const sp&lt;SurfaceComposerClient&gt;&amp; client, const sp&lt;IBinder&gt;&amp; id, uint32_t flags, uint32_t mask) &#123; Mutex::Autolock _l(mLock); layer_state_t* s = getLayerStateLocked(client, id); ...... if ((mask &amp; layer_state_t::eLayerOpaque) || (mask &amp; layer_state_t::eLayerHidden) || (mask &amp; layer_state_t::eLayerSecure)) &#123; s-&gt;what |= layer_state_t::eFlagsChanged; &#125; s-&gt;flags &amp;= ~mask; s-&gt;flags |= (flags &amp; mask); s-&gt;mask |= mask; return NO_ERROR;&#125; 具体数值就不详细计算了，前面分析【Android 7.1.2 (Android N) Android Graphics 系统分析 [i.wonder~]】可知，SurfaceFlinger接收Vsync信号与App有一个offset间隔时间，当SurfaceFlinger接收Vsync信号时，就可以根据flags是否显示 和 上面设置的一系列数值进行渲染合成，最终显示到屏幕上。 （四）、Activity启动窗口(Starting Window)添加过程4.1、Activity组件的启动窗口(Starting Window)的添加过程时序图： 4.1.1、 ActivityStack.startActivityLocked()123456789101112131415161718192021222324252627282930313233343536373839[-&gt;ActivityStack.java] final void startActivityLocked(ActivityRecord r, boolean newTask, boolean keepCurTransition, ActivityOptions options) &#123; ...... if (!isHomeStack() || numActivities() &gt; 0) &#123; ...... if ((r.intent.getFlags() &amp; Intent.FLAG_ACTIVITY_NO_ANIMATION) != 0) &#123; mWindowManager.prepareAppTransition(TRANSIT_NONE, keepCurTransition); mNoAnimActivities.add(r); &#125; else &#123; mWindowManager.prepareAppTransition(newTask ? r.mLaunchTaskBehind ? TRANSIT_TASK_OPEN_BEHIND : TRANSIT_TASK_OPEN : TRANSIT_ACTIVITY_OPEN, keepCurTransition); mNoAnimActivities.remove(r); &#125; addConfigOverride(r, task); boolean doShow = true; ...... if (r.mLaunchTaskBehind) &#123; ....... &#125; else if (SHOW_APP_STARTING_PREVIEW &amp;&amp; doShow) &#123; ActivityRecord prev = r.task.topRunningActivityWithStartingWindowLocked(); if (prev != null) &#123; // We don't want to reuse the previous starting preview if: // (1) The current activity is in a different task. if (prev.task != r.task) &#123; prev = null; &#125; // (2) The current activity is already displayed. else if (prev.nowVisible) &#123; prev = null; &#125; &#125; r.showStartingWindow(prev, showStartingIcon); &#125; &#125; ...... &#125; 可以看到直接调用ActivityRecord.showStartingWindow()进一步添加启动窗口 4.1.2、 ActivityRecord.showStartingWindow()1234567891011[-&gt;ActivityRecord.java]void showStartingWindow(ActivityRecord prev, boolean createIfNeeded) &#123; final CompatibilityInfo compatInfo = service.compatibilityInfoForPackageLocked(info.applicationInfo); final boolean shown = service.mWindowManager.setAppStartingWindow( appToken, packageName, theme, compatInfo, nonLocalizedLabel, labelRes, icon, logo, windowFlags, prev != null ? prev.appToken : null, createIfNeeded); if (shown) &#123; mStartingWindowState = STARTING_WINDOW_SHOWN; &#125;&#125; 4.1.2、ActivityRecord.setAppStartingWindow()1234567891011121314151617181920[-&gt;ActivityRecord.java]public boolean setAppStartingWindow(IBinder token, String pkg, int theme, CompatibilityInfo compatInfo, CharSequence nonLocalizedLabel, int labelRes, int icon, int logo, int windowFlags, IBinder transferFrom, boolean createIfNeeded) &#123; ...... synchronized(mWindowMap) &#123; AppWindowToken wtoken = findAppWindowToken(token); ...... if (transferStartingWindow(transferFrom, wtoken)) &#123; return true; &#125; ...... wtoken.startingData = new StartingData(pkg, theme, compatInfo, nonLocalizedLabel, labelRes, icon, logo, windowFlags); Message m = mH.obtainMessage(H.ADD_STARTING, wtoken); mH.sendMessageAtFrontOfQueue(m); &#125; return true;&#125; 如果参数transferFrom所描述的Activity组件没有启动窗口或者启动窗口数据转移给参数token所描述的Activity组件，那么接下来就可能需要为参数token所描述的Activity组件创建一个新的启动窗口 4.1.3、 H.handleMessage()12345678910111213141516171819202122232425262728293031323334353637[-&gt;WindowManagerService.java::H]case ADD_STARTING: &#123; final AppWindowToken wtoken = (AppWindowToken)msg.obj; final StartingData sd = wtoken.startingData; ...... View view = null; try &#123; final Configuration overrideConfig = wtoken != null &amp;&amp; wtoken.mTask != null ? wtoken.mTask.mOverrideConfig : null; view = mPolicy.addStartingWindow(wtoken.token, sd.pkg, sd.theme, sd.compatInfo, sd.nonLocalizedLabel, sd.labelRes, sd.icon, sd.logo, sd.windowFlags, overrideConfig); &#125; catch (Exception e) &#123; &#125; if (view != null) &#123; boolean abort = false; synchronized(mWindowMap) &#123; if (wtoken.removed || wtoken.startingData == null) &#123; wtoken.startingWindow = null; wtoken.startingData = null; abort = true; &#125; &#125; else &#123; wtoken.startingView = view; &#125; &#125; if (abort) &#123; try &#123; mPolicy.removeStartingWindow(wtoken.token, view); &#125; catch (Exception e) &#123; &#125; &#125; &#125;&#125; break; PhoneWindowManager实现WindowManagerPolicy，所以会调用PhoneWindowManager中的方法 继续分析PhoneWindowManager.addStartingWindow() 4.1.4、PhoneWindowManager.addStartingWindow()12345678910111213141516171819202122232425262728293031323334353637383940414243[-&gt;PhoneWindowManager.java]public View addStartingWindow(IBinder appToken, String packageName, int theme, CompatibilityInfo compatInfo, CharSequence nonLocalizedLabel, int labelRes, int icon, int logo, int windowFlags, Configuration overrideConfig) &#123; ...... WindowManager wm = null; View view = null; try &#123; Context context = mContext; ...... final PhoneWindow win = new PhoneWindow(context); win.setIsStartingWindow(true); ...... win.setType( WindowManager.LayoutParams.TYPE_APPLICATION_STARTING); ...... win.setLayout(WindowManager.LayoutParams.MATCH_PARENT, WindowManager.LayoutParams.MATCH_PARENT); final WindowManager.LayoutParams params = win.getAttributes(); params.token = appToken; params.packageName = packageName; params.windowAnimations = win.getWindowStyle().getResourceId( com.android.internal.R.styleable.Window_windowAnimationStyle, 0); ...... params.setTitle(\"Starting \" + packageName); wm = (WindowManager)context.getSystemService(Context.WINDOW_SERVICE); view = win.getDecorView(); ...... wm.addView(view, params); return view.getParent() != null ? view : null; &#125; catch (WindowManager.BadTokenException e) &#123; ...... &#125; catch (RuntimeException e) &#123; ...... &#125; finally &#123; if (view != null &amp;&amp; view.getParent() == null) &#123; wm.removeViewImmediate(view); &#125; &#125; return null;&#125; 创建PhoneWindow对象，接下来继续设置所创建的窗口win的以下属性： 1、窗口类型：设置为WindowManager.LayoutParams.TYPE_APPLICATION_STARTING，即设置为启动窗口类型； 2、窗口标题：由参数labelRes、nonLocalizedLabel，以及窗口的运行上下文context来确定； 3、窗口标志：分别将indowManager.LayoutParams.FLAG_NOT_TOUCHABLE、WindowManager.LayoutParams.FLAG_NOT_FOCUSABLE和WindowManager.LayoutParams.FLAG_ALT_FOCUSABLE_IM位设置为1，即不可接受触摸事件和不可获得焦点，但是可以接受输入法窗口； 4、窗口大小：设置为WindowManager.LayoutParams.MATCH_PARENT，即与父窗口一样大，但是由于这是一个顶层窗口，因此实际上是指与屏幕一样大； 5、布局参数：包括窗口所对应的窗口令牌（token）和包名（packageName），以及窗口所使用的动画类型（windowAnimations）和标题（title）。 wm.addView(view, params)，一个新创建的Activity组件的启动窗口就增加到WindowManagerService服务中去了，这样，WindowManagerService服务就可以下次刷新系统UI时，将该启动窗口显示出来 （五）、WMS切换Activity窗口（App Transition）过程WindowManagerService服务在执行Activity窗口的切换操作的时候，会给参与切换操作的Activity组件的设置一个动画，以便可以向用户展现一个Activity组件切换效果，从而提高用户体验。 首先看一下App Transition动态图： 时序图： 我们直接分析App Transition过程的prepareAppTransition、executeAppTransition 关于Activity启动过程请参考：【Android-7-1-2-Android-N-Activity启动流程分析】 5.1、prepareAppTransition()过程5.1.1、ActivityStack.startActivityLocked()12345678[-&gt;ActivityStack.java]......mWindowManager.prepareAppTransition(newTask ? r.mLaunchTaskBehind ? TRANSIT_TASK_OPEN_BEHIND : TRANSIT_TASK_OPEN : TRANSIT_ACTIVITY_OPEN, keepCurTransition);...... 5.1.2、WindowManagerService.prepareAppTransition()123456789101112[-&gt;WindowManagerService.java]@Overridepublic void prepareAppTransition(int transit, boolean alwaysKeepCurrent) &#123; ...... synchronized(mWindowMap) &#123; boolean prepared = mAppTransition.prepareAppTransitionLocked( transit, alwaysKeepCurrent); if (prepared &amp;&amp; okToDisplay()) &#123; mSkipAppTransitionAnimation = false; &#125; &#125;&#125; 发现是直接调用AppTransition.prepareAppTransitionLocked()实现的。 5.1.3、AppTransition.prepareAppTransitionLocked()进一步调用setAppTransition() 12345private void setAppTransition(int transit) &#123; mNextAppTransition = transit; setLastAppTransition(TRANSIT_UNSET, null, null);&#125; 发现只是将transit（即AppTransition动画类型）赋值给变量mNextAppTransition 5.2、AppTransition animation设置过程继续分析ActivityStackSupervisor.realStartActivityLocked() 123456789101112[-&gt;ActivityStackSupervisor.java]final boolean realStartActivityLocked(ActivityRecord r, ProcessRecord app, boolean andResume, boolean checkConfig) throws RemoteException &#123; ...... if (andResume) &#123; r.startFreezingScreenLocked(app, 0); mWindowManager.setAppVisibility(r.appToken, true); // schedule launch ticks to collect information about slow apps. r.startLaunchTickingLocked(); &#125; 首先会通知WindowManagerService服务将参数r.appToken所描述的Activity组件的可见性设置为true 12345678910111213141516171819[-&gt;WindowManagerService.java]@Overridepublic void setAppVisibility(IBinder token, boolean visible) &#123; ...... AppWindowToken wtoken; synchronized(mWindowMap) &#123; wtoken = findAppWindowToken(token); ...... mOpeningApps.remove(wtoken); mClosingApps.remove(wtoken); ...... final long origId = Binder.clearCallingIdentity(); wtoken.inPendingTransaction = false; setTokenVisibilityLocked(wtoken, null, visible, AppTransition.TRANSIT_UNSET, true, wtoken.voiceInteraction); wtoken.updateReportedVisibilityLocked(); &#125; &#125; 调用WindowManagerService.setTokenVisibilityLocked() 1234567891011121314151617[-&gt;WindowManagerService.java] boolean setTokenVisibilityLocked(AppWindowToken wtoken, WindowManager.LayoutParams lp, boolean visible, int transit, boolean performLayout, boolean isVoiceInteraction) &#123; ...... if (wtoken.hidden == visible || (wtoken.hidden &amp;&amp; wtoken.mIsExiting) || (visible &amp;&amp; wtoken.waitingForReplacement())) &#123; ...... if (transit != AppTransition.TRANSIT_UNSET) &#123; ...... if (applyAnimationLocked(wtoken, lp, transit, visible, isVoiceInteraction)) &#123; delayed = runningAppAnimation = true; &#125; ...... changed = true; &#125; &#125; &#125; 调用WindowManagerService.applyAnimationLocked()设置AppTransition动画 12345678910[-&gt;WindowManagerService.java] private boolean applyAnimationLocked(AppWindowToken atoken, WindowManager.LayoutParams lp, int transit, boolean enter, boolean isVoiceInteraction) &#123; ...... if (okToDisplay()) &#123; Animation a = mAppTransition.loadAnimation(lp, transit, enter, mCurConfiguration.uiMode, mCurConfiguration.orientation, frame, displayFrame, insets, surfaceInsets, isVoiceInteraction, freeform, atoken.mTask.mTaskId); &#125;&#125; 终于到了AppTransition真正设置过程了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[-&gt;AppTransition.java]Animation loadAnimation(WindowManager.LayoutParams lp, int transit, boolean enter, int uiMode, int orientation, Rect frame, Rect displayFrame, Rect insets, @Nullable Rect surfaceInsets, boolean isVoiceInteraction, boolean freeform, int taskId) &#123; Animation a; if()&#123;&#125; ...... &#125; else if()&#123; ...... &#125;else &#123; int animAttr = 0; switch (transit) &#123; case TRANSIT_ACTIVITY_OPEN: animAttr = enter ? WindowAnimation_activityOpenEnterAnimation : WindowAnimation_activityOpenExitAnimation; break; case TRANSIT_ACTIVITY_CLOSE: animAttr = enter ? WindowAnimation_activityCloseEnterAnimation : WindowAnimation_activityCloseExitAnimation; break; case TRANSIT_DOCK_TASK_FROM_RECENTS: case TRANSIT_TASK_OPEN: animAttr = enter ? WindowAnimation_taskOpenEnterAnimation : WindowAnimation_taskOpenExitAnimation; break; case TRANSIT_TASK_CLOSE: animAttr = enter ? WindowAnimation_taskCloseEnterAnimation : WindowAnimation_taskCloseExitAnimation; break; case TRANSIT_TASK_TO_FRONT: animAttr = enter ? WindowAnimation_taskToFrontEnterAnimation : WindowAnimation_taskToFrontExitAnimation; break; case TRANSIT_TASK_TO_BACK: animAttr = enter ? WindowAnimation_taskToBackEnterAnimation : WindowAnimation_taskToBackExitAnimation; break; ...... &#125; a = animAttr != 0 ? loadAnimationAttr(lp, animAttr) : null; &#125; return a;&#125; 最后通过loadAnimationAttr加载xml文件加载动画，动画xml文件的存放路径（/frameworks/base/core/res/res/anim/） 5.3、executeAppTransition过程 ActivityStackSupervisor.realStartActivityLocked() ActivityStack.minimalResumeActivityLocked() ActivityStack.completeResumeLocked() ActivityStackSupervisor.reportResumedActivityLocked() 继续分析ActivityStackSupervisor.reportResumedActivityLocked() 1234567891011[-&gt;ActivityStackSupervisor.java]boolean reportResumedActivityLocked(ActivityRecord r) &#123; final ActivityStack stack = r.task.stack; ...... if (allResumedActivitiesComplete()) &#123; ensureActivitiesVisibleLocked(null, 0, !PRESERVE_WINDOWS); mWindowManager.executeAppTransition(); return true; &#125; return false;&#125; 首先是Activity组件的可见性设置，然后执行executeAppTransition() 5.3.1.WindowManagerService.executeAppTransition()123456789101112131415[-&gt;WindowManagerService.java] public void executeAppTransition() &#123; ...... synchronized(mWindowMap) &#123; if (mAppTransition.isTransitionSet()) &#123; mAppTransition.setReady(); ...... try &#123; mWindowPlacerLocked.performSurfacePlacement(); &#125; finally &#123; ...... &#125; &#125; &#125; &#125; 而WindowSurfacePlacer.performSurfacePlacement()请看前面第二节分析 （六）、Activity Window 动画显示过程6.1、动画的设置过程在Android系统中，窗口动画的本质就是对原始窗口施加一个变换（Transformation）。在线性数学中，对物体的形状进行变换是通过乘以一个矩阵（Matrix）来实现，目的就是对物体进行偏移、旋转、缩放、切变、反射和投影等。因此，给窗口设置动画实际上就给窗口设置一个变换矩阵（Transformation Matrix）。 窗口被设置的动画虽然可以达到三个，但是这三个动画可以归结为两类，一类是普通动画，例如，窗口在打开过程中被设置的进入动画和在关闭过程中被设置的退出动画，另一类是切换动画。其中，Self Transformation和Attached Transformation都是属于普通动画，而App Transformation属于切换动画。前面已经分析过App Transformation的设置过程 接下来分析普通动画的设置过程。 6.1、普通动画的设置过程普通动画的设置过程也是通过setTokenVisibilityLocked()设置的 6.1.1、WindowManagerService.setTokenVisibilityLocked()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[-&gt;WindowManagerService.java] boolean setTokenVisibilityLocked(AppWindowToken wtoken, WindowManager.LayoutParams lp, boolean visible, int transit, boolean performLayout, boolean isVoiceInteraction) &#123; ...... if (wtoken.hidden == visible || (wtoken.hidden &amp;&amp; wtoken.mIsExiting) || (visible &amp;&amp; wtoken.waitingForReplacement())) &#123; ...... if (transit != AppTransition.TRANSIT_UNSET) &#123; ...... if (applyAnimationLocked(wtoken, lp, transit, visible, isVoiceInteraction)) &#123; delayed = runningAppAnimation = true; &#125; ...... changed = true; &#125; final int windowsCount = wtoken.allAppWindows.size(); for (int i = 0; i &lt; windowsCount; i++) &#123; WindowState win = wtoken.allAppWindows.get(i); ...... if (visible) &#123; if (!win.isVisibleNow()) &#123; if (!runningAppAnimation) &#123; win.mWinAnimator.applyAnimationLocked( WindowManagerPolicy.TRANSIT_ENTER, true); ...... &#125; changed = true; win.setDisplayLayoutNeeded(); &#125; &#125; else if (win.isVisibleNow()) &#123; if (!runningAppAnimation) &#123; win.mWinAnimator.applyAnimationLocked( WindowManagerPolicy.TRANSIT_EXIT, false); ...... &#125; changed = true; win.setDisplayLayoutNeeded(); &#125; &#125; wtoken.hidden = wtoken.hiddenRequested = !visible; visibilityChanged = true; ...... if (changed) &#123; mInputMonitor.setUpdateInputWindowsNeededLw(); if (performLayout) &#123; updateFocusedWindowLocked(UPDATE_FOCUS_WILL_PLACE_SURFACES, false /*updateInputWindows*/); mWindowPlacerLocked.performSurfacePlacement(); &#125; mInputMonitor.updateInputWindowsLw(false /*force*/); &#125; &#125; &#125; 可以看到普通动画是通过win.mWinAnimator.applyAnimationLocked(WindowManagerPolicy.TRANSIT_ENTER, true) 6.1.1.WindowStateAnimator.applyAnimationLocked()1234567891011121314151617181920212223242526272829303132333435363738[-&gt;WindowStateAnimator.java]boolean applyAnimationLocked(int transit, boolean isEntrance) &#123; ...... if (mService.okToDisplay()) &#123; int anim = mPolicy.selectAnimationLw(mWin, transit); int attr = -1; Animation a = null; if (anim != 0) &#123; a = anim != -1 ? AnimationUtils.loadAnimation(mContext, anim) : null; &#125; else &#123; switch (transit) &#123; case WindowManagerPolicy.TRANSIT_ENTER: attr = com.android.internal.R.styleable.WindowAnimation_windowEnterAnimation; break; case WindowManagerPolicy.TRANSIT_EXIT: attr = com.android.internal.R.styleable.WindowAnimation_windowExitAnimation; break; case WindowManagerPolicy.TRANSIT_SHOW: attr = com.android.internal.R.styleable.WindowAnimation_windowShowAnimation; break; case WindowManagerPolicy.TRANSIT_HIDE: attr = com.android.internal.R.styleable.WindowAnimation_windowHideAnimation; break; &#125; if (attr &gt;= 0) &#123; a = mService.mAppTransition.loadAnimationAttr(mWin.mAttrs, attr); &#125; &#125; if (a != null) &#123; setAnimation(a); mAnimationIsEntrance = isEntrance; &#125; &#125; else &#123; clearAnimation(); &#125; ...... return mAnimation != null;&#125; 可以看到也是根据动画类型从而通过AppTransition.loadAnimationAttr(mWin.mAttrs, attr)加载不同的anim xml文件。 6.2、窗口动画的显示框架通过堆栈信息可以看到，由Vsync信号驱动，然后调用Choreographer.doFrame完成动画的相关操作，关于Vsync这部分之前文章已经分析过，这里不再分析了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130[-&gt;WindowAnimator.java]/** Locked on mService.mWindowMap. */private void animateLocked(long frameTimeNs) &#123; ...... mCurrentTime = frameTimeNs / TimeUtils.NANOS_PER_MS; mBulkUpdateParams = SET_ORIENTATION_CHANGE_COMPLETE; boolean wasAnimating = mAnimating; setAnimating(false); mAppWindowAnimating = false; ...... if (SHOW_TRANSACTIONS) Slog.i( TAG, \"&gt;&gt;&gt; OPEN TRANSACTION animateLocked\"); SurfaceControl.openTransaction(); SurfaceControl.setAnimationTransaction(); try &#123; final int numDisplays = mDisplayContentsAnimators.size(); for (int i = 0; i &lt; numDisplays; i++) &#123; final int displayId = mDisplayContentsAnimators.keyAt(i); //1、Activity组件切换动画的推进过程 updateAppWindowsLocked(displayId); DisplayContentsAnimator displayAnimator = mDisplayContentsAnimators.valueAt(i); final ScreenRotationAnimation screenRotationAnimation = displayAnimator.mScreenRotationAnimation; if (screenRotationAnimation != null &amp;&amp; screenRotationAnimation.isAnimating()) &#123; if (screenRotationAnimation.stepAnimationLocked(mCurrentTime)) &#123; setAnimating(true); &#125; else &#123; mBulkUpdateParams |= SET_UPDATE_ROTATION; screenRotationAnimation.kill(); displayAnimator.mScreenRotationAnimation = null; //TODO (multidisplay): Accessibility supported only for the default display. if (mService.mAccessibilityController != null &amp;&amp; displayId == Display.DEFAULT_DISPLAY) &#123; // We just finished rotation animation which means we did not // anounce the rotation and waited for it to end, announce now. mService.mAccessibilityController.onRotationChangedLocked( mService.getDefaultDisplayContentLocked(), mService.mRotation); &#125; &#125; &#125; // Update animations of all applications, including those // associated with exiting/removed apps //2、窗口动画的推进过程 updateWindowsLocked(displayId); //壁纸动画的推进过程 updateWallpaperLocked(displayId); final WindowList windows = mService.getWindowListLocked(displayId); final int N = windows.size(); //3、通过一个for循环来遍历保存在窗口堆栈的每一个WindowState对象，以便可以对系统中的每一个窗口的绘图表面进行更新 //确定该窗口实际要显示的大小、位置、Alpha通道和变换矩阵等信息 for (int j = 0; j &lt; N; j++) &#123; windows.get(j).mWinAnimator.prepareSurfaceLocked(true); &#125; &#125; for (int i = 0; i &lt; numDisplays; i++) &#123; final int displayId = mDisplayContentsAnimators.keyAt(i); testTokenMayBeDrawnLocked(displayId); final ScreenRotationAnimation screenRotationAnimation = mDisplayContentsAnimators.valueAt(i).mScreenRotationAnimation; if (screenRotationAnimation != null) &#123; screenRotationAnimation.updateSurfacesInTransaction(); &#125; orAnimating(mService.getDisplayContentLocked(displayId).animateDimLayers()); orAnimating(mService.getDisplayContentLocked(displayId).getDockedDividerController() .animate(mCurrentTime)); //TODO (multidisplay): Magnification is supported only for the default display. if (mService.mAccessibilityController != null &amp;&amp; displayId == Display.DEFAULT_DISPLAY) &#123; mService.mAccessibilityController.drawMagnifiedRegionBorderIfNeededLocked(); &#125; &#125; ...... //4、触发下一帧动画逻辑 if (mAnimating) &#123; mService.scheduleAnimationLocked(); &#125; if (mService.mWatermark != null) &#123; mService.mWatermark.drawIfNeeded(); &#125; &#125; catch (RuntimeException e) &#123; ...... &#125; finally &#123; SurfaceControl.closeTransaction(); if (SHOW_TRANSACTIONS) Slog.i( TAG, \"&lt;&lt;&lt; CLOSE TRANSACTION animateLocked\"); &#125; boolean hasPendingLayoutChanges = false; final int numDisplays = mService.mDisplayContents.size(); for (int displayNdx = 0; displayNdx &lt; numDisplays; ++displayNdx) &#123; final DisplayContent displayContent = mService.mDisplayContents.valueAt(displayNdx); final int pendingChanges = getPendingLayoutChanges(displayContent.getDisplayId()); if ((pendingChanges &amp; WindowManagerPolicy.FINISH_LAYOUT_REDO_WALLPAPER) != 0) &#123; mBulkUpdateParams |= SET_WALLPAPER_ACTION_PENDING; &#125; if (pendingChanges != 0) &#123; hasPendingLayoutChanges = true; &#125; &#125; boolean doRequest = false; if (mBulkUpdateParams != 0) &#123; doRequest = mWindowPlacerLocked.copyAnimToLayoutParamsLocked(); &#125; //5、刷新系统UI if (hasPendingLayoutChanges || doRequest) &#123; mWindowPlacerLocked.requestTraversal(); &#125; ...... if (mRemoveReplacedWindows) &#123; removeReplacedWindowsLocked(); &#125; mService.stopUsingSavedSurfaceLocked(); mService.destroyPreservedSurfaceLocked(); mService.mWindowPlacerLocked.destroyPendingSurfaces(); ......&#125; 1、Activity组件切换动画的推进、 2、窗口动画的推进、壁纸动画推进 3、循环遍历保存在窗口堆栈的每一个WindowState对象，以便可以对系统中的每一个窗口的绘图表面进行更新 确定该窗口实际要显示的大小、位置、Alpha通道和变换矩阵等信息 4、触发下一帧动画逻辑 5、刷新系统UI 其中第3点 prepareSurfaceLocked在3.3.小节已经分析过了、第5点最终会调用mWindowPlacerLocked.performSurfacePlacement来刷新UI，也已经分析过了。 接下来分析Activity组件切换动画、窗口动画的推进过程。 6.3、Activity组件切换动画AppWindowAnimator:属于AppWindowToken，它的成员变量mAppAnimator代表了此应用程序所属的AppWindowAnimator WindowStateAnimator:WMS记录了所有窗口的WindowState，其中WindowState.mWinAnimator是一个WindowStateAnimator对象，它和上面AppWindowAnimator一样可以由开发人员定制 WindowAnimator.updateAppWindowsLocked() 123456789101112131415161718192021222324[-&gt;WindowAnimator.java]private void updateAppWindowsLocked(int displayId) &#123; ArrayList&lt;TaskStack&gt; stacks = mService.getDisplayContentLocked(displayId).getStacks(); for (int stackNdx = stacks.size() - 1; stackNdx &gt;= 0; --stackNdx) &#123; final TaskStack stack = stacks.get(stackNdx); final ArrayList&lt;Task&gt; tasks = stack.getTasks(); for (int taskNdx = tasks.size() - 1; taskNdx &gt;= 0; --taskNdx) &#123; final AppTokenList tokens = tasks.get(taskNdx).mAppTokens; for (int tokenNdx = tokens.size() - 1; tokenNdx &gt;= 0; --tokenNdx) &#123; final AppWindowAnimator appAnimator = tokens.get(tokenNdx).mAppAnimator; appAnimator.wasAnimating = appAnimator.animating; if (appAnimator.stepAnimationLocked(mCurrentTime, displayId)) &#123; appAnimator.animating = true; setAnimating(true); mAppWindowAnimating = true; &#125; else if (appAnimator.wasAnimating) &#123; // stopped animating, do one more pass through the layout setAppLayoutChanges(appAnimator, WindowManagerPolicy.FINISH_LAYOUT_REDO_WALLPAPER, \"appToken \" + appAnimator.mAppToken + \" done\", displayId); ...... &#125; &#125; &#125; 调用stepAnimationLocked() 12345678910111213141516171819202122232425[-&gt;AppWindowAnimator.java] // This must be called while inside a transaction. boolean stepAnimationLocked(long currentTime, final int displayId) &#123; if (mService.okToDisplay()) &#123; ...... if ((mAppToken.allDrawn || animating || mAppToken.startingDisplayed) &amp;&amp; animation != null) &#123; ...... if (stepAnimation(currentTime)) &#123; // animation isn't over, step any thumbnail and that's // it for now. if (thumbnail != null) &#123; stepThumbnailAnimation(currentTime); &#125; return true; &#125; &#125; &#125; else if (animation != null) &#123; animating = true; animation = null; &#125; ...... &#125;&#125; 调用stepAnimation() 123456789101112131415161718192021222324252627[-&gt;AppWindowAnimator.java]private boolean stepAnimation(long currentTime) &#123; if (animation == null) &#123; return false; &#125; //1. transformation.clear(); final long animationFrameTime = getAnimationFrameTime(animation, currentTime); //2. boolean hasMoreFrames = animation.getTransformation(animationFrameTime, transformation); if (!hasMoreFrames) &#123; if (deferThumbnailDestruction &amp;&amp; !deferFinalFrameCleanup) &#123; deferFinalFrameCleanup = true; hasMoreFrames = true; &#125; else &#123; deferFinalFrameCleanup = false; if (mProlongAnimation == PROLONG_ANIMATION_AT_END) &#123; hasMoreFrames = true; &#125; else &#123; setNullAnimation(); clearThumbnail(); &#125; &#125; &#125; hasTransformation = hasMoreFrames; return hasMoreFrames;&#125; 1.将成员变量transformation所描述的变换矩阵的数据清空 2.调用Animation.getTransformation()来计算Activity组件切换动画下一步所对应的变换矩阵，并且将这个变换矩阵的数据保存在成员变量transformation 6.4、窗口动画的推进过程继续分析WindowAnimator.animateLocked()的updateWindowsLocked() 6.4.1、WindowAnimator.updateWindowsLocked()1234567891011121314151617181920212223[-&gt;WindowAnimator.java]private void updateWindowsLocked(final int displayId) &#123; ++mAnimTransactionSequence; final WindowList windows = mService.getWindowListLocked(displayId); ...... for (int i = windows.size() - 1; i &gt;= 0; i--) &#123; WindowState win = windows.get(i); WindowStateAnimator winAnimator = win.mWinAnimator; final int flags = win.mAttrs.flags; boolean canBeForceHidden = mPolicy.canBeForceHidden(win, win.mAttrs); boolean shouldBeForceHidden = shouldForceHide(win); if (winAnimator.hasSurface()) &#123; final boolean wasAnimating = winAnimator.mWasAnimating; final boolean nowAnimating = winAnimator.stepAnimationLocked(mCurrentTime); winAnimator.mWasAnimating = nowAnimating; orAnimating(nowAnimating); &#125; &#125; ...... &#125; ......&#125; WindowStateAnimator.stepAnimationLocked() 如果窗口的动画尚未结束显示，那么stepAnimationLocked()会返回一个true值给调用者，否则的话，就会返回一个false值给调用者 12345678910111213141516171819202122232425262728293031323334353637383940[-&gt;WindowStateAnimator.java] boolean stepAnimationLocked(long currentTime) &#123; // Save the animation state as it was before this step so WindowManagerService can tell if // we just started or just stopped animating by comparing mWasAnimating with isAnimationSet(). mWasAnimating = mAnimating; final DisplayContent displayContent = mWin.getDisplayContent(); if (displayContent != null &amp;&amp; mService.okToDisplay()) &#123; // We will run animations as long as the display isn't frozen. if (mWin.isDrawnLw() &amp;&amp; mAnimation != null) &#123; mHasTransformation = true; mHasLocalTransformation = true; if (!mLocalAnimating) &#123; final DisplayInfo displayInfo = displayContent.getDisplayInfo(); if (mAnimateMove) &#123; mAnimateMove = false; mAnimation.initialize(mWin.mFrame.width(), mWin.mFrame.height(), mAnimDx, mAnimDy); &#125; else &#123; mAnimation.initialize(mWin.mFrame.width(), mWin.mFrame.height(), displayInfo.appWidth, displayInfo.appHeight); &#125; mAnimDx = displayInfo.appWidth; mAnimDy = displayInfo.appHeight; mAnimation.setStartTime(mAnimationStartTime != -1 ? mAnimationStartTime : currentTime); mLocalAnimating = true; mAnimating = true; &#125; if ((mAnimation != null) &amp;&amp; mLocalAnimating) &#123; mLastAnimationTime = currentTime; if (stepAnimation(currentTime)) &#123; return true; &#125; &#125; ...... &#125; ...... return false; &#125; 12345678910111213[-&gt;WindowStateAnimator.java]private boolean stepAnimation(long currentTime) &#123; ...... currentTime = getAnimationFrameTime(mAnimation, currentTime); //1. mTransformation.clear(); //2. final boolean more = mAnimation.getTransformation(currentTime, mTransformation); if (mAnimationStartDelayed &amp;&amp; mAnimationIsEntrance) &#123; mTransformation.setAlpha(0f); &#125; return more;&#125; 1、将成员变量mTransformation所描述的变换矩阵的数据清空。 2、调用mAnimation.getTransformation()来计算窗口动画下一步所对应的变换矩阵，并且将这个变换矩阵的数据保存在成员变量mTransformation。 然后就是动画过后，窗口大小计算、渲染合成等等显示步骤了，由于之前已经分析过了，不再分析了： 3、循环遍历保存在窗口堆栈的每一个WindowState对象，以便可以对系统中的每一个窗口的绘图表面进行更新 确定该窗口实际要显示的大小、位置、Alpha通道和变换矩阵等信息 12345[-&gt;WindowAnimator.java::animateLocked()]//确定该窗口实际要显示的大小、位置、Alpha通道和变换矩阵等信息for (int j = 0; j &lt; N; j++) &#123; windows.get(j).mWinAnimator.prepareSurfaceLocked(true);&#125; 4、触发下一帧动画逻辑 1234[-&gt;WindowAnimator.java::animateLocked()]if (mAnimating) &#123; mService.scheduleAnimationLocked();&#125; 5、刷新系统UI 1234[-&gt;WindowAnimator.java::animateLocked()]if (hasPendingLayoutChanges || doRequest) &#123; mWindowPlacerLocked.requestTraversal();&#125; 最终经过SurfaceFlinger合成显示到屏幕上。 总体流程图(…)： （七）、参考文档(特别感谢各位前辈的分析和图示)：浅析 Android 的窗口WMS:窗口大小的计算Android 窗口的计算过程Android Window 机制探索Android 窗口管理 - 且听风吟Android 关于Window OverscanWindowManagerService动画分析深入理解Activity—-Token之旅 - CSDN博客Android 4.4(KitKat)窗口管理子系统 - 体系框架Android窗口系统第四篇—Activity动画的设置过程Android 7.1 GUI系统-窗口管理WMS-Surface管理（四）Android 的窗口管理系统 (View, Canvas, WindowManager)WMS–启动窗口(StartingWindow) - Gityuan博客 | 袁辉辉博客View绘制流程及源码解析(一)—-performTraversals()源码分析Android窗口系统第三篇—WindowManagerService中窗口的组织方式google 进入分屏后在横屏模式按 home 键界面错乱 (二) - Android - 掘金Android应用Activity、Dialog、PopWindow、Toast窗口添加机制及源码分析Android窗口管理服务WindowManagerService的简要介绍和学习计划 - CSDN博客Android窗口管理分析（2）：WindowManagerService窗口管理之Window添加流程Android窗口管理服务WindowManagerService显示窗口动画的原理分析 - CSDN博客Android6.0 WMS（五） WMS计算Activity窗口大小的过程分析（二）WMS的relayoutWindow","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android 7.1.2 (Android N) Android Graphics 系统 分析 [i.wonder~]","slug":"Android-7-1-2-Android-N-Android-Graphics-系统分析","date":"2018-01-31T16:00:00.000Z","updated":"2018-04-19T14:30:01.581Z","comments":true,"path":"2018/02/01/Android-7-1-2-Android-N-Android-Graphics-系统分析/","link":"","permalink":"http://zhoujinjian.cc/2018/02/01/Android-7-1-2-Android-N-Android-Graphics-系统分析/","excerpt":"Android系统图形框架由下往上主要的包括HAL(HWComposer和Gralloc两个moudle)，SurfaceFlinger（BufferQueue的消费者），WindowManagerService（窗口管理者），View（BufferQueue的生产者）四大模块。● HAL: 包括HWComposer和Gralloc两个moudle，Android N上由SurfaceFlinger打开，因此在同一进程。 gralloc 用于BufferQueue的内存分配，同时也有fb的显示接口，HWComposer作为合成SurfaceFlinger里面的Layer，并显示（通过gralloc的post函数）● SurfaceFlinger可以叫做LayerFlinger，作为Layer的管理者，同是也是BufferQueue的消费者，当每个Layer的生产者draw完完整的一帧时，会通知SurfaceFlinger，通知的方式采用BufferQueue。● WindowManagerService: 作为Window的管理者，掌管着计算窗口大小，窗口切换等任务，同时也会将相应的参数设置给SurfaceFlinger，比如Window的在z-order，和窗口的大小。● View: 作为BufferQueue的生产者，每当执行lockCanvas-&gt;draw-&gt;unlockCanvas，之后会存入一帧数据进入BufferQueue中。 嘿嘿(^▽^),但这也是无可奈何的事情，毕竟世上不可能有那么多十全十美的好事，做人在某些时候总是要有些取舍的。","text":"Android系统图形框架由下往上主要的包括HAL(HWComposer和Gralloc两个moudle)，SurfaceFlinger（BufferQueue的消费者），WindowManagerService（窗口管理者），View（BufferQueue的生产者）四大模块。● HAL: 包括HWComposer和Gralloc两个moudle，Android N上由SurfaceFlinger打开，因此在同一进程。 gralloc 用于BufferQueue的内存分配，同时也有fb的显示接口，HWComposer作为合成SurfaceFlinger里面的Layer，并显示（通过gralloc的post函数）● SurfaceFlinger可以叫做LayerFlinger，作为Layer的管理者，同是也是BufferQueue的消费者，当每个Layer的生产者draw完完整的一帧时，会通知SurfaceFlinger，通知的方式采用BufferQueue。● WindowManagerService: 作为Window的管理者，掌管着计算窗口大小，窗口切换等任务，同时也会将相应的参数设置给SurfaceFlinger，比如Window的在z-order，和窗口的大小。● View: 作为BufferQueue的生产者，每当执行lockCanvas-&gt;draw-&gt;unlockCanvas，之后会存入一帧数据进入BufferQueue中。 嘿嘿(^▽^),但这也是无可奈何的事情，毕竟世上不可能有那么多十全十美的好事，做人在某些时候总是要有些取舍的。 【博客原图链接】源码（部分）：/frameworks/native/services/surfaceflinger/ tests/Transaction_test.cpp tests/vsync/vsync.cpp /frameworks/native/include/gui/ BitTube.h BufferSlot.h BufferQueueCore.h BufferQueueProducer.h /frameworks/base/core/java/android/app/ Activity.java ActivityThread.java Instrumentation.java /frameworks/base/core/jni/ android_view_DisplayEventReceiver.cpp android_view_SurfaceControl.cpp android_view_Surface.cpp android_view_SurfaceSession.cpp /frameworks/native/include/gui/ SurfaceComposerClient.h IDisplayEventConnection.h SurfaceComposerClient.h /frameworks/native/services/surfaceflinger/ SurfaceFlinger.cpp Client.cpp main_surfaceflinger.cpp DisplayDevice.cpp DispSync.cpp EventControlThread.cpp EventThread.cpp Layer.cpp MonitoredProducer.cpp /frameworks/base/core/java/android/view/ WindowManagerImpl.java ViewManager.java WindowManagerGlobal.java ViewRootImpl.java Choreographer.java IWindowSession.aidl DisplayEventReceiver.java SurfaceControl.java Surface.java SurfaceSession.java /frameworks/native/include/ui/ GraphicBuffer.h GraphicBufferAllocator.h /frameworks/base/services/core/java/com/android/server/wm/ WindowManagerService.java Session.java WindowState.java WindowStateAnimator.java WindowSurfaceController.java 【博客原图链接】（一）、Android Graphics 系统框架（试用限制？？？万恶的亿图(EDraw)强加水印~火~） App 基于Android系统的GUI框架开发完整的Apk应用。 Android Graphics Stack Client(SurfaceFlinger Client)Android在客户端的绘图堆栈通常包括： OpenGL ES：使用GPU进行3D和2D的绘图的API EGL：衔接GLES和系统的Native Window系统的适配层 Vulkan：Vulkan为Khronos Group推出的下一代跨平台图形开发接口，用于替代历史悠久的OpenGL。Android从7.0(Nougat)开始加入了对其的支持。Vulkan与OpenGL相比，接口更底层，从而使开发者能更直接地控制GPU。由于更好的并行支持，及更小的开销，性能上也有一定的提升。Android Graphics Stack Server（SurfaceFlinger Server）SurfaceFlinger是Android用于管理Display和负责Window Composite（窗口混合），把应用的显示窗口输出到Display的系统服务。 Android Drivers（HAL）Android的驱动层，通过Android本身的HAL（硬件抽象层）机制，运行于User Space，跟渲染相关的包括： Hwcomposer：如果硬件支持，SurfaceFlinger可以请求hwcomposer去做窗口混合而不需要自己来做，这样的效率也会更高，减少对GPU资源的占用 Gralloc：用来管理Graphics Buffer的分配和管理系统的framebuffer OpenGL ES/EGL Linux Kernel and Drivers除了标准的Linux内核和驱动（例如fb是framebuffer驱动），硬件厂商自己的驱动外，Android自己的一些Patches： Ashmem：异步共享内存，用于在进程间共享一块内存区域，并允许系统在资源紧张时回收不加锁的内存块 ION：内存管理器 ION是google在Android4.0 为了解决内存碎片管理而引入的通用内存管理器,在面向程序员编程方面，它和ashmem很相似。但ION比ashmem更强大 Binder：高效的进程间通信机制 Vsync：Android 4.1引入了Vsync(Vertical Syncronization)用于渲染同步，使得App UI和SurfaceFlinger可以按硬件产生的VSync节奏来进行工作 Hardware Display（显示器）、CPU、GPU、VPU（Video Process Unit）、和内存等等 （二）、Android Graphics 测试程序（C++）为了便于观察对原生测试程序显示图像大小做了如下修改： frameworks/native/services/surfaceflinger/tests/Transaction_test.cpp Disable_HWUI_GPU_HWC.patch 原生SurfaceFlinger测试程序编译：1、编译Android 7.1.2源码-userdebug版本，烧录重启2、编译/frameworks/native/services/surfaceflinger/tests/会生成SurfaceFlinger_test3、连接手机执行命令 adb root、adb remount、adb push SurfaceFlinger_test /system/bin/4、adb shell setenforce 0(暂时关闭SELinux权限)5、adb shell、cd system/bin/、chmod 0777 SurfaceFlinger_test6、运行测试程序：./SurfaceFlinger_test 可以看到在Android 显示屏接替绘制了多个图像，并且会变换形状、位置、颜色、透明度等。 我们先看一下主要步骤： 1、 创建SurfaceComposerClient 12sp&lt;SurfaceComposerClient&gt; mComposerClient;mComposerClient = new SurfaceComposerClient; 2、 客户端SurfaceComposerClient请求SurfaceFlinger创建Surface 注：App端对应SurfaceControl&lt;—&gt;SurfaceFlinger对应Layer 12345678sp&lt;SurfaceControl&gt; mBGSurfaceControl;sp&lt;SurfaceControl&gt; mFGSurfaceControl; // Background surfacemBGSurfaceControl = mComposerClient-&gt;createSurface( String8(\"BG Test Surface\"), displayWidth, displayHeight, PIXEL_FORMAT_RGBA_8888, 0);fillSurfaceRGBA8(mBGSurfaceControl, 63, 63, 195); 1234// Foreground surfacemFGSurfaceControl = mComposerClient-&gt;createSurface( String8(\"FG Test Surface\"), 64, 64, PIXEL_FORMAT_RGBA_8888, 0);fillSurfaceRGBA8(mFGSurfaceControl, 195, 63, 63); 3、处理事务，将SurfaceControl（App）的变化更新到Layer（SurfaceFlinger）图层 123456789101112SurfaceComposerClient::openGlobalTransaction(); mComposerClient-&gt;setDisplayLayerStack(display, 0); ASSERT_EQ(NO_ERROR, mBGSurfaceControl-&gt;setLayer(INT_MAX-2)); ASSERT_EQ(NO_ERROR, mBGSurfaceControl-&gt;show()); ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;setLayer(INT_MAX-1)); ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;setPosition(64, 64)); ASSERT_EQ(NO_ERROR, mFGSurfaceControl-&gt;show()); SurfaceComposerClient::closeGlobalTransaction(true); 4、接受Vsync同步信号，渲染合成，推送到显示屏显示 接下来开始Android Graphics系统神秘探索之谜。 （三）、Android Graphics 禁用hwc和GPU3.1、Disable_HWUI_GPU_HWC注：基于Android 7.1.2 Qualcomm MSM89XX源码，由于代码段较长，已放到GitHub Disable_HWUI_GPU_HWC.patch 编译userdebug版本，烧录开机: 运行测试程序：./SurfaceFlinger_test 结果跟上述一致，这里不再贴图了。 3.2、Vsync测试程序Vsync(Vertical Syncronization)用于渲染同步，使得App UI和SurfaceFlinger可以按硬件产生的VSync节奏来进行工作。 查看frameworks/native/services/surfaceflinger/tests/下还有vsync测试程序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;android/looper.h&gt;#include &lt;gui/DisplayEventReceiver.h&gt;#include &lt;utils/Looper.h&gt;using namespace android;int receiver(int fd, int events, void* data)&#123; DisplayEventReceiver* q = (DisplayEventReceiver*)data; ssize_t n; DisplayEventReceiver::Event buffer[1]; static nsecs_t oldTimeStamp = 0; while ((n = q-&gt;getEvents(buffer, 1)) &gt; 0) &#123; for (int i=0 ; i&lt;n ; i++) &#123; if (buffer[i].header.type == DisplayEventReceiver::DISPLAY_EVENT_VSYNC) &#123; printf(\"event vsync: count=%d\\t\", buffer[i].vsync.count); &#125; if (oldTimeStamp) &#123; float t = float(buffer[i].header.timestamp - oldTimeStamp) / s2ns(1); printf(\"%f ms (%f Hz)\\n\", t*1000, 1.0/t); &#125; oldTimeStamp = buffer[i].header.timestamp; &#125; &#125; if (n&lt;0) &#123;printf(\"error reading events (%s)\\n\", strerror(-n));&#125; return 1;&#125;int main(int argc, char** argv)&#123; DisplayEventReceiver myDisplayEvent; sp&lt;Looper&gt; loop = new Looper(false); loop-&gt;addFd(myDisplayEvent.getFd(), 0, ALOOPER_EVENT_INPUT, receiver, &amp;myDisplayEvent); myDisplayEvent.setVsyncRate(1); do &#123; //printf(\"about to poll...\\n\"); int32_t ret = loop-&gt;pollOnce(-1); switch (ret) &#123; case ALOOPER_POLL_WAKE: //(\"ALOOPER_POLL_WAKE\\n\"); break; case ALOOPER_POLL_CALLBACK: //(\"ALOOPER_POLL_CALLBACK\\n\"); break; case ALOOPER_POLL_TIMEOUT: printf(\"ALOOPER_POLL_TIMEOUT\\n\"); break; case ALOOPER_POLL_ERROR: printf(\"ALOOPER_POLL_TIMEOUT\\n\"); break; default: printf(\"ugh? poll returned %d\\n\", ret); break; &#125; &#125; while (1); return 0;&#125; 编译运行看看：可以看到vsync信号每隔16 ms一次，关于vsync知识稍后再分析。 123456event vsync: count=2631 16.168612 ms (61.848231 Hz)event vsync: count=2632 16.168613 ms (61.848224 Hz)event vsync: count=2633 16.168312 ms (61.849378 Hz)event vsync: count=2634 16.168682 ms (61.847961 Hz)event vsync: count=2635 16.168596 ms (61.848288 Hz)event vsync: count=2636 16.168867 ms (61.847255 Hz) （四）、Android SurfaceFlinger 内部机制4.1、APP与SurfaceFlinger的数据结构 4.1.1、BufferQueue介绍BufferQueue 类是 Android 中所有图形处理操作的核心。它的是将生成图形数据缓冲区的一方（生产者Producer）连接到接受数据以进行显示或进一步处理的一方（消费者Consumer）。几乎所有在系统中移动图形数据缓冲区的内容都依赖于 BufferQueue。 从上图APP与SurfaceFlinger交互中可以看出，BufferQueue内部维持着64个BufferSlot，每一个BufferSlot内部有一个GraphicBuffer指向分配的Graphic Buffer。先来看一下图中几个状态代表的含义： 1234567891011frameworks/native/include/gui/BufferSlot.h// A buffer can be in one of five states, represented as below://// | mShared | mDequeueCount | mQueueCount | mAcquireCount |// --------|---------|---------------|-------------|---------------|// FREE | false | 0 | 0 | 0 |// DEQUEUED| false | 1 | 0 | 0 |// QUEUED | false | 0 | 1 | 0 |// ACQUIRED| false | 0 | 0 | 1 |// SHARED | true | any | any | any | FREE : FREE表示缓冲区可由生产者（Producer）DEQUEUED出列。 该BufferSlot由BufferQueue”拥有”。 它转换到DEQUEUED 当调用dequeueBuffer时。 DEQUEUED： DEQUEUED表示缓冲区已经被生产者（Producer）出列，但是尚未queued 或canceled。生产者（Producer）可以修改缓冲区的内容一旦相关的释放围栏被发信号通知。BufferSlot由Producer”拥有”。 它可以转换到QUEUED（通过 queueBuffer或者attachBuffer）或者返回FREE（通过cancelBuffer或者detachBuffer）。 QUEUED： QUEUED表示缓冲区已经被生产者（Producer）填充排队等待消费者（Consumer）使用。 缓冲区内容可能被继续 修改在有限的时间内，所以内容不能被访问，直到关联的栅栏fence发信号。 该BufferSlot由BufferQueue”拥有”。 它 可以转换为ACQUIRED（通过acquireBuffer）或FREE（如果是另一个缓冲区以异步模式排队）。 ACQUIRED： ACQUIRED表示缓冲区已被消费者（Consumer）获取。 如与QUEUED，内容不能被消费者访问，直到 获得栅栏fence信号。 BufferSlot由Consumer”拥有”。 它当releaseBuffer（或detachBuffer）被调用时转换为FREE。 一个 分离的缓冲区也可以通过attachBuffer进入ACQUIRED状态。 SHARED： SHARED表示此缓冲区正在共享缓冲区中使用模式。 它可以同时在其他State的任何组合， 除了FREE （因为这不包括在任何其他State）。 它可以也可以出列，排队或多次获得。 简单描述一下状态转换过程： 1、首先生产者dequeue过来一块Buffer，此时该buffer的状态为DEQUEUED，所有者为PRODUCER，生产者可以填充数据了。在没有dequeue操作时，buffer的状态为free,所有者为BUFFERQUEUE。 2、生产者填充完数据后,进行queue操作，此时buffer的状态由DEQUEUED-&gt;QUEUED的转变，buffer所有者也变成了BufferQueue了。 3、上面已经通知消费者去拿buffer了，这个时候消费者就进行acquire操作将buffer拿过来，此时buffer的状态由QUEUED-&gt;ACQUIRED转变，buffer的拥有者由BufferQueue变成Consumer。 4、当消费者已经消费了这块buffer(已经合成，已经编码等)，就进行release操作释放buffer,将buffer归还给BufferQueue,buffer状态由ACQUIRED变成FREE.buffer拥有者由Consumer变成BufferQueue. 4.1.2、生产者Producer生产者Producer实现IGraphicBufferProducer的接口，在实际运作过程中，应用（Client端）存在代理端BpGraphicBufferProducer，SurfaceFlinger（Server端）存在Native端BnGraphicBufferProducer。生产者代理端Bp通过Binder通信，不断的dequeueBuffer和queueBuffer操作，Native端同样响应这些操作请求，这样buffer就转了起来了。 这里介绍几个非常重要的函数： 1、requestBuffer requestBuffer为给定的索引请求一个新的Buffer。 服务器（即IGraphicBufferProducer实现）分配新创建的Buffer到给定的BufferSlot槽索引，并且客户端可以镜像slot-&gt;Buffer映射，这样就没有必要传输一个GraphicBuffer用于每个出队操作。 12345678// requestBuffer requests a new buffer for the given index. The server (i.e.// the IGraphicBufferProducer implementation) assigns the newly created// buffer to the given slot index, and the client is expected to mirror the// slot-&gt;buffer mapping so that it&apos;s not necessary to transfer a// GraphicBuffer for every dequeue operation.//// The slot must be in the range of [0, NUM_BUFFER_SLOTS).virtual status_t requestBuffer(int slot, sp&lt;GraphicBuffer&gt;* buf) = 0; 2、dequeueBuffer dequeueBuffer请求一个新的Buffer Slot供客户端使用。 插槽的所有权被转移到客户端，这意味着服务器不会使用与该插槽关联的缓冲区的内容。 123456// dequeueBuffer requests a new buffer slot for the client to use. Ownership// of the slot is transfered to the client, meaning that the server will not// use the contents of the buffer associated with that slot.//virtual status_t dequeueBuffer(int* slot, sp&lt;Fence&gt;* fence, uint32_t w, uint32_t h, PixelFormat format, uint32_t usage) = 0; 3、detachBuffer detachBuffer尝试删除给定buffer 的所有权插槽从buffer queue。 如果这个请求成功，该slot将会被free，并且将无法从这个接口获得缓冲区。释放的插槽将保持未分配状态，直到被选中为止在dequeueBuffer中保存一个新分配的缓冲区，或者附加一个缓冲区到插槽。 缓冲区必须已经被取出，并且调用者必须已经拥有sp （即必须调用requestBuffer） 12345678910// detachBuffer attempts to remove all ownership of the buffer in the given// slot from the buffer queue. If this call succeeds, the slot will be// freed, and there will be no way to obtain the buffer from this interface.// The freed slot will remain unallocated until either it is selected to// hold a freshly allocated buffer in dequeueBuffer or a buffer is attached// to the slot. The buffer must have already been dequeued, and the caller// must already possesses the sp&lt;GraphicBuffer&gt; (i.e., must have called// requestBuffer).//virtual status_t detachBuffer(int slot) = 0; 4、attachBuffer attachBuffer尝试将缓冲区的所有权转移给缓冲区队列。 如果这个调用成功，就好像这个缓冲区已经出队一样从返回的插槽号码。 因此，如果连接，这个调用将失败这个缓冲区会导致很多的缓冲区同时出队。 1234567// attachBuffer attempts to transfer ownership of a buffer to the buffer// queue. If this call succeeds, it will be as if this buffer was dequeued// from the returned slot number. As such, this call will fail if attaching// this buffer would cause too many buffers to be simultaneously dequeued.//virtual status_t attachBuffer(int* outSlot, const sp&lt;GraphicBuffer&gt;&amp; buffer) = 0; 4.1.3、消费者Consumer这里介绍几个非常重要的函数： 1、acquireBuffer acquireBuffer尝试获取下一个未决缓冲区的所有权BufferQueue。 如果没有缓冲区等待，则返回NO_BUFFER_AVAILABLE。 如果缓冲区被成功获取，有关缓冲区的信息将在BufferItem中返回。 1234567// acquireBuffer attempts to acquire ownership of the next pending buffer in// the BufferQueue. If no buffer is pending then it returns// NO_BUFFER_AVAILABLE. If a buffer is successfully acquired, the// information about the buffer is returned in BufferItem.//virtual status_t acquireBuffer(BufferItem* buffer, nsecs_t presentWhen, uint64_t maxFrameNumber = 0) = 0; 2、releaseBuffer releaseBuffer从消费者释放一个BufferSlot回到BufferQueue。 这可以在缓冲区的内容仍然存在时完成被访问。 栅栏将在缓冲区不再正在使用时发出信号。 frameNumber用于标识返回的确切缓冲区。 12345678// releaseBuffer releases a buffer slot from the consumer back to the// BufferQueue. This may be done while the buffer&apos;s contents are still// being accessed. The fence will signal when the buffer is no longer// in use. frameNumber is used to indentify the exact buffer returned.//virtual status_t releaseBuffer(int buf, uint64_t frameNumber, EGLDisplay display, EGLSyncKHR fence, const sp&lt;Fence&gt;&amp; releaseFence) = 0; 3、detachBuffer detachBuffer尝试删除给定缓冲区的所有权插槽从缓冲区队列。 如果这个请求成功，该插槽将会是释放，并且将无法从这个接口获得缓冲区。释放的插槽将保持未分配状态，直到被选中为止在dequeueBuffer中保存一个新分配的缓冲区，或者附加一个缓冲区到slot。 缓冲区必须已被acquired。 12345678// detachBuffer attempts to remove all ownership of the buffer in the given// slot from the buffer queue. If this call succeeds, the slot will be// freed, and there will be no way to obtain the buffer from this interface.// The freed slot will remain unallocated until either it is selected to// hold a freshly allocated buffer in dequeueBuffer or a buffer is attached// to the slot. The buffer must have already been acquired.//virtual status_t detachBuffer(int slot) = 0; 4、attachBuffer attachBuffer尝试将缓冲区的所有权转移给缓冲区队列。 如果这个调用成功，就好像这个缓冲区被获取了一样从返回的插槽号码。 因此，如果连接，这个调用将失败这个缓冲区会导致太多的缓冲区被同时acquired。 1234567// attachBuffer attempts to transfer ownership of a buffer to the buffer// queue. If this call succeeds, it will be as if this buffer was acquired// from the returned slot number. As such, this call will fail if attaching// this buffer would cause too many buffers to be simultaneously acquired.//virtual status_t attachBuffer(int *outSlot, const sp&lt;GraphicBuffer&gt;&amp; buffer) = 0; 4.2、App（Java层）请求创建Surface过程4.2.1、Activity启动流程Activity创建过程这里不再叙述。 请参考【Android 7.1.2 (Android N) Activity启动流程分析】 &amp;&amp; 【Android 7.1.2 (Android N) Activity-Window加载显示流程分析】 ● ActivityManagerService接收启动Activity的请求 Activity.startActivity()Activity.startActivityForResult()Instrumentation.execStartActivity()ActivityManagerProxy.startActivity()ActivityManagerNative.onTransact()ActivityManagerService.startActivity()ActivityStarter.startActivityMayWait()ActivityStarter.startActivityLocked()ActivityStarter.startActivityUnchecked()ActivityStackSupervisor.resumeFocusedStackTopActivityLocked() ActivityStack.resumeTopActivityUncheckedLocked()ActivityStack.resumeTopActivityInnerLocked()ActivityStackSupervisor.startSpecificActivityLocked() ●● 创建Activity所属的应用进程 ActivityManagerService.startProcessLocked() ●●● Zygote通过socket通信fork一个新的进程，并根据”android.app.ActivityThread”字符串 ●●● 反射出该对象并执行ActivityThread的main方法 ActivityThread.main() ActivityThread.attach() ActivityManagerProxy.attachApplication() ActivityManagerNative.onTransact() ActivityManagerService.attachApplication() ActivityManagerService.attachApplicationLocked() ActivityStackSupervisor.attachApplicationLocked() ActivityStackSupervisor.realStartActivityLocked() ●●●● 执行启动AcitivityIApplicationThread.scheduleLaunchActivity()ActivityThread.ApplicationThread.scheduleLaunchActivity()ActivityThread.sendMessage() ActivityThread.H.handleMessage()ActivityThread.handleLauncherActivity()ActivityThread.performLauncherActivity()ActivityThread.handleResumeActivity() 4.2.2、Window加载显示流程画图，需要重新分析一下下，嘿嘿(^▽^)~ 4.2.2.1、ActivityThread.handleLaunchActivity()接着从ActivityThread的handleLaunchActivity方法： 123456789101112131415 [-&gt;ActivityThread.java] private void handleLaunchActivity(ActivityClientRecord r, Intent customIntent, String reason)&#123; ...... //创建Activity Activity a = performLaunchActivity(r, customIntent); if (a != null) &#123; ...... //启动Activity handleResumeActivity(r.token, false, r.isForward, !r.activity.mFinished &amp;&amp; !r.startsNotResumed, r.lastProcessedSeq, reason); ...... &#125;&#125; 4.2.2.2、ActivityThread.handleResumeActivity()回到我们刚刚的handleLaunchActivity()方法，在调用完performLaunchActivity()方法之后，其有掉用了handleResumeActivity()法。performLaunchActivity()方法完成了两件事： 1) Activity窗口对象的创建，通过attach函数来完成； 2) Activity视图对象的创建，通过setContentView函数来完成； 这些准备工作完成后，就可以显示该Activity了，应用程序进程通过调用handleResumeActivity函数来启动Activity的显示过程。 [-&gt;ActivityThread.java] 123456789101112131415161718192021222324252627282930 final void handleResumeActivity(IBinder token, boolean clearHide, boolean isForward, boolean reallyResume, int seq, String reason) &#123; ActivityClientRecord r = mActivities.get(token); ...... r = performResumeActivity(token, clearHide, reason); ...... if (r.window == null &amp;&amp; !a.mFinished &amp;&amp; willBeVisible) &#123; //获得为当前Activity创建的窗口PhoneWindow对象 r.window = r.activity.getWindow(); //获取为窗口创建的视图DecorView对象 View decor = r.window.getDecorView(); decor.setVisibility(View.INVISIBLE); //在attach函数中就为当前Activity创建了WindowManager对象 ViewManager wm = a.getWindowManager(); //得到该视图对象的布局参数 WindowManager.LayoutParams l = r.window.getAttributes(); //将视图对象保存到Activity的成员变量mDecor中 a.mDecor = decor; l.type = WindowManager.LayoutParams.TYPE_BASE_APPLICATION; l.softInputMode |= forwardBit; ...... if (a.mVisibleFromClient &amp;&amp; !a.mWindowAdded) &#123; a.mWindowAdded = true; //将创建的视图对象DecorView添加到Activity的窗口管理器中 wm.addView(decor, l); &#125; ...... &#125; &#125;&#125; 在前面的performLaunchActivity函数中完成Activity的创建后，会将当前当前创建的Activity在应用程序进程端的描述符ActivityClientRecord以键值对的形式保存到ActivityThread的成员变量mActivities中：mActivities.put(r.token, r)，r.token就是Activity的身份证，即是IApplicationToken.Proxy代理对象，也用于与AMS通信。上面的函数首先通过performResumeActivity从mActivities变量中取出Activity的应用程序端描述符ActivityClientRecord，然后取出前面为Activity创建的视图对象DecorView和窗口管理器WindowManager，最后将视图对象添加到窗口管理器中。 ViewManager.addView()真正实现的的地方在WindowManagerImpl.java中。 12345public interface ViewManager&#123;public void addView(View view, ViewGroup.LayoutParams params);......&#125; [-&gt;WindowManagerImpl.java] 12345Overridepublic void addView(@NonNull View view, @NonNull ViewGroup.LayoutParams params) &#123; ...... mGlobal.addView(view, params, mContext.getDisplay(), mParentWindow);&#125; [-&gt;WindowManagerGlobal.java] 123456789101112131415161718 public void addView(View view, ViewGroup.LayoutParams params, Display display, Window parentWindow) &#123; ...... ViewRootImpl root; View panelParentView = null; synchronized (mLock) &#123; ...... root = new ViewRootImpl(view.getContext(), display); view.setLayoutParams(wparams); mViews.add(view); mRoots.add(root); mParams.add(wparams); &#125; try &#123; root.setView(view, wparams, panelParentView); &#125; ......&#125; 4.2.2.3、ViewRootImpl()构造过程：[ViewRootImpl.java # ViewRootImpl()] 1234567891011121314151617181920 final W mWindow; final Surface mSurface = new Surface(); final ViewRootHandler mHandler = new ViewRootHandler(); ...... public ViewRootImpl(Context context, Display display) &#123; mContext = context; mWindowSession = WindowManagerGlobal.getWindowSession();//IWindowSession的代理对象，该对象用于和WMS通信。 mDisplay = display; ...... mWindow = new W(this);//创建了一个W本地Binder对象，用于WMS通知应用程序进程 ...... mAttachInfo = new View.AttachInfo(mWindowSession, mWindow, display, this, mHandler, this); ...... mViewConfiguration = ViewConfiguration.get(context); mDensity = context.getResources().getDisplayMetrics().densityDpi; mNoncompatDensity = context.getResources().getDisplayMetrics().noncompatDensityDpi; mFallbackEventHandler = new PhoneFallbackEventHandler(context); mChoreographer = Choreographer.getInstance();//Choreographer对象 ......&#125; 在ViewRootImpl的构造函数中初始化了一些成员变量，ViewRootImpl创建了以下几个主要对象： (1) 通过WindowManagerGlobal.getWindowSession()得到IWindowSession的代理对象，该对象用于和WMS通信。 (2) 创建了一个W本地Binder对象，用于WMS通知应用程序进程。 (3) 采用单例模式创建了一个Choreographer对象，用于统一调度窗口绘图。 (4) 创建ViewRootHandler对象，用于处理当前视图消息。 (5) 构造一个AttachInfo对象； ●●●(6) 创建Surface对象，用于绘制当前视图，当然该Surface对象的真正创建是由WMS来完成的，只不过是WMS传递给应用程序进程的。 4.2.2.4、IWindowSession代理获取过程[-&gt;WindowManagerGlobal.java] 12345678910111213141516171819202122 private static IWindowSession sWindowSession; public static IWindowSession getWindowSession() &#123; synchronized (WindowManagerGlobal.class) &#123; if (sWindowSession == null) &#123; try &#123; ...... //得到IWindowSession代理对象 sWindowSession = windowManager.openSession( new IWindowSessionCallback.Stub() &#123; @Override public void onAnimatorScaleChanged(float scale) &#123; ValueAnimator.setDurationScale(scale); &#125; &#125;, imm.getClient(), imm.getInputContext()); &#125; catch (RemoteException e) &#123; throw e.rethrowFromSystemServer(); &#125; &#125; return sWindowSession; &#125;&#125; 以上函数通过WMS的openSession函数创建应用程序与WMS之间的连接通道，即获取IWindowSession代理对象，并将该代理对象保存到ViewRootImpl的静态成员变量sWindowSession中,因此在应用程序进程中有且只有一个IWindowSession代理对象。 [-&gt;WindowManagerService.java] 12345678Overridepublic IWindowSession openSession(IWindowSessionCallback callback, IInputMethodClient client, IInputContext inputContext) &#123; if (client == null) throw new IllegalArgumentException(\"null client\"); if (inputContext == null) throw new IllegalArgumentException(\"null inputContext\"); Session session = new Session(this, callback, client, inputContext); return session;&#125; 在WMS服务端构造了一个Session实例对象。ViewRootImpl 是一很重要的类，类似 ActivityThread 负责跟AmS通信一样，ViewRootImpl 的一个重要职责就是跟 WmS 通信，它通静态变量 sWindowSession（IWindowSession实例）与 WmS 进行通信。每个应用进程，仅有一个 sWindowSession 对象，它对应了 WmS 中的 Session 子类，WmS 为每一个应用进程分配一个 Session 对象。WindowState 类有一个 IWindow mClient 参数，是在构造方法中赋值的，是由 Session 调用 addWindow 传递过来了，对应了 ViewRootImpl 中的 W 类的实例。 4.2.2.5、视图View添加过程ViewRootImpl.setView()窗口管理器WindowManagerImpl为当前添加的窗口创建好各种对象后，调用ViewRootImpl的setView函数向WMS服务添加一个窗口对象。 [-&gt;ViewRootImpl.java] 123456789101112131415161718192021222324252627282930public void setView(View view, WindowManager.LayoutParams attrs, View panelParentView) &#123; synchronized (this) &#123; if (mView == null) &#123; ////将DecorView保存到ViewRootImpl的成员变量mView中 mView = view; ...... //1）在添加窗口前进行UI布局 requestLayout(); ...... try &#123; ...... //2)将窗口添加到WMS服务中，mWindow为W本地Binder对象，通过Binder传输到WMS服务端后，变为IWindow代理对象 res = mWindowSession.addToDisplay(mWindow, mSeq, mWindowAttributes, getHostVisibility(), mDisplay.getDisplayId(), mAttachInfo.mContentInsets, mAttachInfo.mStableInsets, mAttachInfo.mOutsets, mInputChannel); &#125; ...... //3)建立窗口消息通道 if (mInputChannel != null) &#123; if (mInputQueueCallback != null) &#123; mInputQueue = new InputQueue(); mInputQueueCallback.onInputQueueCreated(mInputQueue); &#125; mInputEventReceiver = new WindowInputEventReceiver(mInputChannel, Looper.myLooper()); &#125; ...... &#125; &#125;&#125; 通过前面的分析可以知道，用户自定义的UI作为一个子View被添加到DecorView中，然后将顶级视图DecorView添加到应用程序进程的窗口管理器中，窗口管理器首先为当前添加的View创建一个ViewRootImpl对象、一个布局参数对象ViewGroup.LayoutParams，然后将这三个对象分别保存到当前应用程序进程的窗口管理器WindowManagerImpl中，最后通过ViewRootImpl对象将当前视图对象注册到WMS服务中。 ViewRootImpl的setView函数向WMS服务添加一个窗口对象过程： (1) requestLayout()在应用程序进程中进行窗口UI布局； (2) WindowSession.addToDisplay()向WMS服务注册一个窗口对象； (3) 注册应用程序进程端的消息接收通道； 4.2.2.6、requestLayout()在应用程序进程中进行窗口UI布局；2.10、窗口UI布局过程 requestLayout函数调用里面使用了Hanlder的一个小手段，那就是利用postSyncBarrier添加了一个Barrier（挡板），这个挡板的作用是阻塞普通的同步消息的执行，在挡板被撤销之前，只会执行异步消息，而requestLayout先添加了一个挡板Barrier，之后自己插入了一个异步任务mTraversalRunnable，其主要作用就是保证mTraversalRunnable在所有同步Message之前被执行，保证View绘制的最高优先级。具体实现如下： 12345678910111213Overridepublic void requestLayout() &#123; scheduleTraversals();&#125;void scheduleTraversals() &#123; if (!mTraversalScheduled) &#123; mTraversalScheduled = true; mTraversalBarrier = mHandler.getLooper().getQueue().postSyncBarrier(); mChoreographer.postCallback( Choreographer.CALLBACK_TRAVERSAL, mTraversalRunnable, null); ...... &#125;&#125; 这里暂时不讨论Choreographer和Vsync知识，稍后再详细分析。 现在先说出结论：Choreographer构造函数中，构造了一个FrameDisplayEventReceiver对象，用于请求并接收Vsync信号。 此时FrameDisplayEventReceiver会Call requestNextVsync()来告诉系统我要在下一个VSYNC需要被trigger。Vsync信号每隔16ms一次，此时Vsync信号还未来到，继续分析mWindowSession.addToDisplay()。 4.2.2.7、mWindowSession.addToDisplay()向WMS服务注册一个窗口对象；[Session.java] 1234567Overridepublic int addToDisplay(IWindow window, int seq, WindowManager.LayoutParams attrs, int viewVisibility, int displayId, Rect outContentInsets, Rect outStableInsets, Rect outOutsets, InputChannel outInputChannel) &#123; return mService.addWindow(this, window, seq, attrs, viewVisibility, displayId, outContentInsets, outStableInsets, outOutsets, outInputChannel);&#125; [WindowManagerService.java] 123456789101112131415161718192021public int addWindow(Session session, IWindow client, int seq, WindowManager.LayoutParams attrs, int viewVisibility, int displayId, Rect outContentInsets, Rect outStableInsets, Rect outOutsets, InputChannel outInputChannel) &#123; ...... synchronized(mWindowMap) &#123; ...... WindowState win = new WindowState(this, session, client, token, attachedWindow, appOp[0], seq, attrs, viewVisibility, displayContent); return WindowManagerGlobal.ADD_APP_EXITING; &#125; ...... if (addToken) &#123; mTokenMap.put(attrs.token, token); &#125; win.attach(); mWindowMap.put(client.asBinder(), win); ...... return res;&#125; 构造一个WindowState对象，并将添加的窗口信息记录到mTokenMap和mWindowMap哈希表中。 在WMS服务端创建了所需对象后，接着调用了WindowState的attach()来进一步完成窗口添加。 [WindowState.java] 12345void attach() &#123; if (WindowManagerService.localLOGV) Slog.v(TAG, \"Attaching \" + this + \" token=\" + mToken + \", list=\" + mToken.windows); mSession.windowAddedLocked();&#125; [Session.java] 12345678910111213 void windowAddedLocked() &#123; if (mSurfaceSession == null) &#123; if (WindowManagerService.localLOGV) Slog.v( TAG_WM, \"First window added to \" + this + \", creating SurfaceSession\"); mSurfaceSession = new SurfaceSession(); if (SHOW_TRANSACTIONS) Slog.i(TAG_WM, \" NEW SURFACE SESSION \" + mSurfaceSession); mService.mSessions.add(this); if (mLastReportedAnimatorScale != mService.getCurrentAnimatorScale()) &#123; mService.dispatchNewAnimatorScaleLocked(this); &#125; &#125; mNumWindow++;&#125; 4.2.2.8、SurfaceSession建立过程SurfaceSession对象承担了应用程序与SurfaceFlinger之间的通信过程，每一个需要与SurfaceFlinger进程交互的应用程序端都需要创建一个SurfaceSession对象。 客户端请求 [SurfaceSession.java] 123public SurfaceSession() &#123; mNativeClient = nativeCreate();&#125; Java层的SurfaceSession对象构造过程会通过JNI在native层创建一个SurfaceComposerClient对象。 [android_view_SurfaceSession.cpp] 12345static jlong nativeCreate(JNIEnv* env, jclass clazz) &#123;SurfaceComposerClient* client = new SurfaceComposerClient();client-&gt;incStrong((void*)nativeCreate);return reinterpret_cast&lt;jlong&gt;(client);&#125; Java层的SurfaceSession对象与C++层的SurfaceComposerClient对象之间是一对一关系。 是否似曾相识，就是前面最开始SurfaceFlinger_Test程序第一步：new SurfaceComposerClient的过程。 [SurfaceComposerClient.cpp] 12345678910111213SurfaceComposerClient::SurfaceComposerClient(): mStatus(NO_INIT), mComposer(Composer::getInstance())&#123;&#125;void SurfaceComposerClient::onFirstRef() &#123;//得到SurfaceFlinger的代理对象BpSurfaceComposer sp&lt;ISurfaceComposer&gt; sm(ComposerService::getComposerService());if (sm != 0) &#123; sp&lt;ISurfaceComposerClient&gt; conn = sm-&gt;createConnection(); if (conn != 0) &#123; mClient = conn; mStatus = NO_ERROR; &#125;&#125;&#125; SurfaceComposerClient继承于RefBase类，当第一次被强引用时，onFirstRef函数被回调，在该函数中SurfaceComposerClient会请求SurfaceFlinger为当前应用程序创建一个Client对象，专门接收该应用程序的请求，在SurfaceFlinger端创建好Client本地Binder对象后，将该Binder代理对象返回给应用程序端，并保存在SurfaceComposerClient的成员变量mClient中。 服务端处理 在SurfaceFlinger服务端为应用程序创建交互的Client对象 [SurfaceFlinger.cpp] 12345678910sp&lt;ISurfaceComposerClient&gt; SurfaceFlinger::createConnection()&#123;sp&lt;ISurfaceComposerClient&gt; bclient;sp&lt;Client&gt; client(new Client(this));status_t err = client-&gt;initCheck();if (err == NO_ERROR) &#123; bclient = client;&#125;return bclient;&#125; 4.2.3、App（C++层）请求创建SurfaceFlinger客户端(client)的过程 继续详细分析AppApp（C++层）请求创建SurfaceFlinger客户端(client)的过程 SurfaceComposerClient第一次强引用时，会执行onFirstRef() [SurfaceComposerClient.cpp] 12345678910111213SurfaceComposerClient::SurfaceComposerClient(): mStatus(NO_INIT), mComposer(Composer::getInstance())&#123;&#125;void SurfaceComposerClient::onFirstRef() &#123;//得到SurfaceFlinger的代理对象BpSurfaceComposer sp&lt;ISurfaceComposer&gt; sm(ComposerService::getComposerService());if (sm != 0) &#123; sp&lt;ISurfaceComposerClient&gt; conn = sm-&gt;createConnection(); if (conn != 0) &#123; mClient = conn; mStatus = NO_ERROR; &#125;&#125;&#125; 第一步：获取”SurfaceFlinger”服务 ComposerService::getComposerService() 12345678910/*static*/ sp&lt;ISurfaceComposer&gt; ComposerService::getComposerService() &#123; ComposerService&amp; instance = ComposerService::getInstance(); Mutex::Autolock _l(instance.mLock); if (instance.mComposerService == NULL) &#123; ComposerService::getInstance().connectLocked(); assert(instance.mComposerService != NULL); ALOGD(\"ComposerService reconnected\"); &#125; return instance.mComposerService;&#125; ComposerService::getInstance()会调用connectLocked()获取”SurfaceFlinger”服务。 12345678910111213ComposerService::ComposerService(): Singleton&lt;ComposerService&gt;() &#123; Mutex::Autolock _l(mLock); connectLocked();&#125;void ComposerService::connectLocked() &#123; const String16 name(&quot;SurfaceFlinger&quot;); while (getService(name, &amp;mComposerService) != NO_ERROR) &#123; usleep(250000); &#125; ......&#125; 所以前面instance.mComposerService其实返回的是”SurfaceFlinger”服务。 第二步：createConnection() 接下来就会调用”SurfaceFlinger”服务的createConnection() 12345678910sp&lt;ISurfaceComposerClient&gt; SurfaceFlinger::createConnection()&#123; sp&lt;ISurfaceComposerClient&gt; bclient; sp&lt;Client&gt; client(new Client(this)); status_t err = client-&gt;initCheck(); if (err == NO_ERROR) &#123; bclient = client; &#125; return bclient;&#125; 4.2.4、APP申请创建Surface过程前面讲ViewRootImpl.setView时，requestLayout()需要Vsync trigger，加入现在Vsync信号来到，于是会继续执行。App申请创建Surface的过程就在其中。 当Vsync事件到来时，就会通过Choreographer的postCallback()，接着执行mTraversalRunnable对象的run()方法。 mTraversalRunnable对象的类型为TraversalRunnable，该类实现了Runnable接口，在其run()函数中调用了doTraversal()函数来完成窗口布局。 [-&gt;ViewRootImpl.java] 12345678910111213141516final class TraversalRunnable implements Runnable &#123; @Override public void run() &#123; doTraversal(); &#125;&#125;final TraversalRunnable mTraversalRunnable = new TraversalRunnable();void doTraversal() &#123; if (mTraversalScheduled) &#123; mTraversalScheduled = false; mHandler.getLooper().getQueue().removeSyncBarrier(mTraversalBarrier); ...... performTraversals(); ...... &#125;&#125; performTraversals函数相当复杂，其主要实现以下几个重要步骤： 1.执行窗口测量； 2.执行窗口注册； 3.执行窗口布局； 4.执行窗口绘图； [-&gt;ViewRootImpl.java] 12345678910111213141516171819202122232425262728293031 private void performTraversals() &#123; ...... /****************执行窗口测量******************/ if (layoutRequested) &#123; windowSizeMayChange |= measureHierarchy(host, lp, res, desiredWindowWidth, desiredWindowHeight); &#125; ...... /****************向WMS服务添加窗口******************/ if (mFirst || windowShouldResize || insetsChanged || viewVisibilityChanged || params != null || mForceNextWindowRelayout) &#123; ...... try &#123; ...... relayoutResult = relayoutWindow(params, viewVisibility, insetsPending); ...... &#125; ...... &#125; /****************执行窗口布局******************/ if (didLayout) &#123; performLayout(lp, mWidth, mHeight); ...... &#125; /****************执行窗口绘制******************/ if (!cancelDraw &amp;&amp; !newSurface) &#123; ...... performDraw(); &#125; ......&#125; 1、执行窗口测量performMeasure() [-&gt;ViewRootImpl.java] 123456 private void performMeasure(int childWidthMeasureSpec, int childHeightMeasureSpec) &#123; Trace.traceBegin(Trace.TRACE_TAG_VIEW, \"measure\"); try &#123; mView.measure(childWidthMeasureSpec, childHeightMeasureSpec); &#125; ......&#125; 4.2.4.1、APP申请创建Surface过程(Java层)2、执行窗口注册relayoutWindow； [-&gt;ViewRootImpl.java] 12345678910111213141516 private int relayoutWindow(WindowManager.LayoutParams params, int viewVisibility, boolean insetsPending) throws RemoteException &#123; ...... int relayoutResult = mWindowSession.relayout( mWindow, mSeq, params, (int) (mView.getMeasuredWidth() * appScale + 0.5f), (int) (mView.getMeasuredHeight() * appScale + 0.5f), viewVisibility, insetsPending ? WindowManagerGlobal.RELAYOUT_INSETS_PENDING : 0, mWinFrame, mPendingOverscanInsets, mPendingContentInsets, mPendingVisibleInsets, mPendingStableInsets, mPendingOutsets, mPendingBackDropFrame, mPendingConfiguration, mSurface); ...... return relayoutResult;&#125; 这里通过前面获取的IWindowSession代理对象请求WMS服务执行窗口布局，mSurface是ViewRootImpl的成员变量 [-&gt;ViewRootImpl.java] 1final Surface mSurface = new Surface(); [-&gt;Surface.java] 123456/** * Create an empty surface, which will later be filled in by readFromParcel(). * @hide */public Surface() &#123;&#125; 该Surface构造函数仅仅创建了一个空Surface对象，并没有对该Surface进程native层的初始化，到此我们知道应用程序进程为每个窗口对象都创建了一个Surface对象。并且将该Surface通过跨进程方式传输给WMS服务进程，我们知道，在Android系统中，如果一个对象需要在不同进程间传输，必须实现Parcelable接口，Surface类正好实现了Parcelable接口。ViewRootImpl通过IWindowSession接口请求WMS的完整过程如下： [-&gt;IWindowSession.java$ Proxy] 123456789101112131415161718192021/** This file is auto-generated. DO NOT MODIFY* * Original file: frameworks/base/core/java/android/view/IWindowSession.aidl*/@Override public int relayout(android.view.IWindow window, int seq, android.view.WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewVisibility, int flags, android.graphics.Rect outFrame, android.graphics.Rect outOverscanInsets, android.graphics.Rect outContentInsets, android.graphics.Rect outVisibleInsets, android.graphics.Rect outStableInsets, android.graphics.Rect outOutsets, android.graphics.Rect outBackdropFrame, android.content.res.Configuration outConfig, android.view.Surface outSurface) throws android.os.RemoteException &#123;android.os.Parcel _data = android.os.Parcel.obtain();android.os.Parcel _reply = android.os.Parcel.obtain();int _result;try &#123; ...... mRemote.transact(Stub.TRANSACTION_relayout, _data, _reply, 0); ...... if ((0 != _reply.readInt())) &#123; outSurface.readFromParcel(_reply); &#125;&#125; finally &#123; ......&#125;return _result;&#125; 从该函数的实现可以看出，应用程序进程中创建的Surface对象并没有传递到WMS服务进程，只是读取WMS服务进程返回来的Surface。那么WMS服务进程是如何响应应用程序进程布局请求的呢？ [-&gt;IWindowSession.java$ Stub] 1234567891011121314151617181920@Override public boolean onTransact(int code, android.os.Parcel data, android.os.Parcel reply, int flags) throws android.os.RemoteException&#123;switch (code)&#123;case TRANSACTION_relayout: &#123; ...... android.view.Surface _arg15; _arg15 = new android.view.Surface(); int _result = this.relayout(_arg0, _arg1, _arg2, _arg3, _arg4, _arg5, _arg6, _arg7, _arg8, _arg9, _arg10, _arg11, _arg12, _arg13, _arg14, _arg15); reply.writeNoException(); reply.writeInt(_result); ...... if ((_arg15!=null)) &#123; reply.writeInt(1); _arg15.writeToParcel(reply, android.os.Parcelable.PARCELABLE_WRITE_RETURN_VALUE); &#125; return true; &#125;&#125; 该函数可以看出，WMS服务在响应应用程序进程请求添加窗口时，首先在当前进程空间创建一个Surface对象 12android.view.Surface _arg15;_arg15 = new android.view.Surface(); 然后调用Session的relayout()函数进一步完成窗口添加过程，最后将WMS服务中创建的Surface返回给应用程序进程。 到目前为止，在应用程序进程和WMS服务进程分别创建了一个Surface对象，但是他们调用的都是Surface的无参构造函数，在该构造函数中并未真正初始化native层的Surface，那native层的Surface是在那里创建的呢？ [-&gt;Session.java] 1234567891011 public int relayout(IWindow window, int seq, WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewFlags, int flags, Rect outFrame, Rect outOverscanInsets, Rect outContentInsets, Rect outVisibleInsets, Rect outStableInsets, Rect outsets, Rect outBackdropFrame, Configuration outConfig, Surface outSurface) &#123; int res = mService.relayoutWindow(this, window, seq, attrs, requestedWidth, requestedHeight, viewFlags, flags, outFrame, outOverscanInsets, outContentInsets, outVisibleInsets, outStableInsets, outsets, outBackdropFrame, outConfig, outSurface); return res;&#125; [-&gt;WindowManagerService.java] 1234567891011121314151617181920212223242526 public int relayoutWindow(Session session, IWindow client, int seq, WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewVisibility, int flags, Rect outFrame, Rect outOverscanInsets, Rect outContentInsets, Rect outVisibleInsets, Rect outStableInsets, Rect outOutsets, Rect outBackdropFrame, Configuration outConfig, Surface outSurface) &#123; int result = 0; ...... if (viewVisibility == View.VISIBLE &amp;&amp; (win.mAppToken == null || !win.mAppToken.clientHidden)) &#123; result = relayoutVisibleWindow(outConfig, result, win, winAnimator, attrChanges, oldVisibility); try &#123; result = createSurfaceControl(outSurface, result, win, winAnimator); &#125; catch (Exception e) &#123; ...... return 0; &#125; ...... &#125; else &#123; ...... &#125; ...... return result;&#125; [-&gt;WindowManagerService.java] 12345678910111213 private int createSurfaceControl(Surface outSurface, int result, WindowState win, WindowStateAnimator winAnimator) &#123; if (!win.mHasSurface) &#123; result |= RELAYOUT_RES_SURFACE_CHANGED; &#125; WindowSurfaceController surfaceController = winAnimator.createSurfaceLocked(); if (surfaceController != null) &#123; surfaceController.getSurface(outSurface); &#125; else &#123; outSurface.release(); &#125; return result;&#125; [-&gt;WindowSurfaceController.java] 123 void getSurface(Surface outSurface) &#123; outSurface.copyFrom(mSurfaceControl);&#125; [-&gt;WindowStateAnimator.java] 123456789101112 WindowSurfaceController createSurfaceLocked() &#123; ...... try &#123; ...... mSurfaceController = new WindowSurfaceController(mSession.mSurfaceSession, attrs.getTitle().toString(), width, height, format, flags, this); w.setHasSurface(true); &#125; ...... return mSurfaceController;&#125; [-&gt;WindowSurfaceController.java] 123456789101112131415public WindowSurfaceController(SurfaceSession s, String name, int w, int h, int format, int flags, WindowStateAnimator animator) &#123; mAnimator = animator; mSurfaceW = w; mSurfaceH = h; ...... if (animator.mWin.isChildWindow() &amp;&amp; animator.mWin.mSubLayer &lt; 0 &amp;&amp; animator.mWin.mAppToken != null) &#123; ...... &#125; else &#123; mSurfaceControl = new SurfaceControl( s, name, w, h, format, flags); &#125;&#125; 4.2.4.1、APP申请创建Surface过程(C++层)SurfaceControl创建过程 [-&gt;SurfaceControl.java] 1234567 public SurfaceControl(SurfaceSession session, String name, int w, int h, int format, int flags) throws OutOfResourcesException &#123; ...... mNativeObject = nativeCreate(session, name, w, h, format, flags); ......&#125; [-&gt;android_view_SurfaceControl.cpp] 12345678910111213static jlong nativeCreate(JNIEnv* env, jclass clazz, jobject sessionObj, jstring nameStr, jint w, jint h, jint format, jint flags) &#123;ScopedUtfChars name(env, nameStr);sp&lt;SurfaceComposerClient&gt; client(android_view_SurfaceSession_getClient(env, sessionObj));sp&lt;SurfaceControl&gt; surface = client-&gt;createSurface( String8(name.c_str()), w, h, format, flags);if (surface == NULL) &#123; jniThrowException(env, OutOfResourcesException, NULL); return 0;&#125;surface-&gt;incStrong((void *)nativeCreate);return reinterpret_cast&lt;jlong&gt;(surface.get());&#125; 该函数首先得到前面创建好的SurfaceComposerClient对象，通过该对象向SurfaceFlinger端的Client对象发送创建Surface的请求，最后得到一个SurfaceControl对象。 [-&gt;SurfaceComposerClient.cpp] 1234567891011121314151617181920sp&lt;SurfaceControl&gt; SurfaceComposerClient::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags) &#123;sp&lt;SurfaceControl&gt; sur;if (mStatus == NO_ERROR) &#123; sp&lt;IBinder&gt; handle; sp&lt;IGraphicBufferProducer&gt; gbp; status_t err = mClient-&gt;createSurface(name, w, h, format, flags, &amp;handle, &amp;gbp); ALOGE_IF(err, \"SurfaceComposerClient::createSurface error %s\", strerror(-err)); if (err == NO_ERROR) &#123; sur = new SurfaceControl(this, handle, gbp); &#125;&#125;return sur;&#125; SurfaceComposerClient将Surface创建请求转交给保存在其成员变量中的Bp SurfaceComposerClient对象来完成，在SurfaceFlinger端的Client本地对象会返回一个ISurface代理对象给应用程序，通过该代理对象为应用程序当前创建的Surface创建一个SurfaceControl对象。 [ISurfaceComposerClient.cpp] 1234567891011 virtual status_t createSurface(const String8&amp; name, uint32_t width, uint32_t height, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp) &#123; Parcel data, reply; ...... remote()-&gt;transact(CREATE_SURFACE, data, &amp;reply); *handle = reply.readStrongBinder(); *gbp = interface_cast&lt;IGraphicBufferProducer&gt;(reply.readStrongBinder()); return reply.readInt32();&#125; [Client.cpp] MessageCreateSurface消息是专门为应用程序请求创建Surface而定义的一种消息类型： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 status_t Client::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp)&#123; /* * createSurface must be called from the GL thread so that it can * have access to the GL context. */ class MessageCreateLayer : public MessageBase &#123; SurfaceFlinger* flinger; Client* client; sp&lt;IBinder&gt;* handle; sp&lt;IGraphicBufferProducer&gt;* gbp; status_t result; const String8&amp; name; uint32_t w, h; PixelFormat format; uint32_t flags; public: MessageCreateLayer(SurfaceFlinger* flinger, const String8&amp; name, Client* client, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp) : flinger(flinger), client(client), handle(handle), gbp(gbp), result(NO_ERROR), name(name), w(w), h(h), format(format), flags(flags) &#123; &#125; status_t getResult() const &#123; return result; &#125; virtual bool handler() &#123; result = flinger-&gt;createLayer(name, client, w, h, format, flags, handle, gbp); return true; &#125; &#125;; sp&lt;MessageBase&gt; msg = new MessageCreateLayer(mFlinger.get(), name, this, w, h, format, flags, handle, gbp); mFlinger-&gt;postMessageSync(msg); return static_cast&lt;MessageCreateLayer*&gt;( msg.get() )-&gt;getResult(); &#125;Client将应用程序创建Surface的请求转换为异步消息投递到SurfaceFlinger的消息队列中，将创建Surface的任务转交给SurfaceFlinger。[-&gt;SurfaceFlinger.cpp] status_t SurfaceFlinger::createLayer( const String8&amp; name, const sp&lt;Client&gt;&amp; client, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp)&#123; //ALOGD(\"createLayer for (%d x %d), name=%s\", w, h, name.string()); ...... status_t result = NO_ERROR; sp&lt;Layer&gt; layer; ////根据flags创建不同类型的layer switch (flags &amp; ISurfaceComposerClient::eFXSurfaceMask) &#123; case ISurfaceComposerClient::eFXSurfaceNormal: result = createNormalLayer(client, name, w, h, flags, format, handle, gbp, &amp;layer); break; case ISurfaceComposerClient::eFXSurfaceDim: result = createDimLayer(client, name, w, h, flags, handle, gbp, &amp;layer); break; default: result = BAD_VALUE; break; &#125; if (result != NO_ERROR) &#123; return result; &#125; //将创建好的Layer对象保存在Client中 result = addClientLayer(client, *handle, *gbp, layer); if (result != NO_ERROR) &#123; return result; &#125; setTransactionFlags(eTransactionNeeded); return result; &#125; SurfaceFlinger根据标志位创建对应类型的Surface，当前系统定义了3种类型的Layer: [-&gt;ISurfaceComposerClient.h] 123eFXSurfaceNormal = 0x00000000,eFXSurfaceDim = 0x00020000,eFXSurfaceMask = 0x000F0000 [-&gt;SurfaceFlinger.cpp] 123456789101112131415161718192021222324status_t SurfaceFlinger::createNormalLayer(const sp&lt;Client&gt;&amp; client, const String8&amp; name, uint32_t w, uint32_t h, uint32_t flags, PixelFormat&amp; format, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp, sp&lt;Layer&gt;* outLayer)&#123;// initialize the surfacesswitch (format) &#123;case PIXEL_FORMAT_TRANSPARENT:case PIXEL_FORMAT_TRANSLUCENT: format = PIXEL_FORMAT_RGBA_8888; break;case PIXEL_FORMAT_OPAQUE: format = PIXEL_FORMAT_RGBX_8888; break;&#125;//在SurfaceFlinger端为应用程序的Surface创建对应的Layer对象 *outLayer = new Layer(this, client, name, w, h, flags);status_t err = (*outLayer)-&gt;setBuffers(w, h, format, flags);if (err == NO_ERROR) &#123; *handle = (*outLayer)-&gt;getHandle(); *gbp = (*outLayer)-&gt;getProducer();&#125;ALOGE_IF(err, \"createNormalLayer() failed (%s)\", strerror(-err));return err;&#125; 在SurfaceFlinger服务端为应用程序创建的Surface创建对应的Layer对象。应用程序请求创建Surface过程如下：第一次强引用Layer对象时，onFirstRef()函数被回调 [Layer.cpp] 123456789101112131415161718192021void Layer::onFirstRef() &#123;// Creates a custom BufferQueue for SurfaceFlingerConsumer to usesp&lt;IGraphicBufferProducer&gt; producer;sp&lt;IGraphicBufferConsumer&gt; consumer;//创建BufferQueue对象BufferQueue::createBufferQueue(&amp;producer, &amp;consumer);mProducer = new MonitoredProducer(producer, mFlinger);mSurfaceFlingerConsumer = new SurfaceFlingerConsumer(consumer, mTextureName, this);mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0));mSurfaceFlingerConsumer-&gt;setContentsChangedListener(this);mSurfaceFlingerConsumer-&gt;setName(mName);#ifdef TARGET_DISABLE_TRIPLE_BUFFERING#warning \"disabling triple buffering\"#elsemProducer-&gt;setMaxDequeuedBufferCount(2);#endifconst sp&lt;const DisplayDevice&gt; hw(mFlinger-&gt;getDefaultDisplayDevice());updateTransformHint(hw);&#125; 根据buffer可用监听器的注册过程，我们知道，当生产者也就是应用程序填充好图形buffer数据后，通过回调方式通知消费者的 4.2.4.2、BufferQueue构造过程[-&gt;BufferQueue.cpp] 1234567891011void BufferQueue::createBufferQueue(sp&lt;IGraphicBufferProducer&gt;* outProducer, sp&lt;IGraphicBufferConsumer&gt;* outConsumer, const sp&lt;IGraphicBufferAlloc&gt;&amp; allocator) &#123;......sp&lt;BufferQueueCore&gt; core(new BufferQueueCore(allocator));sp&lt;IGraphicBufferProducer&gt; producer(new BufferQueueProducer(core));sp&lt;IGraphicBufferConsumer&gt; consumer(new BufferQueueConsumer(core));*outProducer = producer;*outConsumer = consumer;&#125; [-&gt;BufferQueueCore.cpp] 所以核心都是这个BufferQueueCore，他是管理图形缓冲区的中枢。 123456789101112131415161718192021BufferQueueCore::BufferQueueCore(const sp&lt;IGraphicBufferAlloc&gt;&amp; allocator) :mAllocator(allocator),......&#123;if (allocator == NULL) &#123; sp&lt;ISurfaceComposer&gt; composer(ComposerService::getComposerService()); mAllocator = composer-&gt;createGraphicBufferAlloc(); if (mAllocator == NULL) &#123; BQ_LOGE(\"createGraphicBufferAlloc failed\"); &#125;&#125;int numStartingBuffers = getMaxBufferCountLocked();for (int s = 0; s &lt; numStartingBuffers; s++) &#123; mFreeSlots.insert(s);&#125;for (int s = numStartingBuffers; s &lt; BufferQueueDefs::NUM_BUFFER_SLOTS; s++) &#123; mUnusedSlots.push_front(s);&#125;&#125; BufferQueueCore类中定义了一个64项的数据mSlots，是一个容量大小为64的数组，因此BufferQueueCore可以管理最多64块的GraphicBuffer。 [-&gt;ISurfaceComposer.cpp] 1234567 virtual sp&lt;IGraphicBufferAlloc&gt; createGraphicBufferAlloc()&#123; Parcel data, reply; data.writeInterfaceToken(ISurfaceComposer::getInterfaceDescriptor()); remote()-&gt;transact(BnSurfaceComposer::CREATE_GRAPHIC_BUFFER_ALLOC, data, &amp;reply); return interface_cast&lt;IGraphicBufferAlloc&gt;(reply.readStrongBinder());&#125; [-&gt;SurfaceFlinger.cpp] 12345sp&lt;IGraphicBufferAlloc&gt; SurfaceFlinger::createGraphicBufferAlloc()&#123;sp&lt;GraphicBufferAlloc&gt; gba(new GraphicBufferAlloc());return gba;&#125; GraphicBufferAlloc构造过程 [-&gt;GraphicBufferAlloc.cpp] 123456789sp&lt;GraphicBuffer&gt; GraphicBufferAlloc::createGraphicBuffer(uint32_t width, uint32_t height, PixelFormat format, uint32_t usage, std::string requestorName, status_t* error) &#123;sp&lt;GraphicBuffer&gt; graphicBuffer(new GraphicBuffer( width, height, format, usage, std::move(requestorName)));status_t err = graphicBuffer-&gt;initCheck();......return graphicBuffer;&#125; 图形缓冲区创建过程 [-&gt;GraphicBuffer.cpp] 1234567891011121314GraphicBuffer::GraphicBuffer(uint32_t inWidth, uint32_t inHeight, PixelFormat inFormat, uint32_t inUsage, std::string requestorName): BASE(), mOwner(ownData), mBufferMapper(GraphicBufferMapper::get()), mInitCheck(NO_ERROR), mId(getUniqueId()), mGenerationNumber(0) &#123;width =height =stride =format =usage = 0;handle = NULL;mInitCheck = initSize(inWidth, inHeight, inFormat, inUsage, std::move(requestorName)); &#125; 根据图形buffer的宽高、格式等信息为图形缓冲区分配存储空间。 使用GraphicBufferAllocator对象来为图形缓冲区分配内存空间，GraphicBufferAllocator是对Gralloc模块中的gpu设备的封装类。关于GraphicBufferAllocator内存分配过程请以后作分析，图形缓冲区分配完成后，还会映射到SurfaceFlinger服务进程的虚拟地址空间。 Android图形缓冲区分配过程源码分析 [-&gt;Layer.cpp] 123456789101112131415Layer::Layer(SurfaceFlinger* flinger, const sp&lt;Client&gt;&amp; client, const String8&amp; name, uint32_t w, uint32_t h, uint32_t flags): contentDirty(false), sequence(uint32_t(android_atomic_inc(&amp;sSequence))), mFlinger(flinger), mTextureName(-1U), mPremultipliedAlpha(true), mName(\"unnamed\"), mFormat(PIXEL_FORMAT_NONE), ......&#123;mCurrentCrop.makeInvalid();mFlinger-&gt;getRenderEngine().genTextures(1, &amp;mTextureName);mTexture.init(Texture::TEXTURE_EXTERNAL, mTextureName);......&#125; 到此才算真正创建了一个可用于绘图的Surface (Layer)，从上面的分析我们可以看出，在WMS服务进程端，其实创建了两个Java层的Surface对象，第一个Surface使用了无参构造函数，仅仅构造一个Surface对象而已，而第二个Surface却使用了有参构造函数，参数指定了图象宽高等信息，这个Java层Surface对象还会在native层请求SurfaceFlinger创建一个真正能用于绘制图象的native层Surface。最后通过浅拷贝的方式将第二个Surface复制到第一个Surface中，最后通过writeToParcel方式写回到应用程序进程。 到目前为止，应用程序和WMS一共创建了3个Java层Surface（SurfaceControl）对象，如上图所示，而真正能用于绘图的Surface只有3号，那么3号Surface与2号Surface之间是什么关系呢？outSurface.copyFrom(surface) [Surface.java] 12345678910111213 public void copyFrom(SurfaceControl other) &#123; ...... long surfaceControlPtr = other.mNativeObject; ...... long newNativeObject = nativeCreateFromSurfaceControl(surfaceControlPtr); synchronized (mLock) &#123; if (mNativeObject != 0) &#123; nativeRelease(mNativeObject); &#125; setNativeObjectLocked(newNativeObject); &#125;&#125; [android_view_Surface.cpp] 123456789101112131415static jlong nativeCreateFromSurfaceControl(JNIEnv* env, jclass clazz, jlong surfaceControlNativeObj) &#123;/* * This is used by the WindowManagerService just after constructing * a Surface and is necessary for returning the Surface reference to * the caller. At this point, we should only have a SurfaceControl. */sp&lt;SurfaceControl&gt; ctrl(reinterpret_cast&lt;SurfaceControl *&gt;(surfaceControlNativeObj));sp&lt;Surface&gt; surface(ctrl-&gt;getSurface());if (surface != NULL) &#123; surface-&gt;incStrong(&amp;sRefBaseOwner);&#125;return reinterpret_cast&lt;jlong&gt;(surface.get());&#125; 2号Surface引用到了3号Surface的SurfaceControl对象后，通过writeToParcel()函数写会到应用程序进程。 [Surface.java] 12345678910111213141516 @Overridepublic void writeToParcel(Parcel dest, int flags) &#123; if (dest == null) &#123; throw new IllegalArgumentException(\"dest must not be null\"); &#125; synchronized (mLock) &#123; // NOTE: This must be kept synchronized with the native parceling code // in frameworks/native/libs/Surface.cpp dest.writeString(mName); dest.writeInt(mIsSingleBuffered ? 1 : 0); nativeWriteToParcel(mNativeObject, dest); &#125; if ((flags &amp; Parcelable.PARCELABLE_WRITE_RETURN_VALUE) != 0) &#123; release(); &#125;&#125; [android_view_Surface.cpp] 12345678910111213141516static void nativeWriteToParcel(JNIEnv* env, jclass clazz, jlong nativeObject, jobject parcelObj) &#123;Parcel* parcel = parcelForJavaObject(env, parcelObj);if (parcel == NULL) &#123; doThrowNPE(env); return;&#125;sp&lt;Surface&gt; self(reinterpret_cast&lt;Surface *&gt;(nativeObject));android::view::Surface surfaceShim;if (self != nullptr) &#123; surfaceShim.graphicBufferProducer = self-&gt;getIGraphicBufferProducer();&#125;// Calling code in Surface.java has already written the name of the Surface// to the ParcelsurfaceShim.writeToParcel(parcel, /*nameAlreadyWritten*/true);&#125; 应用程序进程中的1号Surface按相反顺序读取WMS服务端返回过来的Binder对象等数据，并构造一个native层的Surface对象。 1234567891011121314151617 public void readFromParcel(Parcel source) &#123; if (source == null) &#123; throw new IllegalArgumentException(\"source must not be null\"); &#125; synchronized (mLock) &#123; // nativeReadFromParcel() will either return mNativeObject, or // create a new native Surface and return it after reducing // the reference count on mNativeObject. Either way, it is // not necessary to call nativeRelease() here. // NOTE: This must be kept synchronized with the native parceling code // in frameworks/native/libs/Surface.cpp mName = source.readString(); mIsSingleBuffered = source.readInt() != 0; setNativeObjectLocked(nativeReadFromParcel(mNativeObject, source)); &#125;&#125; 应用程序进程中的1号Surface按相反顺序读取WMS服务端返回过来的Binder对象等数据，并构造一个native层的Surface对象。 123456789101112131415static jlong nativeCreateFromSurfaceControl(JNIEnv* env, jclass clazz, jlong surfaceControlNativeObj) &#123;/* * This is used by the WindowManagerService just after constructing * a Surface and is necessary for returning the Surface reference to * the caller. At this point, we should only have a SurfaceControl. */sp&lt;SurfaceControl&gt; ctrl(reinterpret_cast&lt;SurfaceControl *&gt;(surfaceControlNativeObj));sp&lt;Surface&gt; surface(ctrl-&gt;getSurface());if (surface != NULL) &#123; surface-&gt;incStrong(&amp;sRefBaseOwner);&#125;return reinterpret_cast&lt;jlong&gt;(surface.get());&#125; 每个Activity可以有一个或多个Surface，默认情况下一个Activity只有一个Surface，当Activity中使用SurfaceView时，就存在多个Surface。Activity默认surface是在relayoutWindow过程中由WMS服务创建的，然后回传给应用程序进程，我们知道一个Surface其实就是应用程序端的本地窗口，关于Surface的初始化过程这里就不在介绍。 4.2.4.2、生产者Producer构造过程12sp&lt;IGraphicBufferProducer&gt; producer(new BufferQueueProducer(core));sp&lt;IGraphicBufferConsumer&gt; consumer(new BufferQueueConsumer(core)); 实例化BufferQueueProducer，这里初始化了mCore(core) 和 mSlots(core-&gt;mSlots) 1234567891011BufferQueueProducer::BufferQueueProducer(const sp&lt;BufferQueueCore&gt;&amp; core) : mCore(core), mSlots(core-&gt;mSlots), mConsumerName(), mStickyTransform(0), mLastQueueBufferFence(Fence::NO_FENCE), mCallbackMutex(), mNextCallbackTicket(0), mCurrentCallbackTicket(0), mCallbackCondition(), mDequeueTimeout(-1) &#123;&#125; 4.2.4.3、消费者Consumer构造过程12sp&lt;IGraphicBufferProducer&gt; producer(new BufferQueueProducer(core));sp&lt;IGraphicBufferConsumer&gt; consumer(new BufferQueueConsumer(core)); 实例化BufferQueueConsumer，这里初始化了mCore(core) 和 mSlots(core-&gt;mSlots) 1234BufferQueueConsumer::BufferQueueConsumer(const sp&lt;BufferQueueCore&gt;&amp; core) : mCore(core), mSlots(core-&gt;mSlots), mConsumerName() &#123;&#125; 4.2.4.4、SurfaceFlinger设置监听12345mSurfaceFlingerConsumer = new SurfaceFlingerConsumer(consumer, mTextureName, this);mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0));mSurfaceFlingerConsumer-&gt;setContentsChangedListener(this);mSurfaceFlingerConsumer-&gt;setName(mName); 4.2.4.5、应用程序本地窗口Surface创建过程从前面分析可知，SurfaceFlinger在处理应用程序请求创建Surface中，在SurfaceFlinger服务端仅仅创建了Layer对象，那么应用程序本地窗口Surface在什么时候、什么地方创建呢？ 为应用程序创建好了Layer对象并返回ISurface的代理对象给应用程序，应用程序通过该代理对象创建了一个SurfaceControl对象，Java层Surface需要通过android_view_Surface.cpp中的JNI函数来操作native层的Surface，在操作native层Surface前，首先需要获取到native的Surface，应用程序本地窗口Surface就是在这个时候创建的。 [-&gt;SurfaceControl.cpp] 12345678910sp&lt;Surface&gt; SurfaceControl::getSurface() const&#123;Mutex::Autolock _l(mLock);if (mSurfaceData == 0) &#123; // This surface is always consumed by SurfaceFlinger, so the // producerControlledByApp value doesn't matter; using false. mSurfaceData = new Surface(mGraphicBufferProducer, false);&#125;return mSurfaceData;&#125; [Surface.cpp] 1234567891011121314151617181920212223242526272829303132333435Surface::Surface( const sp&lt;IGraphicBufferProducer&gt;&amp; bufferProducer, bool controlledByApp): mGraphicBufferProducer(bufferProducer), mCrop(Rect::EMPTY_RECT), mGenerationNumber(0), mSharedBufferMode(false), mAutoRefresh(false), mSharedBufferSlot(BufferItem::INVALID_BUFFER_SLOT), mSharedBufferHasBeenQueued(false), mNextFrameNumber(1) &#123;// Initialize the ANativeWindow function pointers.ANativeWindow::setSwapInterval = hook_setSwapInterval;ANativeWindow::dequeueBuffer = hook_dequeueBuffer;ANativeWindow::cancelBuffer = hook_cancelBuffer;ANativeWindow::queueBuffer = hook_queueBuffer;ANativeWindow::query = hook_query;ANativeWindow::perform = hook_perform;ANativeWindow::dequeueBuffer_DEPRECATED = hook_dequeueBuffer_DEPRECATED;ANativeWindow::cancelBuffer_DEPRECATED = hook_cancelBuffer_DEPRECATED;ANativeWindow::lockBuffer_DEPRECATED = hook_lockBuffer_DEPRECATED;ANativeWindow::queueBuffer_DEPRECATED = hook_queueBuffer_DEPRECATED;const_cast&lt;int&amp;&gt;(ANativeWindow::minSwapInterval) = 0;const_cast&lt;int&amp;&gt;(ANativeWindow::maxSwapInterval) = 1;mReqWidth = 0;mReqHeight = 0;mReqFormat = 0;mReqUsage = 0;......mSwapIntervalZero = false;&#125; 在创建完应用程序本地窗口Surface后，想要在该Surface上绘图，首先需要为该Surface分配图形buffer。我们前面介绍了Android应用程序图形缓冲区的分配都是由SurfaceFlinger服务进程来完成，在请求创建Surface时，在服务端创建了一个BufferQueue本地Binder对象，该对象负责管理应用程序一个本地窗口Surface的图形缓冲区。 4.2.4.5、执行窗口布局performLayout()[-&gt;ViewRootImpl.java] 1234567891011121314151617181920 private void performLayout(WindowManager.LayoutParams lp, int desiredWindowWidth, int desiredWindowHeight) &#123; mLayoutRequested = false; mScrollMayChange = true; mInLayout = true; final View host = mView; try &#123; host.layout(0, 0, host.getMeasuredWidth(), host.getMeasuredHeight()); mInLayout = false; int numViewsRequestingLayout = mLayoutRequesters.size(); if (numViewsRequestingLayout &gt; 0) &#123; ...... measureHierarchy(host, lp, mView.getContext().getResources(), desiredWindowWidth, desiredWindowHeight); mInLayout = true; host.layout(0, 0, host.getMeasuredWidth(), host.getMeasuredHeight()); ...... &#125; mInLayout = false;&#125; 4.2.4.7、执行窗口绘制performDraw()12345678[-&gt;ViewRootImpl.java]private void performDraw() &#123; ...... try &#123; draw(fullRedrawNeeded); &#125; ...... &#125; &#125; Android是怎样将View画出来的？由于之前我们已经关闭了HWC、GPU、HWUI，这里只关注软件绘制。 [-&gt;ViewRootImpl.java] 123456789101112131415 private void draw(boolean fullRedrawNeeded) &#123; Surface surface = mSurface; ...... if (!dirty.isEmpty() || mIsAnimating || accessibilityFocusDirty) &#123; if (mAttachInfo.mHardwareRenderer != null &amp;&amp; mAttachInfo.mHardwareRenderer.isEnabled()) &#123; ...... &#125; else &#123; ...... if (!drawSoftware(surface, mAttachInfo, xOffset, yOffset, scalingRequired, dirty)) &#123; return; &#125; &#125; &#125; .....&#125; 关于渲染这个流程很复杂，我们后续章节再分析。 4.3、APP申请(lock)Buffer的过程 1234567891011121314151617181920212223242526272829303132 private boolean drawSoftware(Surface surface, AttachInfo attachInfo, int xoff, int yoff, boolean scalingRequired, Rect dirty) &#123; // Draw with software renderer. final Canvas canvas; try &#123; ...... canvas = mSurface.lockCanvas(dirty); ...... &#125; ...... try &#123; canvas.translate(-xoff, -yoff); if (mTranslator != null) &#123; mTranslator.translateCanvas(canvas); &#125; canvas.setScreenDensity(scalingRequired ? mNoncompatDensity : 0); attachInfo.mSetIgnoreDirtyState = false; mView.draw(canvas); drawAccessibilityFocusedDrawableIfNeeded(canvas); &#125;...... &#125; finally &#123; try &#123; surface.unlockCanvasAndPost(canvas); &#125; catch (IllegalArgumentException e) &#123; ...... return false; &#125; &#125; return true;&#125; 先看看Surface的lockCanvas方法： [-&gt;Surface.java] 1234567891011//mCanvas 变量直接赋值private final Canvas mCanvas = new CompatibleCanvas();public Canvas lockCanvas(Rect inOutDirty) throws Surface.OutOfResourcesException, IllegalArgumentException &#123;synchronized (mLock) &#123; checkNotReleasedLocked(); ...... mLockedObject = nativeLockCanvas(mNativeObject, mCanvas, inOutDirty); return mCanvas;&#125;&#125; [-&gt;android_view_Surface.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263static jlong nativeLockCanvas(JNIEnv* env, jclass clazz, jlong nativeObject, jobject canvasObj, jobject dirtyRectObj) &#123; //获取java层的Surface保存的long型句柄sp&lt;Surface&gt; surface(reinterpret_cast&lt;Surface *&gt;(nativeObject));if (!isSurfaceValid(surface)) &#123; doThrowIAE(env); return 0;&#125;Rect dirtyRect(Rect::EMPTY_RECT);Rect* dirtyRectPtr = NULL;//获取java层dirty Rect的位置大小信息if (dirtyRectObj) &#123; dirtyRect.left = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.left); dirtyRect.top = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.top); dirtyRect.right = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.right); dirtyRect.bottom = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.bottom); dirtyRectPtr = &amp;dirtyRect;&#125;ANativeWindow_Buffer outBuffer; //调用Surface的lock方法,将申请的图形缓冲区赋给outBufferstatus_t err = surface-&gt;lock(&amp;outBuffer, dirtyRectPtr);......SkImageInfo info = SkImageInfo::Make(outBuffer.width, outBuffer.height, convertPixelFormat(outBuffer.format), outBuffer.format == PIXEL_FORMAT_RGBX_8888 ? kOpaque_SkAlphaType : kPremul_SkAlphaType);SkBitmap bitmap;//创建一个SkBitmap//图形缓冲区每一行像素大小ssize_t bpr = outBuffer.stride * bytesPerPixel(outBuffer.format);bitmap.setInfo(info, bpr);if (outBuffer.width &gt; 0 &amp;&amp; outBuffer.height &gt; 0) &#123; bitmap.setPixels(outBuffer.bits);&#125; else &#123; // be safe with an empty bitmap. bitmap.setPixels(NULL);&#125;Canvas* nativeCanvas = GraphicsJNI::getNativeCanvas(env, canvasObj);nativeCanvas-&gt;setBitmap(bitmap);if (dirtyRectPtr) &#123; nativeCanvas-&gt;clipRect(dirtyRect.left, dirtyRect.top, dirtyRect.right, dirtyRect.bottom);&#125;if (dirtyRectObj) &#123; env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.left, dirtyRect.left); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.top, dirtyRect.top); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.right, dirtyRect.right); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.bottom, dirtyRect.bottom);&#125;......sp&lt;Surface&gt; lockedSurface(surface);lockedSurface-&gt;incStrong(&amp;sRefBaseOwner);return (jlong) lockedSurface.get();&#125; 这段代码逻辑主要如下： 1）获取java层dirty 的Rect大小和位置信息； 2）调用Surface的lock方法,将申请的图形缓冲区赋给outBuffer； 3）创建一个Skbitmap，填充它用来保存申请的图形缓冲区，并赋值给Java层的Canvas对象； 4）将剪裁位置大小信息赋给java层Canvas对象。 4.3.1、Surface管理图形缓冲区-APP申请(lock)Buffer的过程我们上边分析到了申请图形缓冲区，用到了Surface的lock函数，我们继续查看。 [-&gt;Surface.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384status_t Surface::lock( ANativeWindow_Buffer* outBuffer, ARect* inOutDirtyBounds) &#123;......ANativeWindowBuffer* out;int fenceFd = -1;//调用dequeueBuffer函数，申请图形缓冲区status_t err = dequeueBuffer(&amp;out, &amp;fenceFd);ALOGE_IF(err, \"dequeueBuffer failed (%s)\", strerror(-err));if (err == NO_ERROR) &#123; //获取图形缓冲区区域大小,赋给后备缓冲区变量backBuffer sp&lt;GraphicBuffer&gt; backBuffer(GraphicBuffer::getSelf(out)); const Rect bounds(backBuffer-&gt;width, backBuffer-&gt;height); Region newDirtyRegion; if (inOutDirtyBounds) &#123; //如果上层指定乐刷新脏矩形区域，则用这个区域和缓冲区区域求交集， //然后将交集的结果设给需要去刷新的新区域 newDirtyRegion.set(static_cast&lt;Rect const&amp;&gt;(*inOutDirtyBounds)); newDirtyRegion.andSelf(bounds); &#125; else &#123; /如果上层没有指定脏矩形区域，所以刷新整个图形缓冲区 newDirtyRegion.set(bounds); &#125; // figure out if we can copy the frontbuffer back //上一次绘制的信息保存在mPostedBuffer中，而这个mPostedBuffer则要在unLockAndPost函数中设置 int backBufferSlot(getSlotFromBufferLocked(backBuffer.get())); const sp&lt;GraphicBuffer&gt;&amp; frontBuffer(mPostedBuffer); const bool canCopyBack = (frontBuffer != 0 &amp;&amp; backBuffer-&gt;width == frontBuffer-&gt;width &amp;&amp; backBuffer-&gt;height == frontBuffer-&gt;height &amp;&amp; backBuffer-&gt;format == frontBuffer-&gt;format); if (canCopyBack) &#123; Mutex::Autolock lock(mMutex); Region oldDirtyRegion; if(mSlots[backBufferSlot].dirtyRegion.isEmpty()) &#123; oldDirtyRegion.set(bounds); &#125; else &#123; for(int i = 0 ; i &lt; NUM_BUFFER_SLOTS; i++ ) &#123; if(i != backBufferSlot &amp;&amp; !mSlots[i].dirtyRegion.isEmpty()) oldDirtyRegion.orSelf(mSlots[i].dirtyRegion); &#125; &#125; const Region copyback(oldDirtyRegion.subtract(newDirtyRegion)); if (!copyback.isEmpty()) //这里把mPostedBuffer中的旧数据拷贝到BackBuffer中。 //后续的绘画只要更新脏区域就可以了，这会节约不少资源 copyBlt(backBuffer, frontBuffer, copyback); &#125; else &#123; // if we can't copy-back anything, modify the user's dirty // region to make sure they redraw the whole buffer //如果两次图形缓冲区大小不一致，我们就要修改用户指定的dirty区域大小为整个缓冲区大小， //然后去更新整个缓冲区 newDirtyRegion.set(bounds); Mutex::Autolock lock(mMutex); for (size_t i=0 ; i&lt;NUM_BUFFER_SLOTS ; i++) &#123; mSlots[i].dirtyRegion.clear(); &#125; &#125; &#123; // scope for the lock Mutex::Autolock lock(mMutex); //将新的dirty赋给这个bufferslot mSlots[backBufferSlot].dirtyRegion = newDirtyRegion; &#125; if (inOutDirtyBounds) &#123; *inOutDirtyBounds = newDirtyRegion.getBounds(); &#125; void* vaddr; //lock和unlock分别用来锁定和解锁一个指定的图形缓冲区，在访问一块图形缓冲区的时候， //例如，向一块图形缓冲写入内容的时候，需要将该图形缓冲区锁定，用来避免访问冲突, //锁定之后，就可以获得由参数参数l、t、w和h所圈定的一块缓冲区的起始地址，保存在输出参数vaddr中 status_t res = backBuffer-&gt;lockAsync( GRALLOC_USAGE_SW_READ_OFTEN | GRALLOC_USAGE_SW_WRITE_OFTEN, newDirtyRegion.bounds(), &amp;vaddr, fenceFd); ......&#125;return err;&#125; Surface的lock函数用来申请图形缓冲区和一些操作，方法不长，大概工作有： 1）调用connect函数完成一些初始化； 2）调用dequeueBuffer函数，申请图形缓冲区； 3）计算需要绘制的新的dirty区域，旧的区域原样copy数据。 [-&gt;BufferQueueProducer.cpp] 123456789101112131415161718192021222324252627int Surface::dequeueBuffer(android_native_buffer_t** buffer, int* fenceFd) &#123;uint32_t reqWidth;uint32_t reqHeight;PixelFormat reqFormat;uint32_t reqUsage;&#123; ......//申请图形缓冲区status_t result = mGraphicBufferProducer-&gt;dequeueBuffer(&amp;buf, &amp;fence, reqWidth, reqHeight, reqFormat, reqUsage);......//根据index获取缓冲区sp&lt;GraphicBuffer&gt;&amp; gbuf(mSlots[buf].buffer);......if ((result &amp; IGraphicBufferProducer::BUFFER_NEEDS_REALLOCATION) || gbuf == 0) &#123; //由于申请的内存是在surfaceflinger进程中， //BufferQueue中的图形缓冲区也是通过匿名共享内存和binder传递描述符映射过去的， //Surface通过调用requestBuffer将图形缓冲区映射到Surface所在进程 result = mGraphicBufferProducer-&gt;requestBuffer(buf, &amp;gbuf); ......&#125;......//获取这个这个buffer对象的指针内容*buffer = gbuf.get();......return OK;&#125; [-&gt;BufferQueueProducer.cpp] 12345678910status_t BufferQueueProducer::requestBuffer(int slot, sp&lt;GraphicBuffer&gt;* buf) &#123;ATRACE_CALL();Mutex::Autolock lock(mCore-&gt;mMutex);......mSlots[slot].mRequestBufferCalled = true;*buf = mSlots[slot].mGraphicBuffer;return NO_ERROR;&#125; 这个比较简单，还是很好理解的额，就是根据指定index取出mSlots中的slot中的buffer。 4.4、APP提交(unlockAndPost)Buffer的过程Surface绘制完毕后，unlockCanvasAndPost操作。 [-&gt;android_view_Surface.cpp] 1234567891011121314151617static void nativeUnlockCanvasAndPost(JNIEnv* env, jclass clazz, jlong nativeObject, jobject canvasObj) &#123;sp&lt;Surface&gt; surface(reinterpret_cast&lt;Surface *&gt;(nativeObject));if (!isSurfaceValid(surface)) &#123; return;&#125;// detach the canvas from the surfaceCanvas* nativeCanvas = GraphicsJNI::getNativeCanvas(env, canvasObj);nativeCanvas-&gt;setBitmap(SkBitmap());// unlock surfacestatus_t err = surface-&gt;unlockAndPost();if (err &lt; 0) &#123; doThrowIAE(env);&#125;&#125; [-&gt;Surface.cpp] 123456789101112131415status_t Surface::unlockAndPost()&#123;......int fd = -1;//解锁图形缓冲区，和前面的lockAsync成对出现status_t err = mLockedBuffer-&gt;unlockAsync(&amp;fd);//queueBuffer去归还图形缓冲区err = queueBuffer(mLockedBuffer.get(), fd);mPostedBuffer = mLockedBuffer;mLockedBuffer = 0;return err;&#125; 这里也比较简单，核心也是分两步： 1）解锁图形缓冲区，和前面的lockAsync成对出现； 2）queueBuffer去归还图形缓冲区； 所以我们还是重点分析第二步，查看queueBuffer的实现： [-&gt;Surface.cpp] 1234567int Surface::queueBuffer(android_native_buffer_t* buffer, int fenceFd) &#123;......status_t err = mGraphicBufferProducer-&gt;queueBuffer(i, input, &amp;output);mLastQueueDuration = systemTime() - now;......return err;&#125; 调用BufferQueueProducer的queueBuffer归还缓冲区，将绘制后的图形缓冲区queue回去。 [-&gt;BufferQueueProducer.cpp] 1234567891011121314151617181920status_t BufferQueueProducer::queueBuffer(int slot, const QueueBufferInput &amp;input, QueueBufferOutput *output) &#123;......&#123; // scope for the lock Mutex::Autolock lock(mCallbackMutex); while (callbackTicket != mCurrentCallbackTicket) &#123; mCallbackCondition.wait(mCallbackMutex); &#125; if (frameAvailableListener != NULL) &#123; frameAvailableListener-&gt;onFrameAvailable(item); &#125; else if (frameReplacedListener != NULL) &#123; frameReplacedListener-&gt;onFrameReplaced(item); &#125; ......&#125;......return NO_ERROR;&#125; 总结： 1）从传入的QueueBufferInput ，解析填充一些变量； 2）改变入队Slot的状态为QUEUED，每次推进来，mFrameCounter都加1。这里的slot，上一篇讲分配缓冲区返回最老的FREE状态buffer，就是用这个mFrameCounter最小值判断，就是上一篇LRU算法的判断； 3）创建一个BufferItem来描述GraphicBuffer，用mSlots[slot]中的slot填充BufferItem； 4）将BufferItem塞进mCore的mQueue队列，依照指定规则； 5）然后通知SurfaceFlinger去消费。 Folw： （五）、通知SF消费合成当绘制完毕的GraphicBuffer入队之后，会通知SurfaceFlinger去消费，就是BufferQueueProducer的queueBuffer函数的最后几行，listener-&gt;onFrameAvailable()。 listener最终通过回调，会回到Layer当中，所以最终调用Layer的onFrameAvailable接口，我们看看它的实现： [Layer.cpp] 123456789101112void Layer::onFrameAvailable(const BufferItem&amp; item) &#123;// Add this buffer from our internal queue tracker&#123; // Autolock scope ...... mQueueItems.push_back(item); android_atomic_inc(&amp;mQueuedFrames); // Wake up any pending callbacks mLastFrameNumberReceived = item.mFrameNumber; mQueueItemCondition.broadcast();&#125;mFlinger-&gt;signalLayerUpdate();&#125; 这里又调用SurfaceFlinger的signalLayerUpdate函数，继续查看： [SurfaceFlinger.cpp] 123void SurfaceFlinger::signalLayerUpdate() &#123;mEventQueue.invalidate();&#125; 这里又调用MessageQueue的invalidate函数： [MessageQueue.cpp] 123void MessageQueue::invalidate() &#123;mEvents-&gt;requestNextVsync();&#125; 贴一下SurfaceFlinger的初始化请求vsync信号流程图： 最终结果会走到SurfaceFlinger的vsync信号接收逻辑，即SurfaceFlinger的onMessageReceived函数： [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627282930void SurfaceFlinger::onMessageReceived(int32_t what) &#123;ATRACE_CALL();switch (what) &#123; case MessageQueue::INVALIDATE: &#123; bool frameMissed = !mHadClientComposition &amp;&amp; mPreviousPresentFence != Fence::NO_FENCE &amp;&amp; mPreviousPresentFence-&gt;getSignalTime() == INT64_MAX; ATRACE_INT(\"FrameMissed\", static_cast&lt;int&gt;(frameMissed)); if (mPropagateBackpressure &amp;&amp; frameMissed) &#123; signalLayerUpdate(); break; &#125; bool refreshNeeded = handleMessageTransaction(); refreshNeeded |= handleMessageInvalidate(); refreshNeeded |= mRepaintEverything; if (refreshNeeded) &#123; // Signal a refresh if a transaction modified the window state, // a new buffer was latched, or if HWC has requested a full // repaint signalRefresh(); &#125; break; &#125; case MessageQueue::REFRESH: &#123; handleMessageRefresh(); break; &#125;&#125;&#125; SurfaceFlinger收到了VSync信号后，调用了handleMessageRefresh函数 [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627void SurfaceFlinger::handleMessageRefresh() &#123;ATRACE_CALL();nsecs_t refreshStartTime = systemTime(SYSTEM_TIME_MONOTONIC);preComposition();rebuildLayerStacks();setUpHWComposer();doDebugFlashRegions();doComposition();postComposition(refreshStartTime);mPreviousPresentFence = mHwc-&gt;getRetireFence(HWC_DISPLAY_PRIMARY);mHadClientComposition = false;for (size_t displayId = 0; displayId &lt; mDisplays.size(); ++displayId) &#123; const sp&lt;DisplayDevice&gt;&amp; displayDevice = mDisplays[displayId]; mHadClientComposition = mHadClientComposition || mHwc-&gt;hasClientComposition(displayDevice-&gt;getHwcDisplayId());&#125;// Release any buffers which were replaced this framefor (auto&amp; layer : mLayersWithQueuedFrames) &#123; layer-&gt;releasePendingBuffer();&#125;mLayersWithQueuedFrames.clear();&#125; 我们主要看下下面几个函数。 [SurfaceFlinger.cpp] 123456preComposition();rebuildLayerStacks();setUpHWComposer();doDebugFlashRegions();doComposition();postComposition(refreshStartTime); 一、preComposition()函数我们先来看第一个函数preComposition() [SurfaceFlinger.cpp] 1234567891011121314void SurfaceFlinger::preComposition()&#123;bool needExtraInvalidate = false;const LayerVector&amp; layers(mDrawingState.layersSortedByZ);const size_t count = layers.size();for (size_t i=0 ; i&lt;count ; i++) &#123; if (layers[i]-&gt;onPreComposition()) &#123; needExtraInvalidate = true; &#125;&#125;if (needExtraInvalidate) &#123; signalLayerUpdate();&#125;&#125; 上面函数先是调用了mDrawingState的layersSortedByZ来得到上次绘图的Layer层列表。并不是所有的Layer都会参与屏幕图像的绘制，因此SurfaceFlinger用state对象来记录参与绘制的Layer对象。 记得我们之前分析过createLayer函数来创建Layer，创建之后会调用addClientLayer函数。 [SurfaceFlinger.cpp] 1234567891011121314151617181920status_t SurfaceFlinger::addClientLayer(const sp&lt;Client&gt;&amp; client, const sp&lt;IBinder&gt;&amp; handle, const sp&lt;IGraphicBufferProducer&gt;&amp; gbc, const sp&lt;Layer&gt;&amp; lbc) &#123;// add this layer to the current state list&#123; Mutex::Autolock _l(mStateLock); if (mCurrentState.layersSortedByZ.size() &gt;= MAX_LAYERS) &#123; return NO_MEMORY; &#125; mCurrentState.layersSortedByZ.add(lbc); mGraphicBufferProducerList.add(IInterface::asBinder(gbc));&#125;// attach this layer to the clientclient-&gt;attachLayer(handle, lbc);return NO_ERROR;&#125; 我们来看下addClientLayer函数，这里会把Layer对象放在mCurrentState的layersSortedByZ对象中。而mDrawingState和mCurrentState什么关系呢？在后面我们会介绍，mDrawingState代表上一次绘图时的状态，处理完之后会把mCurrentState赋给mDrawingState。 回到preComposition函数，遍历所有的Layer对象，调用其onPreComposition函数来检测Layer层中的图像是否有变化。 1.1、每个Layer的onFrameAvailable函数onPreComposition函数来根据mQueuedFrames来判断图像是否发生了变化，或者是mSidebandStreamChanged、mAutoRefresh。 [Layer.cpp] 1234bool Layer::onPreComposition() &#123;mRefreshPending = false;return mQueuedFrames &gt; 0 || mSidebandStreamChanged || mAutoRefresh;&#125; 当Layer所对应的Surface更新图像后，它所对应的Layer对象的onFrameAvailable函数会被调用来通知这种变化。 在SurfaceFlinger的preComposition函数中当有Layer的图像改变了，最后也会调用SurfaceFlinger的signalLayerUpdate函数。 SurfaceFlinger::signalLayerUpdate是调用了MessageQueue的invalidate函数 最后处理还是调用了SurfaceFlinger的onMessageReceived函数。看看SurfaceFlinger的onMessageReceived函数对NVALIDATE的处理 handleMessageInvalidate函数中调用了handlePageFlip函数，这个函数将会处理Layer中的缓冲区，把更新过的图像缓冲区切换到前台，等待VSync信号更新到FrameBuffer。 1.2、绘制流程用户进程更新Surface图像，将导致SurfaceFlinger中的Layer发送invalidate消息，处理该消息会调用handleTransaction函数和handlePageFilp函数来更新Layer对象。一旦VSync信号到来，再调用rebuildlayerStacks setUpHWComposer doComposition postComposition函数将所有Layer的图像混合后更新到显示设备上去。 二、handleTransaction handPageFlip更新Layer对象在上一节中的绘图的流程中，我们看到了handleTransaction和handPageFlip这两个函数通常是在用户进程更新Surface图像时会调用，来更新Layer对象。这节就主要讲解这两个函数。 2.1、handleTransaction函数handleTransaction函数的参数是transactionFlags，不过函数中没有使用这个参数，而是通过getTransactionFlags(eTransactionMask)来重新对transactionFlags赋值，然后使用它作为参数来调用函数 handleTransactionLocked。 [SurfaceFlinger.cpp] 123456789101112131415void SurfaceFlinger::handleTransaction(uint32_t transactionFlags)&#123;ATRACE_CALL();Mutex::Autolock _l(mStateLock);const nsecs_t now = systemTime();mDebugInTransaction = now;transactionFlags = getTransactionFlags(eTransactionMask);handleTransactionLocked(transactionFlags);mLastTransactionTime = systemTime() - now;mDebugInTransaction = 0;invalidateHwcGeometry();&#125; getTransactionFlags函数的参数是eTransactionMask只是屏蔽其他位。 handleTransactionLocked函数会调用每个Layer类的doTransaction函数，在分析handleTransactionLocked函数之前，我们先看看Layer类 的doTransaction函数。 2.2、Layer的doTransaction函数下面是Layer的doTransaction函数代码 [Layer.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475uint32_t Layer::doTransaction(uint32_t flags) &#123;ATRACE_CALL();pushPendingState();//上次绘制的State对象 Layer::State c = getCurrentState();//当前使用的State对象const Layer::State&amp; s(getDrawingState());const bool sizeChanged = (c.requested.w != s.requested.w) || (c.requested.h != s.requested.h);if (sizeChanged) &#123; // the size changed, we need to ask our client to request a new buffer //如果Layer的尺寸发生变化，就要改变Surface的缓冲区的尺寸 // record the new size, form this point on, when the client request // a buffer, it'll get the new size. mSurfaceFlingerConsumer-&gt;setDefaultBufferSize( c.requested.w, c.requested.h);&#125;const bool resizePending = (c.requested.w != c.active.w) || (c.requested.h != c.active.h);if (!isFixedSize()) &#123; if (resizePending &amp;&amp; mSidebandStream == NULL) &#123; //如果Layer不是固定尺寸的类型，比较它的实际大小和要求的改变大小 flags |= eDontUpdateGeometryState; &#125;&#125;//如果没有eDontUpdateGeometryState标志，更新active的值为request if (flags &amp; eDontUpdateGeometryState) &#123;&#125; else &#123; Layer::State&amp; editCurrentState(getCurrentState()); if (mFreezePositionUpdates) &#123; float tx = c.active.transform.tx(); float ty = c.active.transform.ty(); c.active = c.requested; c.active.transform.set(tx, ty); editCurrentState.active = c.active; &#125; else &#123; editCurrentState.active = editCurrentState.requested; c.active = c.requested; &#125;&#125;// 如果当前state的active和以前的State的active不等，设置更新标志 if (s.active != c.active) &#123; // invalidate and recompute the visible regions if needed flags |= Layer::eVisibleRegion;&#125;//如果当前state的sequence和以前state的sequence不等，设置更新标志if (c.sequence != s.sequence) &#123; // invalidate and recompute the visible regions if needed flags |= eVisibleRegion; this-&gt;contentDirty = true; // we may use linear filtering, if the matrix scales us const uint8_t type = c.active.transform.getType(); mNeedsFiltering = (!c.active.transform.preserveRects() || (type &gt;= Transform::SCALE));&#125;// If the layer is hidden, signal and clear out all local sync points so// that transactions for layers depending on this layer's frames becoming// visible are not blockedif (c.flags &amp; layer_state_t::eLayerHidden) &#123; Mutex::Autolock lock(mLocalSyncPointMutex); for (auto&amp; point : mLocalSyncPoints) &#123; point-&gt;setFrameAvailable(); &#125; mLocalSyncPoints.clear();&#125;// Commit the transactioncommitTransaction(c);return flags;&#125; Layer类中的两个类型为Layer::State的成员变量mDrawingState、mCurrentState，这里为什么要两个对象呢？Layer对象在绘制图形时，使用的是mDrawingState变量，用户调用接口设置Layer对象属性是，设置的值保存在mCurrentState对象中，这样就不会因为用户的操作而干扰Layer对象的绘制了。 Layer的doTransaction函数据你是比较这两个变量，如果有不同的地方，说明在上次绘制以后，用户改变的Layer的设置，要把这种变化通过flags返回。 State的结构中有两个Geometry字段，active和requested。他们表示layer的尺寸，其中requested保存是用户设置的尺寸，而active保存的值通过计算后的实际尺寸。 State中的z字段的值就是Layer在显示轴的位置，值越小位置越靠下。 layerStack字段是用户指定的一个值，用户可以给DisplayDevice也指定一个layerStack值，只有Layer对象和DisplayDevice对象的layerStack相等，这个Layer才能在这个显示设备上输出，这样的好处是可以让显示设备只显示某个Surface的内容。例如，可以让HDMI显示设备只显示手机上播放视频的Surface窗口，但不显示Activity窗口。 sequence字段是个序列值，每当用户调用了Layer的接口，例如setAlpha、setSize或者setLayer等改变Layer对象属性的哈数，这个值都会加1。因此在doTransaction函数中能通过比较sequence值来判断Layer的属性值有没有变化。 doTransaction函数最后会调用commitTransaction函数，就是把mCurrentState赋值给mDrawingState [Layer.cpp] 123void Layer::commitTransaction(const State&amp; stateToCommit) &#123;mDrawingState = stateToCommit;&#125; 2.3、handleTransactionLocked函数下面我们来分析handleTransactionLocked函数，这个函数比较长，我们分段分析 2.3.1 处理Layer的事务 [SurfaceFlinger.cpp] 123456789101112131415161718192021void SurfaceFlinger::handleTransactionLocked(uint32_t transactionFlags)&#123;const LayerVector&amp; currentLayers(mCurrentState.layersSortedByZ);const size_t count = currentLayers.size();// Notify all layers of available framesfor (size_t i = 0; i &lt; count; ++i) &#123; currentLayers[i]-&gt;notifyAvailableFrames();&#125;if (transactionFlags &amp; eTraversalNeeded) &#123; for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); uint32_t trFlags = layer-&gt;getTransactionFlags(eTransactionNeeded); if (!trFlags) continue; const uint32_t flags = layer-&gt;doTransaction(0); if (flags &amp; Layer::eVisibleRegion) mVisibleRegionsDirty = true; &#125;&#125; 在SurfaceFlinger中也有两个类型为State的变量mCurrentState和mDrawingState，但是和Layer中的不要混起来。它的名字相同而已 1234 struct State &#123; LayerVector layersSortedByZ; DefaultKeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt; displays;&#125;; 结构layersSortedByZ字段保存所有参与绘制的Layer对象，而字段displays保存的是所有输出设备的DisplayDeviceState对象 这里用两个变量的目的是和Layer中使用两个变量是一样的。 上面代码根据eTraversalNeeded标志来决定是否要检查所有的Layer对象。如果某个Layer对象中有eTransactionNeeded标志，将调用它的doTransaction函数。Layer的doTransaction函数返回的flags如果有eVisibleRegion，说明这个Layer需要更新，就把mVisibleRegionsDirty设置为true 2.3.2、处理显示设备的变化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172 if (transactionFlags &amp; eDisplayTransactionNeeded) &#123; // here we take advantage of Vector's copy-on-write semantics to // improve performance by skipping the transaction entirely when // know that the lists are identical const KeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt;&amp; curr(mCurrentState.displays); const KeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt;&amp; draw(mDrawingState.displays); if (!curr.isIdenticalTo(draw)) &#123; mVisibleRegionsDirty = true; const size_t cc = curr.size(); size_t dc = draw.size(); // find the displays that were removed // (ie: in drawing state but not in current state) // also handle displays that changed // (ie: displays that are in both lists) for (size_t i=0 ; i&lt;dc ; i++) &#123; const ssize_t j = curr.indexOfKey(draw.keyAt(i)); if (j &lt; 0) &#123; // in drawing state but not in current state if (!draw[i].isMainDisplay()) &#123; // Call makeCurrent() on the primary display so we can // be sure that nothing associated with this display // is current. const sp&lt;const DisplayDevice&gt; defaultDisplay(getDefaultDisplayDevice()); defaultDisplay-&gt;makeCurrent(mEGLDisplay, mEGLContext); sp&lt;DisplayDevice&gt; hw(getDisplayDevice(draw.keyAt(i))); if (hw != NULL) hw-&gt;disconnect(getHwComposer()); if (draw[i].type &lt; DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES) mEventThread-&gt;onHotplugReceived(draw[i].type, false); mDisplays.removeItem(draw.keyAt(i)); &#125; else &#123; ALOGW(\"trying to remove the main display\"); &#125; &#125; else &#123; // this display is in both lists. see if something changed. const DisplayDeviceState&amp; state(curr[j]); const wp&lt;IBinder&gt;&amp; display(curr.keyAt(j)); const sp&lt;IBinder&gt; state_binder = IInterface::asBinder(state.surface); const sp&lt;IBinder&gt; draw_binder = IInterface::asBinder(draw[i].surface); if (state_binder != draw_binder) &#123; // changing the surface is like destroying and // recreating the DisplayDevice, so we just remove it // from the drawing state, so that it get re-added // below. sp&lt;DisplayDevice&gt; hw(getDisplayDevice(display)); if (hw != NULL) hw-&gt;disconnect(getHwComposer()); mDisplays.removeItem(display); mDrawingState.displays.removeItemsAt(i); dc--; i--; // at this point we must loop to the next item continue; &#125; const sp&lt;DisplayDevice&gt; disp(getDisplayDevice(display)); if (disp != NULL) &#123; if (state.layerStack != draw[i].layerStack) &#123; disp-&gt;setLayerStack(state.layerStack); &#125; if ((state.orientation != draw[i].orientation) || (state.viewport != draw[i].viewport) || (state.frame != draw[i].frame)) &#123; disp-&gt;setProjection(state.orientation, state.viewport, state.frame); &#125; if (state.width != draw[i].width || state.height != draw[i].height) &#123; disp-&gt;setDisplaySize(state.width, state.height); &#125; &#125; &#125; &#125; // find displays that were added // (ie: in current state but not in drawing state) for (size_t i=0 ; i&lt;cc ; i++) &#123; if (draw.indexOfKey(curr.keyAt(i)) &lt; 0) &#123; const DisplayDeviceState&amp; state(curr[i]); sp&lt;DisplaySurface&gt; dispSurface; sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferProducer&gt; bqProducer; sp&lt;IGraphicBufferConsumer&gt; bqConsumer; BufferQueue::createBufferQueue(&amp;bqProducer, &amp;bqConsumer, new GraphicBufferAlloc()); int32_t hwcDisplayId = -1; if (state.isVirtualDisplay()) &#123; // Virtual displays without a surface are dormant: // they have external state (layer stack, projection, // etc.) but no internal state (i.e. a DisplayDevice). if (state.surface != NULL) &#123; int width = 0; DisplayUtils* displayUtils = DisplayUtils::getInstance(); int status = state.surface-&gt;query( NATIVE_WINDOW_WIDTH, &amp;width); ALOGE_IF(status != NO_ERROR, \"Unable to query width (%d)\", status); int height = 0; status = state.surface-&gt;query( NATIVE_WINDOW_HEIGHT, &amp;height); ALOGE_IF(status != NO_ERROR, \"Unable to query height (%d)\", status); if (MAX_VIRTUAL_DISPLAY_DIMENSION == 0 || (width &lt;= MAX_VIRTUAL_DISPLAY_DIMENSION &amp;&amp; height &lt;= MAX_VIRTUAL_DISPLAY_DIMENSION)) &#123; int usage = 0; status = state.surface-&gt;query( NATIVE_WINDOW_CONSUMER_USAGE_BITS, &amp;usage); ALOGW_IF(status != NO_ERROR, \"Unable to query usage (%d)\", status); if ( (status == NO_ERROR) &amp;&amp; displayUtils-&gt;canAllocateHwcDisplayIdForVDS(usage)) &#123; hwcDisplayId = allocateHwcDisplayId(state.type); &#125; &#125; displayUtils-&gt;initVDSInstance(mHwc, hwcDisplayId, state.surface, dispSurface, producer, bqProducer, bqConsumer, state.displayName, state.isSecure, state.type); &#125; &#125; else &#123; ALOGE_IF(state.surface!=NULL, \"adding a supported display, but rendering \" \"surface is provided (%p), ignoring it\", state.surface.get()); hwcDisplayId = allocateHwcDisplayId(state.type); // for supported (by hwc) displays we provide our // own rendering surface dispSurface = new FramebufferSurface(*mHwc, state.type, bqConsumer); producer = bqProducer; &#125; const wp&lt;IBinder&gt;&amp; display(curr.keyAt(i)); if (dispSurface != NULL &amp;&amp; producer != NULL) &#123; sp&lt;DisplayDevice&gt; hw = new DisplayDevice(this, state.type, hwcDisplayId, mHwc-&gt;getFormat(hwcDisplayId), state.isSecure, display, dispSurface, producer, mRenderEngine-&gt;getEGLConfig()); hw-&gt;setLayerStack(state.layerStack); hw-&gt;setProjection(state.orientation, state.viewport, state.frame); hw-&gt;setDisplayName(state.displayName); // When a new display device is added update the active // config by querying HWC otherwise the default config // (config 0) will be used. if (hwcDisplayId &gt;= DisplayDevice::DISPLAY_PRIMARY &amp;&amp; hwcDisplayId &lt; DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES) &#123; int activeConfig = mHwc-&gt;getActiveConfig(hwcDisplayId); if (activeConfig &gt;= 0) &#123; hw-&gt;setActiveConfig(activeConfig); &#125; &#125; mDisplays.add(display, hw); if (state.isVirtualDisplay()) &#123; if (hwcDisplayId &gt;= 0) &#123; mHwc-&gt;setVirtualDisplayProperties(hwcDisplayId, hw-&gt;getWidth(), hw-&gt;getHeight(), hw-&gt;getFormat()); &#125; &#125; else &#123; mEventThread-&gt;onHotplugReceived(state.type, true); &#125; &#125; &#125; &#125; &#125;&#125; 这段代码的作用是处理显示设备的变化，分成3种情况： 1.显示设备减少了，需要把显示设备对应的DisplayDevice移除 2.显示设备发生了变化，例如用户设置了Surface、重新设置了layerStack、旋转了屏幕等，这就需要重新设置显示对象的属性 3.显示设备增加了，创建新的DisplayDevice加入系统中。 2.3.3、设置TransfromHit12345678910111213141516171819202122232425262728293031323334353637383940 if (transactionFlags &amp; (eTraversalNeeded|eDisplayTransactionNeeded)) &#123; ...... sp&lt;const DisplayDevice&gt; disp; uint32_t currentlayerStack = 0; for (size_t i=0; i&lt;count; i++) &#123; // NOTE: we rely on the fact that layers are sorted by // layerStack first (so we don't have to traverse the list // of displays for every layer). const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); uint32_t layerStack = layer-&gt;getDrawingState().layerStack; if (i==0 || currentlayerStack != layerStack) &#123; currentlayerStack = layerStack; // figure out if this layerstack is mirrored // (more than one display) if so, pick the default display, // if not, pick the only display it's on. disp.clear(); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); if (hw-&gt;getLayerStack() == currentlayerStack) &#123; if (disp == NULL) &#123; disp = hw; &#125; else &#123; disp = NULL; break; &#125; &#125; &#125; &#125; if (disp == NULL) &#123; // NOTE: TEMPORARY FIX ONLY. Real fix should cause layers to // redraw after transform hint changes. See bug 8508397. // could be null when this layer is using a layerStack // that is not visible on any display. Also can occur at // screen off/on times. disp = getDefaultDisplayDevice(); &#125; layer-&gt;updateTransformHint(disp); &#125;&#125; 这段代码的作用是根据每种显示设备的不同，设置和显示设备关联在一起的Layer（主要看Layer的layerStack是否和DisplayDevice的layerStack）的TransformHint（主要指设备的显示方向orientation）。 2.3.4、处理Layer增加情况123456789101112131415161718192021222324252627282930/* * Perform our own transaction if needed */const LayerVector&amp; layers(mDrawingState.layersSortedByZ);if (currentLayers.size() &gt; layers.size()) &#123; // layers have been added mVisibleRegionsDirty = true;&#125;// some layers might have been removed, so// we need to update the regions they're exposing.if (mLayersRemoved) &#123; mLayersRemoved = false; mVisibleRegionsDirty = true; const size_t count = layers.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); if (currentLayers.indexOf(layer) &lt; 0) &#123; // this layer is not visible anymore // TODO: we could traverse the tree from front to back and // compute the actual visible region // TODO: we could cache the transformed region const Layer::State&amp; s(layer-&gt;getDrawingState()); Region visibleReg = s.active.transform.transform( Region(Rect(s.active.w, s.active.h))); invalidateLayerStack(s.layerStack, visibleReg); &#125; &#125;&#125; 这段代码处理Layer的增加情况，如果Layer增加了，需要重新计算设备的更新区域，因此把mVisibleRegionsDirty设为true，如果Layer删除了，需要把Layer的可见区域加入到系统需要更新的区域中。 2.3.5、设置mDrawingState12commitTransaction();updateCursorAsync(); 调用commitTransaction和updateCursorAsync函数 commitTransaction函数作用是把mDrawingState的值设置成mCurrentState的值。而updateCursorAsync函数会更新所有显示设备中光标的位置。 2.3.6 小结 handleTransaction函数的作用的就是处理系统在两次刷新期间的各种变化。SurfaceFlinger模块中不管是SurfaceFlinger类还是Layer类，都采用了双缓冲的方式来保存他们的属性，这样的好处是刚改变SurfaceFlinger对象或者Layer类对象的属性是，不需要上锁，大大的提高了系统效率。只有在最后的图像输出是，才进行一次上锁，并进行内存的属性变化处理。正因此，应用进程必须收到VSync信号才开始改变Surface的内容。 2.4、handlePageFlip函数handlePageFlip函数代码如下： [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051bool SurfaceFlinger::handlePageFlip()&#123;Region dirtyRegion;bool visibleRegions = false;const LayerVector&amp; layers(mDrawingState.layersSortedByZ);bool frameQueued = false;// Store the set of layers that need updates. This set must not change as// buffers are being latched, as this could result in a deadlock.// Example: Two producers share the same command stream and:// 1.) Layer 0 is latched// 2.) Layer 0 gets a new frame// 2.) Layer 1 gets a new frame// 3.) Layer 1 is latched.// Display is now waiting on Layer 1's frame, which is behind layer 0's// second frame. But layer 0's second frame could be waiting on display.Vector&lt;Layer*&gt; layersWithQueuedFrames;for (size_t i = 0, count = layers.size(); i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); if (layer-&gt;hasQueuedFrame()) &#123; frameQueued = true; if (layer-&gt;shouldPresentNow(mPrimaryDispSync)) &#123; layersWithQueuedFrames.push_back(layer.get()); &#125; else &#123; layer-&gt;useEmptyDamage(); &#125; &#125; else &#123; layer-&gt;useEmptyDamage(); &#125;&#125;for (size_t i = 0, count = layersWithQueuedFrames.size() ; i&lt;count ; i++) &#123; Layer* layer = layersWithQueuedFrames[i]; const Region dirty(layer-&gt;latchBuffer(visibleRegions)); layer-&gt;useSurfaceDamage(); const Layer::State&amp; s(layer-&gt;getDrawingState()); invalidateLayerStack(s.layerStack, dirty);&#125;mVisibleRegionsDirty |= visibleRegions;// If we will need to wake up at some time in the future to deal with a// queued frame that shouldn't be displayed during this vsync period, wake// up during the next vsync period to check again.if (frameQueued &amp;&amp; layersWithQueuedFrames.empty()) &#123; signalLayerUpdate();&#125;// Only continue with the refresh if there is actually new work to doreturn !layersWithQueuedFrames.empty();&#125; handlePageFlip函数先调用每个Layer对象的hasQueuedFrame函数，确定这个Layer对象是否有需要更新的图层，然后把需要更新的Layer对象放到layersWithQueuedFrames中。 我们先来看Layer的hasQueuedFrame方法就是看其mQueuedFrames是否大于0 和mSidebandStreamChanged。前面小节分析只要Surface有数据写入，就会调用Layer的onFrameAvailable函数，然后mQueuedFrames值加1. 继续看handlePageFlip函数，接着调用需要更新的Layer对象的latchBuffer函数，然后根据返回的更新区域调用invalidateLayerStack函数来设置更新设备对象的更新区域。 下面我们看看latchBuffer函数 LatchBuffer函数调用updateTextImage来得到需要的图像。这里参数r是Reject对象，其作用是判断在缓冲区的尺寸是否符合要求。调用updateTextImage函数如果得到的结果是PRESENT_LATER,表示推迟处理，然后调用signalLayerUpdate函数来发送invalidate消息，这次绘制过程就不处理这个Surface的图像了。 如果不需要推迟处理，把mQueuedFrames的值减1. 最后LatchBuffer函数调用mSurfaceFlingerConsumer的getCurrentBuffer来取回当前的图像缓冲区指针，保存在mActiveBuffer中。 2.5 小结这样经过handleTransaction handlePageFlip两个函数处理，SurfaceFlinger中无论是Layer属性的变化还是图像的变化都处理好了，只等VSync信号到来就可以输出了。 三、rebuildLayerStacks函数前面介绍，VSync信号到来后，先是调用了rebuildLayerStacks函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445void SurfaceFlinger::rebuildLayerStacks() &#123;updateExtendedMode();// rebuild the visible layer list per screenif (CC_UNLIKELY(mVisibleRegionsDirty)) &#123; ATRACE_CALL(); mVisibleRegionsDirty = false; invalidateHwcGeometry(); //计算每个显示设备上可见的Layer const LayerVector&amp; layers(mDrawingState.layersSortedByZ); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; Region opaqueRegion; Region dirtyRegion; Vector&lt; sp&lt;Layer&gt; &gt; layersSortedByZ; const sp&lt;DisplayDevice&gt;&amp; hw(mDisplays[dpy]); const Transform&amp; tr(hw-&gt;getTransform()); const Rect bounds(hw-&gt;getBounds()); if (hw-&gt;isDisplayOn()) &#123; //计算每个layer的可见区域，确定设备需要重新绘制的区域 computeVisibleRegions(hw-&gt;getHwcDisplayId(), layers, hw-&gt;getLayerStack(), dirtyRegion, opaqueRegion); const size_t count = layers.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); &#123; //只需要和显示设备的LayerStack相同的layer Region drawRegion(tr.transform( layer-&gt;visibleNonTransparentRegion)); drawRegion.andSelf(bounds); if (!drawRegion.isEmpty()) &#123; //如果Layer的显示区域和显示设备的窗口有交集 //把Layer加入列表中 layersSortedByZ.add(layer); &#125; &#125; &#125; &#125; //设置显示设备的可见Layer列表 hw-&gt;setVisibleLayersSortedByZ(layersSortedByZ); hw-&gt;undefinedRegion.set(bounds); hw-&gt;undefinedRegion.subtractSelf(tr.transform(opaqueRegion)); hw-&gt;dirtyRegion.orSelf(dirtyRegion); &#125;&#125;&#125; rebuildLayerStacks函数的作用是重建每个显示设备的可见layer对象列表。对于按显示轴（Z轴）排列的Layer对象，排在最前面的当然会优先显示，但是Layer图像可能有透明域，也可能有尺寸没有覆盖整个屏幕，因此下面的layer也有显示的机会。rebuildLayerStacks函数对每个显示设备，先计算和显示设备具有相同layerStack值的Layer对象在该显示设备上的可见区域。然后将可见区域和显示设备的窗口区域有交集的layer组成一个新的列表，最后把这个列表设置到显示设备对象中。 computeVisibleRegions函数首先计算每个Layer在设备上的可见区域visibleRegion。计算方法就是用整个Layer的区域减去上层所有不透明区域aboveOpaqueLayers。而上层所有不透明区域值是一个逐层累计的过程，每层都需要把自己的不透明区域累加到aboveOpaqueLayers中。 而每层的不透明区域的计算方法：如果Layer的alpha的值为255，并且layer的isOpaque函数为true，则本层的不透明区域等于Layer所在区域，否则为0.这样一层层算下来，就很容易得到每层的可见区域大小了。 其次，计算整个显示设备需要更新的区域outDirtyRegion。outDirtyRegion的值也是累计所有层的需要重回的区域得到的。如果Layer中的显示内容发生了变化，则整个可见区域visibleRegion都需要更新，同时还要包括上一次的可见区域，然后在去掉被上层覆盖后的区域得到的就是Layer需要更新的区域。如果Layer显示的内容没有变化，但是考虑到窗口大小的变化或者上层窗口的变化，因此Layer中还是有区域可以需要重绘的地方。这种情况下最简单的算法是用Layer计算出可见区域减去以前的可见区域就可以了。但是在computeVisibleRegions函数还引入了被覆盖区域，通常被覆盖区域和可见区域并不重复，因此函数中计算暴露区域是用可见区域减去被覆盖区域的。 四、setUpHWComposer函数setUpHWComposer函数的作用是更新HWComposer对象中图层对象列表以及图层属性。 [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102void SurfaceFlinger::setUpHWComposer() &#123;for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; bool dirty = !mDisplays[dpy]-&gt;getDirtyRegion(false).isEmpty(); bool empty = mDisplays[dpy]-&gt;getVisibleLayersSortedByZ().size() == 0; bool wasEmpty = !mDisplays[dpy]-&gt;lastCompositionHadVisibleLayers; ...... bool mustRecompose = dirty &amp;&amp; !(empty &amp;&amp; wasEmpty); ...... mDisplays[dpy]-&gt;beginFrame(mustRecompose); if (mustRecompose) &#123; mDisplays[dpy]-&gt;lastCompositionHadVisibleLayers = !empty; &#125;&#125;//得到系统HWComposer对象 HWComposer&amp; hwc(getHwComposer());if (hwc.initCheck() == NO_ERROR) &#123; // build the h/w work list if (CC_UNLIKELY(mHwWorkListDirty)) &#123; mHwWorkListDirty = false; for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;= 0) &#123; const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers( hw-&gt;getVisibleLayersSortedByZ()); const size_t count = currentLayers.size(); //根据Layer数量在HWComposer中创建hwc_layer_list_t列表 if (hwc.createWorkList(id, count) == NO_ERROR) &#123; HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i=0 ; cur!=end &amp;&amp; i&lt;count ; ++i, ++cur) &#123; const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); layer-&gt;setGeometry(hw, *cur); if (mDebugDisableHWC || mDebugRegion || mDaltonize || mHasColorMatrix) &#123; cur-&gt;setSkip(true); &#125; &#125; &#125; &#125; &#125; &#125; // set the per-frame data for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;= 0) &#123; bool freezeSurfacePresent = false; isfreezeSurfacePresent(freezeSurfacePresent, hw, id); const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers( hw-&gt;getVisibleLayersSortedByZ()); const size_t count = currentLayers.size(); HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i=0 ; cur!=end &amp;&amp; i&lt;count ; ++i, ++cur) &#123; /* * update the per-frame h/w composer data for each layer * and build the transparent region of the FB */ const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); //将Layer的mActiveBuffer设置到HWComposer中 layer-&gt;setPerFrameData(hw, *cur); setOrientationEventControl(freezeSurfacePresent,id); &#125; &#125; &#125; // If possible, attempt to use the cursor overlay on each display. for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;= 0) &#123; const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers( hw-&gt;getVisibleLayersSortedByZ()); const size_t count = currentLayers.size(); HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i=0 ; cur!=end &amp;&amp; i&lt;count ; ++i, ++cur) &#123; const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); if (layer-&gt;isPotentialCursor()) &#123; cur-&gt;setIsCursorLayerHint(); break; &#125; &#125; &#125; &#125; dumpDrawCycle(true); status_t err = hwc.prepare(); ALOGE_IF(err, \"HWComposer::prepare failed (%s)\", strerror(-err)); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); hw-&gt;prepareFrame(hwc); &#125;&#125;&#125; HWComposer中有一个类型为DisplayData结构的数组mDisplayData，它维护着每个显示设备的信息。DisplayData结构中有一个类型为hwc_display_contents_l字段list，这个字段又有一个hwc_layer_l类型的数组hwLayers，记录该显示设备所有需要输出的Layer信息。 setUpHWComposer函数调用HWComposer的createWorkList函数就是根据每种显示设备的Layer数量，创建和初始化hwc_display_contents_l对象和hwc_layer_l数组 创建完HWComposer中的列表后，接下来是对每个Layer对象调用它的setPerFrameData函数，参数是HWComposer和HWCLayerInterface。setPerFrameData函数将Layer对象的当前图像缓冲区mActiveBuffer设置到HWCLayerInterface对象对应的hwc_layer_l对象中。 HWComposer类中除了前面介绍的Gralloc还管理着Composer模块，这个模块实现了硬件的图像合成功能。setUpHWComposer函数接下来调用HWComposer类的prepare函数，而prepare函数会调用Composer模块的prepare接口。最后到各个厂家的实现hwc_prepare函数将每种HWComposer中的所有图层的类型都设置为HWC_FRAMEBUFFER就结束了。 五、合成所有层的图像 （doComposition()函数）doComposition函数是合成所有层的图像，代码如下： [SurfaceFlinger.cpp] 123456789101112131415161718192021void SurfaceFlinger::doComposition() &#123;ATRACE_CALL();const bool repaintEverything = android_atomic_and(0, &amp;mRepaintEverything);for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; const sp&lt;DisplayDevice&gt;&amp; hw(mDisplays[dpy]); if (hw-&gt;isDisplayOn()) &#123; // transform the dirty region into this screen's coordinate space const Region dirtyRegion(hw-&gt;getDirtyRegion(repaintEverything)); // repaint the framebuffer (if needed) doDisplayComposition(hw, dirtyRegion); hw-&gt;dirtyRegion.clear(); hw-&gt;flip(hw-&gt;swapRegion); hw-&gt;swapRegion.clear(); &#125; // inform the h/w that we're done compositing hw-&gt;compositionComplete();&#125;postFramebuffer();&#125; doComposition函数针对每种显示设备调用doDisplayComposition函数来合成，合成后调用postFramebuffer函数，我们先来看看doDisplayComposition函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void SurfaceFlinger::doDisplayComposition(const sp&lt;const DisplayDevice&gt;&amp; hw, const Region&amp; inDirtyRegion) &#123;// We only need to actually compose the display if:// 1) It is being handled by hardware composer, which may need this to// keep its virtual display state machine in sync, or// 2) There is work to be done (the dirty region isn't empty)bool isHwcDisplay = hw-&gt;getHwcDisplayId() &gt;= 0;if (!isHwcDisplay &amp;&amp; inDirtyRegion.isEmpty()) &#123; ALOGV(\"Skipping display composition\"); return;&#125;ALOGV(\"doDisplayComposition\");Region dirtyRegion(inDirtyRegion);// compute the invalid region//swapRegion设置为需要更新的区域 hw-&gt;swapRegion.orSelf(dirtyRegion);uint32_t flags = hw-&gt;getFlags();//获得显示设备支持的更新方式标志 if (flags &amp; DisplayDevice::SWAP_RECTANGLE) &#123; // we can redraw only what's dirty, but since SWAP_RECTANGLE only // takes a rectangle, we must make sure to update that whole // rectangle in that case dirtyRegion.set(hw-&gt;swapRegion.bounds());&#125; else &#123; if (flags &amp; DisplayDevice::PARTIAL_UPDATES) &#123;//支持部分更新 // We need to redraw the rectangle that will be updated // (pushed to the framebuffer). // This is needed because PARTIAL_UPDATES only takes one // rectangle instead of a region (see DisplayDevice::flip()) //将更新区域调整为整个窗口大小 dirtyRegion.set(hw-&gt;swapRegion.bounds()); &#125; else &#123; // we need to redraw everything (the whole screen) dirtyRegion.set(hw-&gt;bounds()); hw-&gt;swapRegion = dirtyRegion; &#125;&#125;//合成 if (!doComposeSurfaces(hw, dirtyRegion)) return;// update the swap region and clear the dirty regionhw-&gt;swapRegion.orSelf(dirtyRegion);//没有硬件composer的情况，输出图像// swap buffers (presentation)hw-&gt;swapBuffers(getHwComposer());&#125; doDisplayComposition函数根据显示设备支持的更新方式，重新设置需要更新区域的大小。 真正的合成工作是在doComposerSurfaces函数中完成，这个函数在layer的类型为HWC_FRAMEBUFFER,或者不支持硬件的composer的情况下，调用layer的draw函数来一层一层低合成最后的图像。 合成完后，doDisplayComposition函数调用了hw的swapBuffers函数，这个函数前面介绍过了，它将在系统不支持硬件的composer情况下调用eglSwapBuffers来输出图像到显示设备。 六、postFramebuffer()函数上一节的doComposition函数最后调用了postFramebuffer函数，代码如下： [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051void SurfaceFlinger::postFramebuffer()&#123;ATRACE_CALL();const nsecs_t now = systemTime();mDebugInSwapBuffers = now;HWComposer&amp; hwc(getHwComposer());if (hwc.initCheck() == NO_ERROR) &#123; if (!hwc.supportsFramebufferTarget()) &#123; // EGL spec says: // \"surface must be bound to the calling thread's current context, // for the current rendering API.\" getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext); &#125; hwc.commit();&#125;// make the default display current because the VirtualDisplayDevice code cannot// deal with dequeueBuffer() being called outside of the composition loop; however// the code below can call glFlush() which is allowed (and does in some case) call// dequeueBuffer().getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext);for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers(hw-&gt;getVisibleLayersSortedByZ()); hw-&gt;onSwapBuffersCompleted(hwc); const size_t count = currentLayers.size(); int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;=0 &amp;&amp; hwc.initCheck() == NO_ERROR) &#123; HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i = 0; cur != end &amp;&amp; i &lt; count; ++i, ++cur) &#123; currentLayers[i]-&gt;onLayerDisplayed(hw, &amp;*cur); &#125; &#125; else &#123; for (size_t i = 0; i &lt; count; i++) &#123; currentLayers[i]-&gt;onLayerDisplayed(hw, NULL); &#125; &#125;&#125;mLastSwapBufferTime = systemTime() - now;mDebugInSwapBuffers = 0;uint32_t flipCount = getDefaultDisplayDevice()-&gt;getPageFlipCount();if (flipCount % LOG_FRAME_STATS_PERIOD == 0) &#123; logFrameStats();&#125;&#125; postFramebuffer先判断系统是否支持composer，如果不支持，我们知道图像已经在doComposition函数时调用hw-&gt;swapBuffers输出了，就返回了。如果支持硬件composer，postFramebuffer函数将调用HWComposer的commit函数继续执行。 [HWComposer.cpp] 12345678910111213141516171819202122232425262728293031323334353637status_t HWComposer::commit() &#123;int err = NO_ERROR;if (mHwc) &#123; if (!hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123; // On version 1.0, the OpenGL ES target surface is communicated // by the (dpy, sur) fields and we are guaranteed to have only // a single display. mLists[0]-&gt;dpy = eglGetCurrentDisplay(); mLists[0]-&gt;sur = eglGetCurrentSurface(EGL_DRAW); &#125; for (size_t i=VIRTUAL_DISPLAY_ID_BASE; i&lt;mNumDisplays; i++) &#123; DisplayData&amp; disp(mDisplayData[i]); if (disp.outbufHandle) &#123; mLists[i]-&gt;outbuf = disp.outbufHandle; mLists[i]-&gt;outbufAcquireFenceFd = disp.outbufAcquireFence-&gt;dup(); &#125; &#125; err = mHwc-&gt;set(mHwc, mNumDisplays, mLists); for (size_t i=0 ; i&lt;mNumDisplays ; i++) &#123; DisplayData&amp; disp(mDisplayData[i]); disp.lastDisplayFence = disp.lastRetireFence; disp.lastRetireFence = Fence::NO_FENCE; if (disp.list) &#123; if (disp.list-&gt;retireFenceFd != -1) &#123; disp.lastRetireFence = new Fence(disp.list-&gt;retireFenceFd); disp.list-&gt;retireFenceFd = -1; &#125; disp.list-&gt;flags &amp;= ~HWC_GEOMETRY_CHANGED; &#125; &#125;&#125;return (status_t)err;&#125; 合成效果图： （六）、Android SurfaceFlinger - VSync工作原理一、VSYNC 总体概念6.1.1、VSYNC 概念VSYNC（Vertical Synchronization）是一个相当古老的概念，对于游戏玩家，它有一个更加大名鼎鼎的中文名字—垂直同步。 “垂直同步(vsync)”指的是显卡的输出帧数和屏幕的垂直刷新率相同，这完全是一个CRT显示器上的概念。其实无论是VSYNC还是垂直同步这个名字，因为LCD根本就没有垂直扫描的这种东西，因此这个名字本身已经没有意义。但是基于历史的原因，这个名称在图形图像领域被沿袭下来。 在当下，垂直同步的含义我们可以理解为，使得显卡生成帧的速度和屏幕刷新的速度的保持一致。举例来说，如果屏幕的刷新率为60Hz，那么生成帧的速度就应该被固定在1/60 s。 6.1.2、Android VSYNC – 黄油计划谷歌为解决Android系统流畅性问题。在4.1版本引入了一个重大的改进–Project Butter黄油计划。 Project Butter对Android Display系统进行了重构，引入了三个核心元素，即VSYNC、Triple Buffer和Choreographer。 VSYNC最重要的作用是防止出现画面撕裂（screentearing）。所谓画面撕裂，就是指一个画面上出现了两帧画面的内容，如下图。 为什么会出现这种情况呢？这种情况一般是因为显卡输出帧的速度高于显示器的刷新速度，导致显示器并不能及时处理输出的帧，而最终出现了多个帧的画面都留在了显示器上的问题。这也就是我们所说的画面撕裂。 这个图中有三个元素，Display是显示屏幕，GPU和CPU负责渲染帧数据，每个帧以方框表示，并以数字进行编号，如0、1、2等等。VSync用于指导双缓冲区的交换。 以时间的顺序来看下将会发生的异常： Step1. Display显示第0帧数据，此时CPU和GPU渲染第1帧画面，而且赶在Display显示下一帧前完成 Step2. 因为渲染及时，Display在第0帧显示完成后，也就是第1个VSync后，正常显示第1帧 Step3. 由于某些原因，比如CPU资源被占用，系统没有及时地开始处理第2帧，直到第2个VSync快来前才开始处理 Step4. 第2个VSync来时，由于第2帧数据还没有准备就绪，显示的还是第1帧。这种情况被Android开发组命名为”Jank”。 Step5. 当第2帧数据准备完成后，它并不会马上被显示，而是要等待下一个VSync。 所以总的来说，就是屏幕平白无故地多显示了一次第1帧。原因大家应该都看到了，就是CPU没有及时地开始着手处理第2帧的渲染工作，以致”延误军机”。 其实总结上面的这个情况之所以发生，首先的原因就在于第二帧没有及时的绘制（当然即使第二帧及时绘制，也依然可能出现Jank，这就是同时引入三重缓冲的作用。我们将在三重缓冲一节中再讲解这种情况）。那么如何使得第二帧即使被绘制呢？ 这就是我们在Graphic系统中引入VSYNC的原因，考虑下面这张图： 如上图所示，一旦VSync出现后，立刻就开始执行下一帧的绘制工作。这样就可以大大降低Jank出现的概率。另外，VSYNC引入后，要求绘制也只能在收到VSYNC消息之后才能进行，因此，也就杜绝了另外一种极端情况的出现—CPU（GPU）一直不停的进行绘制，帧的生成速度高于屏幕的刷新速度，导致生成的帧不能被显示，只能丢弃，这样就出现了丢帧的情况—引入VSYNC后，绘制的速度就和屏幕刷新的速度保持一致了。 二、VSync信号产生那么VSYNC信号是如何生成的呢？ Android系统中VSYNC信号分为两种，一种是硬件生成的信号，一种是软件模拟的信号。 硬件信号是由HardwareComposer提供的，HWC封装了相关的HAL层，如果硬件厂商提供的HAL层实现能定时产生VSYNC中断，则直接使用硬件的VSYNC中断，否则HardwareComposer内部会通过VSyncThread来模拟产生VSYNC中断（其实现很简单，就是sleep固定时间，然后唤醒）。 SurfaceFlinger的启动过程中inti()会创建一个HWComposer对象。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071HWComposer::HWComposer( const sp&lt;SurfaceFlinger&gt;&amp; flinger, EventHandler&amp; handler) : mFlinger(flinger), mFbDev(0), mHwc(0), mNumDisplays(1), mCBContext(new cb_context), mEventHandler(handler), mDebugForceFakeVSync(false) &#123; ... //首先是一些和VSYNC有关的信息的初始化 //因为在硬件支持的情况下，VSYNC的功能就是由HWC提供的 for (size_t i=0 ; i&lt;HWC_NUM_PHYSICAL_DISPLAY_TYPES ; i++) &#123; mLastHwVSync[i] = 0; mVSyncCounts[i] = 0; &#125; //根据配置来看是否需要模拟VSYNC消息 char value[PROPERTY_VALUE_MAX]; property_get(\"debug.sf.no_hw_vsync\", value, \"0\"); mDebugForceFakeVSync = atoi(value); ... // don't need a vsync thread if we have a hardware composer needVSyncThread = false; // always turn vsync off when we start,只是暂时关闭信号，后面会再开启 eventControl(HWC_DISPLAY_PRIMARY, HWC_EVENT_VSYNC, 0); //显然，如果需要模拟VSync信号的话，我们需要线程来做这个工作 if (needVSyncThread) &#123; // we don't have VSYNC support, we need to fake it //VSyncThread类的实现很简单，无非就是一个计时器而已，定时发送消息而已 //TODO VSYNC专题 mVSyncThread = new VSyncThread(*this); &#125; ... &#125; HWComposer::HWComposer( const sp&lt;SurfaceFlinger&gt;&amp; flinger, EventHandler&amp; handler) : mFlinger(flinger), mFbDev(0), mHwc(0), mNumDisplays(1), mCBContext(new cb_context), mEventHandler(handler), mDebugForceFakeVSync(false) &#123; ... //首先是一些和VSYNC有关的信息的初始化 //因为在硬件支持的情况下，VSYNC的功能就是由HWC提供的 for (size_t i=0 ; i&lt;HWC_NUM_PHYSICAL_DISPLAY_TYPES ; i++) &#123; mLastHwVSync[i] = 0; mVSyncCounts[i] = 0; &#125; //根据配置来看是否需要模拟VSYNC消息 char value[PROPERTY_VALUE_MAX]; property_get(\"debug.sf.no_hw_vsync\", value, \"0\"); mDebugForceFakeVSync = atoi(value); ... // don't need a vsync thread if we have a hardware composer needVSyncThread = false; // always turn vsync off when we start,只是暂时关闭信号，后面会再开启 eventControl(HWC_DISPLAY_PRIMARY, HWC_EVENT_VSYNC, 0); //显然，如果需要模拟VSync信号的话，我们需要线程来做这个工作 if (needVSyncThread) &#123; // we don't have VSYNC support, we need to fake it //VSyncThread类的实现很简单，无非就是一个计时器而已，定时发送消息而已 //TODO VSYNC专题 mVSyncThread = new VSyncThread(*this); &#125; ... &#125; 我们来看下上面这段代码。 首先mDebugForceFakeVSync是为了调制，可以通过这个变量设置强制使用软件VSYNC模拟。 然后针对不同的屏幕，初始化了他们的mLastHwVSync和mVSyncCounts值。 如果硬件支持，那么就把needVSyncThread设置为false，表示不需要软件模拟。 接着通过eventControl来暂时的关闭了VSYNC信号，这一点将在下面讲解eventControl时一并讲解。 最后，如果需要软件模拟Vsync信号的话，那么我们将通过一个单独的VSyncThread线程来做这个工作(fake VSYNC是这个线程唯一的作用)。我们来看下这个线程。 软件模拟 1234567891011121314151617181920212223242526272829303132333435bool HWComposer::VSyncThread::threadLoop() &#123; const nsecs_t period = mRefreshPeriod; //当前的时间 const nsecs_t now = systemTime(CLOCK_MONOTONIC); //下一次VSYNC到来的时间 nsecs_t next_vsync = mNextFakeVSync; //为了等待下个时间到来应该休眠的时间 nsecs_t sleep = next_vsync - now; //错过了VSYNC的时间 if (sleep &lt; 0) &#123; // we missed, find where the next vsync should be //重新计算下应该休息的时间 sleep = (period - ((now - next_vsync) % period)); //更新下次VSYNC的时间 next_vsync = now + sleep; &#125; //更新下下次VSYNC的时间 mNextFakeVSync = next_vsync + period; struct timespec spec; spec.tv_sec = next_vsync / 1000000000; spec.tv_nsec = next_vsync % 1000000000; int err; do &#123; //纳秒精度级的休眠 err = clock_nanosleep(CLOCK_MONOTONIC, TIMER_ABSTIME, &amp;spec, NULL); &#125; while (err&lt;0 &amp;&amp; errno == EINTR); if (err == 0) &#123; //休眠之后，到了该发生VSYNC的时间了 mHwc.mEventHandler.onVSyncReceived(0, next_vsync); &#125; return true; &#125; 这个函数其实很简单，无非就是一个简单的时间计算，计算过程我已经写在了程序注释里面。总之到了应该发生VSYNC信号的时候，就调用了mHwc.mEventHandler.onVSyncReceived(0, next_vsync)函数来通知VSYNC的到来。 我们注意到mEventHandler实际上是在HWC创建时被传入的，我们来看下HWC创建时的代码. 123456mHwc = new HWComposer(this, *static_cast&lt;HWComposer::EventHandler *&gt;(this)); class SurfaceFlinger : public BnSurfaceComposer, private IBinder::DeathRecipient, private HWComposer::EventHandler 可以看到这个mEventHandler实际上就是SurfaceFlinger。也就是说，VSYNC信号到来时，SurfaceFlinger的onVSyncReceived函数处理了这个消息。 这里我们暂时先不展开SurfaceFlinger内的逻辑处理，等我们下面分析完硬件实现后，一并进行分析 硬件实现 上面我们讲了软件如何模拟一个VSYNC信号并通知SurfaceFlinger,那么硬件又是如何实现这一点的呢？ 我们再一次回到HWC的创建过程中来： 123456789101112131415if (mHwc) &#123; ALOGE(\"Lee Using %s version %u.%u\", HWC_HARDWARE_COMPOSER, (hwcApiVersion(mHwc) &gt;&gt; 24) &amp; 0xff, (hwcApiVersion(mHwc) &gt;&gt; 16) &amp; 0xff); if (mHwc-&gt;registerProcs) &#123; mCBContext-&gt;hwc = this; mCBContext-&gt;procs.invalidate = &amp;hook_invalidate; mCBContext-&gt;procs.vsync = &amp;hook_vsync; if (hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) mCBContext-&gt;procs.hotplug = &amp;hook_hotplug; else mCBContext-&gt;procs.hotplug = NULL; memset(mCBContext-&gt;procs.zero, 0, sizeof(mCBContext-&gt;procs.zero)); mHwc-&gt;registerProcs(mHwc, &amp;mCBContext-&gt;procs); &#125; 来看下上面这段实现。 当HWC有vsync信号生成时，硬件模块会通过procs.vsync来通知软件部分，因此也就是调用了hook_vsync函数。 123456789101112131415void HWComposer::hook_vsync(const struct hwc_procs* procs, int disp, int64_t timestamp) &#123; cb_context* ctx = reinterpret_cast&lt;cb_context*&gt;( const_cast&lt;hwc_procs_t*&gt;(procs)); ctx-&gt;hwc-&gt;vsync(disp, timestamp); &#125; void HWComposer::vsync(int disp, int64_t timestamp) &#123; //只有真实的硬件设备才会产生VSYNC if (uint32_t(disp) &lt; HWC_NUM_PHYSICAL_DISPLAY_TYPES) &#123; &#123; mLastHwVSync[disp] = timestamp; &#125; mEventHandler.onVSyncReceived(disp, timestamp); &#125; 我们发现最后殊途同归，硬件信号最终也通过onVSyncReceived函数通知到了SurfaceFlinger了。下面我们来分析下SurfaceFlinger的处理过程。 三、Surfaceflinger对VSYNC消息的处理先来直接看下Surfaceflinger的onVSyncReceived函数： 12345678910111213141516void SurfaceFlinger::onVSyncReceived(int32_t type, nsecs_t timestamp) &#123; bool needsHwVsync = false; &#123; // Scope for the lock Mutex::Autolock _l(mHWVsyncLock); if (type == 0 &amp;&amp; mPrimaryHWVsyncEnabled) &#123; needsHwVsync = mPrimaryDispSync.addResyncSample(timestamp); &#125; &#125; if (needsHwVsync) &#123; enableHardwareVsync(); &#125; else &#123; disableHardwareVsync(false); &#125;&#125; mPrimaryDispSync是什么？addResyncSample有什么作用？ 要回答这三个问题，我们首先还是得回到SurfaceFlinger的init函数中来。 6.3.1、Surfaceflinger.init()先看一下总体flow： 123456789101112131415161718192021void SurfaceFlinger::init() &#123; ALOGI( &quot;SurfaceFlinger&apos;s main thread ready to run. &quot; &quot;Initializing graphics H/W...&quot;); &#123; ...... // start the EventThread sp&lt;VSyncSource&gt; vsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, vsyncPhaseOffsetNs, true, &quot;app&quot;); mEventThread = new EventThread(vsyncSrc, *this); sp&lt;VSyncSource&gt; sfVsyncSrc = new DispSyncSource(&amp;mPrimaryDispSync, sfVsyncPhaseOffsetNs, true, &quot;sf&quot;); mSFEventThread = new EventThread(sfVsyncSrc, *this); mEventQueue.setEventThread(mSFEventThread); ...... &#125; ...... mEventControlThread = new EventControlThread(this); mEventControlThread-&gt;run(&quot;EventControl&quot;, PRIORITY_URGENT_DISPLAY); ......&#125; 2个EventThread对象分别是mEventThread，给app用，mSFEventThread，给surfaceflinger自己用。 下面给出这4个Thread关系图。 这两个DispSyncSource就是KK引入的重大变化。Android 4.4(KitKat)引入了VSync的虚拟化，即把硬件的VSync信号先同步到一个本地VSync模型中，再从中一分为二，引出两条VSync时间与之有固定偏移的线程。示意图如下： Google这样修改的目的又是什么呢？ =在当前三重缓冲区的架构下，即对于一帧内容，先等App UI画完了，SurfaceFlinger再出场对其进行合并渲染后放入framebuffer，最后整到屏幕上。而现有的VSync模型是让大家一起开始干活。 这个架构其实会产生一个问题，因为App和SurfaceFlinger被同时唤醒，导致他们二者总是一起工作，必然导致VSync来临的时刻，这二者之间产生了CPU资源的抢占。因此，谷歌给这两个工作都加上一个小小的延迟，让这两个工作并不是同时被唤醒，这样大家就可以错开使用资源的高峰期，提高工作的效率。 这两个延迟，其实就分别对应上面代码中的vsyncSrc（绘制延迟）和sfVsyncSrc（合成延迟）。 在创建了两个DispSyncSource变量后，我们使用它们来初始化了两个EventThread。下面我们来详细看下EventThread的创建流程： 123456789101112131415161718192021222324EventThread::EventThread(const sp&lt;VSyncSource&gt;&amp; src, SurfaceFlinger&amp; flinger) : mVSyncSource(src), mFlinger(flinger), mUseSoftwareVSync(false), mVsyncEnabled(false), mDebugVsyncEnabled(false), mVsyncHintSent(false) &#123; for (int32_t i=0 ; i&lt;DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES ; i++) &#123; mVSyncEvent[i].header.type = DisplayEventReceiver::DISPLAY_EVENT_VSYNC; mVSyncEvent[i].header.id = 0; mVSyncEvent[i].header.timestamp = 0; mVSyncEvent[i].vsync.count = 0; &#125; struct sigevent se; se.sigev_notify = SIGEV_THREAD; se.sigev_value.sival_ptr = this; se.sigev_notify_function = vsyncOffCallback; se.sigev_notify_attributes = NULL; timer_create(CLOCK_MONOTONIC, &amp;se, &amp;mTimerId);&#125;void EventThread::onFirstRef() &#123; run(\"EventThread\", PRIORITY_URGENT_DISPLAY + PRIORITY_MORE_FAVORABLE);&#125; EventThread的构造函数很简单。重点是它的onFirstRef函数启动了一个EventThread线程，于是下面的代码才是重点： 123456789101112131415bool EventThread::threadLoop() &#123; DisplayEventReceiver::Event event; Vector&lt; sp&lt;EventThread::Connection&gt; &gt; signalConnections; signalConnections = waitForEvent(&amp;event); // dispatch events to listeners... const size_t count = signalConnections.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Connection&gt;&amp; conn(signalConnections[i]); // now see if we still need to report this event status_t err = conn-&gt;postEvent(event); ...... &#125; return true;&#125; 上面的函数本身并不复杂，其中调用了一个waitForEvent的函数。这个函数相当之长，为了防止代码展开太多，我们这里暂时不再详细分析这个函数。我们目前只需要知道这个函数的最重要的作用是等待Event的到来，并且查找对event感兴趣的监听者，而在没有event到来时，线程处于休眠状态，等待event的唤醒（我们将下一篇VSYNC的接收和处理中展开分析这个函数）。 这样，EventThread线程就运行起来，处在等待被event唤醒的状态下。 MessageQueue和EventThread建立连接 简单说明完EventThread之后，我们再次回到SurfaceFlinger的init过程中来。回到init()函数代码中来： 将SurfaceFlinger的MessageQueue真正和我们刚才创建的EventThread建立起了连接，这样SurfaceFlinger才能真正接收到来自HWC的VSYNC信号。 我们来看下这段代码： 12345678void MessageQueue::setEventThread(const sp&lt;EventThread&gt;&amp; eventThread) &#123; mEventThread = eventThread; mEvents = eventThread-&gt;createEventConnection(); mEventTube = mEvents-&gt;getDataChannel(); mLooper-&gt;addFd(mEventTube-&gt;getFd(), 0, ALOOPER_EVENT_INPUT, MessageQueue::cb_eventReceiver, this); &#125; 这里代码逻辑其实很简单，就是创建了一个到EventThread的连接，得到了发送VSYNC事件通知的BitTube，然后监控这个BitTube中的套接字，并且指定了收到通知后的回调函数，MessageQueue::cb_eventReceiver。这样一旦VSync信号传来，函数cb_eventReceiver将被调用。 向Eventhread注册一个事件的监听者—-createEventConnection 在SurfaceFlinger的init函数中，我们调用了mEventQueue.setEventThread(mSFEventThread)函数，我们在前面一章中已经提到过，这个函数将SurfaceFlinger的MessageQueue真正和我们刚才创建的EventThread建立起了连接。我们来看下这段代码： 12345678910111213141516sp&lt;EventThread::Connection&gt; EventThread::createEventConnection() const &#123; return new Connection(const_cast&lt;EventThread*&gt;(this)); &#125; EventThread::Connection::Connection( const sp&lt;EventThread&gt;&amp; eventThread) : count(-1), mEventThread(eventThread), mChannel(new BitTube()) &#123; &#125; void EventThread::Connection::onFirstRef() &#123; mEventThread-&gt;registerDisplayEventConnection(this); &#125; status_t EventThread::registerDisplayEventConnection( const sp&lt;EventThread::Connection&gt;&amp; connection) &#123; mDisplayEventConnections.add(connection); mCondition.broadcast(); &#125; 这个函数会导致一个Connection类的创建，而这个connection类会被保存在EventThread下的一个容器内。 通过createEventConnection这样一个简单的方法，我们其实就注册了一个事件的监听者，得到了发送VSYNC事件通知的BitTube，然后监控这个BitTube中的套接字，并且指定了收到通知后的回调函数，MessageQueue::cb_eventReceiver。这样一旦VSync信号传来，函数cb_eventReceiver将被调用。 6.3.2、VSync信号的处理我们在前面一章也提到了无论是软件方式还是硬件方式，SurfaceFlinger收到VSync信号后，处理函数都是onVSyncReceived函数： VSync消息处理—-addResyncSample 12345678bool DispSync::addResyncSample(nsecs_t timestamp) &#123; size_t idx = (mFirstResyncSample + mNumResyncSamples) % MAX_RESYNC_SAMPLES; mResyncSamples[idx] = timestamp; ...... updateModelLocked(); .......&#125; 粗略浏览下这个函数，发现前半部分其实在做一些简单的计数统计，重点实现显然是updateModelLocked函数： 123456789101112131415161718192021222324252627282930void DispSync::updateModelLocked() &#123; if (mNumResyncSamples &gt;= MIN_RESYNC_SAMPLES_FOR_UPDATE) &#123; nsecs_t durationSum = 0; for (size_t i = 1; i &lt; mNumResyncSamples; i++) &#123; size_t idx = (mFirstResyncSample + i) % MAX_RESYNC_SAMPLES; size_t prev = (idx + MAX_RESYNC_SAMPLES - 1) % MAX_RESYNC_SAMPLES; durationSum += mResyncSamples[idx] - mResyncSamples[prev]; &#125; mPeriod = durationSum / (mNumResyncSamples - 1); double sampleAvgX = 0; double sampleAvgY = 0; double scale = 2.0 * M_PI / double(mPeriod); for (size_t i = 0; i &lt; mNumResyncSamples; i++) &#123; size_t idx = (mFirstResyncSample + i) % MAX_RESYNC_SAMPLES; nsecs_t sample = mResyncSamples[idx]; double samplePhase = double(sample % mPeriod) * scale; sampleAvgX += cos(samplePhase); sampleAvgY += sin(samplePhase); &#125; sampleAvgX /= double(mNumResyncSamples); sampleAvgY /= double(mNumResyncSamples); mPhase = nsecs_t(atan2(sampleAvgY, sampleAvgX) / scale); ...... mThread-&gt;updateModel(mPeriod, mPhase); &#125; &#125; 不得不说，前面大段的数学计算让人有些困惑，我们暂且跳过，先分析下主线流程，也就是mThread-&gt;updateModel(mPeriod, mPhase)这个调用： DispSyncThread.updateModel的用途 123456void updateModel(nsecs_t period, nsecs_t phase) &#123; Mutex::Autolock lock(mMutex); mPeriod = period; mPhase = phase; mCond.signal(); &#125; updateModel是DispSyncThread类的函数，这个函数本身代码很短，其实它的主要作用是mCond.signal发送一个信号给等待中的线程。那么究竟是谁在等待这个条件呢？ 其实等待这个条件的正是DispSyncThread的循环函数： 12345678910111213141516171819202122232425262728virtual bool threadLoop() &#123; status_t err; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); nsecs_t nextEventTime = 0; while (true) &#123; Vector&lt;CallbackInvocation&gt; callbackInvocations; nsecs_t targetTime = 0; &#123; // Scope for lock Mutex::Autolock lock(mMutex); ...... if (mPeriod == 0) &#123; err = mCond.wait(mMutex); ...... &#125; nextEventTime = computeNextEventTimeLocked(now); targetTime = nextEventTime; ...... &#125; now = systemTime(SYSTEM_TIME_MONOTONIC); ...... callbackInvocations = gatherCallbackInvocationsLocked(now); &#125; if (callbackInvocations.size() &gt; 0) &#123; fireCallbackInvocations(callbackInvocations); &#125; &#125; return false; &#125; 大量的时间相关的计算和状态的转变我们不再深入研究，我们来看下这个线程被通知唤醒之后做的两个主要的函数的处理，gatherCallbackInvocationsLocked()和fireCallbackInvocations()。 gatherCallbackInvocationsLocked()的代码其实很简单： 12345678910111213141516Vector&lt;CallbackInvocation&gt; gatherCallbackInvocationsLocked(nsecs_t now) &#123; Vector&lt;CallbackInvocation&gt; callbackInvocations; nsecs_t ref = now - mPeriod; for (size_t i = 0; i &lt; mEventListeners.size(); i++) &#123; nsecs_t t = computeListenerNextEventTimeLocked(mEventListeners[i], ref); if (t &lt; now) &#123; CallbackInvocation ci; ci.mCallback = mEventListeners[i].mCallback; ci.mEventTime = t; callbackInvocations.push(ci); mEventListeners.editItemAt(i).mLastEventTime = t; &#125; &#125; return callbackInvocations; &#125; 其实就是从mEventListeners取出之前注册的事件监听者，放入callbackInvocations中，等待后面的调用。至于监听者从何处而来？在waitforevent时通过enableVSyncLocked注册的。 继续看下fireCallbackInvocations()函数： 12345void fireCallbackInvocations(const Vector&lt;CallbackInvocation&gt;&amp; callbacks) &#123; for (size_t i = 0; i &lt; callbacks.size(); i++) &#123; callbacks[i].mCallback-&gt;onDispSyncEvent(callbacks[i].mEventTime); &#125; &#125;` 我们目前只分析主线的走向,接下来调用了DispSyncSource的onDispSyncEvent在： 1234567891011121314151617virtual void onDispSyncEvent(nsecs_t when) &#123; sp&lt;VSyncSource::Callback&gt; callback; &#123; callback = mCallback; &#125; if (callback != NULL) &#123; callback-&gt;onVSyncEvent(when); &#125; &#125; void EventThread::onVSyncEvent(nsecs_t timestamp) &#123; Mutex::Autolock _l(mLock); mVSyncEvent[0].header.type = DisplayEventReceiver::DISPLAY_EVENT_VSYNC; mVSyncEvent[0].header.id = 0; mVSyncEvent[0].header.timestamp = timestamp; mVSyncEvent[0].vsync.count++; mCondition.broadcast(); &#125; 我们看到这里mCondition.broadcas发出了命令，那么EventThread中waitforEvent的等待就会被唤醒。而一旦唤醒，我们就回到了EventThread的loop中，我们来看下代码： 123456789101112131415bool EventThread::threadLoop() &#123; DisplayEventReceiver::Event event; Vector&lt; sp&lt;EventThread::Connection&gt; &gt; signalConnections; signalConnections = waitForEvent(&amp;event); // dispatch events to listeners... const size_t count = signalConnections.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Connection&gt;&amp; conn(signalConnections[i]); // now see if we still need to report this event status_t err = conn-&gt;postEvent(event); ...... &#125; return true; &#125; 这里主要就是通过conn-&gt;postEvent来分发事件： 12345678910status_t EventThread::Connection::postEvent( const DisplayEventReceiver::Event&amp; event) &#123; ssize_t size = DisplayEventReceiver::sendEvents(mChannel, &amp;event, 1); return size &lt; 0 ? status_t(size) : status_t(NO_ERROR); &#125; ssize_t DisplayEventReceiver::sendEvents(const sp&lt;BitTube&gt;&amp; dataChannel, Event const* events, size_t count) &#123; return BitTube::sendObjects(dataChannel, events, count); &#125; 其实看到这里的BitTube我们就明白了，在本文开始时候我们提到： 通过createEventConnection这样一个简单的方法，我们其实就注册了一个事件的监听者，得到了发送VSYNC事件通知的BitTube，然后监控这个BitTube中的套接字，并且指定了收到通知后的回调函数，MessageQueue::cb_eventReceiver。这样一旦VSync信号传来，函数cb_eventReceiver将被调用。 所以我们这里可以来看看MessageQueue::cb_eventReceiver函数了： 123456789101112131415161718int MessageQueue::cb_eventReceiver(int fd, int events, void* data) &#123; MessageQueue* queue = reinterpret_cast&lt;MessageQueue *&gt;(data); return queue-&gt;eventReceiver(fd, events); &#125; int MessageQueue::eventReceiver(int fd, int events) &#123; ssize_t n; DisplayEventReceiver::Event buffer[8]; while ((n = DisplayEventReceiver::getEvents(mEventTube, buffer, 8)) &gt; 0) &#123; for (int i=0 ; i&lt;n ; i++) &#123; if (buffer[i].header.type == DisplayEventReceiver::DISPLAY_EVENT_VSYNC) &#123; mHandler-&gt;dispatchInvalidate(); break; &#125; &#125; &#125; return 1; &#125; 我们看到收到消息之后MessageQueue对消息进行了分发，我们目前走的是dispatchInvalidate()。 123456789101112131415161718192021222324252627282930313233343536373839void MessageQueue::Handler::dispatchInvalidate() &#123; if ((android_atomic_or(eventMaskInvalidate, &amp;mEventMask) &amp; eventMaskInvalidate) == 0) &#123; mQueue.mLooper-&gt;sendMessage(this, Message(MessageQueue::INVALIDATE)); &#125; &#125; void MessageQueue::Handler::handleMessage(const Message&amp; message) &#123; switch (message.what) &#123; case INVALIDATE: android_atomic_and(~eventMaskInvalidate, &amp;mEventMask); mQueue.mFlinger-&gt;onMessageReceived(message.what); break; case REFRESH: android_atomic_and(~eventMaskRefresh, &amp;mEventMask); mQueue.mFlinger-&gt;onMessageReceived(message.what); break; case TRANSACTION: android_atomic_and(~eventMaskTransaction, &amp;mEventMask); mQueue.mFlinger-&gt;onMessageReceived(message.what); break; &#125; &#125; void SurfaceFlinger::onMessageReceived(int32_t what) &#123; ATRACE_CALL(); switch (what) &#123; case MessageQueue::TRANSACTION: handleMessageTransaction(); break; case MessageQueue::INVALIDATE: handleMessageTransaction(); handleMessageInvalidate(); signalRefresh(); break; case MessageQueue::REFRESH: handleMessageRefresh(); break; &#125; &#125; 到了这里，就进入了SurfaceFlinger的处理流程，我们看到对于INVALIDATE的消息，实际上系统在处理过程中实际还是会发送一个Refresh消息。 6.4、App向Eventhread注册一个事件的监听者–createEventConnection()在ViewRootImpl的构造函数中会实例化Choreographer对象 1234public ViewRootImpl(Context context, Display display) &#123; . . . . . mChoreographer = Choreographer.getInstance(); &#125; 在mChoreographer 的构造函数中实例化FrameDisplayEventReceiver对象 1234private Choreographer(Looper looper) &#123; . . . . . . mDisplayEventReceiver = USE_VSYNC ? new FrameDisplayEventReceiver(looper) : null; &#125; 在FrameDisplayEventReceiver的父类构造函数中会调用到，android_view_DisplayEventReceiver.cpp中的nativeInit方法,在nativeInit方法中有如下过程 1234567static jlong nativeInit(JNIEnv* env, jclass clazz, jobject receiverWeak, jobject messageQueueObj) &#123; . . . . . . sp&lt;NativeDisplayEventReceiver&gt; receiver = new NativeDisplayEventReceiver(env, receiverWeak, messageQueue); status_t status = receiver-&gt;initialize(); . . . . . . 创建NativeDisplayEventReceiver类 类型指针 在NativeDisplayEventReceiver的构造函数中会调用DisplayEventReceiver类的无参构造函数实例化成员mReceiver； 123456789DisplayEventReceiver::DisplayEventReceiver() &#123; sp&lt;ISurfaceComposer&gt; sf(ComposerService::getComposerService()); if (sf != NULL) &#123; mEventConnection = sf-&gt;createDisplayEventConnection(); if (mEventConnection != NULL) &#123; mDataChannel = mEventConnection-&gt;getDataChannel(); &#125; &#125;&#125; 在这段代码中获取Surfaceflinger服务的代理对象，然后通过Binder IPC创建BpDisplayEventConnection对象 该函数经由BnSurfaceComposer.onTransact函数辗转调用到SurfaceFlinger.createDisplayEventConnection函数： 123sp&lt;IDisplayEventConnection&gt; SurfaceFlinger::createDisplayEventConnection() &#123; return mEventThread-&gt;createEventConnection();&#125; 出现了熟悉的面孔mEventThread，该对象是一个EventThread对象，该对象在SurfaceFlinger.init函数里面创建，但是创建运行以后，貌似还没有进行任何的动作，这里调用createEventConnection函数： 123sp&lt;EventThread::Connection&gt; EventThread::createEventConnection() const &#123; return new Connection(const_cast&lt;EventThread*&gt;(this));&#125; 然后mEventConnection-&gt;getDataChannel()方法再次通过Binder IPC创建 BitTube对象mDataChannel ，在Binder IPC创建mDataChannel 过程中会从服务端EventThread::Connection::Connection中（在EventThread类中定义）接收一个socketpair创建的FIFO文件描述符； EventThread::Connection::Connection创建描述符的代码： Connection构造函数调用BitTube的无参构造函数，在BitTube的构造函数中调用init函数； 123456789101112131415161718void BitTube::init(size_t rcvbuf, size_t sndbuf) &#123; int sockets[2]; if (socketpair(AF_UNIX, SOCK_SEQPACKET, 0, sockets) == 0) &#123; size_t size = DEFAULT_SOCKET_BUFFER_SIZE; setsockopt(sockets[0], SOL_SOCKET, SO_RCVBUF, &amp;rcvbuf, sizeof(rcvbuf)); setsockopt(sockets[1], SOL_SOCKET, SO_SNDBUF, &amp;sndbuf, sizeof(sndbuf)); // sine we don't use the \"return channel\", we keep it small... setsockopt(sockets[0], SOL_SOCKET, SO_SNDBUF, &amp;size, sizeof(size)); setsockopt(sockets[1], SOL_SOCKET, SO_RCVBUF, &amp;size, sizeof(size)); fcntl(sockets[0], F_SETFL, O_NONBLOCK); fcntl(sockets[1], F_SETFL, O_NONBLOCK); mReceiveFd = sockets[0]; mSendFd = sockets[1]; &#125; else &#123; mReceiveFd = -errno; ALOGE(\"BitTube: pipe creation failed (%s)\", strerror(-mReceiveFd)); &#125;&#125; 调用到NativeDisplayEventReceiver类的父类DisplayEventDispatcher中的initialize()方法， 将BpDisplayEventConnection对象获取到的mDataChannel （BitTube类型）中的文件描述符添加到UI主线程Looper的epoll中， 当文件描述符中被写入数据时，该epoll_wait会被唤醒； 直接看代码： 123456789101112status_t NativeDisplayEventReceiver::initialize() &#123; status_t result = mReceiver.initCheck(); if (result) &#123; ALOGW(\"Failed to initialize display event receiver, status=%d\", result); return result; &#125; int rc = mMessageQueue-&gt;getLooper()-&gt;addFd(mReceiver.getFd(), 0, Looper::EVENT_INPUT, this, NULL); if (rc &lt; 0) &#123; return UNKNOWN_ERROR; &#125; return OK;&#125; 这里的主要代码是mMessageQueue-&gt;getLooper()-&gt;addFd()这一行，其中的参数mReceiver.getFd()返回的是在创建NativeDisplayEventReceiver时从SurfaceFlinger服务端接收回来的socket接收端描述符，前面分析到 mMessageQueue是与当前应用线程关联的java层的MessageQueue对应的native层的MessageQueue对象，下面看一下Looper.addFd这个函数，上面调用时传进来的this指针对应的是一个NativeDisplayEventReceiver对象，该类继承了LooperCallback： 1234567891011121314151617181920212223242526272829303132333435363738int Looper::addFd(int fd, int ident, int events, Looper_callbackFunc callback, void* data) &#123; return addFd(fd, ident, events, callback ? new SimpleLooperCallback(callback) : NULL, data);&#125;int Looper::addFd(int fd, int ident, int events, const sp&lt;LooperCallback&gt;&amp; callback, void* data) &#123; int epollEvents = 0; if (events &amp; EVENT_INPUT) epollEvents |= EPOLLIN; if (events &amp; EVENT_OUTPUT) epollEvents |= EPOLLOUT; &#123; // acquire lock AutoMutex _l(mLock); Request request; request.fd = fd; request.ident = ident; request.callback = callback; request.data = data; struct epoll_event eventItem; memset(&amp; eventItem, 0, sizeof(epoll_event)); // zero out unused members of data field union eventItem.events = epollEvents; eventItem.data.fd = fd; ssize_t requestIndex = mRequests.indexOfKey(fd); if (requestIndex &lt; 0) &#123; int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, fd, &amp; eventItem); if (epollResult &lt; 0) &#123; &#125; mRequests.add(fd, request); &#125; else &#123; int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_MOD, fd, &amp; eventItem); if (epollResult &lt; 0) &#123; return -1; &#125; mRequests.replaceValueAt(requestIndex, request); &#125; &#125; // release lock return 1;&#125; 首先将上面传进来的NativeDisplayEventReceiver对象封装成一个SimpleLooperCallback对象，调用下面的addFd函数的时候主要步骤如下： （1）创建一个struct epoll_event结构体对象，将对应的内存全部用清0，并作对应的初始化； （2）查询通过addFd方法已经添加到epoll中监听的文件描述符； （3）查询不到的话，则调用epoll_ctl方法设置EPOLL_CTL_ADD属性将对应的文件描述符添加到epoll监听的描述符中； （4）根据前面addFd传入的参数EVENT_INPUT，说明当前应用线程的native层的Looper对象中的epoll机制已经开始监听来自于SurfaceFlinger服务端socket端的写入事件。 6.5、App请求Vsync信号前面讲解ViewRootImpl.setView()的时候，因涉及到Vsync信号知识，requestLayout()没有具体讲解，现在继续。 12345678910111213Overridepublic void requestLayout() &#123; scheduleTraversals();&#125;void scheduleTraversals() &#123; if (!mTraversalScheduled) &#123; mTraversalScheduled = true; mTraversalBarrier = mHandler.getLooper().getQueue().postSyncBarrier(); mChoreographer.postCallback( Choreographer.CALLBACK_TRAVERSAL, mTraversalRunnable, null); ...... &#125;&#125; [-&gt;Choreographer.java] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public void postCallback(int callbackType, Runnable action, Object token) &#123; postCallbackDelayed(callbackType, action, token, 0);&#125;public void postCallbackDelayed(int callbackType, Runnable action, Object token, long delayMillis) &#123; ...... postCallbackDelayedInternal(callbackType, action, token, delayMillis);&#125;private void postCallbackDelayedInternal(int callbackType, Object action, Object token, long delayMillis) &#123; ...... synchronized (mLock) &#123; final long now = SystemClock.uptimeMillis(); final long dueTime = now + delayMillis; //将要执行的回调封装成CallbackRecord对象，保存到mCallbackQueues数组中 mCallbackQueues[callbackType].addCallbackLocked(dueTime, action, token); if (dueTime &lt;= now) &#123; scheduleFrameLocked(now); &#125; else &#123; Message msg = mHandler.obtainMessage(MSG_DO_SCHEDULE_CALLBACK, action); msg.arg1 = callbackType; msg.setAsynchronous(true); mHandler.sendMessageAtTime(msg, dueTime); &#125; &#125;&#125;private void scheduleFrameLocked(long now) &#123; if (!mFrameScheduled) &#123; mFrameScheduled = true; if (USE_VSYNC) &#123; if (isRunningOnLooperThreadLocked()) &#123; scheduleVsyncLocked(); &#125; else &#123; Message msg = mHandler.obtainMessage(MSG_DO_SCHEDULE_VSYNC); msg.setAsynchronous(true); mHandler.sendMessageAtFrontOfQueue(msg); &#125; &#125; else &#123; final long nextFrameTime = Math.max( mLastFrameTimeNanos / TimeUtils.NANOS_PER_MS + sFrameDelay, now); Message msg = mHandler.obtainMessage(MSG_DO_FRAME); msg.setAsynchronous(true); mHandler.sendMessageAtTime(msg, nextFrameTime); &#125; &#125;&#125; 消息处理： 12345678910111213141516171819202122232425 private final class FrameHandler extends Handler &#123; public FrameHandler(Looper looper) &#123; super(looper); &#125; @Override public void handleMessage(Message msg) &#123; switch (msg.what) &#123; case MSG_DO_SCHEDULE_VSYNC: doScheduleVsync(); break; &#125; &#125;&#125; void doScheduleVsync() &#123; synchronized (mLock) &#123; if (mFrameScheduled) &#123; scheduleVsyncLocked(); &#125; &#125;&#125;private void scheduleVsyncLocked() &#123; //申请Vsync信号 mDisplayEventReceiver.scheduleVsync(); &#125; 在该函数中考虑了两种情况，一种是系统没有使用Vsync机制，在这种情况下，首先根据屏幕刷新频率计算下一次刷新时间，通过异步消息方式延时执行doFrame()函数实现屏幕刷新。如果系统使用了Vsync机制，并且当前线程具备消息循环，则直接请求Vsync信号，否则就通过主线程来请求Vsync信号。 6.5.1、Vsync请求过程我们知道在Choreographer构造函数中，构造了一个FrameDisplayEventReceiver对象，用于请求并接收Vsync信号，Vsync信号请求过程如下： 1234private void scheduleVsyncLocked() &#123; //申请Vsync信号 mDisplayEventReceiver.scheduleVsync(); &#125; [-&gt;DisplayEventReceiver.java] 12345678public void scheduleVsync() &#123; if (mReceiverPtr == 0) &#123; Log.w(TAG, \"Attempted to schedule a vertical sync pulse but the display event \" + \"receiver has already been disposed.\"); &#125; else &#123; nativeScheduleVsync(mReceiverPtr); &#125;&#125; [-&gt;android_view_DisplayEventReceiver.cpp ] 12345678910static void nativeScheduleVsync(JNIEnv* env, jclass clazz, jlong receiverPtr) &#123;sp&lt;NativeDisplayEventReceiver&gt; receiver = reinterpret_cast&lt;NativeDisplayEventReceiver*&gt;(receiverPtr);status_t status = receiver-&gt;scheduleVsync();if (status) &#123; String8 message; message.appendFormat(\"Failed to schedule next vertical sync pulse. status=%d\", status); jniThrowRuntimeException(env, message.string());&#125;&#125; VSync请求过程又转交给了DisplayEventReceiver： [-&gt;DisplayEventReceiver.cpp] 1234567status_t DisplayEventReceiver::requestNextVsync() &#123;if (mEventConnection != NULL) &#123; mEventConnection-&gt;requestNextVsync(); return NO_ERROR;&#125;return NO_INIT;&#125; 这里的mEventConnection也是前面创建native层对象NativeDisplayEventReceiver时创建的，实际对象是一个BpDisplayEventConnection对象，也就是一个Binder客户端，对应的Binder服务端BnDisplayEventConnection是一个EventThread::Connection对象，对应的BpDisplayEventConnection.requestNextVsync函数和BnDisplayEventConnection.onTransact(REQUEST_NEXT_VSYNC)函数没有进行特别的处理，下面就调用到EventThread::Connection.requestNextVsync函数，从BnDisplayEventConnection.onTransact(REQUEST_NEXT_VSYNC)开始已经从用户进程将需要垂直同步信号的请求发送到了SurfaceFlinger进程，下面的函数调用开始进入SF进程： 123void EventThread::Connection::requestNextVsync() &#123; mEventThread-&gt;requestNextVsync(this); &#125; 辗转调用到EventThread.requestNextVsync函数，注意里面传了参数this，也就是当前的EventThread::Connection对象，需要明确的是，这里的mEventThread对象是创建EventThread::Connection对象的时候保存的，对应的是SurfaceFlinger对象的里面的mEventThread成员，该对象是一个在SurfaceFlinger.init里面创建并启动的线程对象，可见设计的时候就专门用这个SurfaceFlinger.mEventThread线程来接收来自应用进程的同步信号请求，每来一个应用进程同步信号请求，就通过SurfaceFlinger.mEventThread创建一个EventThread::Connection对象，并通过EventThread.registerDisplayEventConnection函数将创建的EventThread::Connection对象保存到EventThread.mDisplayEventConnections里面，上面有调用到了EventThread.requestNextVsync函数： 1234567void EventThread::requestNextVsync(const sp&lt;EventThread::Connection&gt;&amp; connection) &#123; Mutex::Autolock _l(mLock); if (connection-&gt;count &lt; 0) &#123; connection-&gt;count = 0; mCondition.broadcast(); &#125;&#125; 传进来的是一个前面创建的EventThread::Connection对象，里面判断到了EventThread::Connection.count成员变量，看一下EventThread::Connection构造函数中初始变量的值： 123EventThread::Connection::Connection(const sp&lt;EventThread&gt;&amp; eventThread) : count(-1), mEventThread(eventThread), mChannel(new BitTube())&#123;&#125; 可以看到初始值是-1，这个值就是前面那个问题的关键，EventThread::Connection.count标示了这次应用进程的垂直同步信号的请求是一次性的，还是多次重复的，看一下注释里面对于这个变量的说明： 1234// count &gt;= 1 : continuous event. count is the vsync rate// count == 0 : one-shot event that has not fired// count ==-1 : one-shot event that fired this round / disabledint32_t count; 很清楚的说明了，count = 0说明当前的垂直同步信号请求是一个一次性的请求，并且还没有被处理。上面EventThread::requestNextVsync里面将count设置成0，同时调用了mCondition.broadcast()唤醒所有正在等待mCondition的线程，这个会触发EventThread.waitForEvent函数从： 1mCondition.wait(mLock); 中醒来，醒来之后经过一轮do…while循环就会返回，返回以后调用序列如下： （1）EventThread::Connection.postEvent(event) （2）DisplayEventReceiver::sendEvents(mChannel, &amp;event, 1)，mChannel参数就是前面创建DisplayEventReceiver是创建的BitTube对象 （3）BitTube::sendObjects(dataChannel, events, count)，static函数，通过dataChannel指向BitTube对象 最终调用到BitTube::sendObjects函数： 12345ssize_t BitTube::sendObjects(const sp&lt;BitTube&gt;&amp; tube, void const* events, size_t count, size_t objSize)&#123; const char* vaddr = reinterpret_cast&lt;const char*&gt;(events); ssize_t size = tube-&gt;write(vaddr, count*objSize); return size &lt; 0 ? size : size / static_cast&lt;ssize_t&gt;(objSize);&#125; 继续调用到BitTube::write函数： 123456789ssize_t BitTube::write(void const* vaddr, size_t size)&#123; ssize_t err, len; do &#123; len = ::send(mSendFd, vaddr, size, MSG_DONTWAIT | MSG_NOSIGNAL); // cannot return less than size, since we're using SOCK_SEQPACKET err = len &lt; 0 ? errno : 0; &#125; while (err == EINTR); return err == 0 ? len : -err;&#125; 这里调用到了::send函数，::是作用域描述符，如果前面没有类名之类的，代表的就是全局的作用域，也就是调用全局函数send，这里很容易就能想到这是一个socket的写入函数，也就是将event事件数据写入到BitTube中互联的socket中，这样在另一端马上就能收到写入的数据，前面分析到这个BitTube的socket的两端连接着SurfaceFlinger进程和应用进程，也就是说通过调用BitTube::write函数，将最初由SurfaceFlinger捕获到的垂直信号事件经由BitTube中互联的socket从SurfaceFlinger进程发送到了应用进程中BitTube的socket接收端。 下面就要分析应用进程是如何接收并使用这个垂直同步信号事件的。 6.5.2、应用进程接收VSync6.5.2.1、解析VSync事件VSync同步信号事件已经发送到用户进程中的socket接收端，在前面NativeDisplayEventReceiver.initialize中分析到应用进程端的socket接收描述符已经被添加到Choreographer所在线程的native层的Looper机制中，在epoll中监听EPOLLIN事件，当socket收到数据后，epoll会马上返回，下面分步骤看一下Looper.pollInner()数： （1）epoll_wait 12struct epoll_event eventItems[EPOLL_MAX_EVENTS];int eventCount = epoll_wait(mEpollFd, eventItems, EPOLL_MAX_EVENTS, timeoutMillis); 在监听到描述符对应的事件后，epoll_wait会马上返回，并将产生的具体事件类型写入到参数eventItems里面，最终返回的eventCount是监听到的事件的个数 （2）事件分析 12345678910111213141516171819202122232425for (int i = 0; i &lt; eventCount; i++) &#123; int fd = eventItems[i].data.fd; uint32_t epollEvents = eventItems[i].events; if (fd == mWakeReadPipeFd) &#123; //判断是不是pipe读管道的事件 这里如果是EventThread,这里就是一个socket的描述符,而不是mWakeReadPipeFd if (epollEvents &amp; EPOLLIN) &#123; awoken(); // 清空读管道中的数据 &#125; else &#123; ALOGW(\"Ignoring unexpected epoll events 0x%x on wake read pipe.\", epollEvents); &#125; &#125; else &#123; //EventThread接收到同步信号走的这里 ssize_t requestIndex = mRequests.indexOfKey(fd); if (requestIndex &gt;= 0) &#123; int events = 0; if (epollEvents &amp; EPOLLIN) events |= EVENT_INPUT; if (epollEvents &amp; EPOLLOUT) events |= EVENT_OUTPUT; if (epollEvents &amp; EPOLLERR) events |= EVENT_ERROR; if (epollEvents &amp; EPOLLHUP) events |= EVENT_HANGUP; pushResponse(events, mRequests.valueAt(requestIndex)); &#125; else &#123; ALOGW(\"Ignoring unexpected epoll events 0x%x on fd %d that is \" \"no longer registered.\", epollEvents, fd); &#125; &#125; &#125; Looper目前了解到的主要监听的文件描述符种类有两种： 1）消息事件，epoll_wait监听pipe管道的接收端描述符mWakeReadPipeFd 2）与VSync信号，epoll_wait监听socket接收端描述符，并在addFd的过程中将相关的信息封装在一个Request结构中，并以fd为key存储到了mRequests中，具体可以回过头看3.1.2关于addFd的分析； 因此，上面走的是else的分支，辨别出当前的事件类型后，调用pushResponse： 123456void Looper::pushResponse(int events, const Request&amp; request) &#123; Response response; response.events = events; response.request = request; //复制不是引用，调用拷贝构造函数 mResponses.push(response);&#125; 该函数将Request和events封装在一个Response对象里面，存储到了mResponses里面，也就是mResponses里面放的是”某某fd上接收到了类别为events的时间”记录，继续向下看Looper.pollInner函数 （3）事件分发处理 1234567891011121314151617// Invoke all response callbacks.for (size_t i = 0; i &lt; mResponses.size(); i++) &#123; Response&amp; response = mResponses.editItemAt(i); if (response.request.ident == POLL_CALLBACK) &#123; int fd = response.request.fd; int events = response.events; void* data = response.request.data; int callbackResult = response.request.callback-&gt;handleEvent(fd, events, data); if (callbackResult == 0) &#123; removeFd(fd); &#125; // Clear the callback reference in the response structure promptly because we // will not clear the response vector itself until the next poll. response.request.callback.clear(); result = POLL_CALLBACK; &#125;&#125; 这里的response.request是从pushResponse里面复制过来的，里面的request对应的Request对象是在addFd的时候创建的，ident成员就是POLL_CALLBACK，所以继续走到response.request.callback-&gt;handleEvent这个函数，回忆一下3.1.2里面的addFd函数，这里的callback实际上是一个SimpleLooperCallback（定义在Looper.cpp中）对象，看一下里面的handleEvent函数： 123int SimpleLooperCallback::handleEvent(int fd, int events, void* data) &#123; return mCallback(fd, events, data);&#125; 这里的mCallback就是当时在addFd的时候传进来的callBack参数，实际上对应的就是NativeDisplayEventReceiver对象本身，因此最终就将垂直同步信号事件分发到了NativeDisplayEventReceiver.handleEvent函数中。 6.5.3、VSync事件分发调用到NativeDisplayEventReceiver.handleEvent函数，该函数定义在android_view_DisplayEventReceiver.cpp中，直接列出该函数： 1234567891011121314151617181920212223int NativeDisplayEventReceiver::handleEvent(int receiveFd, int events, void* data) &#123; if (events &amp; (Looper::EVENT_ERROR | Looper::EVENT_HANGUP)) &#123; ALOGE(\"Display event receiver pipe was closed or an error occurred. \" \"events=0x%x\", events); return 0; // remove the callback &#125; if (!(events &amp; Looper::EVENT_INPUT)) &#123; ALOGW(\"Received spurious callback for unhandled poll event. \" \"events=0x%x\", events); return 1; // keep the callback &#125; // Drain all pending events, keep the last vsync. nsecs_t vsyncTimestamp; int32_t vsyncDisplayId; uint32_t vsyncCount; if (processPendingEvents(&amp;vsyncTimestamp, &amp;vsyncDisplayId, &amp;vsyncCount)) &#123; ALOGV(\"receiver %p ~ Vsync pulse: timestamp=%\" PRId64 \", id=%d, count=%d\", this, vsyncTimestamp, vsyncDisplayId, vsyncCount); mWaitingForVsync = false; dispatchVsync(vsyncTimestamp, vsyncDisplayId, vsyncCount); &#125; return 1; // keep the callback&#125; 首先判断事件是不是正确的Looper::EVENT_INPUT事件，然后调用到NativeDisplayEventReceiver.processPendingEvents函数： 123456789101112131415161718192021222324252627282930bool NativeDisplayEventReceiver::processPendingEvents(nsecs_t* outTimestamp, int32_t* outId, uint32_t* outCount) &#123; bool gotVsync = false; DisplayEventReceiver::Event buf[EVENT_BUFFER_SIZE]; ssize_t n; while ((n = mReceiver.getEvents(buf, EVENT_BUFFER_SIZE)) &gt; 0) &#123; for (ssize_t i = 0; i &lt; n; i++) &#123; const DisplayEventReceiver::Event&amp; ev = buf[i]; switch (ev.header.type) &#123; case DisplayEventReceiver::DISPLAY_EVENT_VSYNC: // Later vsync events will just overwrite the info from earlier // ones. That's fine, we only care about the most recent. gotVsync = true; *outTimestamp = ev.header.timestamp; *outId = ev.header.id; *outCount = ev.vsync.count; break; case DisplayEventReceiver::DISPLAY_EVENT_HOTPLUG: dispatchHotplug(ev.header.timestamp, ev.header.id, ev.hotplug.connected); break; default: ALOGW(\"receiver %p ~ ignoring unknown event type %#x\", this, ev.header.type); break; &#125; &#125; &#125; if (n &lt; 0) &#123; ALOGW(\"Failed to get events from display event receiver, status=%d\", status_t(n)); &#125; return gotVsync;&#125; 这里的mReceiver也就是前面创建NativeDisplayEventReceiver对象是创建的成员变量对象DisplayEventReceiver，下面调用到DisplayEventReceiver.getEvents函数，应该是要从出现同步信号事件的socket中读取数据，上面Looper机制中epoll中监听到socket以后，返回到NativeDisplayEventReceiver.handleEvent里面，但是socket里面的数据还没有读取，下面的调用流程为： （1）mReceiver.getEvents(buf, EVENT_BUFFER_SIZE) —&gt; DisplayEventReceiver::getEvents(DisplayEventReceiver::Event _events, sizet count) （2）BitTube::recvObjects(dataChannel, events, count) —&gt; BitTube::recvObjects(const sp&amp; tube, void events, size_t count, size_t objSize) 看一下这个recvObjects函数： 123456ssize_t BitTube::recvObjects(const sp&lt;BitTube&gt;&amp; tube, void* events, size_t count, size_t objSize)&#123; char* vaddr = reinterpret_cast&lt;char*&gt;(events); ssize_t size = tube-&gt;read(vaddr, count*objSize); return size &lt; 0 ? size : size / static_cast&lt;ssize_t&gt;(objSize);&#125; 这里在NativeDisplayEventReceiver中创建了一个缓冲区，并在recvObjects中将socket中的Event数据读到这个缓冲区中，这个Event.header.type一般都是DISPLAY_EVENT_VSYNC，因此在上面的processPendingEvents函数中会将Event数据保存在outCount所指向的内存中，并返回true。 接下来返回到NativeDisplayEventReceiver.handleEvent后会调用到dispatchVsync函数： 12345void NativeDisplayEventReceiver::dispatchVsync(nsecs_t timestamp, int32_t id, uint32_t count) &#123; JNIEnv* env = AndroidRuntime::getJNIEnv(); env-&gt;CallVoidMethod(mReceiverObjGlobal, gDisplayEventReceiverClassInfo.dispatchVsync, timestamp, id, count); mMessageQueue-&gt;raiseAndClearException(env, \"dispatchVsync\");&#125; 这里的处理很直接，直接调用mReceiverObjGlobal对象在gDisplayEventReceiverClassInfo.dispatchVsync中指定的函数，将后面的timestamp（时间戳） id（设备ID） count（经过的同步信号的数量，一般没有设置采样频率应该都是1），下面分别看一下mReceiverObjGlobal以及gDisplayEventReceiverClassInfo.dispatchVsync代表的是什么？ （1）mReceiverObjGlobal 1234NativeDisplayEventReceiver::NativeDisplayEventReceiver(JNIEnv* env, jobject receiverObj, const sp&lt;MessageQueue&gt;&amp; messageQueue) : mReceiverObjGlobal(env-&gt;NewGlobalRef(receiverObj)), mMessageQueue(messageQueue), mWaitingForVsync(false) &#123; ALOGV(\"receiver %p ~ Initializing input event receiver.\", this);&#125; 可以看到mReceiverObjGlobal是创建NativeDisplayEventReceiver对象时传进来的第二个参数，该对象是在nativeInit函数中创建： 1sp receiver = new NativeDisplayEventReceiver(env, receiverObj, messageQueue); 进一步的，receiverObj是调用nativeInit函数时传进来的第一个参数（第一个参数env是系统用于连接虚拟机时自动加上的），nativeInit函数又是在Choreographer中创建FrameDisplayEventReceiver对象时，在基类DisplayEventReceiver构造器中调用的，因此这里的mReceiverObjGlobal对应的就是Choreographer中的FrameDisplayEventReceiver成员mDisplayEventReceiver。 （2）gDisplayEventReceiverClassInfo.dispatchVsync 在JNI中有很多这样的类似的结构体对象，这些对象都是全局结构体对象，这里的gDisplayEventReceiverClassInfo就是这样的一个对象，里面描述了一些在整个文件内可能会调用到的java层的相关类以及成员函数的相关信息，看一下gDisplayEventReceiverClassInfo： 12345static struct &#123; jclass clazz; jmethodID dispatchVsync; jmethodID dispatchHotplug;&#125; gDisplayEventReceiverClassInfo; 看一下里面的变量名称就能知道大致的含义，clazz成员代表的是某个java层的类的class信息，dispatchVsync和dispatchHotplug代表的是java层类的方法的方法信息，看一下该文件中注册JNI函数的方法： 12345678int register_android_view_DisplayEventReceiver(JNIEnv* env) &#123; int res = RegisterMethodsOrDie(env, \"android/view/DisplayEventReceiver\", gMethods, NELEM(gMethods)); jclass clazz = FindClassOrDie(env, \"android/view/DisplayEventReceiver\"); gDisplayEventReceiverClassInfo.clazz = MakeGlobalRefOrDie(env, clazz); gDisplayEventReceiverClassInfo.dispatchVsync = GetMethodIDOrDie(env, gDisplayEventReceiverClassInfo.clazz, \"dispatchVsync\", \"(JII)V\"); gDisplayEventReceiverClassInfo.dispatchHotplug = GetMethodIDOrDie(env, gDisplayEventReceiverClassInfo.clazz, \"dispatchHotplug\", \"(JIZ)V\"); return res;&#125; RegisterMethodsOrDie调用注册了java层调用native方法时链接到的函数的入口，下面clazz对应的就是java层的”android/view/DisplayEventReceiver.java”类，gDisplayEventReceiverClassInfo.dispatchVsync里面保存的就是clazz类信息中与dispatchVsync方法相关的信息，同样dispatchHotplug也是。 分析到这里，就知道应用进程native接收到同步信号事件后，会调用Choreographer中的FrameDisplayEventReceiver成员mDisplayEventReceiver的dispatchVsync方法。 6.5.4、应用接收Vsync看一下FrameDisplayEventReceiver.dispatchVsync方法，也就是DisplayEventReceiver.dispatchVsync方法(Choreographer.java)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// Called from native code. @SuppressWarnings(&quot;unused&quot;) private void dispatchVsync(long timestampNanos, int builtInDisplayId, int frame) &#123; onVsync(timestampNanos, builtInDisplayId, frame); &#125; 注释表明这个方法是从native代码调用的，该函数然后会调用FrameDisplayEventReceiver.onVsync方法： @Override public void onVsync(long timestampNanos, int builtInDisplayId, int frame) &#123; // Ignore vsync from secondary display. // This can be problematic because the call to scheduleVsync() is a one-shot. // We need to ensure that we will still receive the vsync from the primary // display which is the one we really care about. Ideally we should schedule // vsync for a particular display. // At this time Surface Flinger won&apos;t send us vsyncs for secondary displays // but that could change in the future so let&apos;s log a message to help us remember // that we need to fix this. //注释：忽略来自非主显示器的Vsync信号，但是我们前面调用的scheduleVsync函数只能请求到一次Vsync信号，因此需要重新调用scheduleVsync函数 //请求来自主显示设备的Vsync信号 if (builtInDisplayId != SurfaceControl.BUILT_IN_DISPLAY_ID_MAIN) &#123; Log.d(TAG, &quot;Received vsync from secondary display, but we don&apos;t support &quot; + &quot;this case yet. Choreographer needs a way to explicitly request &quot; + &quot;vsync for a specific display to ensure it doesn&apos;t lose track &quot; + &quot;of its scheduled vsync.&quot;); scheduleVsync(); return; &#125; // Post the vsync event to the Handler. // The idea is to prevent incoming vsync events from completely starving // the message queue. If there are no messages in the queue with timestamps // earlier than the frame time, then the vsync event will be processed immediately. // Otherwise, messages that predate the vsync event will be handled first. long now = System.nanoTime(); if (timestampNanos &gt; now) &#123; Log.w(TAG, &quot;Frame time is &quot; + ((timestampNanos - now) * 0.000001f) + &quot; ms in the future! Check that graphics HAL is generating vsync &quot; + &quot;timestamps using the correct timebase.&quot;); timestampNanos = now; &#125; if (mHavePendingVsync) &#123; Log.w(TAG, &quot;Already have a pending vsync event. There should only be &quot; + &quot;one at a time.&quot;); &#125; else &#123; mHavePendingVsync = true; &#125; mTimestampNanos = timestampNanos; //同步信号时间戳 mFrame = frame; //同步信号的个数，理解就是从调用scheduleVsync到onVsync接收到信号之间经历的同步信号的个数，一般都是1 Message msg = Message.obtain(mHandler, this); msg.setAsynchronous(true); mHandler.sendMessageAtTime(msg, timestampNanos / TimeUtils.NANOS_PER_MS); &#125; 貌似这里的处理只是往Choreographer对象中的mHandler对应的线程Looper中发送一个消息，消息的内容有两个特点： （1）将this，也就是当前的FrameDisplayEventReceiver对象作为参数，后面会回调到FrameDisplayEventReceiver.run方法； （2）为Message设置FLAG_ASYNCHRONOUS属性； 发送这个FLAG_ASYNCHRONOUS消息后，后面会回调到FrameDisplayEventReceiver.run方法，至于为什么，后面再写文章结合View.invalidate方法的过程分析，看一下FrameDisplayEventReceiver.run方法： 12345@Overridepublic void run() &#123; mHavePendingVsync = false; doFrame(mTimestampNanos, mFrame);&#125; 调用Choreographer.doFrame方法，如果是重绘事件doFrame方法会最终调用到ViewRootImpl.performTraversals方法进入实际的绘制流程。经过上面的分析可以知道，调用一次Choreographer.scheduleVsyncLocked只会请求一次同步信号，也就是回调一次FrameDisplayEventReceiver.onVsync方法，在思考一个问题，一个应用进程需要多次请求Vsync同步信号会不会使用同样的一串对象？多个线程又是怎么样的？ 答：一般绘制操作只能在主线程里面进行，因此一般来说只会在主线程里面去请求同步信号，可以认为不会存在同一个应用的多个线程请求SF的Vsync信号，Choreographer是一个线程内的单例模式，存储在了 ThreadLocal sThreadInstance对象里面，所以主线程多次请求使用的是同一个Choreographer对象，所以后面的一串对象应该都是可以复用的。 总体架构： 伐木累:::终于完了，由于Android Graphics系统涉及模块代码纵横交叉复杂，其中代码图示有误的地方请见谅，也没有精力一一核对了，还请海涵~~~主要是分析Android Graphics总体的一个流程思想，有需要再一点点深挖。 （七）、参考文档(特别感谢各位前辈的分析和图示)：Android Vsync 原理林学森的Android专栏Android Graphics了解 Systrace图解Android - Android GUI 系统Android SurfaceFlinger 学习之路&amp;Android多媒体开发Android7.0 基础业务AMS、数据业务、电源管理业务 源码分析【Android 显示模块】 - 深入剖析Android系统 - CSDN博客深入理解Android卷一全文-第八章(深入理解Surface系统)android系统 - armwind的专栏 - CSDN博客android显示系统 - kc58236582的博客 - CSDN博客SurfaceView, TextureView, SurfaceTexture等的区别【Demo】Android graphics 学习－生产者、消费者、BufferQueue介绍深入Android Graphics Pipeline：从按钮到帧缓冲（第一部分）深入Android Graphics Pipeline：从按钮到帧缓冲（第二部分）窗口：Profiling 视图（OpenGL/OpenGL ES* 工作负载）Android N中UI硬件渲染（hwui）的HWUI_NEW_OPS(基于Android 7.1)Android’s Graphics Buffer Management System (Part I: gralloc)Android’s Graphics Buffer Management System (Part II: BufferQueue)Android GDI之SurfaceFlingerAndroid SurfaceFlinger 学习之路(五)—-VSync 工作原理Android 5.1 SurfaceFlinger VSYNC详解Android 5.1 SurfaceFlinger VSYNC详解Android消息机制Looper与VSync的传播Android垂直同步信号VSync的产生及传播结构详解Android 4.4(KitKat)中VSync信号的虚拟化Android 4.4(KitKat)窗口管理子系统 - 体系框架Android中用OpenGL ES Tracer分析绘制过程android view的绘制中，View绘制的时间如何和vsync屏幕刷新频率保持同步的？【深入理解Android卷一全文-第八章】入理解Surface系统Android 窗口管理：Z-Order管理","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android 7.1.2 (Android N) Android Binder 系统 分析","slug":"Android-7-1-2-Android-N-Android-Binder系统分析","date":"2017-12-31T16:00:00.000Z","updated":"2018-04-19T14:29:56.436Z","comments":true,"path":"2018/01/01/Android-7-1-2-Android-N-Android-Binder系统分析/","link":"","permalink":"http://zhoujinjian.cc/2018/01/01/Android-7-1-2-Android-N-Android-Binder系统分析/","excerpt":"Android Binder系统概述： Binder是Android系统中大量使用的IPC（Inter-process communication，进程间通讯）机制。无论是应用程序对系统服务的请求，还是应用程序自身提供对外服务，都需要使用到Binder。因此，Binder机制在Android系统中的地位非常重要，可以说，理解Binder是理解Android系统的绝对必要前提。","text":"Android Binder系统概述： Binder是Android系统中大量使用的IPC（Inter-process communication，进程间通讯）机制。无论是应用程序对系统服务的请求，还是应用程序自身提供对外服务，都需要使用到Binder。因此，Binder机制在Android系统中的地位非常重要，可以说，理解Binder是理解Android系统的绝对必要前提。 framework/base/core/java/ (Java) framework/base/core/jni/ (JNI) framework/native/libs/binder (Native) framework/native/cmds/servicemanager/ (Native) kernel/drivers/staging/android (Driver) Java framework framework/base/core/java/android/os/● IInterface.java● IBinder.java● Parcel.java● IServiceManager.java● ServiceManager.java● ServiceManagerNative.java● Binder.java framework/base/core/jni/● android_os_Parcel.cpp● AndroidRuntime.cpp● android_util_Binder.cpp (核心类) Native framework framework/native/libs/binder● IServiceManager.cpp● BpBinder.cpp● Binder.cpp● IPCThreadState.cpp (核心类)● ProcessState.cpp (核心类) framework/native/include/binder/● IServiceManager.h● IInterface.h framework/native/cmds/servicemanager/● bctest.c● binder.h● binder.c● service_manager.c● servicemanager.rc Kernel kernel/drivers/staging/android/ ● binder.c● binder.h 博客原图链接一、Android Binder系统C程序示例（1）、简述Binder跨进程机制Android系统中，每个应用程序是由Android的Activity，Service，Broadcast，ContentProvider这四组件的中一个或多个组合而成，这四组件所涉及的多进程间的通信底层都是依赖于Binder IPC机制。 从进程角度来看IPC机制 现在Client进程需要访问Server进程中的服务，会经过以下步骤：1、Server进程首先向ServiceManager注册服务（ServiceManager先于Server启动）2、Client进程向ServiceManager查询服务得到一个句柄Handle（Server进程可能不止一个服务，用Handle区分是哪一个服务）3、Client进程 封装数据Buffer通过Binder驱动发送给Server进程，Server进程取得数据后解析数据，使用Server进程的Handle服务对应的函数处理数据，处理完成后通过Binder驱动传输给Client进程 1.1、Server进程向ServiceManager注册服务ServiceManager是一个守护进程。它的main()函数源码如下： ServiceManager是如何启动的？ 这里简要介绍一下ServiceManager的启动方式。当Kernel启动加载完驱动之后，会启动Android的init进程，init进程会解析servicemanager.rc，进而启动servicemanager.rc中定义的守护进程。 1234567891011121314151617[-&gt;ServiceManager.c]int main(int argc, char **argv)&#123; struct binder_state *bs; void *svcmgr = BINDER_SERVICE_MANAGER; bs = binder_open(128*1024); if (binder_become_context_manager(bs)) &#123; ... &#125; svcmgr_handle = svcmgr; binder_loop(bs, svcmgr_handler); return 0;&#125; 123456789101112131415161718192021222324252627282930void binder_loop(struct binder_state *bs, binder_handler func)&#123; int res; struct binder_write_read bwr; unsigned readbuf[32]; bwr.write_size = 0; bwr.write_consumed = 0; bwr.write_buffer = 0; // 告诉Kernel，ServiceManager进程进入了消息循环状态。 readbuf[0] = BC_ENTER_LOOPER; binder_write(bs, readbuf, sizeof(unsigned)); for (;;) &#123; bwr.read_size = sizeof(readbuf); bwr.read_consumed = 0; bwr.read_buffer = (unsigned) readbuf; // 向Kernel中发送消息(先写后读)。 // 先将消息传递给Kernel，然后再从Kernel读取消息反馈 res = ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr); ... // 解析读取的消息反馈 res = binder_parse(bs, 0, readbuf, bwr.read_consumed, func); ... &#125;&#125; binder_loop()主要工作： (1)、通过ioctl(,BINDER_WRITE_READ,)进入消息循环，休眠等待Client请求 (2)、当Client通过驱动请求服务时，binder驱动会唤醒ServiceManager，通过binder_parse()解析处理数据，回复信息 代码调用关系图： 时序流程图： main()主要进行了三项工作： (1) 、通过binder_open()打开”/dev/binder”文件，即打开Binder设备文件。 (2) 、调用binder_become_context_manager()，通过ioctl()告诉Binder驱动程序自己是Binder上下文管理者。 (3) 、调用binder_loop()进入消息循环，等待Client的请求。如果没有Client请求，则进入睡眠等待状态；当有Client请求时，就被唤醒，然后读取并处理Client请求。 1.2、分析Android binder原生示例程序bctest.c：12345678910111213141516int main(int argc, char **argv)&#123; struct binder_state *bs; uint32_t svcmgr = BINDER_SERVICE_MANAGER; uint32_t handle; bs = binder_open(128*1024); ... while (argc &gt; 0) &#123; //svcmgr_lookup方法调用binder_call(bs, &amp;msg, &amp;reply, 0, SVC_MGR_ADD_SERVICE) handle = svcmgr_lookup(bs, svcmgr, \"alt_svc_mgr\"); //svcmgr_publish方法调用binder_call(bs, &amp;msg, &amp;reply, 0, SVC_MGR_CHECK_SERVICE) svcmgr_publish(bs, svcmgr, argv[1], &amp;token); &#125; return 0;&#125; 1.3、示例程序（bctest.c）注册服务、获取服务过程注册服务的过程（bctest.c）: (1) 、bs = binder_open(128*1024) (2) 、binder_call(bs, &amp;msg, &amp;reply, 0, SVC_MGR_ADD_SERVICE) 参数说明： // msg含有服务的名字 // reply它会含有servicemanager回复的数据 // target为0表示servicemanager // code: 表示要调用servicemanager中的”addservice函数” 获取服务的过程（bctest.c）: (1) 、bs = binder_open(128*1024) (2) 、binder_call(bs, &amp;msg, &amp;reply, target, SVC_MGR_CHECK_SERVICE) 参数说明： // msg含有服务的名字 // reply它会含有servicemanager回复的数据, 表示提供服务的进程 // target为0表示servicemanager // code: 表示要调用servicemanager中的”getservice函数” binder_call远程实现： 根据msg、target、code就知道需要调用哪个服务的哪一个函数。 12345678910111213141516171819202122232425262728293031323334353637int binder_call(struct binder_state *bs, struct binder_io *msg, struct binder_io *reply, uint32_t target, uint32_t code)&#123; int res; struct binder_write_read bwr; struct &#123; uint32_t cmd; struct binder_transaction_data txn; &#125; __attribute__((packed)) writebuf; unsigned readbuf[32]; writebuf.cmd = BC_TRANSACTION; writebuf.txn.target.handle = target; writebuf.txn.code = code; writebuf.txn.flags = 0; writebuf.txn.data_size = msg-&gt;data - msg-&gt;data0; writebuf.txn.offsets_size = ((char*) msg-&gt;offs) - ((char*) msg-&gt;offs0); writebuf.txn.data.ptr.buffer = (uintptr_t)msg-&gt;data0; writebuf.txn.data.ptr.offsets = (uintptr_t)msg-&gt;offs0; bwr.write_size = sizeof(writebuf); bwr.write_consumed = 0; bwr.write_buffer = (uintptr_t) &amp;writebuf; hexdump(msg-&gt;data0, msg-&gt;data - msg-&gt;data0); for (;;) &#123; bwr.read_size = sizeof(readbuf); bwr.read_consumed = 0; bwr.read_buffer = (uintptr_t) readbuf; res = ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr); res = binder_parse(bs, reply, (uintptr_t) readbuf, bwr.read_consumed, 0); &#125;&#125; 注： 结构体简介 binder_io 封装一次发送的数据 binder_write_read 存储一次读写操作的数据 binder_transaction_data 存储一次事务的数据 （1）构造参数，使用binder_io 描述（2）数据转换binder_io -&gt; binder_write_read；首先根据binder_io 、target、code三者构造binder_transaction_data，然后将binder_write_read.write_buffer指向binder_transaction_data（3）调用ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr);发送数据 （2）、Android Binder系统_ServiceManager我们先跳过ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr)所涉及的内核知识和流程，稍后再Android Binder系统-Driver层详细介绍。 2.1、ServiceManager中service句柄如何管理前面分析过，ServiceManager开机初始会启动成为一个守护进程， ServiceManager是如何管理service句柄的？ 进程里有一个全局性的svclist变量： 1struct svcinfo *svclist = 0; 它记录着所有添加进系统的”Service”信息，这些信息被组织成一条单向链表，我们不妨称这条链表为”Service向量表”。示意图如下： 链表节点类型为svcinfo 添加服务简单理解就是 新建svcinfo节点插入到单链表中，查询服务就是看单链表是否有此服务。 2.2、解析Binder上传数据-(binder_parse函数)回到ServiceManager的main()函数。binder_loop()会先向binder驱动发出了BC_ENTER_LOOPER命令，接着进入一个for循环不断调用ioctl()读取发来的数据，接着解析这些数据。假设现在Client有请求，Binder驱动就通过会上传数据。读取数据后会交由binder_parse()解析。 1binder_loop(bs, svcmgr_handler); 注意binder_loop()的参数svcmgr_handler()函数指针。而且这个参数会进一步传递给binder_parse()。binder_parse()负责解析从binder驱动读来的数据，其代码截选如下： 1234567891011121314151617181920212223242526272829303132333435363738int binder_parse(struct binder_state *bs, struct binder_io *bio, uintptr_t ptr, size_t size, binder_handler func)&#123; int r = 1; uintptr_t end = ptr + (uintptr_t) size; while (ptr &lt; end) &#123; uint32_t cmd = *(uint32_t *) ptr; ptr += sizeof(uint32_t); switch(cmd) &#123; ... //驱动有数据后会返回次cmd case BR_TRANSACTION: &#123; struct binder_transaction_data *txn = (struct binder_transaction_data *) ptr; binder_dump_txn(txn); if (func) &#123; unsigned rdata[256/4]; struct binder_io msg; struct binder_io reply; int res; bio_init(&amp;reply, rdata, sizeof(rdata), 4); bio_init_from_txn(&amp;msg, txn); res = func(bs, txn, &amp;msg, &amp;reply); if (txn-&gt;flags &amp; TF_ONE_WAY) &#123; binder_free_buffer(bs, txn-&gt;data.ptr.buffer); &#125; else &#123; binder_send_reply(bs, &amp;reply, txn-&gt;data.ptr.buffer, res); &#125; ... &#125; ptr += sizeof(*txn); break; &#125; ... &#125; return r;&#125; 从前文的代码我们可以看到，binder_loop()声明了一个128节的buffer（即uint32_t readbuf[32]），每次用BINDER_WRITE_READ命令从驱动读取一些内容，并传入binder_parse()。 binder_parse()在合适的时机，会回调其func参数（binder_handler func）指代的回调函数，即前文说到的svcmgr_handler()函数。 binder_loop()就这样一直循环下去，完成了整个ServiceManager的工作。 2.3、数据转换binder_transaction_data-&gt;binder_io初始化reply；根据txt(Binder驱动反馈的信息)初始化msg 12bio_init(&amp;reply, rdata, sizeof(rdata), 4);bio_init_from_txn(&amp;msg, txn); 2.4、如何添加服务SVC_MGR_ADD_SERVICE前面讲过 binder_parse()在合适的时机，会回调其func参数（binder_handler func）指代的回调函数，即前文说到的svcmgr_handler()函数。并且会根据binder_transaction_data的code判断具体调用哪一个函数。 12345678910111213141516171819202122232425262728293031323334int svcmgr_handler(struct binder_state *bs, struct binder_transaction_data *txn, struct binder_io *msg, struct binder_io *reply)&#123; struct svcinfo *si; uint16_t *s; size_t len; uint32_t handle; uint32_t strict_policy; int allow_isolated; ...... switch(txn-&gt;code) &#123; case SVC_MGR_GET_SERVICE: case SVC_MGR_CHECK_SERVICE: s = bio_get_string16(msg, &amp;len); handle = do_find_service(s, len, txn-&gt;sender_euid, txn-&gt;sender_pid); bio_put_ref(reply, handle); return 0; case SVC_MGR_ADD_SERVICE: s = bio_get_string16(msg, &amp;len); handle = bio_get_ref(msg); allow_isolated = bio_get_uint32(msg) ? 1 : 0; if (do_add_service(bs, s, len, handle, txn-&gt;sender_euid, allow_isolated, txn-&gt;sender_pid)) return -1; break;... &#125; bio_put_uint32(reply, 0); return 0;&#125; 由代码可知code = SVC_MGR_ADD_SERVICE 会调用do_add_service()函数 123456789101112131415161718192021222324252627int do_add_service(struct binder_state *bs, const uint16_t *s, size_t len, uint32_t handle, uid_t uid, int allow_isolated, pid_t spid)&#123; struct svcinfo *si; ... si = find_svc(s, len); if (si) &#123; ... &#125; else &#123; si = malloc(sizeof(*si) + (len + 1) * sizeof(uint16_t)); ... si-&gt;handle = handle; si-&gt;len = len; memcpy(si-&gt;name, s, (len + 1) * sizeof(uint16_t)); si-&gt;name[len] = '\\0'; si-&gt;death.func = (void*) svcinfo_death; si-&gt;death.ptr = si; si-&gt;allow_isolated = allow_isolated; si-&gt;next = svclist; svclist = si; &#125; binder_acquire(bs, handle); binder_link_to_death(bs, handle, &amp;si-&gt;death); return 0;&#125; 可见添加Service只是新建了一个svcinfo然后插入到前面所说的”Service向量表”中。 2.5、如何获取服务SVC_MGR_CHECK_SERVICE123456uint32_t do_find_service(const uint16_t *s, size_t len, uid_t uid, pid_t spid)&#123; struct svcinfo *si = find_svc(s, len); ... return si-&gt;handle;&#125; 获取服务会查询”Service向量表”是否有此服务，然后返回Service的句柄handle。 2.6、ServiceManager回复数据前面分析回调svcmgr_handler()函数处理数据后，会调用binder_send_reply()函数 回复消息给驱动。 1binder_send_reply(bs, &amp;reply, txn-&gt;data.ptr.buffer, res) 2.7、总结：示例程序（bctest.c）注册、获取服务一般分以下步骤： （1）源进程通过binder_open()打开”/dev/binder”文件，即打开Binder设备文件。 （2）源进程构造数据：[a].构造binder_io [b].转为binder_transaction_data [c].放入binder_write_read （3）源进程调用ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr);发送数据给驱动 （4）驱动上报数据到目的进程ServiceManager （5）目的进程ServiceManager处理完数据，重新构造数据，通过调用ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr);发送数据给驱动 （6）驱动然后将数据反馈到源进程 （3）、Android Binder系统C程序3.1、Android Binder系统C程序_框架总结bctest.c注册服务获取服务的一般流程框架： 3.2、Android Binder系统C程序_编码参考bctest.c编码： test_server：向ServiceManager添加服务”hello” &amp;&amp; “goodbye” Service test_client ：查询获取服务(ServiceManager) 链接：Binder_C_App 3.3、Android Binder系统C程序_测试./test_server &amp; ./test_client hello ./test_client hello 100ask.taobao.com ./test_client goodbye ./test_client goodbye 100ask.taobao.com 二、Android Binder系统-Driver层前面打开驱动binder_open(128*1024)、ServiceManager启动是如何与驱动交互成为管理者的，以及添加服务获取服务 驱动部分都没有详细讲解，现在一起来看下。 （1）、Binder驱动概述1.1 概述Binder驱动是Android专用的，但底层的驱动架构与Linux驱动一样。binder驱动在以misc设备进行注册，作为虚拟字符设备，没有直接操作硬件，只是对设备内存的处理。主要是驱动设备的初始化(binder_init)，打开 (binder_open)，映射(binder_mmap)，数据操作(binder_ioctl)。如启动ServiceManager调用: 1.2 系统调用用户态的程序调用Kernel层驱动是需要陷入内核态，进行系统调用(syscall)，比如打开Binder驱动方法的调用链为： open-&gt; open() -&gt; binder_open()。 open()为用户空间的方法，open()便是系统调用中相应的处理方法，通过查找，对应调用到内核binder驱动的binder_open()方法，至于其他的从用户态陷入内核态的流程也基本一致。 （2）、Binder核心方法2.1、binder_init()主要工作是为了注册misc设备 binder_init函数中最主要的工作其实下面这行： 1ret = misc_register(&amp;binder_miscdev); 该行代码真正向内核中注册了Binder设备。binder_miscdev的定义如下： 12345static struct miscdevice binder_miscdev = &#123; .minor = MISC_DYNAMIC_MINOR, .name = \"binder\", .fops = &amp;binder_fops&#125;; 这里指定了Binder设备的名称是”binder”。这样，在用户空间便可以通过对/dev/binder文件进行操作来使用Binder。 binder_miscdev同时也指定了该设备的fops。fops是另外一个结构体，这个结构中包含了一系列的函数指针，其定义如下： 12345678910static const struct file_operations binder_fops = &#123; .owner = THIS_MODULE, .poll = binder_poll, .unlocked_ioctl = binder_ioctl, .compat_ioctl = binder_ioctl, .mmap = binder_mmap, .open = binder_open, .flush = binder_flush, .release = binder_release,&#125;; 2.2、主要结构Binder驱动中包含了很多的结构体。为了便于下文讲解，这里我们先对这些结构体做一些介绍。 驱动中的结构体可以分为两类： 一类是与用户空间共用的，这些结构体在Binder通信协议过程中会用到。因此，这些结构体定义在binder.h中，包括： 结构体名称 说明 flat_binder_object 描述在Binder IPC中传递的对象，见下文 binder_write_read 存储一次读写操作的数据 binder_version 存储Binder的版本号 transaction_flags 描述事务的flag，例如是否是异步请求，是否支持fd binder_transaction_data 存储一次事务的数据 binder_ptr_cookie 包含了一个指针和一个cookie binder_handle_cookie 包含了一个句柄和一个cookie binder_pri_desc 暂未用到 binder_pri_ptr_cookie 暂未用到 从前面Binder系统C程序框架分析，这其中，binder_write_read和binder_transaction_data这两个结构体最为重要，它们存储了IPC调用过程中的数据。关于这一点，我们在下文中会讲解。 Binder驱动中，还有一类结构体是仅仅Binder驱动内部实现过程中需要的，它们定义在binder.c中，包括： 结构体名称 说明 binder_node 描述Binder实体节点，即：对应了一个Server binder_ref 描述对于Binder实体的引用 binder_buffer 描述Binder通信过程中存储数据的Buffer binder_proc 描述使用Binder的进程 binder_thread 描述使用Binder的线程 binder_work 描述通信过程中的一项任务 binder_transaction 描述一次事务的相关信息 binder_deferred_state 描述延迟任务 binder_ref_death 描述Binder实体死亡的信息 binder_transaction_log debugfs日志 binder_transaction_log_entry debugfs日志条目 这里需要读者关注的结构体已经用加粗做了标注。 2.3、Binder协议Binder协议可以分为控制协议和驱动协议两类。 控制协议是进程通过ioctl(“/dev/binder”) 与Binder设备进行通讯的协议，该协议包含以下几种命令： 结构体名称 说明 参数类型 BINDER_WRITE_READ 读写操作，最常用的命令。IPC过程就是通过这个命令进行数据传递 binder_write_read BINDER_SET_MAX_THREADS 设置进程支持的最大线程数量 size_t BINDER_SET_CONTEXT_MGR 设置自身为ServiceManager 无 BINDER_THREAD_EXIT 通知驱动Binder线程退出 无 BINDER_VERSION 获取Binder驱动的版本号 binder_version BINDER_SET_IDLE_PRIORITY 暂未用到 - BINDER_SET_IDLE_TIMEOUT 暂未用到 - Binder的驱动协议描述了对于Binder驱动的具体使用过程。驱动协议又可以分为两类： 一类是binder_driver_command_protocol，描述了进程发送给Binder驱动的命令 一类是binder_driver_return_protocol，描述了Binder驱动发送给进程的命令 binder_driver_command_protocol共包含17个命令，分别是： 结构体名称 说明 参数类型 BC_TRANSACTION Binder事务，即：Client对于Server的请求 binder_transaction_data BC_REPLY 事务的应答，即：Server对于Client的回复 binder_transaction_data BC_FREE_BUFFER 通知驱动释放Buffer binder_uintptr_t BC_ACQUIRE 强引用计数+1 __u32 BC_RELEASE 强引用计数-1 __u32 BC_INCREFS 弱引用计数+1 __u32 BC_DECREFS 弱引用计数-1 __u32 BC_ACQUIRE_DONE BR_ACQUIRE的回复 binder_ptr_cookie BC_INCREFS_DONE BR_INCREFS的回复 binder_ptr_cookie BC_ENTER_LOOPER 通知驱动主线程ready void BC_REGISTER_LOOPER 通知驱动子线程ready void BC_EXIT_LOOPER 通知驱动线程已经退出 void BC_REQUEST_DEATH_NOTIFICATION 请求接收死亡通知 binder_handle_cookie BC_CLEAR_DEATH_NOTIFICATION 去除接收死亡通知 binder_handle_cookie BC_DEAD_BINDER_DONE 已经处理完死亡通知 binder_uintptr_t BC_ATTEMPT_ACQUIRE 暂未实现 - BC_ACQUIRE_RESULT 暂未实现 - binder_driver_return_protocol共包含18个命令，分别是： 结构体名称 说明 参数类型 BR_OK 操作完成 void BR_NOOP 操作完成 void BR_ERROR 发生错误 __s32 BR_TRANSACTION 通知进程收到一次Binder请求（Server端） binder_transaction_data BR_REPLY 通知进程收到Binder请求的回复（Client） binder_transaction_data BR_TRANSACTION_COMPLETE 驱动对于接受请求的确认回复 void BR_FAILED_REPLY 告知发送方通信目标不存在 void BR_SPAWN_LOOPER 通知Binder进程创建一个新的线程 void BR_ACQUIRE 强引用计数+1请求 binder_ptr_cookie BR_RELEASE 强引用计数-1请求 binder_ptr_cookie BR_INCREFS 弱引用计数+1请求 binder_ptr_cookie BR_DECREFS 若引用计数-1请求 binder_ptr_cookie BR_DEAD_BINDER 发送死亡通知 binder_uintptr_t BR_CLEAR_DEATH_NOTIFICATION_DONE 清理死亡通知完成 binder_uintptr_t BR_DEAD_REPLY 告知发送方对方已经死亡 void BR_ACQUIRE_RESULT 暂未实现 - BR_ATTEMPT_ACQUIRE 暂未实现 - BR_FINISHED 暂未实现 - 单独看上面的协议可能很难理解，这里我们以一次Binder请求过程来详细看一下Binder协议是如何通信的，就比较好理解了。 这幅图的说明如下： Binder是C/S架构的，通信过程牵涉到：Client，Server以及Binder驱动三个角色 Client对于Server的请求以及Server对于Client回复都需要通过Binder驱动来中转数据 BC_XXX命令是进程发送给驱动的命令 BR_XXX命令是驱动发送给进程的命令 整个通信过程由Binder驱动控制 2.4、binder_open()任何进程在使用Binder之前，都需要先通过open(“/dev/binder”)打开Binder设备。上文已经提到，用户空间的open系统调用对应了驱动中的binder_open函数。在这个函数，Binder驱动会为调用的进程做一些初始化工作。binder_open函数代码如下所示： 1234567891011121314151617181920212223242526272829static int binder_open(struct inode *nodp, struct file *filp)&#123; struct binder_proc *proc; // 创建进程对应的binder_proc对象 proc = kzalloc(sizeof(*proc), GFP_KERNEL); if (proc == NULL) return -ENOMEM; get_task_struct(current); proc-&gt;tsk = current; // 初始化binder_proc INIT_LIST_HEAD(&amp;proc-&gt;todo); init_waitqueue_head(&amp;proc-&gt;wait); proc-&gt;default_priority = task_nice(current); // 锁保护 binder_lock(__func__); binder_stats_created(BINDER_STAT_PROC); // 添加到全局列表binder_procs中 hlist_add_head(&amp;proc-&gt;proc_node, &amp;binder_procs); proc-&gt;pid = current-&gt;group_leader-&gt;pid; INIT_LIST_HEAD(&amp;proc-&gt;delivered_death); filp-&gt;private_data = proc; binder_unlock(__func__); return 0;&#125; 在Binder驱动中，通过binder_procs记录了所有使用Binder的进程。每个初次打开Binder设备的进程都会被添加到这个列表中的。 另外，请读者回顾一下上文介绍的Binder驱动中的几个关键结构体： binder_proc binder_node binder_thread binder_ref binder_buffer 在实现过程中，为了便于查找，这些结构体互相之间都留有字段存储关联的结构。 下面这幅图描述了这里说到的这些内容： 2.5、binder_mmap()在打开Binder设备之后，进程还会通过mmap进行内存映射。mmap的作用有如下两个： 申请一块内存空间，用来接收Binder通信过程中的数据 对这块内存进行地址映射，以便将来访问 binder_mmap函数对应了mmap系统调用的处理，这个函数也是Binder驱动的精华所在（这里说的binder_mmap函数也包括其内部调用的binder_update_page_range函数，见下文）。 前文我们说到，使用Binder机制，数据只需要经历一次拷贝就可以了，其原理就在这个函数中。 binder_mmap这个函数中，会申请一块物理内存，然后在用户空间和内核空间同时对应到这块内存上。在这之后，当有Client要发送数据给Server的时候，只需一次，将Client发送过来的数据拷贝到Server端的内核空间指定的内存地址即可，由于这个内存地址在服务端已经同时映射到用户空间，因此无需再做一次复制，Server即可直接访问，整个过程如下图所示： 这幅图的说明如下： Server在启动之后，调用对/dev/binder设备调用mmap 内核中的binder_mmap函数进行对应的处理：申请一块物理内存，然后在用户空间和内核空间同时进行映射 Client通过BINDER_WRITE_READ命令发送请求，这个请求将先到驱动中，同时需要将数据从Client进程的用户空间拷贝到内核空间 驱动通过BR_TRANSACTION通知Server有人发出请求，Server进行处理。由于这块内存也在用户空间进行了映射，因此Server进程的代码可以直接访问 了解原理之后，我们再来看一下Binder驱动的相关源码。这段代码有两个函数： binder_mmap函数对应了mmap的系统调用的处理 binder_update_page_range函数真正实现了内存分配和地址映射 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697static int binder_mmap(struct file *filp, struct vm_area_struct *vma)&#123; int ret; struct vm_struct *area; struct binder_proc *proc = filp-&gt;private_data; const char *failure_string; struct binder_buffer *buffer; ... // 在内核空间获取一块地址范围 area = get_vm_area(vma-&gt;vm_end - vma-&gt;vm_start, VM_IOREMAP); if (area == NULL) &#123; ret = -ENOMEM; failure_string = \"get_vm_area\"; goto err_get_vm_area_failed; &#125; proc-&gt;buffer = area-&gt;addr; // 记录内核空间与用户空间的地址偏移 proc-&gt;user_buffer_offset = vma-&gt;vm_start - (uintptr_t)proc-&gt;buffer; mutex_unlock(&amp;binder_mmap_lock); ... proc-&gt;pages = kzalloc(sizeof(proc-&gt;pages[0]) * ((vma-&gt;vm_end - vma-&gt;vm_start) / PAGE_SIZE), GFP_KERNEL); if (proc-&gt;pages == NULL) &#123; ret = -ENOMEM; failure_string = \"alloc page array\"; goto err_alloc_pages_failed; &#125; proc-&gt;buffer_size = vma-&gt;vm_end - vma-&gt;vm_start; vma-&gt;vm_ops = &amp;binder_vm_ops; vma-&gt;vm_private_data = proc; /* binder_update_page_range assumes preemption is disabled */ preempt_disable(); // 通过下面这个函数真正完成内存的申请和地址的映射 // 初次使用，先申请一个PAGE_SIZE大小的内存 ret = binder_update_page_range(proc, 1, proc-&gt;buffer, proc-&gt;buffer + PAGE_SIZE, vma); ...&#125;static int binder_update_page_range(struct binder_proc *proc, int allocate, void *start, void *end, struct vm_area_struct *vma)&#123; void *page_addr; unsigned long user_page_addr; struct vm_struct tmp_area; struct page **page; struct mm_struct *mm; ... for (page_addr = start; page_addr &lt; end; page_addr += PAGE_SIZE) &#123; int ret; struct page **page_array_ptr; page = &amp;proc-&gt;pages[(page_addr - proc-&gt;buffer) / PAGE_SIZE]; BUG_ON(*page); // 真正进行内存的分配 *page = alloc_page(GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO); if (*page == NULL) &#123; pr_err(\"%d: binder_alloc_buf failed for page at %p\\n\", proc-&gt;pid, page_addr); goto err_alloc_page_failed; &#125; tmp_area.addr = page_addr; tmp_area.size = PAGE_SIZE + PAGE_SIZE /* guard page? */; page_array_ptr = page; // 在内核空间进行内存映射 ret = map_vm_area(&amp;tmp_area, PAGE_KERNEL, &amp;page_array_ptr); if (ret) &#123; pr_err(\"%d: binder_alloc_buf failed to map page at %p in kernel\\n\", proc-&gt;pid, page_addr); goto err_map_kernel_failed; &#125; user_page_addr = (uintptr_t)page_addr + proc-&gt;user_buffer_offset; // 在用户空间进行内存映射 ret = vm_insert_page(vma, user_page_addr, page[0]); if (ret) &#123; pr_err(\"%d: binder_alloc_buf failed to map page at %lx in userspace\\n\", proc-&gt;pid, user_page_addr); goto err_vm_insert_page_failed; &#125; /* vm_insert_page does not seem to increment the refcount */ &#125; if (mm) &#123; up_write(&amp;mm-&gt;mmap_sem); mmput(mm); &#125; preempt_disable(); return 0;... binder_update_page_range主要完成工作：分配物理空间，将物理空间映射到内核空间，将物理空间映射到进程空间. 另外，不同参数下该方法也可以释放物理页面。 2.6、binder_ioctl()内存管理上文中，我们看到binder_mmap的时候，会申请一个PAGE_SIZE(通常是4K)的内存。而实际使用过程中，一个PAGE_SIZE的大小通常是不够的。 在驱动中，会根据实际的使用情况进行内存的分配。有内存的分配，当然也需要内存的释放。这里我们就来看看Binder驱动中是如何进行内存的管理的。 首先，我们还是从一次IPC请求说起。 当一个Client想要对Server发出请求时，它首先将请求发送到Binder设备上，由Binder驱动根据请求的信息找到对应的目标节点，然后将请求数据传递过去。 进程通过ioctl系统调用来发出请求：ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr) 这里的bs-&gt;fd对应了打开Binder设备时的fd。BINDER_WRITE_READ对应了具体要做的操作码，这个操作码将由Binder驱动解析。bwr存储了请求数据，其类型是binder_write_read。 binder_write_read其实是一个相对外层的数据结构，其内部会包含一个binder_transaction_data结构的数据。binder_transaction_data包含了发出请求者的标识，请求的目标对象以及请求所需要的参数。它们的关系如下图所示： binder_ioctl函数对应了ioctl系统调用的处理。这个函数的逻辑比较简单，就是根据ioctl的命令来确定进一步处理的逻辑，具体如下: ● 如果命令是BINDER_WRITE_READ，并且● 如果 bwr.write_size &gt; 0，则调用binder_thread_write● 如果 bwr.read_size &gt; 0，则调用binder_thread_read● 如果命令是BINDER_SET_MAX_THREADS，则设置进程的max_threads，即进程支持的最大线程数● 如果命令是BINDER_SET_CONTEXT_MGR，则设置当前进程为ServiceManager，见下文● 如果命令是BINDER_THREAD_EXIT，则调用binder_free_thread，释放binder_thread● 如果命令是BINDER_VERSION，则返回当前的Binder版本号 这其中，最关键的就是binder_thread_write方法。当Client请求Server的时候，便会发送一个BINDER_WRITE_READ命令，同时框架会将将实际的数据包装好。此时，binder_transaction_data中的code将是BC_TRANSACTION，由此便会调用到binder_transaction方法，这个方法是对一次Binder事务的处理，这其中会调用binder_alloc_buf函数为此次事务申请一个缓存。 12345678910111213141516struct binder_buffer &#123; struct list_head entry; struct rb_node rb_node; unsigned free:1; unsigned allow_user_free:1; unsigned async_transaction:1; unsigned debug_id:29; struct binder_transaction *transaction; struct binder_node *target_node; size_t data_size; size_t offsets_size; uint8_t data[0];&#125;; 而在binder_proc（描述了使用Binder的进程）中，包含了几个字段用来管理进程在Binder IPC过程中缓存，如下： 12345678910struct binder_proc &#123; ... struct list_head buffers; // 进程拥有的buffer列表 struct rb_root free_buffers; // 空闲buffer列表 struct rb_root allocated_buffers; // 已使用的buffer列表 size_t free_async_space; // 剩余的异步调用的空间 size_t buffer_size; // 缓存的上限 ...&#125;; 进程在mmap时，会设定支持的总缓存大小的上限（下文会讲到）。而进程每当收到BC_TRANSACTION，就会判断已使用缓存加本次申请的和有没有超过上限。如果没有，就考虑进行内存的分配。 进程的空闲缓存记录在binder_proc的free_buffers中，这是一个以红黑树形式存储的结构。每次尝试分配缓存的时候，会从这里面按大小顺序进行查找，找到最接近需要的一块缓存。查找的逻辑如下： 123456789101112131415while (n) &#123; buffer = rb_entry(n, struct binder_buffer, rb_node); BUG_ON(!buffer-&gt;free); buffer_size = binder_buffer_size(proc, buffer); if (size &lt; buffer_size) &#123; best_fit = n; n = n-&gt;rb_left; &#125; else if (size &gt; buffer_size) n = n-&gt;rb_right; else &#123; best_fit = n; break; &#125;&#125; 找到之后，还需要对binder_proc中的字段进行相应的更新： 123456789101112131415161718192021rb_erase(best_fit, &amp;proc-&gt;free_buffers);buffer-&gt;free = 0;binder_insert_allocated_buffer(proc, buffer);if (buffer_size != size) &#123; struct binder_buffer *new_buffer = (void *)buffer-&gt;data + size; list_add(&amp;new_buffer-&gt;entry, &amp;buffer-&gt;entry); new_buffer-&gt;free = 1; binder_insert_free_buffer(proc, new_buffer);&#125;binder_debug(BINDER_DEBUG_BUFFER_ALLOC, \"%d: binder_alloc_buf size %zd got %p\\n\", proc-&gt;pid, size, buffer);buffer-&gt;data_size = data_size;buffer-&gt;offsets_size = offsets_size;buffer-&gt;async_transaction = is_async;if (is_async) &#123; proc-&gt;free_async_space -= size + sizeof(struct binder_buffer); binder_debug(BINDER_DEBUG_BUFFER_ALLOC_ASYNC, \"%d: binder_alloc_buf size %zd async free %zd\\n\", proc-&gt;pid, size, proc-&gt;free_async_space);&#125; 下面我们再来看看内存的释放。 BC_FREE_BUFFER命令是通知驱动进行内存的释放，binder_free_buf函数是真正实现的逻辑，这个函数与binder_alloc_buf是刚好对应的。在这个函数中，所做的事情包括： 重新计算进程的空闲缓存大小 通过binder_update_page_range释放内存 更新binder_proc的buffers，free_buffers，allocated_buffers字段 2.7、Binder中的”面向对象”Binder机制淡化了进程的边界，使得跨越进程也能够调用到指定服务的方法，其原因是因为Binder机制在底层处理了在进程间的”对象”传递。 在Binder驱动中，并不是真的将对象在进程间来回序列化，而是通过特定的标识来进行对象的传递。Binder驱动中，通过flat_binder_object来描述需要跨越进程传递的对象。其定义如下： 12345678910struct flat_binder_object &#123; __u32 type; __u32 flags; union &#123; binder_uintptr_t binder; /* local object */ __u32 handle; /* remote object */ &#125;; binder_uintptr_t cookie;&#125;; 这其中，type有如下5种类型。 1234567enum &#123; BINDER_TYPE_BINDER = B_PACK_CHARS('s', 'b', '*', B_TYPE_LARGE), BINDER_TYPE_WEAK_BINDER = B_PACK_CHARS('w', 'b', '*', B_TYPE_LARGE), BINDER_TYPE_HANDLE = B_PACK_CHARS('s', 'h', '*', B_TYPE_LARGE), BINDER_TYPE_WEAK_HANDLE = B_PACK_CHARS('w', 'h', '*', B_TYPE_LARGE), BINDER_TYPE_FD = B_PACK_CHARS('f', 'd', '*', B_TYPE_LARGE),&#125;; 当对象传递到Binder驱动中的时候，由驱动来进行翻译和解释，然后传递到接收的进程。 例如当Server把Binder实体传递给Client时，在发送数据流中，flat_binder_object中的type是BINDER_TYPE_BINDER，同时binder字段指向Server进程用户空间地址。但这个地址对于Client进程是没有意义的（Linux中，每个进程的地址空间是互相隔离的），驱动必须对数据流中的flat_binder_object做相应的翻译：将type该成BINDER_TYPE_HANDLE；为这个Binder在接收进程中创建位于内核中的引用并将引用号填入handle中。对于发生数据流中引用类型的Binder也要做同样转换。经过处理后接收进程从数据流中取得的Binder引用才是有效的，才可以将其填入数据包binder_transaction_data的target.handle域，向Binder实体发送请求。 由于每个请求和请求的返回都会经历内核的翻译，因此这个过程从进程的角度来看是完全透明的。进程完全不用感知这个过程，就好像对象真的在进程间来回传递一样。 2.8、驱动层的线程管理上文多次提到，Binder本身是C/S架构。由Server提供服务，被Client使用。既然是C/S架构，就可能存在多个Client会同时访问Server的情况。 在这种情况下，如果Server只有一个线程处理响应，就会导致客户端的请求可能需要排队而导致响应过慢的现象发生。解决这个问题的方法就是引入多线程。 Binder机制的设计从最底层–驱动层，就考虑到了对于多线程的支持。具体内容如下： 使用Binder的进程在启动之后，通过BINDER_SET_MAX_THREADS告知驱动其支持的最大线程数量 驱动会对线程进行管理。在binder_proc结构中，这些字段记录了进程中线程的信息：max_threads，requested_threads，requested_threads_started，ready_threads binder_thread结构对应了Binder进程中的线程 驱动通过BR_SPAWN_LOOPER命令告知进程需要创建一个新的线程 进程通过BC_ENTER_LOOPER命令告知驱动其主线程已经ready 进程通过BC_REGISTER_LOOPER命令告知驱动其子线程（非主线程）已经ready 进程通过BC_EXIT_LOOPER命令告知驱动其线程将要退出 在线程退出之后，通过BINDER_THREAD_EXIT告知Binder驱动。驱动将对应的binder_thread对象销毁 2.9、再聊ServiceManager上文已经说过，每一个Binder Server在驱动中会有一个binder_node进行对应。同时，Binder驱动会负责在进程间传递服务对象，并负责底层的转换。另外，我们也提到，每一个Binder服务都需要有一个唯一的名称。由ServiceManager来管理这些服务的注册和查找。 而实际上，为了便于使用，ServiceManager本身也实现为一个Server对象。任何进程在使用ServiceManager的时候，都需要先拿到指向它的标识。然后通过这个标识来使用ServiceManager。 这似乎形成了一个互相矛盾的现象： 通过ServiceManager我们才能拿到Server的标识 ServiceManager本身也是一个Server 解决这个矛盾的办法其实也很简单：Binder机制为ServiceManager预留了一个特殊的位置。这个位置是预先定好的，任何想要使用ServiceManager的进程只要通过这个特定的位置就可以访问到ServiceManager了（而不用再通过ServiceManager的接口）。 在Binder驱动中，有一个全局的binder_node 变量： 一般情况下，对于每一个Server驱动层会对应一个binder_node节点，然而binder_context_mgr_node比较特殊，它没有对应的应用层binder实体。在整个系统里，它是如此特殊，以至于系统规定，任何应用都必须使用句柄0来跨进程地访问它。 1static struct binder_node *binder_context_mgr_node; 这个变量指向的就是ServiceManager。 当有进程通过ioctl并指定命令为BINDER_SET_CONTEXT_MGR的时候，驱动被认定这个进程是ServiceManager，binder_ioctl()函数中对应的处理如下： 12345678910111213case BINDER_SET_CONTEXT_MGR: if (binder_context_mgr_node != NULL) &#123; &#125; ret = security_binder_set_context_mgr(proc-&gt;tsk); else &#123; binder_context_mgr_uid = current-&gt;cred-&gt;euid; binder_context_mgr_node = binder_new_node(proc, 0, 0);//在Binder驱动层创建binder_node结构体对象 binder_context_mgr_node-&gt;local_weak_refs++; binder_context_mgr_node-&gt;local_strong_refs++; binder_context_mgr_node-&gt;has_strong_ref = 1; binder_context_mgr_node-&gt;has_weak_ref = 1; &#125; break; ServiceManager应当要先于所有Binder Server之前启动。在它启动完成并告知Binder驱动之后，驱动便设定好了这个特定的节点。 在这之后，当有其他模块想要使用ServerManager的时候，只要将请求指向ServiceManager所在的位置即可。 在Binder驱动中，通过handle = 0这个位置来访问ServiceManager。例如，binder_transaction中，判断如果target.handler为0，则认为这个请求是发送给ServiceManager的，相关代码如下： 123456789101112131415161718if (tr-&gt;target.handle) &#123; struct binder_ref *ref; ref = binder_get_ref(proc, tr-&gt;target.handle, true); if (ref == NULL) &#123; binder_user_error(\"%d:%d got transaction to invalid handle\\n\", proc-&gt;pid, thread-&gt;pid); return_error = BR_FAILED_REPLY; goto err_invalid_target_handle; &#125; target_node = ref-&gt;node;&#125; else &#123; target_node = binder_context_mgr_node; if (target_node == NULL) &#123; return_error = BR_DEAD_REPLY; goto err_no_context_mgr_node; &#125;&#125; 2.10、binder_node等重要结构体 binder_proc binder_node binder_thread binder_ref binder_buffer 1. Binder实体binder_node Binder实体，是各个Server以及ServiceManager在内核中的存在形式。 Binder实体实际上是内核中binder_node结构体的对象，它的作用是在内核中保存Server和ServiceManager的信息(例如，Binder实体中保存了Server对象在用户空间的地址)。简言之，Binder实体是Server在Binder驱动中的存在形式，内核通过Binder实体可以找到用户空间的Server对象。 在上图中，Server和ServiceManager在Binder驱动中都对应的存在一个Binder实体。 2. Binder引用binder_ref 说到Binder实体，就不得不说”Binder引用”。所谓Binder引用，实际上是内核中binder_ref结构体的对象，它的作用是在表示”Binder实体”的引用。换句话说，每一个Binder引用都是某一个Binder实体的引用，通过Binder引用可以在内核中找到它对应的Binder实体。 如果将Server看作是Binder实体的话，那么Client就好比Binder引用。Client要和Server通信，它就是通过保存一个Server对象的Binder引用，再通过该Binder引用在内核中找到对应的Binder实体，进而找到Server对象，然后将通信内容发送给Server对象。 Binder实体和Binder引用都是内核(即，Binder驱动)中的数据结构。每一个Server在内核中就表现为一个Binder实体，而每一个Client则表现为一个Binder引用。这样，每个Binder引用都对应一个Binder实体，而每个Binder实体则可以多个Binder引用。 3、Binder buffer：binder_buffer 4、Binder进程binder_proc 5、Binder线程binder_thread binder机制到底是如何从Binder对象找到其对应的Binder实体呢？ 注意其中的那4个rb_root域，”rb”的意思是”red black”，可见binder_proc里搞出了4个红黑树。 其中，nodes树用于记录binder实体，refs_by_desc树和refs_by_node树则用于记录binder代理。之所以会有两个代理树，是为了便于快速查找，我们暂时只关心其中之一就可以了。threads树用于记录执行传输动作的线程信息。 在一个进程中，有多少”被其他进程进行跨进程调用的”binder实体，就会在该进程对应的nodes树中生成多少个红黑树节点。另一方面，一个进程要访问多少其他进程的binder实体，则必须在其refs_by_desc树中拥有对应的引用节点。 这4棵树的节点类型是不同的，threads树的节点类型为binder_thread，nodes树的节点类型为binder_node，refs_by_desc树和refs_by_node树的节点类型相同，为binder_ref。这些节点内部都会包含rb_node子结构，该结构专门负责连接节点的工作，和前文的hlist_node有点儿异曲同工，这也是linux上一个常用的小技巧。我们以nodes树为例 nodes树是用于记录binder实体的，所以nodes树中的每个binder_node节点，必须能够记录下相应binder实体的信息。因此请大家注意binder_node的ptr域和cookie域。 另一方面，refs_by_desc树和refs_by_node树的每个binder_ref节点则和上层的一个BpBinder对应，而且更重要的是，它必须具有和”目标binder实体的binder_node”进行关联的信息。 请注意binder_ref的那个node域，它负责和binder_node关联。另外，binder_ref中有两个类型为rb_node的域：rb_node_desc域和rb_node_node域，它们分别用于连接refs_by_desc树和refs_by_node。也就是说虽然binder_proc中有两棵引用树，但这两棵树用到的具体binder_ref节点其实是复用的。 binder_node.ptr对应于flat_binder_object.binder； binder_node.cookie对应于flat_binder_object.cookie。 OK，现在我们可以更深入地说明binder句柄的作用了，比如进程1的BpBinder在发起跨进程调用时，向binder驱动传入了自己记录的句柄值，binder驱动就会在”进程1对应的binder_proc结构”的引用树中查找和句柄值相符的binder_ref节点，一旦找到binder_ref节点，就可以通过该节点的node域找到对应的binder_node节点，这个目标binder_node当然是从属于进程2的binder_proc啦，不过不要紧，因为binder_ref和binder_node都处于binder驱动的地址空间中，所以是可以用指针直接指向的。目标binder_node节点的cookie域，记录的其实是进程2中BBinder的地址，binder驱动只需把这个值反映给应用层，应用层就可以直接拿到BBinder了。这就是Binder完成精确打击的大体过程。 三、Android Binder系统驱动情景分析为了更深刻的了解Binder系统 注册服务、获取服务、使用服务的过程，在Driver层(kernel/drivers/staging/android/binder.c)的binder_thread_read()函数、binder_transaction()函数入打印log，让前面编写的C程序示例与binder驱动交互打印更详细的过程。 12345static void binder_transaction(struct binder_proc *proc, struct binder_thread *thread, struct binder_transaction_data *tr, int reply)int binder_thread_write(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed) 已添加好打印log的binder.c文件见GitHub（注：搜索[/* print] 关键字） 事先已经准备好打印log，现在结合log和Binder事务处理开始详细分析。注：log稍后分析再贴出。 （1）、Binder系统驱动情景分析–服务”Hello”注册过程 1.1、ServiceManager休眠等待回顾一下ServiceManager启动流程，ServiceManager进入binder_loop()后 会休眠等待响应client请求。 12345678910111213141516171819202122binder_loop()&#123; // 告诉Kernel，ServiceManager进程进入了消息循环状态。 readbuf[0] = BC_ENTER_LOOPER; binder_write(bs, readbuf, sizeof(unsigned)); bwr.write_size = 0; bwr.write_consumed = 0; bwr.write_buffer = 0; for (;;) &#123; bwr.read_size = sizeof(readbuf); bwr.read_consumed = 0; bwr.read_buffer = (unsigned) readbuf; // 向Kernel中发送消息(先写后读)。 // 先将消息传递给Kernel，然后再从Kernel读取消息反馈 res = ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr); // 解析读取的消息反馈 res = binder_parse(bs, 0, readbuf, bwr.read_consumed, func); ... &#125;&#125; binder_write(bs, readbuf, sizeof(unsigned));会调用ioctl向内核发送数据。 1234567891011121314 int binder_write(struct binder_state *bs, void *data, size_t len)&#123; struct binder_write_read bwr; int res; bwr.write_size = len; bwr.write_consumed = 0; bwr.write_buffer = (uintptr_t) data; bwr.read_size = 0; bwr.read_consumed = 0; bwr.read_buffer = 0; res = ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr); return res;&#125; 如果 bwr.write_size &gt; 0，则调用binder_thread_write 如果 bwr.read_size &gt;0，则调用binder_thread_read 123456789101112131415161718192021222324252627282930313233343536373839static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)&#123; int ret; struct binder_proc *proc = filp-&gt;private_data; struct binder_thread *thread; unsigned int size = _IOC_SIZE(cmd); void __user *ubuf = (void __user *)arg; // 中断等待函数。 ret = wait_event_interruptible(...); // 在proc进程中查找该线程对应的binder_thread；若查找失败，则新建一个binder_thread，并添加到proc-&gt;threads中。 thread = binder_get_thread(proc); ... switch (cmd) &#123; case BINDER_WRITE_READ: &#123; struct binder_write_read bwr; ... // 将binder_write_read从\"用户空间\" 拷贝到 \"内核空间\" if (copy_from_user(&amp;bwr, ubuf, sizeof(bwr))) &#123; ... &#125; // 如果write_size&gt;0，则进行写操作 if (bwr.write_size &gt; 0) &#123; ret = binder_thread_write(proc, thread, (void __user *)bwr.write_buffer, bwr.write_size, &amp;bwr.write_consumed); ... &#125; // 如果read_size&gt;0，则进行读操作 if (bwr.read_size &gt; 0) &#123; ret = binder_thread_read(proc, thread, (void __user *)bwr.read_buffer, bwr.read_size, &amp;bwr.read_consumed, filp-&gt;f_flags &amp; O_NONBLOCK); ... &#125; ... if (copy_to_user(ubuf, &amp;bwr, sizeof(bwr))) &#123; &#125; break; &#125; &#125; return ret;&#125; bwr.write_size &gt; 0; 继续查看binder_thread_write() 注：只有BR_TRANSACTION、BR_REPLY、BC_TRANSACTION、BC_REPLY涉及两进程 其他所有BC_XXX、BR_XXX都只是App和驱动交互用于改变报告状态。 1234567891011121314151617181920212223242526int binder_thread_write(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed)&#123; uint32_t cmd; void __user *ptr = buffer + *consumed; void __user *end = buffer + size; // 读取binder_write_read.write_buffer中的内容。 // 每次读取32bit(即4个字节) while (ptr &lt; end &amp;&amp; thread-&gt;return_error == BR_OK) &#123; // 从用户空间读取32bit到内核中，并赋值给cmd。 if (get_user(cmd, (uint32_t __user *)ptr)) return -EFAULT; ptr += sizeof(uint32_t); switch (cmd) &#123; case BC_ENTER_LOOPER: thread-&gt;looper |= BINDER_LOOPER_STATE_ENTERED; break; ... &#125; // 更新bwr.write_consumed的值 *consumed = ptr - buffer; &#125; return 0;&#125; 当前线程进入BC_ENTER_LOOPER状态，等待请求。 继续binder_loop()中的for(;;;)循环，bwr.read_size &gt;0;会通过binder_thread_read()读操作。 123456789101112131415161718static int binder_thread_read(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed, int non_block)&#123; void __user *ptr = buffer + *consumed; void __user *end = buffer + size; int ret = 0; int wait_for_proc_work; // 如果*consumed=0，则写入BR_NOOP到用户传进来的bwr.read_buffer缓存区 if (*consumed == 0) &#123; if (put_user(BR_NOOP, (uint32_t __user *)ptr)) return -EFAULT; // 修改指针位置 ptr += sizeof(uint32_t); &#125; ...&#125; 可以看到驱动put_user(BR_NOOP, (uint32_t __user *)ptr)发送BR_NOOP到ServiceManager 对于所有的读操作，数据头都是BR_NOOP，如BR_REPLY 1234&gt; ./service_manager &amp;&gt; [ 32.566620] service_manager (1362, 1362), binder_thread_write : BC_ENTER_LOOPER&gt; [ 32.566712] service_manager (1362, 1362), binder_thread_read : BR_NOOP&gt; 1.2、Clent（此处为Test_server）请求SM添加服务构造数据发送给驱动 我们执行Test_server时，打印了很多数据，我们首先看一下数据的构造过程 和 组织格式，这有助于加深我们对binder系统的理解。 123456789101112131415161718int svcmgr_publish(struct binder_state *bs, uint32_t target, const char *name, void *ptr)&#123; int status; unsigned iodata[512/4]; struct binder_io msg, reply; bio_init(&amp;msg, iodata, sizeof(iodata), 4); bio_put_uint32(&amp;msg, 0); // strict mode header bio_put_string16_x(&amp;msg, SVC_MGR_NAME); bio_put_string16_x(&amp;msg, name); bio_put_obj(&amp;msg, ptr); if (binder_call(bs, &amp;msg, &amp;reply, target, SVC_MGR_ADD_SERVICE)) return -1; status = bio_get_uint32(&amp;reply); binder_done(bs, &amp;msg, &amp;reply); return status;&#125; bio_init()、bio_put_uint32()、bio_put_string16_x()函数比较简洁。我们看下bio_put_obj()函数。 构建初始化flat_binder_object结构体： 12345678910111213void bio_put_obj(struct binder_io *bio, void *ptr)&#123; struct flat_binder_object *obj; obj = bio_alloc_obj(bio); if (!obj) return; obj-&gt;flags = 0x7f | FLAT_BINDER_FLAG_ACCEPTS_FDS;// obj-&gt;type = BINDER_TYPE_BINDER;// obj-&gt;binder = (uintptr_t)ptr;// obj-&gt;cookie = 0;//0&#125; 数据结构示意图： Clent（此处为Test_server），test_server.c调用流程： -&gt;svcmgr_publish() -&gt;binder_call() -&gt;ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr) -&gt;binder_thread_write() -&gt;binder_transaction() 现在数据构造好了，binder_call()会调用ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr) 123456789101112131415161718192021222324252627282930313233343536int binder_call(struct binder_state *bs, struct binder_io *msg, struct binder_io *reply, uint32_t target, uint32_t code)&#123; int res; struct binder_write_read bwr; struct &#123; uint32_t cmd; struct binder_transaction_data txn; &#125; __attribute__((packed)) writebuf; unsigned readbuf[32]; writebuf.cmd = BC_TRANSACTION; writebuf.txn.target.handle = target; writebuf.txn.code = code; writebuf.txn.flags = 0; writebuf.txn.data_size = msg-&gt;data - msg-&gt;data0; writebuf.txn.offsets_size = ((char*) msg-&gt;offs) - ((char*) msg-&gt;offs0); writebuf.txn.data.ptr.buffer = (uintptr_t)msg-&gt;data0; writebuf.txn.data.ptr.offsets = (uintptr_t)msg-&gt;offs0; bwr.write_size = sizeof(writebuf); bwr.write_consumed = 0; bwr.write_buffer = (uintptr_t) &amp;writebuf; hexdump(msg-&gt;data0, msg-&gt;data - msg-&gt;data0); for (;;) &#123; bwr.read_size = sizeof(readbuf); bwr.read_consumed = 0; bwr.read_buffer = (uintptr_t) readbuf; res = ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr); ... res = binder_parse(bs, reply, (uintptr_t) readbuf, bwr.read_consumed, 0); &#125;&#125; [ 38.320197] test_server (1363, 1363), binder_thread_write : BC_TRANSACTION 发送数据，进而会调用binder_thread_write()处理数据。 123456789101112131415161718192021222324252627282930313233343536373839binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)&#123; int ret; struct binder_proc *proc = filp-&gt;private_data; struct binder_thread *thread; unsigned int size = _IOC_SIZE(cmd); void __user *ubuf = (void __user *)arg; // 中断等待函数。 ret = wait_event_interruptible(...); // 在proc进程中查找该线程对应的binder_thread；若查找失败，则新建一个binder_thread，并添加到proc-&gt;threads中。 thread = binder_get_thread(proc); ... switch (cmd) &#123; case BINDER_WRITE_READ: &#123; struct binder_write_read bwr; ... // 将binder_write_read从\"用户空间\" 拷贝到 \"内核空间\" if (copy_from_user(&amp;bwr, ubuf, sizeof(bwr))) &#123; ... &#125; // 如果write_size&gt;0，则进行写操作 if (bwr.write_size &gt; 0) &#123; ret = binder_thread_write(proc, thread, (void __user *)bwr.write_buffer, bwr.write_size, &amp;bwr.write_consumed); ... &#125; // 如果read_size&gt;0，则进行读操作 if (bwr.read_size &gt; 0) &#123; ret = binder_thread_read(proc, thread, (void __user *)bwr.read_buffer, bwr.read_size, &amp;bwr.read_consumed, filp-&gt;f_flags &amp; O_NONBLOCK); ... &#125; ... if (copy_to_user(ubuf, &amp;bwr, sizeof(bwr))) &#123; &#125; break; &#125; &#125; return ret;&#125; 由于write_size&gt;0，调用binder_thread_write()处理数据： 1234567891011121314151617181920212223242526272829303132333435int binder_thread_write(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed)&#123; uint32_t cmd; void __user *ptr = buffer + *consumed; void __user *end = buffer + size; // 读取binder_write_read.write_buffer中的内容。 // 每次读取32bit(即4个字节) while (ptr &lt; end &amp;&amp; thread-&gt;return_error == BR_OK) &#123; // 从用户空间读取32bit到内核中，并赋值给cmd。 if (get_user(cmd, (uint32_t __user *)ptr)) return -EFAULT; ptr += sizeof(uint32_t); ... switch (cmd) &#123; ... case BC_TRANSACTION: case BC_REPLY: &#123; struct binder_transaction_data tr; if (copy_from_user(&amp;tr, ptr, sizeof(tr))) return -EFAULT; ptr += sizeof(tr); binder_transaction(proc, thread, &amp;tr, cmd == BC_REPLY); break; &#125; ... &#125; // 更新bwr.write_consumed的值 *consumed = ptr - buffer; &#125; return 0;&#125; 由之前binder_call()分析，writebuf.cmd = BC_TRANSACTION;会执行binder_transaction()函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143static void binder_transaction(struct binder_proc *proc, struct binder_thread *thread, struct binder_transaction_data *tr, int reply)&#123; struct binder_transaction *t; struct binder_work *tcomplete; size_t *offp, *off_end; struct binder_proc *target_proc; struct binder_thread *target_thread = NULL; struct binder_node *target_node = NULL; struct list_head *target_list; wait_queue_head_t *target_wait; struct binder_transaction *in_reply_to = NULL; struct binder_transaction_log_entry *e; uint32_t return_error; ... if (reply) &#123; &#125; else &#123; if (tr-&gt;target.handle) &#123; target_node = ref-&gt;node; &#125; else &#123; // 事务目标对象是ServiceManager的binder实体 // 即，该事务是交给Service Manager来处理的。 target_node = binder_context_mgr_node; &#125; // 设置处理事务的目标进程 target_proc = target_node-&gt;proc; ... &#125; if (target_thread) &#123; ... &#125; else &#123; target_list = &amp;target_proc-&gt;todo; target_wait = &amp;target_proc-&gt;wait; &#125; ... // 分配一个待处理的事务t，t是binder事务(binder_transaction对象) t = kzalloc(sizeof(*t), GFP_KERNEL); // 分配一个待完成的工作tcomplete，tcomplete是binder_work对象。 tcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL); ... // 事务将交给target_proc进程进行处理 t-&gt;to_proc = target_proc; // 事务将交给target_thread线程进行处理 t-&gt;to_thread = target_thread; ... // 分配空间,从目的进程映射的空间分配buf t-&gt;buffer = binder_alloc_buf(target_proc, tr-&gt;data_size, tr-&gt;offsets_size, !reply &amp;&amp; (t-&gt;flags &amp; TF_ONE_WAY)); ... // 保存事务 t-&gt;buffer-&gt;transaction = t; // 保存事务的目标对象(即处理该事务的binder对象) t-&gt;buffer-&gt;target_node = target_node; offp = (size_t *)(t-&gt;buffer-&gt;data + ALIGN(tr-&gt;data_size, sizeof(void *))); // 将\"用户空间的数据\"拷贝到内核中 // tr-&gt;data.ptr.buffer就是用户空间数据的起始地址，tr-&gt;data_size就是数据大小 if (copy_from_user(t-&gt;buffer-&gt;data, tr-&gt;data.ptr.buffer, tr-&gt;data_size)) &#123; ... &#125; // 将\"用户空间的数据中所含对象的偏移地址\"拷贝到内核中 // tr-&gt;data.ptr.offsets就是数据中的对象偏移地址数组，tr-&gt;offsets_size就数据中的对象个数 // 拷贝之后，offp就是flat_binder_object对象数组在内核空间的偏移数组的起始地址 if (copy_from_user(offp, tr-&gt;data.ptr.offsets, tr-&gt;offsets_size)) &#123; ... &#125; ... // off_end就是flat_binder_object对象数组在内核空间的偏移地址的结束地址 off_end = (void *)offp + tr-&gt;offsets_size; // 将所有的flat_binder_object对象读取出来 // 对TestServer而言，只有一个flat_binder_object对象。 for (; offp &lt; off_end; offp++) &#123; struct flat_binder_object *fp; ... fp = (struct flat_binder_object *)(t-&gt;buffer-&gt;data + *offp); switch (fp-&gt;type) &#123; case BINDER_TYPE_BINDER: case BINDER_TYPE_WEAK_BINDER: &#123; struct binder_ref *ref; // 在proc中查找binder实体对应的binder_node struct binder_node *node = binder_get_node(proc, fp-&gt;binder); // 若找不到，则新建一个binder_node；下次就可以直接使用了。 if (node == NULL) &#123; node = binder_new_node(proc, fp-&gt;binder, fp-&gt;cookie); &#125; ... // 在target_proc(即，ServiceManager的进程上下文)中查找是否包行\"该Binder实体的引用\"， // 如果没有找到的话，则将\"该binder实体的引用\"添加到target_proc-&gt;refs_by_node红黑树中。这样，就可以通过Service Manager对该Binder实体进行管理了。 ref = binder_get_ref_for_node(target_proc, node); // 现在修改目的进程type，表示ServiceManager持有TestServer引用，TestServer进程才能拥有实体。 if (fp-&gt;type == BINDER_TYPE_BINDER) fp-&gt;type = BINDER_TYPE_HANDLE; else fp-&gt;type = BINDER_TYPE_WEAK_HANDLE; // 修改handle。handle和binder是联合体，这里将handle设为引用的描述。 // 根据该handle可以找到\"该binder实体在target_proc中的binder引用\"； // 即，可以根据该handle，可以从Service Manager找到对应的Binder实体的引用，从而获取Binder实体。 fp-&gt;handle = ref-&gt;desc; // 增加引用计数，防止\"该binder实体\"在使用过程中被销毁。 binder_inc_ref(ref, fp-&gt;type == BINDER_TYPE_HANDLE, &amp;thread-&gt;todo); ... &#125; break; ... &#125; &#125; if (reply) &#123; .. &#125; else if (!(t-&gt;flags &amp; TF_ONE_WAY)) &#123; BUG_ON(t-&gt;buffer-&gt;async_transaction != 0); t-&gt;need_reply = 1; t-&gt;from_parent = thread-&gt;transaction_stack; // 将当前事务添加到当前线程的事务栈中 thread-&gt;transaction_stack = t; &#125; else &#123; ... &#125; // 设置事务的类型为BINDER_WORK_TRANSACTION t-&gt;work.type = BINDER_WORK_TRANSACTION; // 将事务添加到target_list队列中，即target_list的待处理事务中 list_add_tail(&amp;t-&gt;work.entry, target_list); // 设置待完成工作的类型为BINDER_WORK_TRANSACTION_COMPLETE tcomplete-&gt;type = BINDER_WORK_TRANSACTION_COMPLETE; // 将待完成工作添加到thread-&gt;todo队列中，即当前线程的待完成工作中。 list_add_tail(&amp;tcomplete-&gt;entry, &amp;thread-&gt;todo); // 唤醒目标进程 if (target_wait) wake_up_interruptible(target_wait); return; ...&#125; 说明：这里的tr-&gt;target.handle=0，因此，会设置target_node为ServiceManager对应的Binder实体。下面是target_node,target_proc等值初始化之后的值。 1234target_node = binder_context_mgr_node; // 目标节点为Service Manager对应的Binder实体target_proc = target_node-&gt;proc; // 目标进程为Service Manager对应的binder_proc进程上下文信息target_list = &amp;target_thread-&gt;todo; // 待处理事务队列target_wait = &amp;target_thread-&gt;wait; // 等待队列 小结： 驱动接收到TestServer发送的数据后，驱动主要工作： （1）根据Handle = 0 找到目的进程ServiceManager （2）把数据通过copy_from_user()放到目的进程ServiceManager的空间（mmap） （3）处理offs数据，即解析flat_binder_object结构体 a. 为TestServer构造binder_node node = binder_new_node(proc, fp-&gt;binder, fp-&gt;cookie); b.构造binder_ref给目的进程ServiceManager ref = binder_get_ref_for_node(target_proc, node); c.增加引用计数TestServer binder_inc_ref(ref, fp-&gt;type == BINDER_TYPE_HANDLE, &amp;thread-&gt;todo); 增加引用计数会添加work.entry（BR_INCREFS、BR_ACQUIR）到TestServer todod队列 list_add_tail(&amp;node-&gt;work.entry, target_list) 说明：就新建Binder实体的引用，并将其添加到target_proc-&gt;refs_by_node红黑树 和 target_proc-&gt;refs_by_desc红黑树中。 这样，ServiceManager的进程上下文中就存在Hello Service的Binder引用，ServiceManager也就可以对Hello Service进行管理了！然后，修改fp-&gt;type=BINDER_TYPE_HANDLE，并使fp-&gt;handle = ref-&gt;desc。 （4)新建一个待处理事务t和待完成的工作tcomplete，并对它们进行初始化。待处理事务t会被提交给目标(即ServiceManager对应的Binder实体)进行处理；而待完成的工作tcomplete则是为了反馈给TestServer服务，告诉TestServer它的请求Binder驱动已经收到了。注意，这里仅仅是告诉TestServer该请求已经被收到，而不是处理完毕！待ServiceManager处理完毕该请求之后，Binder驱动会再次反馈相应的消息给TestServer。 （5）binder_thread_write()中执行binder_transaction()后，会更新*consumed的值，即bwr.write_consumed的值 （6）此时，TestServer进程还会继续运行，而且它也通过wake_up_interruptible()唤醒了ServiceManager进程。 12// 更新bwr.write_consumed的值*consumed = ptr - buffer; 接下来，ioctl()会执行binder_thread_read()来设置反馈数据给TestServer进程 123456789101112131415161718192021222324252627282930313233343536 static int binder_thread_read(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed, int non_block)&#123; // 如果*consumed=0，则写入BR_NOOP到用户传进来的bwr.read_buffer缓存区 if (*consumed == 0) &#123; if (put_user(BR_NOOP, (uint32_t __user *)ptr)) return -EFAULT; ptr += sizeof(uint32_t); &#125; ... while (1) &#123; uint32_t cmd; struct binder_transaction_data tr; struct binder_work *w; struct binder_transaction *t = NULL; // 如果当前线程的\"待完成工作\"不为空，则取出待完成工作。 if (!list_empty(&amp;thread-&gt;todo)) w = list_first_entry(&amp;thread-&gt;todo, struct binder_work, entry); &#125; ... switch (w-&gt;type) &#123; ... case BINDER_WORK_TRANSACTION_COMPLETE: &#123; cmd = BR_TRANSACTION_COMPLETE; // 将BR_TRANSACTION_COMPLETE写入到用户缓冲空间中 if (put_user(cmd, (uint32_t __user *)ptr)) return -EFAULT; &#125; break; ... // 更新bwr.read_consumed的值 *consumed = ptr - buffer;&#125; 首先发送BR_NOOP给TestServer，然后处理todo队列，处理完成后会发送BR_TRANSACTION_COMPLETE。 现在内核已经处理完数据，我们从log看看数据发生了哪些变化： 我们发现flat_binder_object结构体的type值发生了变化，binder变成了Handle，看一下结构体，handler 和 binder是一个union，占用同一个位置；Handle为1代表第一个引用，意思是在ServiceManager进程里面根据1能找到第一个binder_ref，根据binder_ref能找到服务hello的binder_node实体。 接下来就等待ServiceManager处理完成后，回复消息。 1.3、唤醒ServiceManager执行添加”hello”服务前面驱动已经创建好TestServer的binder_node，现在唤醒ServiceManager添加svcinfo 看看ServiceManager被唤醒后，会干些什么。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112static int binder_thread_read(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed, int non_block)&#123; ... wait_for_proc_work = thread-&gt;transaction_stack == NULL &amp;&amp; list_empty(&amp;thread-&gt;todo); while (1) &#123; struct binder_transaction_data tr; struct binder_work *w; struct binder_transaction *t = NULL; // 如果当前线程的\"待完成工作\"不为空，则取出待完成工作。 if (!list_empty(&amp;thread-&gt;todo)) w = list_first_entry(&amp;thread-&gt;todo, struct binder_work, entry); else if (!list_empty(&amp;proc-&gt;todo) &amp;&amp; wait_for_proc_work) ... switch (w-&gt;type) &#123; case BINDER_WORK_TRANSACTION: &#123; t = container_of(w, struct binder_transaction, work); &#125; break; ... &#125; // t-&gt;buffer-&gt;target_node是目标节点。 // 这里，addService请求的目标是ServiceManager，因此target_node是ServiceManager对应的节点； // 它的值在事务交互时(binder_transaction中)，被赋值为ServiceManager对应的Binder实体。 if (t-&gt;buffer-&gt;target_node) &#123; // 事务目标对应的Binder实体(即，ServiceManager对应的Binder实体) struct binder_node *target_node = t-&gt;buffer-&gt;target_node; // Binder实体在用户空间的地址(ServiceManager的ptr为NULL) tr.target.ptr = target_node-&gt;ptr; // Binder实体在用户空间的其它数据(ServiceManager的cookie为NULL) tr.cookie = target_node-&gt;cookie; t-&gt;saved_priority = task_nice(current); if (t-&gt;priority &lt; target_node-&gt;min_priority &amp;&amp; !(t-&gt;flags &amp; TF_ONE_WAY)) binder_set_nice(t-&gt;priority); else if (!(t-&gt;flags &amp; TF_ONE_WAY) || t-&gt;saved_priority &gt; target_node-&gt;min_priority) binder_set_nice(target_node-&gt;min_priority); **cmd = BR_TRANSACTION;//将命令改为BR_TRANSACTION &#125; else &#123; tr.target.ptr = NULL; tr.cookie = NULL; cmd = BR_REPLY; &#125; // 交易码 tr.code = t-&gt;code; tr.flags = t-&gt;flags; tr.sender_euid = t-&gt;sender_euid; if (t-&gt;from) &#123; struct task_struct *sender = t-&gt;from-&gt;proc-&gt;tsk; tr.sender_pid = task_tgid_nr_ns(sender, current-&gt;nsproxy-&gt;pid_ns); &#125; else &#123; tr.sender_pid = 0; &#125; // 数据大小 tr.data_size = t-&gt;buffer-&gt;data_size; // 数据中对象的偏移数组的大小(即对象的个数) tr.offsets_size = t-&gt;buffer-&gt;offsets_size; // 数据 tr.data.ptr.buffer = (void *)t-&gt;buffer-&gt;data + proc-&gt;user_buffer_offset; // 数据中对象的偏移数组 tr.data.ptr.offsets = tr.data.ptr.buffer + ALIGN(t-&gt;buffer-&gt;data_size, sizeof(void *)); // 将cmd指令写入到ptr，即传递到用户空间 if (put_user(cmd, (uint32_t __user *)ptr)) return -EFAULT; // 将tr数据拷贝到用户空间 ptr += sizeof(uint32_t); if (copy_to_user(ptr, &amp;tr, sizeof(tr))) return -EFAULT; ptr += sizeof(tr); ... // 删除已处理的事务 list_del(&amp;t-&gt;work.entry); t-&gt;buffer-&gt;allow_user_free = 1; // 设置回复信息 if (cmd == BR_TRANSACTION &amp;&amp; !(t-&gt;flags &amp; TF_ONE_WAY)) &#123; // 该事务会发送给Service Manager守护进程进行处理。 // Service Manager处理之后，还需要给Binder驱动回复处理结果。 // 这里设置Binder驱动回复信息。 t-&gt;to_parent = thread-&gt;transaction_stack; // to_thread表示Service Manager反馈后，将反馈结果交给当前thread进行处理 t-&gt;to_thread = thread; // transaction_stack交易栈保存当前事务。用于之处反馈是针对哪个事务的。 thread-&gt;transaction_stack = t; &#125; else &#123; ... &#125; break; &#125;done: // 更新bwr.read_consumed的值 *consumed = ptr - buffer; ... return 0;&#125; 说明：ServiceManager进程在调用wait_event_interruptible_exclusive(proc-&gt;wait, binder_has_proc_work(proc, thread))进入等待之后，被TestServer进程唤醒。唤醒之后，binder_has_thread_work()为true，因为ServiceManager的待处理事务队列中有个待处理事务(即，TestServer添加服务的请求)。 (01) 进入while循环后，首先取出待处理事务。 (02) 事务的类型是BINDER_WORK_TRANSACTION，得到对应的binder_transaction*类型指针t之后，跳出switch语句。很显然，此时t不为NULL，因此继续往下执行。下面的工作的目的，是将t中的数据转移到tr中(tr是事务交互数据包结构体binder_transaction_data对应的指针)，然后将指令和tr数据都拷贝到用户空间，让ServiceManager读取后进行处理。 Service Manager守护进程在处理完事务之后，需要反馈结果给Binder驱动。因此，接下来会设置t-&gt;to_thread和t-&gt;transaction_stack等成员。最后，修改*consumed的值，即bwr.read_consumed的值，表示待读取内容的大小。 执行完binder_thread_read()之后，回到binder_ioctl()中，执行copy_to_user()将数据拷贝到用户空间。接下来，就回到了Service Manager的守护进程当中，即回到binder_loop()中。 binder_loop()会将ioctl()反馈的数据发送给binder_parse()进行解析。 123456789101112131415161718192021222324252627int binder_parse(struct binder_state *bs, struct binder_io *bio, uintptr_t ptr, size_t size, binder_handler func)&#123; switch(cmd) &#123; case BR_NOOP: case BR_TRANSACTION: &#123; struct binder_transaction_data *txn = (struct binder_transaction_data *) ptr; if ((end - ptr) &lt; sizeof(*txn)) &#123; ALOGE(\"parse: txn too small!\\n\"); return -1; &#125; binder_dump_txn(txn); if (func) &#123; unsigned rdata[256/4]; struct binder_io msg; struct binder_io reply; int res; bio_init(&amp;reply, rdata, sizeof(rdata), 4); bio_init_from_txn(&amp;msg, txn); res = func(bs, txn, &amp;msg, &amp;reply); binder_send_reply(bs, &amp;reply, txn-&gt;data.ptr.buffer, res); &#125; ptr += sizeof(*txn); break; &#125;&#125; 首先会调用svcmgr_handler()-&gt;do_add_service() 1234567891011121314151617181920212223242526272829int do_add_service(struct binder_state *bs, const uint16_t *s, size_t len, uint32_t handle, uid_t uid, int allow_isolated, pid_t spid)&#123; struct svcinfo *si; si = find_svc(s, len); if (si) &#123; if (si-&gt;handle) &#123; svcinfo_death(bs, si); &#125; si-&gt;handle = handle; &#125; else &#123; si = malloc(sizeof(*si) + (len + 1) * sizeof(uint16_t)); si-&gt;handle = handle; si-&gt;len = len; memcpy(si-&gt;name, s, (len + 1) * sizeof(uint16_t)); si-&gt;name[len] = '\\0'; si-&gt;death.func = (void*) svcinfo_death; si-&gt;death.ptr = si; si-&gt;allow_isolated = allow_isolated; si-&gt;next = svclist; svclist = si; &#125; binder_acquire(bs, handle); binder_link_to_death(bs, handle, &amp;si-&gt;death); return 0;&#125; 可以看到首先为hello服务新分配一个结构体svcinfo，然后将handle赋值给svcinfo，这也是以后我们查找服务所得到的handle。 然后调动了binder_acquire、binder_link_to_death发送信息给驱动。 [ 38.467270] service_manager (1362, 1362), binder_thread_write : BC_ACQUIRE [ 38.480122] service_manager (1362, 1362), binder_thread_write : BC_REQUEST_DEATH_NOTIFICATION 接着看binder_send_reply(bs, &amp;reply, txn-&gt;data.ptr.buffer, res); 1234567891011121314void binder_send_reply(struct binder_state *bs, struct binder_io *reply, binder_uintptr_t buffer_to_free, int status)&#123; data.cmd_free = BC_FREE_BUFFER; data.buffer = buffer_to_free; data.cmd_reply = BC_REPLY; data.txn.target.ptr = 0; data.txn.cookie = 0; data.txn.code = 0; ... binder_write(bs, &amp;data, sizeof(data));&#125; 可以看到有BC_FREE_BUFFER、BC_REPLY，通过binder_write(bs, &amp;data, sizeof(data))回复BC_REPLY到驱动。 驱动处理消息跟之前流程类似，这里不再分析。简单总结： 1、驱动接收到BC_REPLY请求，会新建一个待处理事务t（TestServer处理）和待完成的工作tcomplete（service_manager处理） 2、然后唤醒TestServer处理BC_REPLY请求 至此，已经成功添加Hello Service svcmgr: add_service(‘hello’), handle = 1 （2）、Binder系统驱动情景分析–TestClent获取”Hello”服务过程 2.0、构造数据 2.1、发送数据给ServiceManagerbwr初始化完成之后，调用ioctl(,BINDER_WRITE_READ,)和Binder驱动进行交互。 12345678910111213141516171819202122232425262728293031static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)&#123; int ret; struct binder_proc *proc = filp-&gt;private_data; struct binder_thread *thread; unsigned int size = _IOC_SIZE(cmd); void __user *ubuf = (void __user *)arg; switch (cmd) &#123; case BINDER_WRITE_READ: &#123; struct binder_write_read bwr; // 将binder_write_read从\"用户空间\" 拷贝到 \"内核空间\" if (copy_from_user(&amp;bwr, ubuf, sizeof(bwr))) &#123; &#125; // 如果write_size&gt;0，则进行写操作 if (bwr.write_size &gt; 0) &#123; ret = binder_thread_write(proc, thread, (void __user *)bwr.write_buffer, bwr.write_size, &amp;bwr.write_consumed); &#125; // 如果read_size&gt;0，则进行读操作 if (bwr.read_size &gt; 0) &#123; ret = binder_thread_read(proc, thread, (void __user *)bwr.read_buffer, bwr.read_size, &amp;bwr.read_consumed, filp-&gt;f_flags &amp; O_NONBLOCK); &#125; if (copy_to_user(ubuf, &amp;bwr, sizeof(bwr))) &#123; ret = -EFAULT; goto err; &#125; break; &#125;&#125; 首先，会将binder_write_read从用户空间拷贝到内核空间之后。拷贝之后，读取出来的bwr.write_size和bwr.read_size都&gt;0，因此先写后读。即，先执行binder_thread_write()，然后执行binder_thread_read()。 2.2、binder_thread_write()处理数据1234567891011121314151617181920212223242526272829303132333435int binder_thread_write(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed)&#123; uint32_t cmd; void __user *ptr = buffer + *consumed; void __user *end = buffer + size; // 读取binder_write_read.write_buffer中的内容。 // 每次读取32bit(即4个字节) while (ptr &lt; end &amp;&amp; thread-&gt;return_error == BR_OK) &#123; // 从用户空间读取32bit到内核中，并赋值给cmd。 if (get_user(cmd, (uint32_t __user *)ptr)) return -EFAULT; ptr += sizeof(uint32_t); ... switch (cmd) &#123; ... case BC_TRANSACTION: case BC_REPLY: &#123; struct binder_transaction_data tr; if (copy_from_user(&amp;tr, ptr, sizeof(tr))) return -EFAULT; ptr += sizeof(tr); binder_transaction(proc, thread, &amp;tr, cmd == BC_REPLY); break; &#125; ... &#125; // 更新bwr.write_consumed的值 *consumed = ptr - buffer; &#125; return 0;&#125; 说明：MediaPlayer发送的指令是BC_TRANSACTION，这里只关心与BC_TRANSACTION相关的部分。在通过copy_from_user()将数据拷贝从用户空间拷贝到内核空间之后，就调用binder_transaction()进行处理。 2.3、Binder驱动中binder_transaction()的源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133static void binder_transaction(struct binder_proc *proc, struct binder_thread *thread, struct binder_transaction_data *tr, int reply)&#123; struct binder_transaction *t; struct binder_work *tcomplete; size_t *offp, *off_end; struct binder_proc *target_proc; struct binder_thread *target_thread = NULL; struct binder_node *target_node = NULL; struct list_head *target_list; wait_queue_head_t *target_wait; struct binder_transaction *in_reply_to = NULL; struct binder_transaction_log_entry *e; uint32_t return_error; ... if (reply) &#123; ... &#125; else &#123; if (tr-&gt;target.handle) &#123; ... &#125; else &#123; // 该getService是从ServiceManager中获取MediaPlayer； // 因此事务目标对象是ServiceManager的binder实体。 target_node = binder_context_mgr_node; ... &#125; ... // 设置处理事务的目标进程 target_proc = target_node-&gt;proc; ... &#125; if (target_thread) &#123; ... &#125; else &#123; target_list = &amp;target_proc-&gt;todo; target_wait = &amp;target_proc-&gt;wait; &#125; ... // 分配一个待处理的事务t，t是binder事务(binder_transaction对象) t = kzalloc(sizeof(*t), GFP_KERNEL); ... // 分配一个待完成的工作tcomplete，tcomplete是binder_work对象。 tcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL); ... t-&gt;debug_id = ++binder_last_id; ... // 设置from，表示该事务是MediaPlayer线程发起的 if (!reply &amp;&amp; !(tr-&gt;flags &amp; TF_ONE_WAY)) t-&gt;from = thread; else t-&gt;from = NULL; // 下面的一些赋值是初始化事务t t-&gt;sender_euid = proc-&gt;tsk-&gt;cred-&gt;euid; // 事务将交给target_proc进程进行处理 t-&gt;to_proc = target_proc; // 事务将交给target_thread线程进行处理 t-&gt;to_thread = target_thread; // 事务编码 t-&gt;code = tr-&gt;code; // 事务标志 t-&gt;flags = tr-&gt;flags; // 事务优先级 t-&gt;priority = task_nice(current); ... // 分配空间 t-&gt;buffer = binder_alloc_buf(target_proc, tr-&gt;data_size, tr-&gt;offsets_size, !reply &amp;&amp; (t-&gt;flags &amp; TF_ONE_WAY)); ... t-&gt;buffer-&gt;allow_user_free = 0; t-&gt;buffer-&gt;debug_id = t-&gt;debug_id; // 保存事务 t-&gt;buffer-&gt;transaction = t; // 保存事务的目标对象(即处理该事务的binder对象) t-&gt;buffer-&gt;target_node = target_node; trace_binder_transaction_alloc_buf(t-&gt;buffer); if (target_node) binder_inc_node(target_node, 1, 0, NULL); offp = (size_t *)(t-&gt;buffer-&gt;data + ALIGN(tr-&gt;data_size, sizeof(void *))); // 将\"用户空间的数据\"拷贝到内核中 // tr-&gt;data.ptr.buffer就是用户空间数据的起始地址，tr-&gt;data_size就是数据大小 if (copy_from_user(t-&gt;buffer-&gt;data, tr-&gt;data.ptr.buffer, tr-&gt;data_size)) &#123; ... &#125; // 将\"用户空间的数据中所含对象的偏移地址\"拷贝到内核中 // MediaPlayer中不包含对象, offp=null if (copy_from_user(offp, tr-&gt;data.ptr.offsets, tr-&gt;offsets_size)) &#123; ... &#125; ... // MediaPlayer中不包含对象, off_end为null off_end = (void *)offp + tr-&gt;offsets_size; // MediaPlayer中不包含对象, offp=off_end for (; offp &lt; off_end; offp++) &#123; ... &#125; if (reply) &#123; .. &#125; else if (!(t-&gt;flags &amp; TF_ONE_WAY)) &#123; BUG_ON(t-&gt;buffer-&gt;async_transaction != 0); t-&gt;need_reply = 1; t-&gt;from_parent = thread-&gt;transaction_stack; // 将当前事务添加到当前线程的事务栈中 thread-&gt;transaction_stack = t; &#125; else &#123; ... &#125; // 设置事务的类型为BINDER_WORK_TRANSACTION t-&gt;work.type = BINDER_WORK_TRANSACTION; // 将事务添加到target_list队列中，即target_list的待处理事务中 list_add_tail(&amp;t-&gt;work.entry, target_list); // 设置待完成工作的类型为BINDER_WORK_TRANSACTION_COMPLETE tcomplete-&gt;type = BINDER_WORK_TRANSACTION_COMPLETE; // 将待完成工作添加到thread-&gt;todo队列中，即当前线程的待完成工作中。 list_add_tail(&amp;tcomplete-&gt;entry, &amp;thread-&gt;todo); // 唤醒目标进程 if (target_wait) wake_up_interruptible(target_wait); return; ...&#125; 说明：参数reply=0，表明这是个请求事务，而不是反馈。binder_transaction新建会新建”一个待处理事务t”和”待完成的工作tcomplete”，并根据请求的数据对它们进行初始化。 (01) TestClent的getService请求是提交给ServiceManager进行处理的，因此，”待处理事务t”会被添加到ServiceManager的待处理事务队列中。此时的target_thread是ServiceManager对应的线程，而target_proc则是ServiceManager对应的进程上下文环境。 (02) 此时，Binder驱动已经收到了TestClent的getService请求；于是，将一个BINDER_WORK_TRANSACTION_COMPLETE类型的”待完成工作tcomplete”添加到当前线程(即，TestClent线程)的待处理事务队列中。目的是告诉TestClent，Binder驱动已经收到它的getService请求了。 (03) 最后，调用wake_up_interruptible(target_wait)将ServiceManager唤醒。 接下来，还是先分析完TestClent线程，再看ServiceManager被唤醒后做了些什么。 binder_transaction()执行完毕之后，就会返回到binder_thread_write()中。binder_thread_write()更新bwr.write_consumed的值后，就返回到binder_ioctl()继续执行”读”动作。即执行binder_thread_read()。 2.4、Binder驱动中binder_thread_read()的源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687static int binder_thread_read(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed, int non_block)&#123; void __user *ptr = buffer + *consumed; void __user *end = buffer + size; int ret = 0; int wait_for_proc_work; // 如果*consumed=0，则写入BR_NOOP到用户传进来的bwr.read_buffer缓存区 if (*consumed == 0) &#123; if (put_user(BR_NOOP, (uint32_t __user *)ptr)) return -EFAULT; ptr += sizeof(uint32_t); &#125;retry: // 等待proc进程的事务标记。 // 当线程的事务栈为空 并且 待处理事务队列为空时，该标记位true。 wait_for_proc_work = thread-&gt;transaction_stack == NULL &amp;&amp; list_empty(&amp;thread-&gt;todo); ... if (wait_for_proc_work) &#123; ... &#125; else &#123; if (non_block) &#123; ... &#125; else ret = wait_event_interruptible(thread-&gt;wait, binder_has_thread_work(thread)); &#125; ... while (1) &#123; uint32_t cmd; struct binder_transaction_data tr; struct binder_work *w; struct binder_transaction *t = NULL; // 如果当前线程的\"待完成工作\"不为空，则取出待完成工作。 if (!list_empty(&amp;thread-&gt;todo)) w = list_first_entry(&amp;thread-&gt;todo, struct binder_work, entry); else if (!list_empty(&amp;proc-&gt;todo) &amp;&amp; wait_for_proc_work) ... else &#123; if (ptr - buffer == 4 &amp;&amp; !(thread-&gt;looper &amp; BINDER_LOOPER_STATE_NEED_RETURN)) /* no data added */ goto retry; break; &#125; ... switch (w-&gt;type) &#123; ... case BINDER_WORK_TRANSACTION_COMPLETE: &#123; cmd = BR_TRANSACTION_COMPLETE; // 将BR_TRANSACTION_COMPLETE写入到用户缓冲空间中 if (put_user(cmd, (uint32_t __user *)ptr)) return -EFAULT; ptr += sizeof(uint32_t); ... // 待完成事务已经处理完毕，将其从待完成事务队列中删除。 list_del(&amp;w-&gt;entry); kfree(w); binder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE); &#125; break; ... &#125; if (!t) continue; ... &#125; ... // 更新bwr.read_consumed的值 *consumed = ptr - buffer; ... return 0;&#125; 说明： (01) bwr.read_consumed=0，即if (*consumed == 0)为true。因此，会将BR_NOOP写入到bwr.read_buffer中。 (02) thread-&gt;transaction_stack不为空，thread-&gt;todo也不为空。因为，前面在binder_transaction()中有将一个BINDER_WORK_TRANSACTION_COMPLETE类型的待完成工作添加到thread的待完成工作队列中。因此，wait_for_proc_work为false。 (03) binder_has_thread_work(thread)为true。因此，在调用wait_event_interruptible()时，不会进入等待状态，而是继续运行。 (04) 进入while循环后，通过list_first_entry()取出待完成工作w。w的类型w-&gt;type=BINDER_WORK_TRANSACTION_COMPLETE，进入到对应的switch分支。随后，将BR_TRANSACTION_COMPLETE写入到bwr.read_buffer中。此时，待处理工作已经完成，将其从当前线程的待处理工作队列中删除。 (05) 最后，更新bwr.read_consumed的值。 经过binder_thread_read()处理之后，bwr.read_buffer中包含了两个指令：BR_NOOP和BR_TRANSACTION_COMPLETE。 2.5、ServiceManager处理getService请求下面看看ServiceManager被唤醒之后，是如何处理getService请求的 12345678910111213141516171819202122int binder_parse(struct binder_state *bs, struct binder_io *bio, uint32_t *ptr, uint32_t size, binder_handler func)&#123; while (ptr &lt; end) &#123; case BR_TRANSACTION: &#123; struct binder_txn *txn = (void *) ptr; ... if (func) &#123; unsigned rdata[256/4]; struct binder_io msg; // 用于保存&quot;Binder驱动反馈的信息&quot; struct binder_io reply; // 用来保存&quot;回复给Binder驱动的信息&quot; int res; // 初始化reply bio_init(&amp;reply, rdata, sizeof(rdata), 4); // 根据txt(Binder驱动反馈的信息)初始化msg bio_init_from_txn(&amp;msg, txn); // 消息处理 res = func(bs, txn, &amp;msg, &amp;reply); // 反馈消息给Binder驱动。 binder_send_reply(bs, &amp;reply, txn-&gt;data, res);&#125; binder_send_reply(bs, &amp;reply, txn-&gt;data, res);-&gt;binder_write() 123456789101112131415161718binder_write()int binder_write(struct binder_state *bs, void *data, unsigned len)&#123; struct binder_write_read bwr; int res; bwr.write_size = len; bwr.write_consumed = 0; bwr.write_buffer = (unsigned) data; bwr.read_size = 0; bwr.read_consumed = 0; bwr.read_buffer = 0; res = ioctl(bs-&gt;fd, BINDER_WRITE_READ, &amp;bwr); if (res &lt; 0) &#123; fprintf(stderr,\"binder_write: ioctl failed (%s)\\n\", strerror(errno)); &#125; return res;&#125; 说明：binder_write()会将数据封装到binder_write_read的变量bwr中；其中，bwr.read_size=0，而bwr.write_size&gt;0。接着，便通过ioctl(,BINDER_WRITE_READ,)和Binder驱动交互，将数据反馈给Binder驱动。 再次回到Binder驱动的binder_ioctl()对应的BINDER_WRITE_READ分支中。此时，由于bwr.read_size=0，而bwr.write_size&gt;0；因此，Binder驱动只调用binder_thread_write进行写操作，而不会进行读。 返回数据： handle = 1 代表第一个 2.6、Binder驱动中处理ServiceManager返回数据12345678910111213141516171819202122232425262728293031323334353637int binder_thread_write(struct binder_proc *proc, struct binder_thread *thread, void __user *buffer, int size, signed long *consumed)&#123; uint32_t cmd; void __user *ptr = buffer + *consumed; void __user *end = buffer + size; // 读取binder_write_read.write_buffer中的内容。 // 每次读取32bit(即4个字节) while (ptr &lt; end &amp;&amp; thread-&gt;return_error == BR_OK) &#123; // 从用户空间读取32bit到内核中，并赋值给cmd。 if (get_user(cmd, (uint32_t __user *)ptr)) return -EFAULT; ptr += sizeof(uint32_t); ... switch (cmd) &#123; case BC_FREE_BUFFER: ... case BC_TRANSACTION: case BC_REPLY: &#123; struct binder_transaction_data tr; if (copy_from_user(&amp;tr, ptr, sizeof(tr))) return -EFAULT; ptr += sizeof(tr); binder_transaction(proc, thread, &amp;tr, cmd == BC_REPLY); break; &#125; ... &#125; // 更新bwr.write_consumed的值 *consumed = ptr - buffer; &#125; return 0;&#125; binder_thread_write()进入BC_REPLY之后，会将数据拷贝到内核空间，然后调用binder_transaction()进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170static void binder_transaction(struct binder_proc *proc, struct binder_thread *thread, struct binder_transaction_data *tr, int reply)&#123; struct binder_transaction *t; struct binder_work *tcomplete; size_t *offp, *off_end; struct binder_proc *target_proc; struct binder_thread *target_thread = NULL; struct binder_node *target_node = NULL; struct list_head *target_list; wait_queue_head_t *target_wait; struct binder_transaction *in_reply_to = NULL; struct binder_transaction_log_entry *e; uint32_t return_error; ... if (reply) &#123; // 事务栈 in_reply_to = thread-&gt;transaction_stack; ... // 设置优先级 binder_set_nice(in_reply_to-&gt;saved_priority); ... thread-&gt;transaction_stack = in_reply_to-&gt;to_parent; // 发起请求的线程，即MediaPlayer所在线程。 // from的值，是MediaPlayer发起请求时在binder_transaction()中赋值的。 target_thread = in_reply_to-&gt;from; ... // MediaPlayer对应的进程 target_proc = target_thread-&gt;proc; &#125; else &#123; ... &#125; if (target_thread) &#123; e-&gt;to_thread = target_thread-&gt;pid; target_list = &amp;target_thread-&gt;todo; target_wait = &amp;target_thread-&gt;wait; &#125; else &#123; ... &#125; e-&gt;to_proc = target_proc-&gt;pid; // 分配一个待处理的事务t，t是binder事务(binder_transaction对象) t = kzalloc(sizeof(*t), GFP_KERNEL); if (t == NULL) &#123; return_error = BR_FAILED_REPLY; goto err_alloc_t_failed; &#125; // 分配一个待完成的工作tcomplete，tcomplete是binder_work对象。 tcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL); if (tcomplete == NULL) &#123; return_error = BR_FAILED_REPLY; goto err_alloc_tcomplete_failed; &#125; binder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE); t-&gt;debug_id = ++binder_last_id; e-&gt;debug_id = t-&gt;debug_id; if (!reply &amp;&amp; !(tr-&gt;flags &amp; TF_ONE_WAY)) t-&gt;from = thread; else t-&gt;from = NULL; // 下面的一些赋值是初始化事务t t-&gt;sender_euid = proc-&gt;tsk-&gt;cred-&gt;euid; // 事务将交给target_proc进程进行处理 t-&gt;to_proc = target_proc; // 事务将交给target_thread线程进行处理 t-&gt;to_thread = target_thread; // 事务编码 t-&gt;code = tr-&gt;code; // 事务标志 t-&gt;flags = tr-&gt;flags; // 事务优先级 t-&gt;priority = task_nice(current); // 分配空间 t-&gt;buffer = binder_alloc_buf(target_proc, tr-&gt;data_size, tr-&gt;offsets_size, !reply &amp;&amp; (t-&gt;flags &amp; TF_ONE_WAY)); if (t-&gt;buffer == NULL) &#123; return_error = BR_FAILED_REPLY; goto err_binder_alloc_buf_failed; &#125; t-&gt;buffer-&gt;allow_user_free = 0; t-&gt;buffer-&gt;debug_id = t-&gt;debug_id; // 保存事务 t-&gt;buffer-&gt;transaction = t; // target_node为NULL t-&gt;buffer-&gt;target_node = target_node; trace_binder_transaction_alloc_buf(t-&gt;buffer); if (target_node) binder_inc_node(target_node, 1, 0, NULL); offp = (size_t *)(t-&gt;buffer-&gt;data + ALIGN(tr-&gt;data_size, sizeof(void *))); // 将\"用户传入的数据\"保存到事务中 if (copy_from_user(t-&gt;buffer-&gt;data, tr-&gt;data.ptr.buffer, tr-&gt;data_size)) &#123; ... &#125; // 将\"用户传入的数据偏移地址\"保存到事务中 if (copy_from_user(offp, tr-&gt;data.ptr.offsets, tr-&gt;offsets_size)) &#123; ... &#125; ... off_end = (void *)offp + tr-&gt;offsets_size; // 将flat_binder_object对象读取出来， // 这里就是Service Manager中反馈的MediaPlayerService对象。 for (; offp &lt; off_end; offp++) &#123; struct flat_binder_object *fp; ... fp = (struct flat_binder_object *)(t-&gt;buffer-&gt;data + *offp); switch (fp-&gt;type) &#123; ... case BINDER_TYPE_HANDLE: case BINDER_TYPE_WEAK_HANDLE: &#123; // 根据handle获取对应的Binder引用，即得到MediaPlayerService的Binder引用 struct binder_ref *ref = binder_get_ref(proc, fp-&gt;handle); if (ref == NULL) &#123; ... &#125; // ref-&gt;node-&gt;proc是MediaPlayerService的进程上下文环境， // 而target_proc是MediaPlayer的进程上下文环境 if (ref-&gt;node-&gt;proc == target_proc) &#123; ... &#125; else &#123; struct binder_ref *new_ref; // 在MediaPlayer进程中引用\"MediaPlayerService\"。 // 表现为，执行binder_get_ref_for_node()会，会先在MediaPlayer进程中查找是否存在MediaPlayerService对应的Binder引用； // 很显然是不存在的。于是，并新建MediaPlayerService对应的Binder引用，并将其添加到MediaPlayer的Binder引用红黑树中。 new_ref = binder_get_ref_for_node(target_proc, ref-&gt;node); if (new_ref == NULL) &#123; ... &#125; // 将new_ref的引用描述复制给fp-&gt;handle。 fp-&gt;handle = new_ref-&gt;desc; binder_inc_ref(new_ref, fp-&gt;type == BINDER_TYPE_HANDLE, NULL); ... &#125; &#125; break; &#125; &#125; if (reply) &#123; binder_pop_transaction(target_thread, in_reply_to); &#125; else if (!(t-&gt;flags &amp; TF_ONE_WAY)) &#123; ... &#125; else &#123; ... &#125; // 设置事务的类型为BINDER_WORK_TRANSACTION t-&gt;work.type = BINDER_WORK_TRANSACTION; // 将事务添加到target_list队列中，即target_list的待处理事务中 list_add_tail(&amp;t-&gt;work.entry, target_list); // 设置待完成工作的类型为BINDER_WORK_TRANSACTION_COMPLETE tcomplete-&gt;type = BINDER_WORK_TRANSACTION_COMPLETE; // 将待完成工作添加到thread-&gt;todo队列中，即当前线程的待完成工作中。 list_add_tail(&amp;tcomplete-&gt;entry, &amp;thread-&gt;todo); // 唤醒目标进程 if (target_wait) wake_up_interruptible(target_wait); return; ...&#125; 说明：reply=1，这里只关注reply部分。 (01) 此反馈最终是要回复给TestClient的。因此，target_thread被赋值为TestServer所在的线程，target_proc则是TestClient对应的进程，target_node为null。 (02) 这里，先看看for循环里面的内容，取出BR_REPLY指令所发送的数据，然后获取数据中的flat_binder_object变量fp。因为fp-&gt;type为BINDER_TYPE_HANDLE，因此进入BINDER_TYPE_HANDLE对应的分支。接着，通过binder_get_ref()获取Hello Service对应的Binder引用；很明显，能够正常获取到Hello Service的Binder引用。因为在Hello Service调用addService请求时，已经创建了它的Binder引用。 binder_get_ref_for_node()的作用是在TestClent进程上下文中添加”TestServer对应的Binder引用”。这样，后面就可以根据该Binder引用一步步的获取TestServer对象。 最后，将Binder引用的描述赋值给fp-&gt;handle。 (03) 此时，Service Manager已经处理了getService请求。便调用binder_pop_transaction(target_thread, in_reply_to)将事务从”target_thread的事务栈”中删除，即从MediaPlayer线程的事务栈中删除该事务。 (04) 新建的”待处理事务t”的type为设为BINDER_WORK_TRANSACTION后，会被添加到MediaPlayer的待处理事务队列中。 (05) 此时，Service Manager已经处理了getService请求，而Binder驱动在等待它的回复。于是，将一个BINDER_WORK_TRANSACTION_COMPLETE类型的”待完成工作tcomplete”(作为回复)添加到当前线程(即，Service Manager线程)的待处理事务队列中。 (06) 最后，调用wake_up_interruptible()唤醒TestServer。TestServer被唤醒后，会对事务BINDER_WORK_TRANSACTION进行处理。 OK，到现在为止，还有两个待处理事务：(01) ServiceManager待处理事务列表中有个BINDER_WORK_TRANSACTION_COMPLETE类型的事务 (02) TestServer待处理事务列表中有个BINDER_WORK_TRANSACTION事务。 2.7. Testclient获取handle （3）、Binder系统驱动情景分析–TestClent使用”Hello”服务过程构造数据发送数据”weidongshan” 四、Android Binder系统-Native层前面我们分析内核驱动Binder使用过程，可以看到，binder系统在内核能正常完成IPC通信，接下来分析Android framwork层，最后是App层。 Framework是一个中间层，它对接了底层实现，封装了复杂的内部逻辑，并提供供外部使用的接口。Framework层是应用程序开发的基础。 Binder Framework层分为C++和Java两个部分，为了达到功能的复用，中间通过JNI进行衔接。 Binder Framework的C++部分，头文件位于这个路径：/frameworks/native/include/binder/，实现位于这个路径：/frameworks/native/libs/binder/ 。Binder库最终会编译成一个动态链接库：libbinder.so，供其他进程链接使用。 为了便于说明，下文中我们将Binder Framework 的C++部分称之为libbinder。首先说一下ServiceManager，然后详细介绍。 (1)、ServiceManager类图(Native层)IServiceManager相关类如下图所示： IServiceManager是表示servicemanager的接口，有如下方法： 1) getService获得binder service引用， 2) checkService获得binder service引用， 3) addService添加binder service， 4) listServices 列举所有binder service。 servicemanager的binder service服务端其实是在frameworks/base/cmds/servicemanager 里实现，BnServiceMananger实际上并未使用。BpServiceMananger就是利用获得的IBinder指针建立的IServiceMananger对象的实际类型。 (2)、Binder框架Native层libbinder中，将实现分为Proxy和Native两端。Proxy对应了上文提到的Client端，是服务对外提供的接口。而Native是服务实现的一端，对应了上文提到的Server端。类名中带有小写字母p的（例如BpInterface），就是指Proxy端。类名带有小写字母n的（例如BnInterface），就是指Native端。 Proxy代表了调用方，通常与服务的实现不在同一个进程，因此下文中，我们也称Proxy端为”远程”端。Native端是服务实现的自身，因此下文中，我们也称Native端为”本地”端。 这里，我们先对libbinder中的主要类做一个简要说明，了解一下它们的关系，然后再详细的讲解。 类名 说明 BpRefBase RefBase的子类，提供remote()方法获取远程Binder IInterface Binder服务接口的基类，Binder服务通常需要同时提供本地接口和远程接口 BpInterface 远程接口的基类，远程接口是供客户端调用的接口集 BnInterface 本地接口的基类，本地接口是需要服务中真正实现的接口集 IBiner Binder对象的基类，BBinder和BpBinder都是这个类的子类 BpBinder 远程Binder，这个类提供transact方法来发送请求，BpXXX实现中会用到 BBinder 本地Binder，服务实现方的基类，提供了onTransact接口来接收请求 ProcessState 代表了使用Binder的进程 IPCThreadState 代表了使用Binder的线程，这个类中封装了与Binder驱动通信的逻辑 Parcel 在Binder上传递的数据的包装器 下图描述了这些类之间的关系： 另外说明一下，Binder服务的实现类（图中紫色部分）通常都会遵守下面的命名规则： ☯ 服务的接口使用I字母作为前缀 ☯ 远程接口使用Bp作为前缀 ☯ 本地接口使用Bn作为前缀 看了上面这些介绍，你可能还是不太容易理解。不过不要紧，下面我们会逐步拆分讲解这些内容。 在这幅图中，浅黄色部分的结构是最难理解的，因此我们先从它们着手。 我们先来看看IBinder这个类。这个类描述了所有在Binder上传递的对象，它既是Binder本地对象BBinder的父类，也是Binder远程对象BpBinder的父类。这个类中的主要方法说明如下： 方法名 说明 localBinder 获取本地Binder对象 remoteBinder 获取远程Binder对象 transact 进行一次Binder操作 queryLocalInterface 尝试获取本地Binder，如何失败返回NULL getInterfaceDescriptor 获取Binder的服务接口描述，其实就是Binder服务的唯一标识 isBinderAlive 查询Binder服务是否还活着 pingBinder 发送PING_TRANSACTION给Binder服务 BpBinder的实例代表了远程Binder，这个类的对象将被客户端调用。其中handle方法会返回指向Binder服务实现者的句柄，这个类最重要就是提供了transact方法，这个方法会将远程调用的参数封装好发送的Binder驱动。 由于每个Binder服务通常都会提供多个服务接口，而这个方法中的uint32_t code参数就是用来对服务接口进行编号区分的。Binder服务的每个接口都需要指定一个唯一的code，这个code要在Proxy和Native端配对好。当客户端将请求发送到服务端的时候，服务端根据这个code（onTransact方法中）来区分调用哪个接口方法。 BBinder的实例代表了本地Binder，它描述了服务的提供方，所有Binder服务的实现者都要继承这个类（的子类），在继承类中，最重要的就是实现onTransact方法，因为这个方法是所有请求的入口。因此，这个方法是和BpBinder中的transact方法对应的，这个方法同样也有一个uint32_t code参数，在这个方法的实现中，由服务提供者通过code对请求的接口进行区分，然后调用具体实现服务的方法。 IBinder中定义了uint32_t code允许的范围： 12FIRST_CALL_TRANSACTION = 0x00000001,LAST_CALL_TRANSACTION = 0x00ffffff, Binder服务要保证自己提供的每个服务接口有一个唯一的code，例如hello服务: 123#define HELLO_SVR_CMD_SAYHELLO 1#define HELLO_SVR_CMD_SAYHELLO_TO 2#define HELLO_SVR_CMD_GET_FD 3 讲完了IBinder，BpBinder和BBinder三个类，我们再来看看BpReBase，IInterface，BpInterface和BnInterface。 每个Binder服务都是为了某个功能而实现的，因此其本身会定义一套接口集（通常是C++的一个类）来描述自己提供的所有功能。而Binder服务既有自身实现服务的类，也要有给客户端进程调用的类。为了便于开发，这两中类里面的服务接口应当是一致的，例如：假设服务实现方提供了一个接口为sayhello(void)的服务方法，那么其远程接口中也应当有一个sayhello(void)方法。因此为了实现方便，本地实现类和远程接口类需要有一个公共的描述服务接口的基类（即上图中的IXXXService）来继承。而这个基类通常是IInterface的子类，IInterface的定义如下： 1234567891011class IInterface : public virtual RefBase&#123;public: IInterface(); static sp&lt;IBinder&gt; asBinder(const IInterface*); static sp&lt;IBinder&gt; asBinder(const sp&lt;IInterface&gt;&amp;);protected: virtual ~IInterface(); virtual IBinder* onAsBinder() = 0;&#125;; 之所以要继承自IInterface类是因为这个类中定义了onAsBinder让子类实现。onAsBinder在本地对象的实现类中返回的是本地对象，在远程对象的实现类中返回的是远程对象。onAsBinder方法被两个静态方法asBinder方法调用。有了这些接口之后，在代码中便可以直接通过IXXX::asBinder方法获取到不用区分本地还是远程的IBinder对象。这个在跨进程传递Binder对象的时候有很大的作用（因为不用区分具体细节，只要直接调用和传递就好）。 下面，我们来看一下BpInterface和BnInterface的定义： 12345678910111213141516171819202122template&lt;typename INTERFACE&gt;class BnInterface : public INTERFACE, public BBinder&#123;public: virtual sp&lt;IInterface&gt; queryLocalInterface(const String16&amp; _descriptor); virtual const String16&amp; getInterfaceDescriptor() const;protected: virtual IBinder* onAsBinder();&#125;;// ----------------------------------------------------------------------template&lt;typename INTERFACE&gt;class BpInterface : public INTERFACE, public BpRefBase&#123;public: BpInterface(const sp&lt;IBinder&gt;&amp; remote);protected: virtual IBinder* onAsBinder();&#125;; 这两个类都是模板类，它们在继承自INTERFACE的基础上各自继承了另外一个类。这里的INTERFACE便是我们Binder服务接口的基类。另外，BnInterface继承了BBinder类，由此可以通过复写onTransact方法来提供实现。BpInterface继承了BpRefBase，通过这个类的remote方法可以获取到指向服务实现方的句柄。在客户端接口的实现类中，每个接口在组装好参数之后，都会调用remote()-&gt;transact来发送请求，而这里其实就是调用的BpBinder的transact方法，这样请求便通过Binder到达了服务实现方的onTransact中。这个过程如下图所示： 基于Binder框架开发的服务，除了满足上文提到的类名规则之外，还需要遵守其他一些共同的规约： ☯为了进行服务的区分，每个Binder服务需要指定一个唯一的标识，这个标识通过getInterfaceDescriptor返回，类型是一个字符串。通常，Binder服务会在类中定义static const android::String16 descriptor;这样一个常量来描述这个标识符，然后在getInterfaceDescriptor方法中返回这个常量。 ☯为了便于调用者获取到调用接口，服务接口的公共基类需要提供一个android::sp asInterface方法来返回基类对象指针。由于上面提到的这两点对于所有Binder服务的实现逻辑都是类似的。为了简化开发者的重复工作，在libbinder中，定义了两个宏来简化这些重复工作，它们是： 12345678910111213141516171819202122232425262728293031#define DECLARE_META_INTERFACE(INTERFACE) \\ static const android::String16 descriptor; \\ static android::sp&lt;I##INTERFACE&gt; asInterface( \\ const android::sp&lt;android::IBinder&gt;&amp; obj); \\ virtual const android::String16&amp; getInterfaceDescriptor() const; \\ I##INTERFACE(); \\ virtual ~I##INTERFACE(); \\#define IMPLEMENT_META_INTERFACE(INTERFACE, NAME) \\ const android::String16 I##INTERFACE::descriptor(NAME); \\ const android::String16&amp; \\ I##INTERFACE::getInterfaceDescriptor() const &#123; \\ return I##INTERFACE::descriptor; \\ &#125; \\ android::sp&lt;I##INTERFACE&gt; I##INTERFACE::asInterface( \\ const android::sp&lt;android::IBinder&gt;&amp; obj) \\ &#123; \\ android::sp&lt;I##INTERFACE&gt; intr; \\ if (obj != NULL) &#123; \\ intr = static_cast&lt;I##INTERFACE*&gt;( \\ obj-&gt;queryLocalInterface( \\ I##INTERFACE::descriptor).get()); \\ if (intr == NULL) &#123; \\ intr = new Bp##INTERFACE(obj); \\ &#125; \\ &#125; \\ return intr; \\ &#125; \\ I##INTERFACE::I##INTERFACE() &#123; &#125; \\ I##INTERFACE::~I##INTERFACE() &#123; &#125; \\ 有了这两个宏之后，开发者只要在接口基类（IXXX）头文件中，使用DECLARE_META_INTERFACE宏便完成了需要的组件的声明。然后在cpp文件中使用IMPLEMENT_META_INTERFACE便完成了这些组件的实现。 2.1、Binder的初始化ProcessState在讲解Binder驱动的时候我们就提到：任何使用Binder机制的进程都必须要对/dev/binder设备进行open以及mmap之后才能使用，这部分逻辑是所有使用Binder机制进程共同的。对于这种共同逻辑的封装便是Framework层的职责之一。libbinder中，ProcessState类封装了这个逻辑，相关代码见下文。 这里是ProcessState构造函数，在这个函数中，初始化mDriverFD的时候调用了open_driver方法打开binder设备，然后又在函数体中，通过mmap进行内存映射。 12345678910111213141516171819ProcessState::ProcessState() : mDriverFD(open_driver()) , mVMStart(MAP_FAILED) , mThreadCountLock(PTHREAD_MUTEX_INITIALIZER) , mThreadCountDecrement(PTHREAD_COND_INITIALIZER) , mExecutingThreadsCount(0) , mMaxThreads(DEFAULT_MAX_BINDER_THREADS) , mStarvationStartTimeMs(0) , mManagesContexts(false) , mBinderContextCheckFunc(NULL) , mBinderContextUserData(NULL) , mThreadPoolStarted(false) , mThreadPoolSeq(1)&#123; if (mDriverFD &gt;= 0) &#123; mVMStart = mmap(0, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0); ... &#125;&#125; open_driver的函数实现如下所示。在这个函数中完成了三个工作： ☯首先通过open系统调用打开了dev/binder设备 ☯然后通过ioctl获取Binder实现的版本号，并检查是否匹配 ☯最后通过ioctl设置进程支持的最大线程数量 关于这部分逻辑背后的处理，在讲解Binder驱动的时候，我们已经讲解过了。 1234567891011121314static int open_driver()&#123; int fd = open(\"/dev/binder\", O_RDWR | O_CLOEXEC); if (fd &gt;= 0) &#123; int vers = 0; status_t result = ioctl(fd, BINDER_VERSION, &amp;vers); ... size_t maxThreads = DEFAULT_MAX_BINDER_THREADS; result = ioctl(fd, BINDER_SET_MAX_THREADS, &amp;maxThreads); &#125; else &#123; ... &#125; return fd;&#125; ProcessState是一个Singleton（单例）类型的类，在一个进程中，只会存在一个实例。通过ProcessState::self()接口获取这个实例。一旦获取这个实例，便会执行其构造函数，由此完成了对于Binder设备的初始化工作。 2.2、关于Binder传递数据的大小限制由于Binder的数据需要跨进程传递，并且还需要在内核上开辟空间，因此允许在Binder上传递的数据并不是无无限大的。mmap中指定的大小便是对数据传递的大小限制： 12#define BINDER_VM_SIZE ((1*1024*1024) - (4096 *2)) // 1M - 8kmVMStart = mmap(0, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0); 这里我们看到，在进行mmap的时候，指定了最大size为BINDER_VM_SIZE，即 1M - 8k的大小。 因此我们在开发过程中，一次Binder调用的数据总和不能超过这个大小。 对于这个区域的大小，我们也可以在设备上进行确认。这里我们还之前提到的system_server为例。上面我们讲解了通过procfs来获取映射的内存地址，除此之外，我们也可以通过showmap命令，来确定这块区域的大小，相关命令如下： 1234angler:/ # ps | grep system_server system 1889 526 2353404 135968 SyS_epoll_ 72972eeaf4 S system_serverangler:/ # showmap 1889 | grep \"/dev/binder\" 1016 4 4 0 0 4 0 0 1 /dev/binder 这里可以看到，这块区域的大小正是 1M - 8K = 1016k。 Tips: 通过showmap命令可以看到进程的详细内存占用情况。在实际的开发过程中，当我们要对某个进程做内存占用分析的时候，这个命令是相当有用的。建议读者尝试通过showmap命令查看system_server或其他感兴趣进程的完整map，看看这些进程都依赖了哪些库或者模块，以及内存占用情况是怎样的。 2.3、与驱动的通信IPCThreadState上文提到ProcessState是一个单例类，一个进程只有一个实例。而负责与Binder驱动通信的IPCThreadState也是一个单例类。但这个类不是一个进程只有一个实例，而是一个线程有一个实例。 IPCThreadState负责了与驱动通信的细节处理。这个类中的关键几个方法说明如下： 方法 说明 transact 公开接口。供Proxy发送数据到驱动，并读取返回结果 sendReply 供Server端写回请求的返回结果 waitForResponse 发送请求后等待响应结果 talkWithDriver 通过ioctl BINDER_WRITE_READ来与驱动通信 writeTransactionData 写入一次事务的数据 executeCommand 处理binder_driver_return_protocol协议命令 freeBuffer 通过BC_FREE_BUFFER命令释放Buffer BpBinder::transact方法在发送请求的时候，其实就是直接调用了IPCThreadState对应的方法来发送请求到Binder驱动的，相关代码如下： 123456789101112status_t BpBinder::transact( uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; if (mAlive) &#123; status_t status = IPCThreadState::self()-&gt;transact( mHandle, code, data, reply, flags); if (status == DEAD_OBJECT) mAlive = 0; return status; &#125; return DEAD_OBJECT;&#125; 而IPCThreadState::transact方法主要逻辑如下： 123456789101112131415161718192021222324252627282930status_t IPCThreadState::transact(int32_t handle, uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags)&#123; status_t err = data.errorCheck(); flags |= TF_ACCEPT_FDS; if (err == NO_ERROR) &#123; err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL); &#125; if (err != NO_ERROR) &#123; if (reply) reply-&gt;setError(err); return (mLastError = err); &#125; if ((flags &amp; TF_ONE_WAY) == 0) &#123; if (reply) &#123; err = waitForResponse(reply); &#125; else &#123; Parcel fakeReply; err = waitForResponse(&amp;fakeReply); &#125; &#125; else &#123; err = waitForResponse(NULL, NULL); &#125; return err;&#125; 这段代码应该还是比较好理解的：首先通过writeTransactionData写入数据，然后通过waitForResponse等待返回结果。TF_ONE_WAY表示此次请求是单向的，即：不用真正等待结果即可返回。 而writeTransactionData方法其实就是在组装binder_transaction_data数据： 1234567891011121314151617181920212223242526272829303132333435status_t IPCThreadState::writeTransactionData(int32_t cmd, uint32_t binderFlags, int32_t handle, uint32_t code, const Parcel&amp; data, status_t* statusBuffer)&#123; binder_transaction_data tr; tr.target.ptr = 0; /* Don't pass uninitialized stack data to a remote process */ tr.target.handle = handle; tr.code = code; tr.flags = binderFlags; tr.cookie = 0; tr.sender_pid = 0; tr.sender_euid = 0; const status_t err = data.errorCheck(); if (err == NO_ERROR) &#123; tr.data_size = data.ipcDataSize(); tr.data.ptr.buffer = data.ipcData(); tr.offsets_size = data.ipcObjectsCount()*sizeof(binder_size_t); tr.data.ptr.offsets = data.ipcObjects(); &#125; else if (statusBuffer) &#123; tr.flags |= TF_STATUS_CODE; *statusBuffer = err; tr.data_size = sizeof(status_t); tr.data.ptr.buffer = reinterpret_cast&lt;uintptr_t&gt;(statusBuffer); tr.offsets_size = 0; tr.data.ptr.offsets = 0; &#125; else &#123; return (mLastError = err); &#125; mOut.writeInt32(cmd); mOut.write(&amp;tr, sizeof(tr)); return NO_ERROR;&#125; 对于binder_transaction_data在讲解Binder驱动的时候我们已经详细讲解过了。而这里的Parcel我们还不了解，那么接下来我们马上就来看一下这个类。 数据包装器：Parcel Binder上提供的是跨进程的服务，每个服务包含了不同的接口，每个接口的参数数量和类型都不一样。那么当客户端想要调用服务端的接口，参数是如何跨进程传递给服务端的呢？除此之外，服务端想要给客户端返回结果，结果又是如何传递回来的呢？ 这些问题的答案就是：Parcel。Parcel就像一个包装器，调用者可以以任意顺序往里面放入需要的数据，所有写入的数据就像是被打成一个整体的包，然后可以直接在Binde上传输。 Parcel提供了所有基本类型的写入和读出接口，下面是其中的一部分： 123456789...status_t writeInt32(int32_t val);status_t writeUint32(uint32_t val);......status_t readUtf8FromUtf16(std::string* str) const;status_t readUtf8FromUtf16(std::unique_ptr&lt;std::string&gt;* str) const;const char* readCString() const;... 因此对于基本类型，开发者可以直接调用接口写入和读出。而对于非基本类型，需要由开发者将其拆分成基本类型然后写入到Parcel中（读出的时候也是一样）。 Parcel会将所有写入的数据进行打包，Parcel本身可以作为一个整体在进程间传递。接收方在收到Parcel之后，只要按写入同样的顺序读出即可。 这个过程，和我们现实生活中寄送包裹做法是一样的：我们将需要寄送的包裹放到硬纸盒中交给快递公司。快递公司将所有的包裹进行打包，然后集中放到运输车中送到目的地，到了目的地之后然后再进行拆分。 Parcel既包含C++部分的实现，也同时提供了Java的接口，中间通过JNI衔接。Java层的接口其实仅仅是一层包装，真正的实现都是位于C++部分中。 特别需要说明一下的是，Parcel类除了可以传递基本数据类型，还可以传递Binder对象： 1234status_t Parcel::writeStrongBinder(const sp&lt;IBinder&gt;&amp; val)&#123; return flatten_binder(ProcessState::self(), val, this);&#125; 这个方法写入的是sp 类型的对象，而IBinder既可能是本地Binder，也可能是远程Binder，这样我们就不可以不用关心具体细节直接进行Binder对象的传递。 这也是为什么IInterface中定义了两个asBinder的static方法，如果你不记得了，请回忆一下这两个方法： 12static sp&lt;IBinder&gt; asBinder(const IInterface*);static sp&lt;IBinder&gt; asBinder(const sp&lt;IInterface&gt;&amp;); 而对于Binder驱动，我们前面已经讲解过：Binder驱动并不是真的将对象在进程间序列化传递，而是由Binder驱动完成了对于Binder对象指针的解释和翻译，使调用者看起来就像在进程间传递对象一样。 2.4、Framework层的线程管理在讲解Binder驱动的时候，我们就讲解过驱动中对应线程的管理。这里我们再来看看，Framework层是如何与驱动层对接进行线程管理的。 ProcessState::setThreadPoolMaxThreadCount 方法中，会通过BINDER_SET_MAX_THREADS命令设置进程支持的最大线程数量： 123456789101112#define DEFAULT_MAX_BINDER_THREADS 15status_t ProcessState::setThreadPoolMaxThreadCount(size_t maxThreads) &#123; status_t result = NO_ERROR; if (ioctl(mDriverFD, BINDER_SET_MAX_THREADS, &amp;maxThreads) != -1) &#123; mMaxThreads = maxThreads; &#125; else &#123; result = -errno; ALOGE(\"Binder ioctl to set max threads failed: %s\", strerror(-result)); &#125; return result;&#125; 由此驱动便知道了该Binder服务支持的最大线程数。驱动在运行过程中，会根据需要，并在没有超过上限的情况下，通过BR_SPAWN_LOOPER命令通知进程创建线程： IPCThreadState在收到BR_SPAWN_LOOPER请求之后，便会调用ProcessState::spawnPooledThread来创建线程： 12345678status_t IPCThreadState::executeCommand(int32_t cmd)&#123; ... case BR_SPAWN_LOOPER: mProcess-&gt;spawnPooledThread(false); break; ...&#125; ProcessState::spawnPooledThread方法负责为线程设定名称并创建线程： 123456789void ProcessState::spawnPooledThread(bool isMain)&#123; if (mThreadPoolStarted) &#123; String8 name = makeBinderThreadName(); ALOGV(\"Spawning new pooled thread, name=%s\\n\", name.string()); sp&lt;Thread&gt; t = new PoolThread(isMain); t-&gt;run(name.string()); &#125;&#125; 线程在run之后，会调用threadLoop将自身添加的线程池中： 12345virtual bool threadLoop()&#123; IPCThreadState::self()-&gt;joinThreadPool(mIsMain); return false;&#125; 而IPCThreadState::joinThreadPool方法中，会根据当前线程是否是主线程发送BC_ENTER_LOOPER或者BC_REGISTER_LOOPER命令告知驱动线程已经创建完毕。整个调用流程如下图所示： （3）、Android Binder系统-Native层添加hello服务3.1、Client构造数据，发送数据给驱动首先看一下Native ServiceManager架构图 只讲数据构造过程。。 构造： [-&gt; IServiceManager.cpp ::BpServiceManager] 1234567891011virtual status_t addService(const String16&amp; name, const sp&lt;IBinder&gt;&amp; service, bool allowIsolated) &#123; Parcel data, reply; //Parcel是数据通信包 //写入头信息\"android.os.IServiceManager\" data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor()); data.writeString16(name); // name为 \"hello\" data.writeStrongBinder(service); // HelloService对象，把一个binder实体“打扁”并写入parcel data.writeInt32(allowIsolated ? 1 : 0); // allowIsolated= false //remote()指向的是BpBinder对象 status_t err = remote()-&gt;transact(ADD_SERVICE_TRANSACTION, data, &amp;reply); return err == NO_ERROR ? reply.readExceptionCode() : err;&#125; 服务注册过程：向ServiceManager注册服务hello Service，服务名为”hello”； 请大家注意上面data.writeStrongBinder()一句，它专门负责把一个binder实体”打扁”并写入parcel。其代码如下： 3.2.1、* writeStrongBinder()[-&gt; parcel.cpp] 1234status_t Parcel::writeStrongBinder(const sp&lt;IBinder&gt;&amp; val)&#123; return flatten_binder(ProcessState::self(), val, this);&#125; 3.2.2、flatten_binder()[-&gt; parcel.cpp] 12345678910111213141516171819202122232425status_t flatten_binder(const sp&lt;ProcessState&gt;&amp; /*proc*/, const sp&lt;IBinder&gt;&amp; binder, Parcel* out)&#123; flat_binder_object obj; obj.flags = 0x7f | FLAT_BINDER_FLAG_ACCEPTS_FDS; if (binder != NULL) &#123; IBinder *local = binder-&gt;localBinder(); //本地Binder不为空 if (!local) &#123; BpBinder *proxy = binder-&gt;remoteBinder(); const int32_t handle = proxy ? proxy-&gt;handle() : 0; obj.type = BINDER_TYPE_HANDLE; obj.binder = 0; obj.handle = handle; obj.cookie = 0; &#125; else &#123; //进入该分支 obj.type = BINDER_TYPE_BINDER; obj.binder = reinterpret_cast&lt;uintptr_t&gt;(local-&gt;getWeakRefs()); obj.cookie = reinterpret_cast&lt;uintptr_t&gt;(local); &#125; &#125; else &#123; ... &#125; return finish_flatten_binder(binder, obj, out);&#125; 将Binder对象扁平化，转换成flat_binder_object对象。 看到了吗？”打扁”的意思就是把binder对象整理成flat_binder_object变量，如果打扁的是binder实体，那么flat_binder_object用cookie域记录binder实体的指针，即BBinder指针，而如果打扁的是binder代理，那么flat_binder_object用handle域记录的binder代理的句柄值。 总结：Parcel的数据区域分两个部分：mData和mObjects，所有的数据不管是基础数据类型还是对象实体，全都追加到mData里，mObjects是一个偏移量数组，记录所有存放在mData中的flat_binder_object实体的偏移量。 3.2.3、finish_flatten_binder()将flat_binder_object写入out。 12345inline static status_t finish_flatten_binder( const sp&lt;IBinder&gt;&amp; , const flat_binder_object&amp; flat, Parcel* out)&#123; return out-&gt;writeObject(flat, false);&#125; 然后flatten_binder()调用了一个关键的finish_flatten_binder()函数。这个函数内部会记录下刚刚被扁平化的flat_binder_object在parcel中的位置。说得更详细点儿就是，parcel对象内部会有一个buffer，记录着parcel中所有扁平化的数据，有些扁平数据是普通数据，而另一些扁平数据则记录着binder对象。所以parcel中会构造另一个mObjects数组，专门记录那些binder扁平数据所在的位置，示意图如下： 一旦到了向驱动层传递数据的时候，IPCThreadState::writeTransactionData()会先把Parcel数据整理成一个binder_transaction_data数据 1234567891011121314151617181920212223242526272829303132333435status_t IPCThreadState::writeTransactionData(int32_t cmd, uint32_t binderFlags, int32_t handle, uint32_t code, const Parcel&amp; data, status_t* statusBuffer)&#123; binder_transaction_data tr; tr.target.ptr = 0; /* Don't pass uninitialized stack data to a remote process */ tr.target.handle = handle; tr.code = code; tr.flags = binderFlags; tr.cookie = 0; tr.sender_pid = 0; tr.sender_euid = 0; const status_t err = data.errorCheck(); if (err == NO_ERROR) &#123; tr.data_size = data.ipcDataSize(); tr.data.ptr.buffer = data.ipcData(); tr.offsets_size = data.ipcObjectsCount()*sizeof(binder_size_t); tr.data.ptr.offsets = data.ipcObjects(); &#125; else if (statusBuffer) &#123; tr.flags |= TF_STATUS_CODE; *statusBuffer = err; tr.data_size = sizeof(status_t); tr.data.ptr.buffer = reinterpret_cast&lt;uintptr_t&gt;(statusBuffer); tr.offsets_size = 0; tr.data.ptr.offsets = 0; &#125; else &#123; return (mLastError = err); &#125; mOut.writeInt32(cmd); mOut.write(&amp;tr, sizeof(tr)); return NO_ERROR;&#125; 3.2.4 、waitForResponse()1234567891011121314151617181920212223status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult)&#123; int32_t cmd; int32_t err; while (1) &#123; if ((err=talkWithDriver()) &lt; NO_ERROR) break;//目的就是把上面打包的mOut数据给kernel,接着看taklWithDriver(); cmd = mIn.readInt32(); &#125; switch (cmd) &#123; case BR_REPLY: ...... goto finish; default: err = executeCommand(cmd); break; &#125; &#125;finish: ..... return err;&#125; 该函数是与serviceManager通信的主要函数，首先会调用talkWithDriver()方法，将之前的打包在mOut中的数据打包成struct binder_write_read 对象，并通过ioctrl发送给kernel。 3.2.5、 IPCThreadState::talkWithDriver12345678910111213141516171819202122232425262728status_t IPCThreadState::talkWithDriver(bool doReceive)&#123; binder_write_read bwr; const bool needRead = mIn.dataPosition() &gt;= mIn.dataSize(); const size_t outAvail = (!doReceive || needRead) ? mOut.dataSize() : 0; //doReceive参数，默认是为true,上面我们看到没有传参数，那么doReceive = 1； bwr.write_size = outAvail; bwr.write_buffer = (uintptr_t)mOut.data(); //将mOut数据指针存放到这里,这就是我们上面打包的数据。 // This is what we'll read. if (doReceive &amp;&amp; needRead) &#123; bwr.read_size = mIn.dataCapacity(); //注意这里数据的大小，在我们new IPCThreadState对象时，已经初始化为256. bwr.read_buffer = (uintptr_t)mIn.data(); //mIn数据指针，放到这里 &#125; else &#123; bwr.read_size = 0; bwr.read_buffer = 0; &#125; //...... bwr.write_consumed = 0; bwr.read_consumed = 0; status_t err; do &#123; if (ioctl(mProcess-&gt;mDriverFD, BINDER_WRITE_READ, &amp;bwr) &gt;= 0) //这里通过ioctl将数据写给kernel ..... &#125; while (err == -EINTR); return err;&#125; 该函数的作用就是将之前打包的数据通过系统调用ioctl发送给kernel，最终发送给kernel的数据是struct binder_write_read对象。该对象已经被打包了3次，它们的包含关系如下所示。 3.2.6、Client获取服务、处理回复数据过程内核会唤醒Client进程处理回复消息。 12345678910111213141516171819202122232425262728status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult)&#123; int32_t cmd; int32_t err; while (1) &#123; if ((err=talkWithDriver()) &lt; NO_ERROR) break; ... cmd = mIn.readInt32(); switch (cmd) &#123; case BR_REPLY: &#123; binder_transaction_data tr; err = mIn.read(&amp;tr, sizeof(tr)); if (reply) &#123; if ((tr.flags &amp; TF_STATUS_CODE) == 0) &#123; //当reply对象回收时，则会调用freeBuffer来回收内存 reply-&gt;ipcSetDataReference( reinterpret_cast&lt;const uint8_t*&gt;(tr.data.ptr.buffer), tr.data_size, reinterpret_cast&lt;const binder_size_t*&gt;(tr.data.ptr.offsets), tr.offsets_size/sizeof(binder_size_t), freeBuffer, this); &#125; ... &#125; ... return err;&#125; 3.2.7、Parcel::ipcSetDataReference12345678910111213141516171819202122232425262728void Parcel::ipcSetDataReference(const uint8_t* data, size_t dataSize, const binder_size_t* objects, size_t objectsCount, release_func relFunc, void* relCookie)&#123; binder_size_t minOffset = 0; freeDataNoInit(); mError = NO_ERROR; mData = const_cast&lt;uint8_t*&gt;(data); //这是有4个字节的buffer。且存放的数据是0 mDataSize = mDataCapacity = dataSize; //之前申请的大小就是4个字节。 //ALOGI(\"setDataReference Setting data size of %p to %lu (pid=%d)\", this, mDataSize, getpid()); mDataPos = 0; ALOGV(\"setDataReference Setting data pos of %p to %zu\", this, mDataPos); mObjects = const_cast&lt;binder_size_t*&gt;(objects); //binder对象其实地址 mObjectsSize = mObjectsCapacity = objectsCount; //binder对象的个数。 mNextObjectHint = 0; mOwner = relFunc; //释放内存的函数，后面我们就不进行了。 mOwnerCookie = relCookie; for (size_t i = 0; i &lt; mObjectsSize; i++) &#123; binder_size_t offset = mObjects[i]; if (offset &lt; minOffset) &#123; ALOGE(\"%s: bad object offset %\"PRIu64\" &lt; %\"PRIu64\"\\n\", __func__, (uint64_t)offset, (uint64_t)minOffset); mObjectsSize = 0; break; &#125; minOffset = offset + sizeof(flat_binder_object); &#125; scanForFds();&#125; 上面做的工作只是将事务数据分别安放到当前Parcel对象的相应位置。其中scanForFds（）是为了查找返回来的数据中是否有binder对象，这个在获取代理对象时有用。 3.2.8、readStrongBinder()[-&gt; Parcel.java] readStrongBinder的过程基本是writeStrongBinder逆过程。 1234567static jobject android_os_Parcel_readStrongBinder(JNIEnv* env, jclass clazz, jlong nativePtr) &#123; Parcel* parcel = reinterpret_cast&lt;Parcel*&gt;(nativePtr); if (parcel != NULL) &#123; return javaObjectForIBinder(env, parcel-&gt;readStrongBinder()); &#125; return NULL;&#125; javaObjectForIBinder 将native层BpBinder对象转换为Java层BinderProxy对象。 3.2.9、readStrongBinder(C++)[-&gt; Parcel.cpp] 123456sp&lt;IBinder&gt; Parcel::readStrongBinder() const&#123; sp&lt;IBinder&gt; val; unflatten_binder(ProcessState::self(), *this, &amp;val); return val;&#125; 3.2.10、unflatten_binder()[-&gt; Parcel.cpp] 12345678910111213141516status_t unflatten_binder(const sp&lt;ProcessState&gt;&amp; proc, const Parcel&amp; in, sp&lt;IBinder&gt;* out) &#123; const flat_binder_object* flat = in.readObject(false); if (flat) &#123; switch (flat-&gt;type) &#123; case BINDER_TYPE_BINDER: *out = reinterpret_cast&lt;IBinder*&gt;(flat-&gt;cookie); return finish_unflatten_binder(NULL, *flat, in); case BINDER_TYPE_HANDLE: *out = proc-&gt;getStrongProxyForHandle(flat-&gt;handle); //创建BpBinder对象 return finish_unflatten_binder( static_cast&lt;BpBinder*&gt;(out-&gt;get()), *flat, in); &#125; &#125; return BAD_TYPE;&#125; 说明：readObject()的作用是从Parcel中读取出它所保存的flat_binder_object类型的对象。该对象的类型是BINDER_TYPE_HANDLE，因此会指向BINDER_TYPE_HANDLE对应的switch分支。 (01) 这里的proc是ProcessState对象，执行proc-&gt;getStrongProxyForHandle()会将句柄(MediaPlayerService的Binder引用描述)保存到ProcessState的链表中，然后再创建并返回该句柄的BpBinder对象(即Binder的代理)。在Android Binder机制(四) defaultServiceManager()的实现中有getStrongProxyForHandle()的详细说明，下面只给出getStrongProxyForHandle()代码。 (02) finish_unflatten_binder()中只有return NO_ERROR。 3.2.11、getStrongProxyForHandle()[-&gt; ProcessState.cpp] 123456789101112131415161718192021222324sp&lt;IBinder&gt; ProcessState::getStrongProxyForHandle(int32_t handle)&#123; sp&lt;IBinder&gt; result; AutoMutex _l(mLock); //查找handle对应的资源项 handle_entry* e = lookupHandleLocked(handle); if (e != NULL) &#123; IBinder* b = e-&gt;binder; if (b == NULL || !e-&gt;refs-&gt;attemptIncWeak(this)) &#123; ... //当handle值所对应的IBinder不存在或弱引用无效时，则创建BpBinder对象 b = new BpBinder(handle); e-&gt;binder = b; if (b) e-&gt;refs = b-&gt;getWeakRefs(); result = b; &#125; else &#123; result.force_set(b); e-&gt;refs-&gt;decWeak(this); &#125; &#125; return result;&#125; 经过该方法，最终创建了指向Binder服务端的BpBinder代理对象。 （4）、Android Binder系统-Native层获取hello服务经过前面的分析，知道流程基本类似，这里不再继续分析获取hello服务 五、Android Binder系统-Framwork-Java层（1）、Android Binder系统Java层主要结构 Android应用程序使用Java语言开发，Binder框架自然也少不了在Java层提供接口。 前文中我们看到，Binder机制在C++层已经有了完整的实现。因此Java层完全不用重复实现，而是通过JNI衔接了C++层以复用其实现。 下图描述了Binder Framework Java层到C++层的衔接关系。 这里对图中Java层和JNI层的几个类做一下说明( 关于C++层的讲解请看这里 )： 这里的IInterface，IBinder和C++层的两个类是同名的。这个同名并不是巧合：它们不仅仅同名，它们所起的作用，以及其中包含的接口都是几乎一样的，区别仅仅在于一个是C++层，一个是Java层而已。 除了IInterface，IBinder之外，这里Binder与BinderProxy类也是与C++的类对应的，下面列出了Java层和C++层类的对应关系： 除了IInterface，IBinder之外，这里Binder与BinderProxy类也是与C++的类对应的，下面列出了Java层和C++层类的对应关系： （2）、JNI的衔接JNI全称是Java Native Interface，这个是由Java虚拟机提供的机制。这个机制使得native代码可以和Java代码互相通讯。简单来说就是：我们可以在C/C++端调用Java代码，也可以在Java端调用C/C++代码。 关于JNI的详细说明，可以参见Oracle的官方文档：Java Native Interface ，这里不多说明。 实际上，在Android中很多的服务或者机制都是在C/C++层实现的，想要将这些实现复用到Java层，就必须通过JNI进行衔接。AOSP源码中，/frameworks/base/core/jni/ 目录下的源码就是专门用来对接Framework层的JNI实现的。 看一下Binder.java的实现就会发现，这里面有不少的方法都是用native关键字修饰的，并且没有方法实现体，这些方法其实都是在C++中android_util_Binder.cpp实现的： 那么，那么，C++是如何调用Java的呢？最关键的，libbinder中的BBinder::onTransact是如何能够调用到Java中的Binder::onTransact的呢？ 这段逻辑就是android_util_Binder.cpp中JavaBBinder::onTransact中处理的了。JavaBBinder是BBinder子类，其类结构如下：libbinder中的BBinder::onTransact是如何能够调用到Java中的Binder::onTransact的呢？ JavaBBinder::onTransact关键代码如下： 123456789101112virtual status_t onTransact( uint32_t code, const Parcel&amp; data, Parcel* reply, uint32_t flags = 0)&#123; JNIEnv* env = javavm_to_jnienv(mVM); IPCThreadState* thread_state = IPCThreadState::self(); const int32_t strict_policy_before = thread_state-&gt;getStrictModePolicy(); jboolean res = env-&gt;CallBooleanMethod(mObject, gBinderOffsets.mExecTransact, code, reinterpret_cast&lt;jlong&gt;(&amp;data), reinterpret_cast&lt;jlong&gt;(reply), flags); ...&#125; 请注意这段代码中的这一行： 12jboolean res = env-&gt;CallBooleanMethod(mObject, gBinderOffsets.mExecTransact, code, reinterpret_cast&lt;jlong&gt;(&amp;data), reinterpret_cast&lt;jlong&gt;(reply), flags); 这一行代码其实是在调用mObject上offset为mExecTransact的方法。这里的几个参数说明如下： mObject 指向了Java端的Binder对象 gBinderOffsets.mExecTransact 指向了Binder类的execTransact方法 data 调用execTransact方法的参数 code, data, reply, flags都是传递给调用方法execTransact的参数 而JNIEnv.CallBooleanMethod这个方法是由虚拟机实现的。即：虚拟机会提供native方法来调用一个Java Object上的方法（关于Android上的Java虚拟机，今后我们会专门讲解）。 这样，就在C++层的JavaBBinder::onTransact中调用了Java层Binder::execTransact方法。而在Binder::execTransact方法中，又调用了自身的onTransact方法，由此保证整个过程串联了起来： 123456789101112131415161718192021222324252627282930313233343536private boolean execTransact(int code, long dataObj, long replyObj, int flags) &#123; Parcel data = Parcel.obtain(dataObj); Parcel reply = Parcel.obtain(replyObj); boolean res; try &#123; res = onTransact(code, data, reply, flags); &#125; catch (RemoteException|RuntimeException e) &#123; if (LOG_RUNTIME_EXCEPTION) &#123; Log.w(TAG, \"Caught a RuntimeException from the binder stub implementation.\", e); &#125; if ((flags &amp; FLAG_ONEWAY) != 0) &#123; if (e instanceof RemoteException) &#123; Log.w(TAG, \"Binder call failed.\", e); &#125; else &#123; Log.w(TAG, \"Caught a RuntimeException from the binder stub implementation.\", e); &#125; &#125; else &#123; reply.setDataPosition(0); reply.writeException(e); &#125; res = true; &#125; catch (OutOfMemoryError e) &#123; RuntimeException re = new RuntimeException(\"Out of memory\", e); reply.setDataPosition(0); reply.writeException(re); res = true; &#125; checkParcel(this, code, reply, \"Unreasonably large binder reply buffer\"); reply.recycle(); data.recycle(); StrictMode.clearGatheredViolations(); return res;&#125; （3）、Java层的ServiceManager 通过这个类图我们看到，Java层的ServiceManager和C++层的接口是一样的。 通过这个类图我们看到，Java层的ServiceManager和C++层的接口是一样的。 然后我们再选取addService方法看一下实现： 1234567891011121314151617public static void addService(String name, IBinder service, boolean allowIsolated) &#123; try &#123; getIServiceManager().addService(name, service, allowIsolated); &#125; catch (RemoteException e) &#123; Log.e(TAG, \"error in addService\", e); &#125;&#125; private static IServiceManager getIServiceManager() &#123; if (sServiceManager != null) &#123; return sServiceManager; &#125; // Find the service manager sServiceManager = ServiceManagerNative.asInterface(BinderInternal.getContextObject()); return sServiceManager;&#125; 很显然，这段代码中，最关键就是下面这个调用： 1ServiceManagerNative.asInterface(BinderInternal.getContextObject()); 然后我们需要再看一下BinderInternal.getContextObject()和ServiceManagerNative.asInterface两个方法。 BinderInternal.getContextObject()是一个JNI方法，其实现代码在android_util_Binder.cpp中： 12345static jobject android_os_BinderInternal_getContextObject(JNIEnv* env, jobject clazz)&#123; sp&lt;IBinder&gt; b = ProcessState::self()-&gt;getContextObject(NULL); return javaObjectForIBinder(env, b);&#125; 而ServiceManagerNative.asInterface的实现和其他的Binder服务是一样的套路： 12345678910111213static public IServiceManager asInterface(IBinder obj)&#123; if (obj == null) &#123; return null; &#125; IServiceManager in = (IServiceManager)obj.queryLocalInterface(descriptor); if (in != null) &#123; return in; &#125; return new ServiceManagerProxy(obj);&#125; 先通过queryLocalInterface查看能不能获得本地Binder，如果无法获取，则创建并返回ServiceManagerProxy对象。 而ServiceManagerProxy自然也是和其他Binder Proxy一样的实现套路： 123456789101112public void addService(String name, IBinder service, boolean allowIsolated) throws RemoteException &#123; Parcel data = Parcel.obtain(); Parcel reply = Parcel.obtain(); data.writeInterfaceToken(IServiceManager.descriptor); data.writeString(name); data.writeStrongBinder(service); data.writeInt(allowIsolated ? 1 : 0); mRemote.transact(ADD_SERVICE_TRANSACTION, data, reply, 0); reply.recycle(); data.recycle();&#125; 接下来的调用流程前面已经分析过了，在此就不再分析了。 六、Android Binder系统-AIDL作为Binder机制的最后一个部分内容，我们来讲解一下开发者经常使用的AIDL机制是怎么回事。 AIDL全称是Android Interface Definition Language，它是Android SDK提供的一种机制。借助这个机制，应用可以提供跨进程的服务供其他应用使用。AIDL的详细说明可以参见官方开发文档：https://developer.android.com/guide/components/aidl.html 。 这里，我们就以官方文档上的例子看来一下AIDL与Binder框架的关系。 开发一个基于AIDL的Service需要三个步骤： 定义一个.aidl文件 实现接口 暴露接口给客户端使用 aidl文件使用Java语言的语法来定义，每个.aidl文件只能包含一个interface，并且要包含interface的所有方法声明。 默认情况下，AIDL支持的数据类型包括： 基本数据类型（即int，long，char，boolean等） String CharSequence List（List的元素类型必须是AIDL支持的） Map（Map中的元素必须是AIDL支持的） 对于AIDL中的接口，可以包含0个或多个参数，可以返回void或一个值。所有非基本类型的参数必须包含一个描述是数据流向的标签，可能的取值是：in，out或者inout。 下面是一个aidl文件的示例： 12345678910111213141516// IRemoteService.aidlpackage com.example.android;// Declare any non-default types here with import statements/** Example service interface */interface IRemoteService &#123; /** Request the process ID of this service, to do evil things with it. */ int getPid(); /** Demonstrates some basic types that you can use as parameters * and return values in AIDL. */ void basicTypes(int anInt, long aLong, boolean aBoolean, float aFloat, double aDouble, String aString);&#125; 这个文件中包含了两个接口 ： getPid 一个无参的接口，返回值类型为int basicTypes，包含了几个基本类型作为参数的接口，无返回值 对于包含.aidl文件的工程，Android IDE（以前是Eclipse，现在是Android Studio）在编译项目的时候，会为aidl文件生成对应的Java文件。 针对上面这个aidl文件生成的java文件中包含的结构如下图所示： 在这个生成的Java文件中，包括了： 一个名称为IRemoteService的interface，该interface继承自android.os.IInterface并且包含了我们在aidl文件中声明的接口方法 IRemoteService中包含了一个名称为Stub的静态内部类，这个类是一个抽象类，它继承自android.os.Binder并且实现了IRemoteService接口。这个类中包含了一个onTransact方法 Stub内部又包含了一个名称为Proxy的静态内部类，Proxy类同样实现了IRemoteService接口 仔细看一下Stub类和Proxy两个中包含的方法，是不是觉得很熟悉？是的，这里和前面介绍的服务实现是一样的模式。这里我们列一下各层类的对应关系： C++层 Java层 AIDL BpXXX XXXProxy IXXX.Stub.Proxy BnXXX XXXNative IXXX.Stub 为了整个结构的完整性，最后我们还是来看一下生成的Stub和Proxy类中的实现逻辑。 Stub是提供给开发者实现业务的父类，而Proxy的实现了对外提供的接口。Stub和Proxy两个类都有一个asBinder的方法。 Stub类中的asBinder实现就是返回自身对象： 1234Overridepublic android.os.IBinder asBinder() &#123; return this;&#125; 而Proxy中asBinder的实现是返回构造函数中获取的mRemote对象，相关代码如下： 12345678910private android.os.IBinder mRemote;Proxy(android.os.IBinder remote) &#123; mRemote = remote;&#125;Overridepublic android.os.IBinder asBinder() &#123; return mRemote;&#125; 而这里的mRemote对象其实就是远程服务在当前进程的标识。 上文我们说了，Stub类是用来提供给开发者实现业务逻辑的父类，开发者者继承自Stub然后完成自己的业务逻辑实现，例如这样： 123456789private final IRemoteService.Stub mBinder = new IRemoteService.Stub() &#123; public int getPid()&#123; return Process.myPid(); &#125; public void basicTypes(int anInt, long aLong, boolean aBoolean, float aFloat, double aDouble, String aString) &#123; // Does something &#125;&#125;; 而这个Proxy类，就是用来给调用者使用的对外接口。我们可以看一下Proxy中的接口到底是如何实现的： Proxy中getPid方法实现如下所示： 12345678910111213141516Overridepublic int getPid() throws android.os.RemoteException &#123; android.os.Parcel _data = android.os.Parcel.obtain(); android.os.Parcel _reply = android.os.Parcel.obtain(); int _result; try &#123; _data.writeInterfaceToken(DESCRIPTOR); mRemote.transact(Stub.TRANSACTION_getPid, _data, _reply, 0); _reply.readException(); _result = _reply.readInt(); &#125; finally &#123; _reply.recycle(); _data.recycle(); &#125; return _result;&#125; 这里就是通过Parcel对象以及transact调用对应远程服务的接口。而在Stub类中，生成的onTransact方法对应的处理了这里的请求： 123456789101112131415161718192021222324252627282930313233343536Overridepublic boolean onTransact(int code, android.os.Parcel data, android.os.Parcel reply, int flags) throws android.os.RemoteException &#123; switch (code) &#123; case INTERFACE_TRANSACTION: &#123; reply.writeString(DESCRIPTOR); return true; &#125; case TRANSACTION_getPid: &#123; data.enforceInterface(DESCRIPTOR); int _result = this.getPid(); reply.writeNoException(); reply.writeInt(_result); return true; &#125; case TRANSACTION_basicTypes: &#123; data.enforceInterface(DESCRIPTOR); int _arg0; _arg0 = data.readInt(); long _arg1; _arg1 = data.readLong(); boolean _arg2; _arg2 = (0 != data.readInt()); float _arg3; _arg3 = data.readFloat(); double _arg4; _arg4 = data.readDouble(); java.lang.String _arg5; _arg5 = data.readString(); this.basicTypes(_arg0, _arg1, _arg2, _arg3, _arg4, _arg5); reply.writeNoException(); return true; &#125; &#125; return super.onTransact(code, data, reply, flags);&#125; onTransact()所要做的就是： 根据code区分请求的是哪个接口 通过data来获取请求的参数 调用由子类实现的抽象方法 有了前文的讲解，对于这部分内容应当不难理解了。 到这里，我们终于讲解完Binder了。 完整框架： 七、参考文档(特别感谢各位前辈的分析和图示)：Binder源码分析深入分析Android BinderBinder系列 - Gityuan博客 | 袁辉辉博客Android Binder机制(1) ~ (12) - Wangkuiwu.github.ioBinder机制-关于Binder的文章 - 泡在网上的日子理解Android Binder机制 - Qiangbo.space博客红茶一杯话Binder - 悠然红茶Binder框架解析Android Binder详解图文详解 Android Binder跨进程通信机制 原理理解Android Binder机制(1/3)：驱动篇-qiangbo.space理解Android Binder机制(2/3)：C++层-qiangbo.space理解Android Binder机制(3/3)：Java层-qiangbo.spaceAndroid Binder 分析–系列-light3moonAndroid学习笔记-Binder | Palance’s Blogandroid系统 -Binder - armwind的专栏 - CSDN博客Bettarwang的专栏 -Android Binder机制深入剖析Android系统 - binder - CSDN博客","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android 7.1.2 (Android N) Android 输入子系统 - Input System 分析","slug":"Android-7-1-2-Android-N-Android-输入子系统-Input-System","date":"2017-11-30T16:00:00.000Z","updated":"2018-04-19T14:29:52.579Z","comments":true,"path":"2017/12/01/Android-7-1-2-Android-N-Android-输入子系统-Input-System/","link":"","permalink":"http://zhoujinjian.cc/2017/12/01/Android-7-1-2-Android-N-Android-输入子系统-Input-System/","excerpt":"Android 输入子系统概述:● 当时输入设备（如触摸屏，键盘等）可用时，Linux Kernel会在/dev/input/下创建名为event0~eventN的设备节点; 当输入设备不可用时，会将相应的设备节点删除。● 当用户操作输入设备时，Linux Kernel会收到相应的硬件中断，然后会将中断加工成原始输入事件（raw input event），并写入到设备节点中。而后在用户空间就可以通过read()函数读取事件数据了。● Android输入系统会监控/dev/input/下的所有设备节点，当某个结点有数据可读时，将数据读出并进行一系列处理，然后在当前系统中的所有窗口（Window）中寻找合适的接收者，并把事件派发给它。● 具体来说，Linux Kernel将raw input event写入到设备节点后，InputReader会通过EventHub将原始事件读取出来并翻译加工为Android输入事件，而后把它交给InputDispatcher。InputDispatcher根据WMS（WindowManagerService）提供的窗口信息将事件传递给合适的窗口，若窗口为壁纸/SurfaceView等，则到了终点；否则会由该Window的ViewRoot继续分发到合适的View。","text":"Android 输入子系统概述:● 当时输入设备（如触摸屏，键盘等）可用时，Linux Kernel会在/dev/input/下创建名为event0~eventN的设备节点; 当输入设备不可用时，会将相应的设备节点删除。● 当用户操作输入设备时，Linux Kernel会收到相应的硬件中断，然后会将中断加工成原始输入事件（raw input event），并写入到设备节点中。而后在用户空间就可以通过read()函数读取事件数据了。● Android输入系统会监控/dev/input/下的所有设备节点，当某个结点有数据可读时，将数据读出并进行一系列处理，然后在当前系统中的所有窗口（Window）中寻找合适的接收者，并把事件派发给它。● 具体来说，Linux Kernel将raw input event写入到设备节点后，InputReader会通过EventHub将原始事件读取出来并翻译加工为Android输入事件，而后把它交给InputDispatcher。InputDispatcher根据WMS（WindowManagerService）提供的窗口信息将事件传递给合适的窗口，若窗口为壁纸/SurfaceView等，则到了终点；否则会由该Window的ViewRoot继续分发到合适的View。 本章涉及的源代码文件名及位置： frameworks/base/services/java/com/android/server/● SystemServer.java frameworks/base/services/java/com/android/server/input/● InputManagerService.java frameworks/base/services/java/com/android/server/wm/● WindowManagerService.java● WindowState.java● InputMonitor.java frameworks/base/core/java/android/view/● View.java● ViewGroup.java● InputEventReceiver.java● ViewRootImpl.java● IWindowSession.aidl● InputChannel.java frameworks/base/core/java/android/app/● Activity.java frameworks/base/services/jni/● android_view_InputChannel.cpp● android_view_InputEventReceiver.cpp● com_android_server_input_InputManagerService.cpp frameworks/native/services/inputflinger/● InputManager.cpp● EventHub.h● EventHub.cpp● InputReader.h● InputReader.cpp● InputListener.h● InputListener.cpp● InputDispatcher.h● InputDispatcher.cpp frameworks/native/libs/input/● InputTransport.cpp /frameworks/native/include/input/● InputTransport.h 博客原图链接一、Input系统必备Linux知识注：必备知识可稍后遇到实际使用的地方再做详细了解。 （一）、必备的Linux知识 inotify和epoll1、INotify介绍与使用INotify是一个Linux内核所提供的一种文件系统变化通知机制。它可以为应用程序监控文件系统的变化，如文件的新建、删除、读写等。INotify机制有两个基本对象，分别为inotify对象与watch对象，都使用文件描述符表示。 inotify对象对应了一个队列，应用程序可以向inotify对象添加多个监听。当被监听的事件发生时，可以通过read()函数从inotify对象中将事件信息读取出来。Inotify对象可以通过以下方式创建： 1int inotifyFd = inotify_init(); 而watch对象则用来描述文件系统的变化事件的监听。它是一个二元组，包括监听目标和事件掩码两个元素。监听目标是文件系统的一个路径，可以是文件也可以是文件夹。而事件掩码则表示了需要需要监听的事件类型，掩码中的每一位代表一种事件。可以监听的事件种类很多，其中就包括文件的创建(IN_CREATE)与删除(IN_DELETE)。读者可以参阅相关资料以了解其他可监听的事件种类。以下代码即可将一个用于监听输入设备节点的创建与删除的watch对象添加到inotify对象中： 1int wd = inotify_add_watch (inotifyFd, “/dev/input”,IN_CREATE | IN_DELETE); 完成上述watch对象的添加后，当/dev/input/下的设备节点发生创建与删除操作时，都会将相应的事件信息写入到inotifyFd所描述的inotify对象中，此时可以通过read()函数从inotifyFd描述符中将事件信息读取出来。 事件信息使用结构体inotify_event进行描述： 1234567struct inotify_event &#123; __s32 wd; /* 事件对应的Watch对象的描述符 */ __u32 mask; /* 事件类型，例如文件被删除，此处值为IN_DELETE */ __u32 cookie; __u32 len; /* name字段的长度 */ char name[0]; /* 可变长的字段，用于存储产生此事件的文件路径*/ &#125;; 当没有监听事件发生时，可以通过如下方式将一个或多个未读取的事件信息读取出来： 1size_t len = read (inotifyFd, events_buf,BUF_LEN); 其中events_buf是inotify_event的数组指针，能够读取的事件数量由取决于数组的长度。成功读取事件信息后，便可根据inotify_event结构体的字段判断事件类型以及产生事件的文件路径了。 总结一下INotify机制的使用过程： · 通过inotify_init()创建一个inotify对象。 · 通过inotify_add_watch将一个或多个监听添加到inotify对象中。 · 通过read()函数从inotify对象中读取监听事件。当没有新事件发生时，inotify对象中无任何可读数据。 通过INotify机制避免了轮询文件系统的麻烦，但是还有一个问题，INotify机制并不是通过回调的方式通知事件，而需要使用者主动从inotify对象中进行事件读取。那么何时才是读取的最佳时机呢？这就需要借助Linux的另一个优秀的机制Epoll了。 使用inotify监听目录实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/inotify.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;//参考: frameworks\\native\\services\\inputflinger\\EventHub.cpp//Usage: inotify &lt;dir&gt;int read_process_inotify_fd(int fd)&#123;int res;char event_buf[512];int event_size;int event_pos = 0;struct inotify_event *event;/* read */ res = read(fd, event_buf, sizeof(event_buf));if(res &lt; (int)sizeof(*event)) &#123; if(errno == EINTR) return 0; printf(\"could not get event, %s\\n\", strerror(errno)); return -1;&#125;//读到的数据是1个或多个inotify_event,它们的长度不一样,逐个处理while(res &gt;= (int)sizeof(*event)) &#123; event = (struct inotify_event *)(event_buf + event_pos); //printf(\"%d: %08x \\\"%s\\\"\\n\", event-&gt;wd, event-&gt;mask, event-&gt;len ? event-&gt;name : \"\"); if(event-&gt;len) &#123; if(event-&gt;mask &amp; IN_CREATE) &#123; printf(\"create file: %s\\n\", event-&gt;name); &#125; else &#123; printf(\"delete file: %s\\n\", event-&gt;name); &#125; &#125; event_size = sizeof(*event) + event-&gt;len; res -= event_size; event_pos += event_size;&#125;return 0;&#125;int main(int argc, char **argv)&#123;int mINotifyFd;int result;if (argc != 2)&#123; printf(\"Usage: %s &lt;dir&gt;\\n\", argv[0]); return -1;&#125;/* inotify_init */mINotifyFd = inotify_init();/* add watch */result = inotify_add_watch(mINotifyFd, argv[1], IN_DELETE | IN_CREATE);/* read */while (1)&#123; read_process_inotify_fd(mINotifyFd);&#125;return 0;&#125; 编译与验证： gcc -o inotify inotify.c //GCC编译 mkdir tmp //创建tmp文件夹 ./inotify tmp &amp; //后台监测tmp文件夹 echo &gt; tmp/1 //tmp文件夹新建文件1 echo &gt; tmp/2 //tmp文件夹新建文件2 rm tmp/1 tmp/2 //移除tmp文件1/2 测试结果可以看到，inotify 成功的监测了tmp文件夹。 2、Epoll介绍与使用无论是从设备节点中获取原始输入事件还是从inotify对象中读取文件系统事件，都面临一个问题，就是这些事件都是偶发的。也就是说，大部分情况下设备节点、inotify对象这些文件描述符中都是无数据可读的，同时又希望有事件到来时可以尽快地对事件作出反应。为解决这个问题，我们不希望不断地轮询这些描述符，也不希望为每个描述符创建一个单独的线程进行阻塞时的读取，因为这都将会导致资源的极大浪费。 此时最佳的办法是使用Epoll机制。Epoll可以使用一次等待监听多个描述符的可读/可写状态。等待返回时携带了可读的描述符或自定义的数据，使用者可以据此读取所需的数据后可以再次进入等待。因此不需要为每个描述符创建独立的线程进行阻塞读取，避免了资源浪费的同时又可以获得较快的响应速度。 Epoll机制的接口只有三个函数，十分简单。 · epoll_create(int max_fds)：创建一个epoll对象的描述符，之后对epoll的操作均使用这个描述符完成。max_fds参数表示了此epoll对象可以监听的描述符的最大数量。 · epoll_ctl (int epfd, int op,int fd, struct epoll_event *event)：用于管理注册事件的函数。这个函数可以增加/删除/修改事件的注册。 · int epoll_wait(int epfd, structepoll_event * events, int maxevents, int timeout)：用于等待事件的到来。当此函数返回时，events数组参数中将会包含产生事件的文件描述符。 接下来以监控若干描述符可读事件为例介绍一下epoll的用法。 （1） 创建epoll对象 首先通过epoll_create()函数创建一个epoll对象： Int epfd = epoll_create(MAX_FDS) （2） 填充epoll_event结构体 接着为每一个需监控的描述符填充epoll_event结构体，以描述监控事件，并通过epoll_ctl()函数将此描述符与epoll_event结构体注册进epoll对象。epoll_event结构体的定义如下: 12345struct epoll_event &#123;__uint32_tevents; /* 事件掩码，指明了需要监听的事件种类*/ epoll_data_t data; /* 使用者自定义的数据，当此事件发生时该数据将原封不动地返回给使用者 */ &#125;; epoll_data_t联合体的定义如下，当然，同一时间使用者只能使用一个字段： 123456typedef union epoll_data &#123;void*ptr;int fd;__uint32_t u32;__uint64_t u64;&#125; epoll_data_t; epoll_event结构中的events字段是一个事件掩码，用以指明需要监听的事件种类，同INotify一样，掩码的每一位代表了一种事件。常用的事件有EPOLLIN（可读），EPOLLOUT（可写），EPOLLERR（描述符发生错误），EPOLLHUP（描述符被挂起）等。更多支持的事件读者可参考相关资料。 data字段是一个联合体，它让使用者可以将一些自定义数据加入到事件通知中，当此事件发生时，用户设置的data字段将会返回给使用者。在实际使用中常设置epoll_event.data.fd为需要监听的文件描述符，事件发生时便可以根据epoll_event.data.fd得知引发事件的描述符。当然也可以设置epoll_event.data.fd为其他便于识别的数据。 填充epoll_event的方法如下： 1234567structepoll_event eventItem;memset(&amp;eventItem, 0, sizeof(eventItem));eventItem.events = EPOLLIN | EPOLLERR | EPOLLHUP; // 监听描述符可读以及出错的事件eventItem.data.fd= listeningFd; // 填写自定义数据为需要监听的描述符 接下来就可以使用epoll_ctl()将事件注册进epoll对象了。epoll_ctl()的参数有四个： · epfd是由epoll_create()函数所创建的epoll对象的描述符。 · op表示了何种操作，包括EPOLL_CTL_ADD/DEL/MOD三种，分别表示增加/删除/修改注册事件。 · fd表示了需要监听的描述符。 · event参数是描述了监听事件的详细信息的epoll_event结构体。 注册方法如下： 123// 将事件监听添加到epoll对象中去result =epoll_ctl(epfd, EPOLL_CTL_ADD, listeningFd, &amp;eventItem); 重复这个步骤可以将多个文件描述符的多种事件监听注册到epoll对象中。完成了监听的注册之后，便可以通过epoll_wait()函数等待事件的到来了。 （3） 使用epoll_wait()函数等待事件 epoll_wait()函数将会使调用者陷入等待状态，直到其注册的事件之一发生之后才会返回，并且携带了刚刚发生的事件的详细信息。其签名如下： int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); · epfd是由epoll_create()函数所创建的epoll对象描述符。 · events是一个epoll_event的数组，此函数返回时，事件的信息将被填充至此。 · maxevents表示此次调用最多可以获取多少个事件，当然，events参数必须能够足够容纳这么多事件。 · timeout表示等待超时的事件。 epoll_wait()函数返回值表示获取了多少个事件。 （4） 处理事件 epoll_wait返回后，便可以根据events数组中所保存的所有epoll_event结构体的events字段与data字段识别事件的类型与来源。 Epoll的使用步骤总结如下： · 通过epoll_create()创建一个epoll对象。 · 为需要监听的描述符填充epoll_events结构体，并使用epoll_ctl()注册到epoll对象中。 · 使用epoll_wait()等待事件的发生。 · 根据epoll_wait()返回的epoll_events结构体数组判断事件的类型与来源并进行处理。 · 继续使用epoll_wait()等待新事件的发生。 使用inotify监听目录实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include &lt;sys/epoll.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;string.h&gt;#if 0typedef union epoll_data &#123;void *ptr; int fd; uint32_t u32; uint64_t u64; &#125; epoll_data_t;#endif#define DATA_MAX_LEN 500/* usage: epoll &lt;file1&gt; [file2] [file3] ... */int add_to_epoll(int fd, int epollFd)&#123;int result;struct epoll_event eventItem;memset(&amp;eventItem, 0, sizeof(eventItem));eventItem.events = EPOLLIN;eventItem.data.fd = fd;result = epoll_ctl(epollFd, EPOLL_CTL_ADD, fd, &amp;eventItem);return result;&#125;void rm_from_epoll(int fd, int epollFd)&#123;epoll_ctl(epollFd, EPOLL_CTL_DEL, fd, NULL);&#125;int main(int argc, char **argv)&#123;int mEpollFd;int i;char buf[DATA_MAX_LEN];// Maximum number of signalled FDs to handle at a time.static const int EPOLL_MAX_EVENTS = 16;// The array of pending epoll events and the index of the next event to be handled.struct epoll_event mPendingEventItems[EPOLL_MAX_EVENTS];if (argc &lt; 2)&#123; printf(\"Usage: %s &lt;file1&gt; [file2] [file3] ...\\n\", argv[0]); return -1;&#125;/* epoll_create */mEpollFd = epoll_create(8);// for each file:* open it// add it to epoll: epoll_ctl(...EPOLL_CTL_ADD...)for (i = 1; i &lt; argc; i++) &#123; //int tmpFd = open(argv[i], O_RDONLY|O_NONBLOCK); int tmpFd = open(argv[i], O_RDWR); add_to_epoll(tmpFd, mEpollFd);&#125;/* epoll_wait */while (1)&#123; int pollResult = epoll_wait(mEpollFd, mPendingEventItems, EPOLL_MAX_EVENTS, -1); for (i = 0; i &lt; pollResult; i++) &#123; printf(\"Reason: 0x%x\\n\", mPendingEventItems[i].events); int len = read(mPendingEventItems[i].data.fd, buf, DATA_MAX_LEN); buf[len] = '\\0'; printf(\"get data: %s\\n\", buf); //sleep(3); &#125;&#125;return 0;&#125; epoll , fifo : o-rdwr-on-named-pipes-with-poll 使用fifo是, 我们的epoll程序是reader echo aa &gt; tmp/1 是writer a. 如果reader以 O_RDONLY|O_NONBLOCK打开FIFO文件, 当writer写入数据时, epoll_wait会立刻返回; 当writer关闭FIFO之后, reader再次调用epoll_wait, 它也会立刻返回(原因是EPPLLHUP, 描述符被挂断) b. 如果reader以 O_RDWR打开FIFO文件 当writer写入数据时, epoll_wait会立刻返回; 当writer关闭FIFO之后, reader再次调用epoll_wait, 它并不会立刻返回, 而是继续等待有数据 编译与验证： gcc -o epoll epoll.c //GCC编译 mkdir tmp //创建tmp文件夹 mkfifo tmp/1 tmp/2 tmp/3 //创建文件1、2、3 ./epoll tmp/1 tmp/2 tmp/3 &amp; //epoll后台监测文件1、2、3 echo aaa &gt; tmp/1 //写人aaa到1 echo bbb &gt; tmp/2 //写入bbb到2 测试结果可以看到，epoll成功的监测了文件内容的改变。 3、INotify与Epoll的小结INotify与Epoll这两套由Linux提供的事件监听机制以最小的开销解决了文件系统变化以及文件描述符可读可写状态变化的监听问题。它们是Reader子系统运行的基石，了解了这两个机制的使用方法之后便为对Reader子系统的分析学习铺平了道路。 参考：https://github.com/weidongshan/APP_0006_inotify_epoll inotify_epoll.c, 用它来监测tmp/目录: 有文件被创建/删除, 有文件可读出数据 a. 当在tmp/下创建文件时, 会立刻监测到，并且使用epoll监测该文件 b. 当文件有数据时，读出数据 c. 当tmp/下文件被删除时，会立刻监测到，并且把它从epoll中移除不再监测 inotify_epoll.c 编译与验证： gcc -o inotify_epoll inotify_epoll.c mkdir tmp ./inotify_epoll tmp/ &amp; mkfifo tmp/1 tmp/2 tmp/3 echo aaa &gt; tmp/1 echo bbb &gt; tmp/2 rm tmp/3 由实例可知，使用inotify 和 epoll 结合就可以监测文件增加和移除 ，还能监测文件内容的改变。 用途简介[稍后进行input system详细分析]： /dev/input 下有多个event文件,对应多个输入设备，如:/dev/input/event0, /dev/input/mouse0, /dev/input/misc 使用inotify 和 epoll 就可以监听输入设备的变化、如Android新连接一个鼠标可检测到改变。同时可监听是否有输入事件。 Lnux IO模式及 select、poll、epoll详解 （二）、必备Linux知识_双向通信(scoketpair)1、进程和APP通信· 创建进程 · 读取、分发 · 进程发送输入事件给APP · 进程读取APP回应的事件 · 输入系统涉及双向的进程间通信 2、回顾Binder系统· Server– 单向发出请求 · Client – 单向回复请求 · 每次请求只可以单方发出 3、引入Socketpair原因：如果创建两组进程（Client，Server）进行双向通信，实现十分复杂 引入Socketpair： Socketpair();两次，获得两个fd，在内核获得缓冲区，一个作为sendbuf区一个作为receivebuf区 APP通过fd1将数据写入fd1的sendbuf区中，通过内核当中的socket机制就会写到fd2中receivebuf区，同理fd2也是如此 socketpair缺点：只适用于线程间、父子进程通信 解决方法：通过Binder机制通信可以访问任意进程，就解决了sockpair缺点 4、socketpair具体使用创建一个线程–pthread_create(); 创建socketpair–socketpair(AF_UNIX, SOCK_SEQPACKET, 0, sockets); 线程处理函数–往socket[1]写入数据，读取socket[0]读取数据 主函数–从socket[1]读取数据，往socket[0]写入数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt; /* See NOTES */#include &lt;sys/socket.h&gt;#define SOCKET_BUFFER_SIZE (32768U)#define MAX 512/* 参考:frameworks/native/libs/input/InputTransport.cpp */ /* 线程执行函数 */int *function_thread(void *arg)&#123;int thread1_fd = (int)arg;int cnt=0;int len;char buf[MAX];while(1)&#123; /* 向 main线程发出: Hello, main thread */ len = sprintf(buf,\"Hello , main thread , cnt = %d\",cnt++); write(thread1_fd,buf,len); /* 读取数据(main线程发回的数据) */ len = read(thread1_fd,buf,MAX); buf[len] = '\\0'; printf(\"thread1 read : %s\\n\",buf); sleep(5);&#125;close(thread1_fd);return NULL;&#125;int main(int argc,char *argv[])&#123;pthread_t threadID;int sockets[2];int bufferSize = SOCKET_BUFFER_SIZE;socketpair(AF_UNIX,SOCK_SEQPACKET,0,sockets); //创建socketpair//初始化setsockopt(sockets[0], SOL_SOCKET, SO_SNDBUF, &amp;bufferSize, sizeof(bufferSize));setsockopt(sockets[0], SOL_SOCKET, SO_RCVBUF, &amp;bufferSize, sizeof(bufferSize));setsockopt(sockets[1], SOL_SOCKET, SO_SNDBUF, &amp;bufferSize, sizeof(bufferSize));setsockopt(sockets[1], SOL_SOCKET, SO_RCVBUF, &amp;bufferSize, sizeof(bufferSize));pthread_create(threadID,NULL,function_thread,(void *)sockets[1]); //创建线程int mainThread_fd = sockets[0];int cnt=0;int len;char buf[MAX];while(1)&#123; /* 读数据: 线程1发出的数据 */ len = read(mainThread_fd,buf,MAX); buf[len] = '\\0'; printf(\"main thread read : %s\\n\",buf); /* main thread向thread1 发出: Hello, thread1 */ len = sprintf(buf,\"Hello , thread1 , cnt = %d\",cnt++); write(mainThread_fd,buf,len); &#125;close(mainThread_fd);return 0;&#125; 使用方法： gcc socketpair.c -o socketpair -pthread 注：出现少量警告，可以忽略 ./socketpair 可以看到main线程 和 thread1双向通信。 main 和 thread1属于两个线程： 父子进程通信： 利用socketpair创建一对无名管道，然后通过sendmsg由服务器进程发送文件的fd给客户端进程，客户端进程通过recvmsg接收服务器进程发来的fd socketpair实现父子进程通信 图示： （三）、必备Linux知识_实现任意进程间双向通信(scoketpair+binder)代码实例，由于代码较多，请往GitHub上查看。 实现任意进程间双向通信(scoketpair+binder) 由第二节最后可知socketpair可实现父子进程通信，图中父进程和子进程可双向通信，假如此时通过binder通信将文件句柄Fd[1]传给另外一个独立进程，我们知道Linux一切皆文件，那个独立进程就可以对Fd[1]读写了，也就是说父进程 就可以和 那个独立进程双向通信了，具体实现请研究上面的代码。 测试： 可以看到两个没有任何关系的进程使用socketpair实现了双向通信。 用途简介[稍后进行input system详细分析]： InputManagerService获取事件后需要发送给App，假如App进程关掉了，需要告知IMS，就不需要接受事件了。可以看到需要进程间相互通信，这就是scoketpair+binder实际作用。 二、输入系统的总体架构（一）、输入子系统分层解析输入子系统的系统架构如下图所示： Android输入系统系统综述： Linux内核会在/dev/input/下创建对应的名为event0~n或其他名称的设备节点。而当输入设备不可用时，则会将对应的节点删除。在用户空间可以通过ioctl的方式从这些设备节点中获取其对应的输入设备的类型、厂商、描述等信息。 当用户操作输入设备时，Linux内核接收到相应的硬件中断，然后将中断加工成原始的输入事件数据并写入其对应的设备节点中，在用户空间可以通过read()函数将事件数据读出。 Android输入系统的工作原理概括来说，就是监控/dev/input/下的所有设备节点，当某个节点有数据可读时，将数据读出并进行一系列的翻译加工，然后在所有的窗口中寻找合适的事件接收者，并派发给它。 1、输入子系统分层解析● Hardware层 硬件层主要就是按键、触摸屏、Sensor等各种输入设备。 ● Kernel层 Kernel 层对Input相关处理只做简单的介绍。 Kernel 层主要分为三层，如下： Input 设备驱动层: 采集输入设备的数据信息，通过 Input Core 的 API 上报数据。 Input Core（核心层）：为事件处理层和设备驱动层提供接口API。 Event Handler（事件处理层）：通过核心层的API获取输入事件上报的数据，定义API与应用层交互。 Event Handler： Event Handler 层以通用的 evdev.c 为例来解析，上层和 Kernel 层的交互在此文件完成。 ● Framework 层 Android系统中Framework 层负责管理输入事件的主要是InputManagerService（IMS）。它主要的任务就是从设备中读事件数据，然后将输入事件发送到焦点窗口中去，另外还需要让系统有机会来处理一些系统按键。显然，要完成这个工作，IMS需要与其它模块打交道，其中最主要的就是WMS和ViewRootImpl。主要的几个模块示意如下： ● App层 WindowManagerService(WMS)是窗口管理服务，核心维护了一个有序的窗口堆栈。PhoneWindowManager(PWM)里有关于手机策略的实现，和输入相关的主要是对系统按键的处理。InputManagerService是输入管理服务，主要干活的是Native层的InputManager。InputManager中的InputReader负责使用EventHub从Input driver中拿事件，然后让InputMapper解析。接着传给InputDispatcher，InputDispatcher负责一方面将事件通过InputManager，InputMonitor一路传给PhoneWindowManager来做系统输入事件的处理，另一方面将这些事件传给焦点及监视窗口。NativeInputManager实现InputReaderPolicyInterface和InputDispatcherPolicyInterface接口，在Native层的InputManager和Java层的IMS间起到一个胶水层的作用。InputMonitor实现了WindowManagerCallbacks接口，起到了IMS到WMS的连接作用。App这边，ViewRootImpl相当于App端一个顶层View的Controller。这个顶层View在WMS中对应一个窗口，用WindowState描述。WindowState中有InputWindowHandle代表一个接收输入事件的窗口句柄。InputDispatcher中的mFocusedWindowHandle指示了焦点窗口的句柄。InputDispatcher管理了一坨连接（一个连接对应一个注册到WMS的窗口），通过这些个连接InputDispatcher可以直接将输入事件发往App端的焦点窗口。输入事件从Driver开始的处理过程大致如下： 事件发往App端后，就进入事件分发阶段，这里简单提下，不做详细分析。 附： Kernel 层生成三个路径及相关设备文件： 123456789101112131415161718192021222324252627282930313233# /sys/class/input/event0 event11 event4 event7 input0 input11 input4 input7event1 event2 event5 event8 input1 input2 input5 input8event10 event3 event6 event9 input10 input3 input6 input9# /dev/inputevent0 event10 event2 event4 event6 event8event1 event11 event3 event5 event7 event9# /proc/bus/input devices handlers# cat devices 查看总线上的已经注册上的输入设备I: Bus=0019 Vendor=0000 Product=0000 Version=0000N: Name=\"ACCDET\"P: Phys=S: Sysfs=/devices/virtual/input/input0U: Uniq=H: Handlers=gpufreq_ib event0B: PROP=0B: EV=3B: KEY=40 0 0 0 0 0 0 1000000000 c000001800000 0...I: Bus=0019 Vendor=0000 Product=0000 Version=0001N: Name=\"fingerprint_key\"P: Phys=S: Sysfs=/devices/virtual/input/input2U: Uniq=H: Handlers=gpufreq_ib event2B: PROP=0B: EV=3B: KEY=2000100000000000 180001f 8000000000000000...cat handlers // 查看注册的handlerN: Number=0 Name=gpufreq_ibN: Number=1 Name=evdev Minor=64 2、getevent与sendevent工具Android系统提供了getevent与sendevent两个工具供开发者从设备节点中直接读取输入事件或写入输入事件。 getevent监听输入设备节点的内容，当输入事件被写入到节点中时，getevent会将其读出并打印在屏幕上。由于getevent不会对事件数据做任何加工，因此其输出的内容是由内核提供的最原始的事件。其用法如下： 1adb shell getevent [-选项] [device_path] 其中device_path是可选参数，用以指明需要监听的设备节点路径。如果省略此参数，则监听所有设备节点的事件。 打开模拟器，执行adb shell getevent –t（-t参数表示打印事件的时间戳），并按一下电源键（不要松手），可以得到以下一条输出，输出的部分数值会因机型的不同而有所差异，但格式一致： 1[1262.443489] /dev/input/event0: 0001 0074 00000001 松开电源键时，又会产生以下一条输出： 1[1262.557130] /dev/input/event0: 0001 0074 00000000 这两条输出便是按下和抬起电源键时由内核生成的原始事件。注意其输出是十六进制的。每条数据有五项信息：产生事件时的时间戳（[ 1262.443489]），产生事件的设备节点（/dev/input/event0），事件类型（0001），事件代码（0074）以及事件的值（00000001）。其中时间戳、类型、代码、值便是原始事件的四项基本元素。除时间戳外，其他三项元素的实际意义依照设备类型及厂商的不同而有所区别。在本例中，类型0x01表示此事件为一条按键事件，代码0x74表示电源键的扫描码，值0x01表示按下，0x00则表示抬起。这两条原始数据被输入系统包装成两个KeyEvent对象，作为两个按键事件派发给Framework中感兴趣的模块或应用程序。 注意一条原始事件所包含的信息量是比较有限的。而在Android API中所使用的某些输入事件，如触摸屏点击/滑动，包含了很多的信息，如XY坐标，触摸点索引等，其实是输入系统整合了多个原始事件后的结果。这个过程将在5.2.4节中详细探讨。 为了对原始事件有一个感性的认识，读者可以在运行getevent的过程中尝试一下其他的输入操作，观察一下每种输入所对应的设备节点及四项元素的取值。 输入设备的节点不仅在用户空间可读，而且是可写的，因此可以将将原始事件写入到节点中，从而实现模拟用户输入的功能。sendevent工具的作用正是如此。其用法如下： 1sendevent &lt;节点路径&gt; &lt;类型&gt;&lt;代码&gt; &lt;值&gt; 可以看出，sendevent的输入参数与getevent的输出是对应的，只不过sendevent的参数为十进制。电源键的代码0x74的十进制为116，因此可以通过快速执行如下两条命令实现点击电源键的效果： 123adb shell sendevent /dev/input/event0 1 116 1 #按下电源键adb shell sendevent /dev/input/event0 1 116 0 #抬起电源键 执行完这两条命令后，可以看到设备进入了休眠或被唤醒，与按下实际的电源键的效果一模一样。另外，执行这两条命令的时间间隔便是用户按住电源键所保持的时间，所以如果执行第一条命令后迟迟不执行第二条，则会产生长按电源键的效果—-关机对话框出现了。很有趣不是么？输入设备节点在用户空间可读可写的特性为自动化测试提供了一条高效的途径。[1] 现在，读者对输入设备节点以及原始事件有了直观的认识，接下来看一下Android输入系统的基本原理。 3、Input driver模拟驱动代码实例： Input driver模拟驱动-作者韦东山 123456789101112131415161718192021222324252627282930313233343536373839404142/* 参考drivers\\input\\keyboard\\gpio_keys.c */#include &lt;linux/module.h&gt;#include &lt;linux/version.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/fs.h&gt;#include &lt;linux/input.h&gt;static struct input_dev *input_emulator_dev;static int input_emulator_init(void)&#123;int i;/* 1\\. 分配一个input_dev结构体 */input_emulator_dev = input_allocate_device();;/* 2\\. 设置 *//* 2.1 能产生哪类事件 */set_bit(EV_KEY, input_emulator_dev-&gt;evbit);set_bit(EV_REP, input_emulator_dev-&gt;evbit);/* 2.2 能产生所有的按键 */for (i = 0; i &lt; BITS_TO_LONGS(KEY_CNT); i++) input_emulator_dev-&gt;keybit[i] = ~0UL;/* 2.3 为android构造一些设备信息 */input_emulator_dev-&gt;name = \"InputEmulatorFrom100ask.net\";input_emulator_dev-&gt;id.bustype = 1;input_emulator_dev-&gt;id.vendor = 0x1234;input_emulator_dev-&gt;id.product = 0x5678;input_emulator_dev-&gt;id.version = 1;/* 3\\. 注册 */input_register_device(input_emulator_dev);return 0;&#125;static void input_emulator_exit(void)&#123;input_unregister_device(input_emulator_dev);input_free_device(input_emulator_dev);&#125;module_init(input_emulator_init);module_exit(input_emulator_exit);MODULE_LICENSE(\"GPL\"); 测试: insmod InputEmulator.ko sendevent /dev/input/event5 1 2 1 // 1 2 1 : EV_KEY, KEY_1, down sendevent /dev/input/event5 1 2 0 // 1 2 0 : EV_KEY, KEY_1, up sendevent /dev/input/event5 0 0 0 // sync sendevent /dev/input/event5 1 3 1 // 1 3 1 : EV_KEY, KEY_2, down sendevent /dev/input/event5 1 3 0 // 1 3 0 : EV_KEY, KEY_1, up sendevent /dev/input/event5 0 0 0 // sync 通过sendevent 最后会成功输入字符1、2。 三、Android Input系统（一）、Android Input 系统关键类介绍上一节讲述了输入事件的源头是位于/dev/input/下的设备节点，而输入系统的终点是由WMS管理的某个窗口。最初的输入事件为内核生成的原始事件，而最终交付给窗口的则是KeyEvent或MotionEvent对象。因此Android输入系统的主要工作是读取设备节点中的原始事件，将其加工封装，然后派发给一个特定的窗口以及窗口中的控件。这个过程由InputManagerService（以下简称IMS）系统服务为核心的多个参与者共同完成。 输入系统的总体流程和参与者如图3-1所示。 上图描述了输入事件的处理流程以及输入系统中最基本的参与者。它们是： · Linux内核，接受输入设备的中断，并将原始事件的数据写入到设备节点中。 · 设备节点，作为内核与IMS的桥梁，它将原始事件的数据暴露给用户空间，以便IMS可以从中读取事件。 · InputManagerService，一个Android系统服务，它分为Java层和Native层两部分。Java层负责与WMS的通信。而Native层则是InputReader和InputDispatcher两个输入系统关键组件的运行容器。 · EventHub，直接访问所有的设备节点。并且正如其名字所描述的，它通过一个名为getEvents()的函数将所有输入系统相关的待处理的底层事件返回给使用者。这些事件包括原始输入事件、设备节点的增删等。 · InputReader，I是IMS中的关键组件之一。它运行于一个独立的线程中，负责管理输入设备的列表与配置，以及进行输入事件的加工处理。它通过其线程循环不断地通过getEvents()函数从EventHub中将事件取出并进行处理。对于设备节点的增删事件，它会更新输入设备列表于配置。对于原始输入事件，InputReader对其进行翻译、组装、封装为包含了更多信息、更具可读性的输入事件，然后交给InputDispatcher进行派发。 · InputReaderPolicy，它为InputReader的事件加工处理提供一些策略配置，例如键盘布局信息等。 · InputDispatcher，是IMS中的另一个关键组件。它也运行于一个独立的线程中。InputDispatcher中保管了来自WMS的所有窗口的信息，其收到来自InputReader的输入事件后，会在其保管的窗口中寻找合适的窗口，并将事件派发给此窗口。 · InputDispatcherPolicy，它为InputDispatcher的派发过程提供策略控制。例如截取某些特定的输入事件用作特殊用途，或者阻止将某些事件派发给目标窗口。一个典型的例子就是HOME键被InputDispatcherPolicy截取到PhoneWindowManager中进行处理，并阻止窗口收到HOME键按下的事件。 · WMS，虽说不是输入系统中的一员，但是它却对InputDispatcher的正常工作起到了至关重要的作用。当新建窗口时，WMS为新窗口和IMS创建了事件传递所用的通道。另外，WMS还将所有窗口的信息，包括窗口的可点击区域，焦点窗口等信息，实时地更新到IMS的InputDispatcher中，使得InputDispatcher可以正确地将事件派发到指定的窗口。 · ViewRootImpl，对于某些窗口，如壁纸窗口、SurfaceView的窗口来说，窗口即是输入事件派发的终点。而对于其他的如Activity、对话框等使用了Android控件系统的窗口来说，输入事件的终点是控件（View）。ViewRootImpl将窗口所接收到的输入事件沿着控件树将事件派发给感兴趣的控件。 简单来说，内核将原始事件写入到设备节点中，InputReader不断地通过EventHub将原始事件取出来并翻译加工成Android输入事件，然后交给InputDispatcher。InputDispatcher根据WMS提供的窗口信息将事件交给合适的窗口。窗口的ViewRootImpl对象再沿着控件树将事件派发给感兴趣的控件。控件对其收到的事件作出响应，更新自己的画面、执行特定的动作。所有这些参与者以IMS为核心，构建了Android庞大而复杂的输入体系。 接下来详细讨论除Linux内核以外的其他参与者的工作原理。 （二）、IMS的创建与启动IMS分为Java层与Native层两个部分，其启动过程是从Java部分的初始化开始，进而完成Native部分的初始化。 IMS在SystemServer.startOtherServices()方法中启动的。IMS的诞生分为两个阶段： · 创建新的IMS对象。 · 调用IMS对象的start()函数完成启动。 我们先看下整个启动过程的序列图，然后根据序列图来一步步分析。 Step 1、 SystemServer.startOtherServices()12345678910111213141516171819202122232425262728 [-&gt;frameworks/base/services/java/com/android/server/SystemServer.java] private void startOtherServices() &#123; ...... try &#123; ...... // ① 新建IMS对象。 traceBeginAndSlog(\"StartInputManagerService\"); inputManager = new InputManagerService(context); Trace.traceEnd(Trace.TRACE_TAG_SYSTEM_SERVER); traceBeginAndSlog(\"StartWindowManagerService\"); wm = WindowManagerService.main(context, inputManager, mFactoryTestMode != FactoryTest.FACTORY_TEST_LOW_LEVEL, !mFirstBoot, mOnlyCore); //将WindowManagerService加入到ServiceManager中 ServiceManager.addService(Context.WINDOW_SERVICE, wm); //将InputManagerService加入到ServiceManager中 ServiceManager.addService(Context.INPUT_SERVICE, inputManager); Trace.traceEnd(Trace.TRACE_TAG_SYSTEM_SERVER); mActivityManagerService.setWindowManager(wm); // 设置向WMS发起回调的callback对象 inputManager.setWindowManagerCallbacks(wm.getInputMonitor()); // ② 正式启动IMS inputManager.start(); &#125;&#125; 在SystemServer中先构造了一个InputManagerService对象和一个WindowManagerService对象，然后将InputManagerService对象传给WindowManagerService对象，WindowManagerService中初始化了一个InputMonitor对象，调用InputManagerService.setWindowManagerCallbacks函数将InputMonitor传进去，后面native层回调时会调用到该InputMonitor对象。 Step 2、 InputManagerService()1234567891011 [-&gt;frameworks/base/services/core/java/com/android/server/input/InputManagerService.java] public InputManagerService(Context context) &#123; this.mContext = context; //注意这里拿了DisplayThread的Handler，意味着IMS中的消息队列处理都是在单独的DisplayThread中进行的。 //它是系统中共享的单例前台线程，主要用作输入输出的处理用。这样可以使用户体验敏感的处理少受其它工作的影响，减少延时。 this.mHandler = new InputManagerHandler(DisplayThread.get().getLooper()); //调用nativeInit来执行C++层的初始化操作 mPtr = nativeInit(this, mContext, mHandler.getLooper().getQueue()); LocalServices.addService(InputManagerInternal.class, new LocalService());&#125; Step 3、 InputManagerService.nativeInit()1234567891011121314[-&gt;frameworks/base/services/core/jni/com_android_server_input_InputManagerService.cpp]static jlong nativeInit(JNIEnv* env, jclass /* clazz */, jobject serviceObj, jobject contextObj, jobject messageQueueObj) &#123;sp&lt;MessageQueue&gt; messageQueue = android_os_MessageQueue_getMessageQueue(env, messageQueueObj);...... // 新建了一个NativeInputManager对象，NativeInputManager，此对象将是Native层组件与 //Java层IMS进行通信的桥梁NativeInputManager* im = new NativeInputManager(contextObj, serviceObj, messageQueue-&gt;getLooper());im-&gt;incStrong(0);// 返回了NativeInputManager对象的指针给Java层的IMS，IMS将其保存在mPtr成员变量中return reinterpret_cast&lt;jlong&gt;(im);&#125; 这个函数主要作用是创建一个NativeInputManager实例，并将其作为返回值保存在InputManagerService.java中的mPtr字段中。 Step 4、NativeInputManager()1234567891011[-&gt;frameworks/base/services/core/jni/com_android_server_input_InputManagerService.cpp]NativeInputManager::NativeInputManager(jobject contextObj, jobject serviceObj, const sp&lt;Looper&gt;&amp; looper) : mLooper(looper), mInteractive(true) &#123;// 出现重点了， NativeInputManager创建了EventHub//构造一个EventHub对象,最原始的输入事件都是通过它收集并且粗加工然后给到InputReader对象sp&lt;EventHub&gt; eventHub = new EventHub();// 接着创建了Native层的InputManagermInputManager = new InputManager(eventHub, this, this);&#125; NativeInputManager构造函数中创建了一个EventHub实例（稍后会详细介绍），并且将这个实例作为参数来创建一个InputManager对象，这个对象会做一些初始化的操作。 Step 5、InputManager()12345678910[-&gt;frameworks/native/services/inputflinger/InputManager.cpp]InputManager::InputManager( const sp&lt;EventHubInterface&gt;&amp; eventHub, const sp&lt;InputReaderPolicyInterface&gt;&amp; readerPolicy, const sp&lt;InputDispatcherPolicyInterface&gt;&amp; dispatcherPolicy) &#123;mDispatcher = new InputDispatcher(dispatcherPolicy);mReader = new InputReader(eventHub, readerPolicy, mDispatcher);initialize();&#125; 这里创建了InputDispatcher对象用于分发按键给当前focus的窗口的，同时创建了一个InputReader用于从EventHub中读取事件。 Step 6、InputManager.initialize()12345678[-&gt;frameworks/native/services/inputflinger/InputManager.cpp]void InputManager::initialize() &#123; // 创建供InputReader运行的线程InputReaderThreadmReaderThread = new InputReaderThread(mReader); // 创建供InputDispatcher运行的线程InputDispatcherThreadmDispatcherThread = new InputDispatcherThread(mDispatcher);&#125; 这里创建了一个InputReaderThread和InputDispatcherThread对象，前面构造函数中创建的InputReader实际上是通过InputReaderThread来读取事件，而InputDispatcher实际通过InputDispatcherThread来分发事件 图3-1： InputManager的构造函数也比较简洁，它创建了四个对象，分别为IMS的核心参与者InputReader与InputDispatcher，以及它们所在的线程InputReaderThread与InputDispatcherThread。注意InputManager的构造函数的参数readerPolicy与dispatcherPolicy，它们都是NativeInputManager。 至此，IMS的创建完成了。在这个过程中，输入系统的重要参与者均完成创建，并得到了如图3-1所描述的一套体系。 依次初始化NativeInputManager，EventHub，InputManager, InputDispatcher，InputReader，InputReaderThread, InputDispatcherThread。NativeInputManager可看作IMS和InputManager的中间层，将IMS的请求转化为对InputManager及其内部对象的操作，同时将InputManager中模块的请求通过JNI调回IMS。InputManager是输入控制中心，它有两个关键线程InputReaderThread和InputDispatcherThread，它们的主要功能部分分别在InputReader和InputDispacher。前者用于从设备中读取事件，后者将事件分发给目标窗口。EventHub是输入设备的控制中心，它直接与input driver打交道。负责处理输入设备的增减，查询，输入事件的处理并向上层提供getEvents()接口接收事件。在它的构造函数中，主要做三件事（结合之前Linux必备知识）： 创建epoll对象，之后就可以把各输入设备的fd挂在上面多路等待输入事件。 建立用于唤醒的pipe，把读端挂到epoll上，以后如果有设备参数的变化需要处理，而getEvents()又阻塞在设备上，就可以调用wake()在pipe的写端写入，就可以让线程从等待中返回。 利用inotify机制监听/dev/input目录下的变更，如有则意味着设备的变化，需要处理。 因为事件的处理是流水线，需要InputReader先读事件，然后InputDispatcher才能进一步处理和分发。因此InputDispatcher需要监听InputReader。这里使用了Listener模式，InputDispacher作为InputReader构造函数的第三个参数，它实现InputListenerInterface接口。到了InputReader的构造函数中，将之包装成QueuedInputListener。QueuedInputListener中的成员变量mArgsQueue是一个缓冲队列，只有在flush()时，才会一次性通知InputDispatcher。QueuedInputListener应用了Command模式，它通过包装InputDispatcher(实现InputListenerInterface接口)，将事件的处理请求封装成NotifyArgs，使其有了缓冲执行的功能。 IMS的成员关系 （三）、IMS启动IMS启动主要是将前面创建的InputReaderThread和InputDispatcherThread启动起来 Step 1、InputManagerService.start()1234567[-&gt;frameworks/base/services/core/java/com/android/server/input/InputManagerService.java]public void start() &#123; Slog.i(TAG, \"Starting input manager\"); nativeStart(mPtr); ...&#125; 该函数主要调用了nativeStart进入native层启动 Step 2. InputManagerService.nativeStart()1234567[-&gt;frameworks/base/services/core/jni/com_android_server_input_InputManagerService.cpp]static void nativeStart(JNIEnv* env, jclass /* clazz */, jlong ptr) &#123;NativeInputManager* im = reinterpret_cast&lt;NativeInputManager*&gt;(ptr);status_t result = im-&gt;getInputManager()-&gt;start();&#125; 进入native层InputManager的start函数 Step 3、InputManager.start()1234567[-&gt;frameworks/native/services/inputflinger/InputManager.cpp]status_t InputManager::start() &#123;status_t result = mDispatcherThread-&gt;run(\"InputDispatcher\", PRIORITY_URGENT_DISPLAY);result = mReaderThread-&gt;run(\"InputReader\", PRIORITY_URGENT_DISPLAY);return OK;&#125; 这个函数实际启动了一个InputReaderThread和InputDispatcherThread来从读取和分发键盘消息，调用它们的run方法后，就会进入threadLoop函数中，只要threadLoop函数返回true，该函数就会循环执行。 InputReaderThread不断调用InputReader的pollOnce()-&gt;getEvents()函数来得到事件，这些事件可以是输入事件，也可以是由inotify监测到设备增减变更所触发的事件，稍后会详细介绍。 Step 4、InputReaderThread.threadLoop()123456[-&gt;frameworks/native/services/inputflinger/InputReader.cpp]bool InputReaderThread::threadLoop() &#123;mReader-&gt;loopOnce();return true;&#125; 这里调用前面创建的InputReaderThread对象的loopOnce进行一次线程循环 Step5、InputReaderThread.loopOnce()123456789101112131415161718192021[-&gt;frameworks/native/services/inputflinger/InputReader.cpp] void InputReader::loopOnce() &#123;....../* ① 通过EventHub抽取事件列表。读取的结果存储在参数mEventBuffer中，返回值表示事件的个数 当EventHub中无事件可以抽取时，此函数的调用将会阻塞直到事件到来或者超时 */size_tcount = mEventHub-&gt;getEvents(timeoutMillis,mEventBuffer, EVENT_BUFFER_SIZE);&#123; AutoMutex _l(mLock); ...... if(count) &#123; // ② 如果有抽得事件，则调用processEventsLocked()函数对事件进行加工处理 processEventsLocked(mEventBuffer, count); &#125; ......&#125;....../* ③ 发布事件。 processEventsLocked()函数在对事件进行加工处理之后，便将处理后的事件存储在 mQueuedListener中。在循环的最后，通过调用flush()函数将所有事件交付给InputDispatcher */ mQueuedListener-&gt;flush(); &#125; InputReader的一次线程循环的工作思路比较清晰，一共三步： · 首先从EventHub中抽取未处理的事件列表。这些事件分为两类，一类是从设备节点中读取的原始输入事件，另一类则是输入设备可用性变化事件，简称为设备事件。 · 通过processEventsLocked()对事件进行处理。对于设备事件，此函数对根据设备的可用性加载或移除设备对应的配置信息。对于原始输入事件，则在进行转译、封装与加工后将结果暂存到mQueuedListener中。 · 所有事件处理完毕后，调用mQueuedListener.flush()将所有暂存的输入事件一次性地交付给InputDispatcher。 这便是InputReader的总体工作流程。而我们接下来将详细讨论这三步的实现。 Step 6、InputDispatcherThread.threadLoop()InputDisptacher的主要任务是把收到的输入事件发送到PhoneWIndowManager或App端的焦点窗口上，稍后详细介绍。 123456[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]bool InputDispatcherThread::threadLoop() &#123;mDispatcher-&gt;dispatchOnce();return true;&#125; 这里调用前面创建的InputDispatcher对象的dispatchOnce函数进行一次按键分发 Step 7、InputDispatcher.dispatchOnce()12345678910111213141516171819[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::dispatchOnce() &#123;nsecs_t nextWakeupTime = LONG_LONG_MAX;&#123; // acquire lock AutoMutex _l(mLock); mDispatcherIsAliveCondition.broadcast(); if (!haveCommandsLocked()) &#123; dispatchOnceInnerLocked(&amp;nextWakeupTime); &#125; if (runCommandsLockedInterruptible()) &#123; nextWakeupTime = LONG_LONG_MIN; &#125;&#125; // release lock// Wait for callback or timeout or wake. (make sure we round up, not down)nsecs_t currentTime = now();int timeoutMillis = toMillisecondTimeoutDelay(currentTime, nextWakeupTime);mLooper-&gt;pollOnce(timeoutMillis);&#125; 上述函数主要是调用dispatchOnceInnerLocked来进行一次按键分发，当没有按键消息时会走到mLooper-&gt;pollOnce(timeoutMillis)；这个函数会进入睡眠状态，当有按键消息发生时该函数会返回，然后走到dispatchOnceInnerLocked函数。这里mLooper-&gt;pollOnce为何会睡眠涉及到Android的Handler机制[☺再总结☺]。 小结：完成IMS的创建之后，InputManagerService.start()函数以启动IMS。InputManager的创建过程分别为InputReader与InputDispatcher创建了承载它们运行的线程，然而并未将这两个线程启动，因此IMS的各员大将仍处于待命状态。此时start()函数的功能就是启动这两个线程，使得InputReader于InputDispatcher开始工作。 当两个线程启动后，InputReader在其线程循环中不断地从EventHub中抽取原始输入事件，进行加工处理后将加工所得的事件放入InputDispatcher的派发发队列中。InputDispatcher则在其线程循环中将派发队列中的事件取出，查找合适的窗口，将事件写入到窗口的事件接收管道中。窗口事件接收线程的Looper从管道中将事件取出，交由事件处理函数进行事件响应。整个过程共有三个线程首尾相接，像三台水泵似的一层层地将事件交付给事件处理函数。如下图所示。 InputManagerService.start()函数的作用，就像为Reader线程、Dispatcher线程这两台水泵按下开关，而Looper这台水泵在窗口创建时便已经处于运行状态了。自此，输入系统动力十足地开始运转，设备节点中的输入事件将被源源不断地抽取给事件处理者。 四、深入理解EventHubInputReaderThread继承自C++的Thread类，Thread类封装了pthread线程工具，提供了与Java层Thread类相似的API。C++的Thread类提供了一个名为threadLoop()的纯虚函数，当线程开始运行后，将会在内建的线程循环中不断地调用threadLoop()，直到此函数返回false，则退出线程循环，从而结束线程。 InputReaderThread启动后，其线程循环将不断地执行InputReader.loopOnce()函数。因此这个loopOnce()函数作为线程循环的循环体包含了InputReader的所有工作。前面一小节 Step5. InputReaderThread.loopOnce() 已经说到InputReaderThread一次线程循环。接下来详细说明EventHub。 · 首先从EventHub中抽取未处理的事件列表。这些事件分为两类，一类是从设备节点中读取的原始输入事件，另一类则是输入设备可用性变化事件，简称为设备事件。 123456789 [-&gt;frameworks/native/services/inputflinger/InputReader.cpp] void InputReader::loopOnce() &#123;....../* ① 通过EventHub抽取事件列表。读取的结果存储在参数mEventBuffer中，返回值表示事件的个数 当EventHub中无事件可以抽取时，此函数的调用将会阻塞直到事件到来或者超时 */size_tcount = mEventHub-&gt;getEvents(timeoutMillis,mEventBuffer, EVENT_BUFFER_SIZE);......&#125; 首先贴一张EventHub-&gt;getEvents()工作时序图，跟着时序图一步步介绍。 （1）、深入理解EventHub1、设备节点监听的建立123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263frameworks/native/services/inputflinger/EventHub.cppEventHub::EventHub(void) : mBuiltInKeyboardId(NO_BUILT_IN_KEYBOARD), mNextDeviceId(1), mControllerNumbers(), mOpeningDevices(0), mClosingDevices(0), mNeedToSendFinishedDeviceScan(false), mNeedToReopenDevices(false), mNeedToScanDevices(true), mPendingEventCount(0), mPendingEventIndex(0), mPendingINotify(false) &#123;acquire_wake_lock(PARTIAL_WAKE_LOCK, WAKE_LOCK_ID);// ① 首先使用epoll_create()函数创建一个epoll对象。EPOLL_SIZE_HINT指定最大监听个数为8//这个epoll对象将用来监听设备节点是否有数据可读（有无事件）mEpollFd = epoll_create(EPOLL_SIZE_HINT);LOG_ALWAYS_FATAL_IF(mEpollFd &lt; 0, \"Could not create epoll instance. errno=%d\", errno);// ② 创建一个inotify对象。这个inotify对象将被用来监听设备节点的增删事件mINotifyFd = inotify_init(); //将存储设备节点的路径/dev/input作为监听对象添加到inotify对象中。当此文件夹下的设备节点 //发生创建与删除事件时，都可以通过mINotifyFd读取事件的详细信息 //static const char *DEVICE_PATH = \"/dev/input\";int result = inotify_add_watch(mINotifyFd, DEVICE_PATH, IN_DELETE | IN_CREATE);LOG_ALWAYS_FATAL_IF(result &lt; 0, \"Could not register INotify for %s. errno=%d\", DEVICE_PATH, errno);//③ 接下来将mINotifyFd作为epoll的一个监控对象。当inotify事件到来时，epoll_wait()将//立刻返回，EventHub便可从mINotifyFd中读取设备节点的增删信息，并作相应处理struct epoll_event eventItem;memset(&amp;eventItem, 0, sizeof(eventItem));eventItem.events = EPOLLIN;// 监听mINotifyFd可读eventItem.data.u32 = EPOLL_ID_INOTIFY; // 注意这里并没有使用fd字段，而使用了自定义的值EPOLL_ID_INOTIFYresult = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, mINotifyFd, &amp;eventItem);// 将对mINotifyFd的监听注册到epoll对象中LOG_ALWAYS_FATAL_IF(result != 0, \"Could not add INotify to epoll instance. errno=%d\", errno);//在构造函数剩余的代码中，EventHub创建了一个名为wakeFds的匿名管道，并将管道读取端的描述符//的可读事件注册到epoll对象中。因为InputReader在执行getEvents()时会因无事件而导致其线程//阻塞在epoll_wait()的调用里，然而有时希望能够立刻唤醒InputReader线程使其处理一些请求。此//时只需向wakeFds管道的写入端写入任意数据，此时读取端有数据可读，使得epoll_wait()得以返回//从而达到唤醒InputReader线程的目的int wakeFds[2];result = pipe(wakeFds);LOG_ALWAYS_FATAL_IF(result != 0, \"Could not create wake pipe. errno=%d\", errno);mWakeReadPipeFd = wakeFds[0];mWakeWritePipeFd = wakeFds[1];result = fcntl(mWakeReadPipeFd, F_SETFL, O_NONBLOCK);LOG_ALWAYS_FATAL_IF(result != 0, \"Could not make wake read pipe non-blocking. errno=%d\", errno);result = fcntl(mWakeWritePipeFd, F_SETFL, O_NONBLOCK);LOG_ALWAYS_FATAL_IF(result != 0, \"Could not make wake write pipe non-blocking. errno=%d\", errno);eventItem.data.u32 = EPOLL_ID_WAKE;result = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, mWakeReadPipeFd, &amp;eventItem);LOG_ALWAYS_FATAL_IF(result != 0, \"Could not add wake read pipe to epoll instance. errno=%d\", errno);int major, minor;getLinuxRelease(&amp;major, &amp;minor);// EPOLLWAKEUP was introduced in kernel 3.5mUsingEpollWakeup = major &gt; 3 || (major == 3 &amp;&amp; minor &gt;= 5);&#125; EventHub的构造函数初识化了Epoll对象和INotify对象，分别监听原始输入事件与设备节点增删事件。同时将INotify对象的可读性事件也注册到Epoll中，因此EventHub可以像处理原始输入事件一样监听设备节点增删事件了。 构造函数同时也揭示了EventHub的监听工作分为设备节点和原始输入事件两个方面，接下来将深入探讨这两方面的内容。 2、getEvents()函数的工作方式正如前文所述，InputReaderThread的线程循环为Reader子系统提供了运转的动力，EventHub的工作也是由它驱动的。InputReader::loopOnce()函数调用EventHub::getEvents()函数获取事件列表，所以这个getEvents()是EventHub运行的动力所在，几乎包含了EventHub的所有工作内容，因此首先要将getEvents()函数的工作方式搞清楚。 getEvents()函数的签名如下： 1size_t EventHub::getEvents(int timeoutMillis,RawEvent* buffer, size_t bufferSize) 此函数将尽可能多地读取设备增删事件与原始输入事件，将它们封装为RawEvent结构体，并放入buffer中供InputReader进行处理。RawEvent结构体的定义如下： [EventHub.h–&gt;RawEvent] 12345678struct RawEvent &#123; nsecs_t when; /* 发生事件时的时间戳 */ int32_t deviceId; //产生事件的设备Id，它是由EventHub自行分配的，InputReader //以根据它从EventHub中获取此设备的详细信息 int32_t type; /* 事件的类型 */ int32_t code; /* 事件代码 */ int32_t value; /* 事件值 */ &#125;; 可以看出，RawEvent结构体与getevent工具的输出十分一致，包含了原始输入事件的四个基本元素，因此用RawEvent结构体表示原始输入事件是非常直观的。RawEvent同时也用来表示设备增删事件，为此，EventHub定义了三个特殊的事件类型DEVICE_ADD、DEVICE_REMOVED以及FINISHED_DEVICE_SCAN，用以与原始输入事件进行区别。 由于getEvents()函数较为复杂，为了给后续分析铺平道路，本节不讨论其细节，先通过伪代码理解此函数的结构与工作方式，在后续深入分析时思路才会比较清晰。 getEvents()函数的本质就是读取并处理Epoll事件与INotify事件 参考以下代码： [EventHub.cpp–&gt;EventHub::getEvents()] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263size_t EventHub::getEvents(int timeoutMillis,RawEvent* buffer, size_t bufferSize) &#123;// event指针指向了在buffer下一个可用于存储事件的RawEvent结构体。每存储一个事件，// event指针都回向后偏移一个元素 */RawEvent* event = buffer;// capacity记录了buffer中剩余的元素数量。当capacity为0时，表示buffer已满，此时需要停// 继续处理新事件，并将已处理的事件返回给调用者size_tcapacity = bufferSize;// 接下来的循环是getEvents()函数的主体。在这个循环中，会先将可用事件放入到buffer中并返回。// 如果没有可用事件，则进入epoll_wait()等待事件的到来，epoll_wait()返回后会重新循环将可用// 将新事件放入bufferfor (;;)&#123; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); /* ① 首先进行与设备相关的工作。某些情况下，如EventHub创建后第一次执行getEvents()函数 */ /* 时，需要扫描/dev/input文件夹下的所有设备节点并将这些设备打开。另外，当设备节点的发生增 */ /* 动作生时，会将设备事件存入到buffer中 */ ...... /* ② 处理未被InputReader取走的输入事件与设备事件。epoll_wait()所取出的epoll_event */ /* 存储在mPendingEventItems中，mPendingEventCount指定了mPendingEventItems数组 */ /* 所存储的事件个数。而mPendingEventIndex指定尚未处理的epoll_event的索引 */ while (mPendingEventIndex &lt; mPendingEventCount) &#123; const struct epoll_event&amp; eventItem = mPendingEventItems[mPendingEventIndex++]; /* 在这里分析每一个epoll_event，如果是表示设备节点可读，则读取原始事件并放置到buffer */ /* 中。如果是表示mINotifyFd可读，则设置mPendingINotify为true，当InputReader */ /* 将现有的输入事件都取出后读取mINotifyFd中的事件，并进行相应的设备加载与卸载操作。 */ /* 另外，如果此epoll_event表示wakeFds的读取端有数据可读，则设置awake标志为true， */ /* 无论此次getEvents()调用有无取到事件，都不会再次进行epoll_wait()进行事件等待 */ ...... &#125; // ③ 如果mINotifyFd有数据可读，说明设备节点发生了增删操作 if(mPendingINotify &amp;&amp; mPendingEventIndex &gt;= mPendingEventCount) &#123; /* 读取mINotifyFd中的事件，同时对输入设备进行相应的加载与卸载操作。这个操作必须当 */ /* InputReader将现有输入事件读取并处理完毕后才能进行，因为现有的输入事件可能来自需要 */ /* 被卸载的输入设备，InputReader处理这些事件依赖于对应的设备信息 */ ...... deviceChanged= true; &#125; // 设备节点增删操作发生时，则重新执行循环体，以便将设备变化的事件放入buffer中 if(deviceChanged) &#123; continue; &#125; // 如果此次getEvents()调用成功获取了一些事件，或者要求唤醒InputReader，则退出循环并 // 结束getEvents()的调用，使InputReader可以立刻对事件进行处理 if(event != buffer || awoken) &#123; break; &#125; /* ④ 如果此次getEvents()调用没能获取事件，说明mPendingEventItems中没有事件可用， */ /* 于是执行epoll_wait()函数等待新的事件到来，将结果存储到mPendingEventItems里，并重 */ /* 置mPendingEventIndex为0 */ mPendingEventIndex = 0; ...... intpollResult = epoll_wait(mEpollFd, mPendingEventItems, EPOLL_MAX_EVENTS,timeoutMillis); ...... mPendingEventCount= size_t(pollResult); // 从epoll_wait()中得到新的事件后，重新循环，对新事件进行处理&#125;// 返回本次getEvents()调用所读取的事件数量returnevent - buffer;&#125; getEvents()函数使用Epoll的核心是mPendingEventItems数组，它是一个事件池。getEvents()函数会优先从这个事件池获取epoll事件进行处理，并将读取相应的原始输入事件返回给调用者。当因为事件池枯竭而导致调用者无法获得任何事件时，会调用epoll_wait()函数等待新事件的到来，将事件池重新注满，然后再重新处理事件池中的Epoll事件。从这个意义来说，getEvents()函数的调用过程，就是消费epoll_wait()所产生的Epoll事件的过程。因此可以将从epoll_wait()的调用开始，到将Epoll事件消费完毕的过程称为EventHub的一个监听周期。依据每次epoll_wait()产生的Epoll事件的数量以及设备节点中原始输入事件的数量，一个监听周期包含一次或多次getEvents()调用。周期中的第一次调用会因为事件池枯竭而直接进入epoll_wait()，而周期中的最后一次调用一定会将最后的事件取走。 注意getEvents()采用事件池机制的根本原因是buffer的容量限制。由于一次epoll_wait()可能返回多个设备节点的可读事件，每个设备节点又有可能读取多条原始输入事件，一段时间内原始输入事件的数量可能大于buffer的容量。因此需要一个事件池以缓存因buffer容量不够而无法处理的epoll事件，以便在下次调用时可以将这些事件优先处理。这是缓冲区操作的一个常用技巧。 当有INotify事件可以从mINotifyFd中读取时，会产生一个epoll事件，EventHub便得知设备节点发生了增删操作。在getEvents()将事件池中的所有事件处理完毕后，便会从mINotifyFd中读取INotify事件，进行输入设备的加载/卸载操作，然后生成对应的RawEvent结构体并返回给调用者。 通过上述分析可以看到，getEvents()包含了原始输入事件读取、输入设备加载/卸载等操作。这几乎是EventHub的全部工作了。如果没有geEvents()的调用，EventHub将对输入事件、设备节点增删事件置若罔闻，因此可以将一次getEvents()调用理解为一次心跳，EventHub的核心功能都会在这次心跳中完成。 getEvents()的代码还揭示了另外一个信息：在一个监听周期内的设备增删事件与Epoll事件的优先级。设备事件的生成逻辑位于Epoll事件的处理之前，因此getEvents()将优先生成设备增删事件，完成所有设备增删事件的生成之前不会处理Epoll事件，也就是不会生成原始输入事件。 接下来我们将从设备管理与原始输入事件处理两个方面深入探讨EventHub。 3、输入设备管理因为输入设备是输入事件的来源，并且决定了输入事件的含义，因此首先讨论EventHub的输入设备管理机制。 输入设备是一个可以为接收用户操作的硬件，内核会为每一个输入设备在/dev/input/下创建一个设备节点，而当输入设备不可用时（例如被拔出），将其设备节点删除。这个设备节点包含了输入设备的所有信息，包括名称、厂商、设备类型，设备的功能等。除了设备节点，某些输入设备还包含一些自定义配置，这些配置以键值对的形式存储在某个文件中。这些信息决定了Reader子系统如何加工原始输入事件。EventHub负责在设备节点可用时加载并维护这些信息，并在设备节点被删除时将其移除。 EventHub通过一个定义在其内部的名为Device的私有结构体来描述一个输入设备。其定义如下： [EventHub.h–&gt;EventHub::Device] 123456789101112131415161718192021222324252627struct Device &#123;Device* next; /* Device结构体实际上是一个单链表 */int fd; /* fd表示此设备的设备节点的描述符，可以从此描述符中读取原始输入事件 */const int32_t id; /* id在输入系统中唯一标识这个设备，由EventHub在加载设备时进行分配 */const String8 path; /* path存储了设备节点在文件系统中的路径 */const InputDeviceIdentifier identifier; /* 厂商信息，存储了设备的供应商、型号等信息 这些信息从设备节点中获得 */uint32_t classes; /* classes表示了设备的类别，键盘设备，触控设备等。一个设备可以同时属于 多个设备类别。类别决定了InputReader如何加工其原始输入事件 *//* 接下来是一系列的事件位掩码，它们详细地描述了设备能够产生的事件类型。设备能够产生的事件类型 决定了此设备所属的类型*/uint8_t keyBitmask[(KEY_MAX + 1) / 8];....../* 配置信息。以键值对的形式存储在一个文件中，其路径取决于identfier字段中的厂商信息，这些 配置信息将会影响InputReader对此设备的事件的加工行为 */String8 configurationFile;PropertyMap* configuration;/* 键盘映射表。对于键盘类型的设备，这些键盘映射表将原始事件中的键盘扫描码转换为Android定义的 的按键值。这个映射表也是从一个文件中加载的，文件路径取决于dentifier字段中的厂商信息 */ VirtualKeyMap* virtualKeyMap;KeyMap keyMap; sp&lt;KeyCharacterMap&gt; overlayKeyMap; sp&lt;KeyCharacterMap&gt; combinedKeyMap;// 力反馈相关的信息。有些设备如高级的游戏手柄支持力反馈功能，目前暂不考虑bool ffEffectPlaying;int16_t ffEffectId;&#125;; Device结构体所存储的信息主要包括以下几个方面： · 设备节点信息：保存了输入设备节点的文件描述符、文件路径等。 · 厂商信息：包括供应商、设备型号、名称等信息，这些信息决定了加载配置文件与键盘映射表的路径。 · 设备特性信息：包括设备的类别，可以上报的事件种类等。这些特性信息直接影响了InputReader对其所产生的事件的加工处理方式。 · 设备的配置信息：包括键盘映射表及其他自定义的信息，从特定位置的配置文件中读取。 另外，Device结构体还存储了力反馈所需的一些数据。在本节中暂不讨论。 EventHub用一个名为mDevices的字典保存当前处于打开状态的设备节点的Device结构体。字典的键为设备Id。 （1）、输入设备的加载EventHub在创建后在第一次调用getEvents()函数时完成对系统中现有输入设备的加载。 再看一下getEvents()函数中相关内容的实现： [EventHub.cpp–&gt;EventHub::getEvents()] 1234567891011121314151617size_t EventHub::getEvents(int timeoutMillis,RawEvent* buffer, size_t bufferSize) &#123;for (;;)&#123; // 处理输入设备卸载操作 ...... /* 在EventHub的构造函数中，mNeedToScanDevices被设置为true，因此创建后第一次调用 getEvents()函数会执行scanDevicesLocked()，加载所有输入设备 */ if(mNeedToScanDevices) &#123; mNeedToScanDevices = false; /*scanDevicesLocked()将会把/dev/input下所有可用的输入设备打开并存储到Device 结构体中 */ scanDevicesLocked(); mNeedToSendFinishedDeviceScan = true; &#125; ......&#125;returnevent – buffer;&#125; 加载所有输入设备由scanDevicesLocked()函数完成。看一下其实现： [EventHub.cpp–&gt;EventHub::scanDevicesLocked()] 12345678910void EventHub::scanDevicesLocked() &#123;// 调用scanDirLocked()函数遍历/dev/input文件夹下的所有设备节点并打开status_tres = scanDirLocked(DEVICE_PATH);......// 错误处理// 打开一个名为VIRTUAL_KEYBOARD的输入设备。这个设备时刻是打开着的。它是一个虚拟的输入设 备，没有对应的输入节点。读者先记住有这么一个输入设备存在于输入系统中 */if(mDevices.indexOfKey(VIRTUAL_KEYBOARD_ID) &lt; 0) &#123; createVirtualKeyboardLocked();&#125;&#125; scanDirLocked()遍历指定文件夹下的所有设备节点，分别对其执行openDeviceLocked()完成设备的打开操作。在这个函数中将为设备节点创建并加载Device结构体。参考其代码： [EventHub.cpp–&gt;EventHub::openDeviceLocked()] 12345678910111213141516171819202122232425262728293031status_t EventHub::openDeviceLocked(const char*devicePath) &#123;// 打开设备节点的文件描述符，用于获取设备信息以及读取原始输入事件int fd =open(devicePath, O_RDWR | O_CLOEXEC);// 接下来的代码通过ioctl()函数从设备节点中获取输入设备的厂商信息InputDeviceIdentifier identifier;......// 分配一个设备Id并创建Device结构体int32_tdeviceId = mNextDeviceId++;Device*device = new Device(fd, deviceId, String8(devicePath), identifier);// 为此设备加载配置信息。 loadConfigurationLocked(device); // ① 通过ioctl函数获取设备的事件位掩码。事件位掩码指定了输入设备可以产生何种类型的输入事件 ioctl(fd, EVIOCGBIT(EV_KEY, sizeof(device-&gt;keyBitmask)),device-&gt;keyBitmask);...... ioctl(fd, EVIOCGPROP(sizeof(device-&gt;propBitmask)),device-&gt;propBitmask); // 接下来的一大段内容是根据事件位掩码为设备分配类别，即设置classes字段。、...... /* ② 将设备节点的描述符的可读事件注册到Epoll中。当此设备的输入事件到来时，Epoll会在 getEvents()函数的调用中产生一条epoll事件 */ structepoll_event eventItem; memset(&amp;eventItem, 0, sizeof(eventItem)); eventItem.events = EPOLLIN; eventItem.data.u32 = deviceId; /* 注意，epoll_event的自定义信息是设备的Id if(epoll_ctl(mEpollFd, EPOLL_CTL_ADD, fd, &amp;eventItem)) &#123; ...... &#125; ...... // ③ 调用addDeviceLocked()将Device添加到mDevices字典中 addDeviceLocked(device); return0; &#125; openDeviceLocked()函数打开指定路径的设备节点，为其创建并填充Device结构体，然后将设备节点的可读事件注册到Epoll中，最后将新建的Device结构体添加到mDevices字典中以供检索之需。整个过程比较清晰，但仍有以下几点需要注意： · openDeviceLocked()函数从设备节点中获取了设备可能上报的事件类型，并据此为设备分配了类别。整个分配过程非常繁琐，由于它和InputReader的事件加工过程关系紧密，因此这部分内容将在5.2.4节再做详细讨论。 · 向Epoll注册设备节点的可读事件时，epoll_event的自定义数据被设置为设备的Id而不是fd。 · addDeviceLocked()将新建的Device对象添加到mDevices字典中的同时也会将其添加到一个名为mOpeningDevices的链表中。这个链表保存了刚刚被加载，但尚未通过getEvents()函数向InputReader发送DEVICE_ADD事件的设备。 完成输入设备的加载之后，通过getEvents()函数便可以读取到此设备所产生的输入事件了。除了在getEvents()函数中使用scanDevicesLockd()一次性加载所有输入设备，当INotify事件告知有新的输入设备节点被创建时，也会通过opendDeviceLocked()将设备加载，稍后再做讨论。 （2）、输入设备的卸载输入设备的卸载由closeDeviceLocked()函数完成。由于此函数的工作内容与openDeviceLocked()函数正好相反，就不列出其代码了。设备的卸载过程为： · 从Epoll中注销对描述符的监听。 · 关闭设备节点的描述符。 · 从mDevices字典中删除对应的Device对象。 · 将Device对象添加到mClosingDevices链表中，与mOpeningDevices类似，这个链表保存了刚刚被卸载，但尚未通过getEvents()函数向InputReader发送DEVICE_REMOVED事件的设备。 同加载设备一样，在getEvents()函数中有根据需要卸载所有输入设备的操作（比如当EventHub要求重新加载所有设备时，会先将所有设备卸载）。并且当INotify事件告知有设备节点删除时也会调用closeDeviceLocked()将设备卸载。 （3）、设备增删事件在分析设备的加载与卸载时发现，新加载的设备与新卸载的设备会被分别放入mOpeningDevices与mClosingDevices链表之中。这两个链表将是在getEvents()函数中向InputReader发送设备增删事件的依据。 参考getEvents()函数的相关代码，以设备卸载事件为例看一下设备增删事件是如何产生的： [EventHub.cpp–&gt;EventHub::getEvents()] 1234567891011121314151617181920212223242526size_t EventHub::getEvents(int timeoutMillis,RawEvent* buffer, size_t bufferSize) &#123;for (;;)&#123; // 遍历mClosingDevices链表，为每一个已卸载的设备生成DEVICE_REMOVED事件 while (mClosingDevices) &#123; Device* device = mClosingDevices; mClosingDevices = device-&gt;next; /* 分析getEvents()函数的工作方式时介绍过，event指针指向buffer中下一个可用于填充 事件的RawEvent对象 */ event-&gt;when = now; // 设置产生事件的事件戳 event-&gt;deviceId = device-&gt;id ==mBuiltInKeyboardId ? BUILT_IN_KEYBOARD_ID : device-&gt;id; event-&gt;type = DEVICE_REMOVED; // 设置事件的类型为DEVICE_REMOVED event += 1; // 将event指针移动到下一个可用于填充事件的RawEvent对象 delete device; // 生成DEVICE_REMOVED事件之后，被卸载的Device对象就不再需要了 mNeedToSendFinishedDeviceScan = true; // 随后发送FINISHED_DEVICE_SCAN事件 /* 当buffer已满则停止继续生成事件，将已生成的事件返回给调用者。尚未生成的事件 将在下次getEvents()调用时生成并返回给调用者 */ if (--capacity == 0) &#123; break; &#125; &#125; // 接下来进行DEVICE_ADDED事件的生成，此过程与 DEVICE_REMOVED事件的生成一致 ......&#125;returnevent – buffer;&#125; 可以看到，在一次getEvents()调用中会尝试为所有尚未发送增删事件的输入设备生成对应的事件返回给调用者。表示设备增删事件的RawEvent对象包含三个信息：产生事件的事件戳、产生事件的设备Id，以及事件类型（DEVICE_ADDED或DEVICE_REMOVED）。 当生成设备增删事件时，会设置mNeedToSendFinishedDeviceSan为true，这个动作的意思是完成所有DEVICE_ADDED/REMOVED事件的生成之后，需要向getEvents()的调用者发送一个FINISHED_DEVICE_SCAN事件，表示设备增删事件的上报结束。这个事件仅包括时间戳与事件类型两个信息。 经过以上分析可知，EventHub可以产生的设备增删事件一共有三种，而且这三种事件拥有固定的优先级，DEVICE_REMOVED事件的优先级最高，DEVICE_ADDED事件次之，FINISHED_DEVICE_SCAN事件最低。而且，getEvents()完成当前高优先级事件的生成之前，不会进行低优先级事件的生成。因此，当发生设备的加载与卸载时，EventHub所生成的完整的设备增删事件序列如图5-5所示，其中R表示DEVICE_REMOVED，A表示DEVICE_ADDED，F表示FINISHED_DEVICE_SCAN。 图：设备增删事件的完整序列 由于参数buffer的容量限制，这个事件序列可能需要通过多次getEvents()调用才能完整地返回给调用者。另外，根据5.2.2节的讨论，设备增删事件相对于Epoll事件拥有较高的优先级，因此从R1事件开始生成到F事件生成之前，getEvents()不会处理Epoll事件，也就是说不会生成原始输入事件。 总结一下设备增删事件的生成原理： · 当发生设备增删时，addDeviceLocked()函数与closeDeviceLocked()函数会将相应的设备放入mOpeningDevices和mClosingDevices链表中。 · getEvents()函数会根据mOpeningDevices和mClosingDevices两个链表生成对应DEVICE_ADDED和DEVICE_REMOVED事件，其中后者的生成拥有高优先级。 · DEVICE_ADDED和DEVICE_REMOVED事件都生成完毕后，getEvents()会生成FINISHED_DEVICE_SCAN事件，标志设备增删事件序列的结束。 （4）、通过INotify动态地加载与卸载设备通过前文的介绍知道了openDeviceLocked()和closeDeviceLocked()可以加载与卸载输入设备。接下来分析EventHub如何通过INotify进行设备的动态加载与卸载。在EventHub的构造函数中创建了一个名为mINotifyFd的INotify对象的描述符，用以监控/dev/input下设备节点的增删。之后将mINotifyFd的可读事件加入到Epoll中。于是可以确定动态加载与卸载设备的工作方式为：首先筛选epoll_wait()函数所取得的Epoll事件，如果Epoll事件表示了mINotifyFd可读，便从mINotifyFd中读取设备节点的增删事件，然后通过执行openDeviceLocked()或closeDeviceLocked()进行设备的加载与卸载。 看一下getEvents()中与INotify相关的代码： [EventHub.cpp–&gt;EventHub::getEvents()] 12345678910111213141516171819202122232425262728293031size_t EventHub::getEvents(int timeoutMillis,RawEvent* buffer, size_t bufferSize) &#123;for (;;)&#123; ...... // 设备增删事件处理 while(mPendingEventIndex &lt; mPendingEventCount) &#123; const struct epoll_event&amp; eventItem = mPendingEventItems[mPendingEventIndex++]; /* ① 通过Epoll事件的data字段确定此事件表示了mINotifyFd可读 注意EPOLL_ID_INOTIFY在EventHub的构造函数中作为data字段向 Epoll注册mINotifyFd的可读事件 */ if (eventItem.data.u32 == EPOLL_ID_INOTIFY) &#123; if (eventItem.events &amp; EPOLLIN) &#123; mPendingINotify = true; // 标记INotify事件待处理 &#125; else &#123; ...... &#125; continue; // 继续处理下一条Epoll事件 &#125; ...... // 其他Epoll事件的处理 &#125; // 如果INotify事件待处理 if(mPendingINotify &amp;&amp; mPendingEventIndex &gt;= mPendingEventCount) &#123; mPendingINotify = false; /* ② 调用readNotifyLocked()函数读取并处理存储在mINotifyFd中的INotify事件 这个函数将完成设备的加载与卸载 */ readNotifyLocked(); deviceChanged = true; &#125; //③ 如果处理了INotify事件，则返回到循环开始处，生成设备增删事件 if(deviceChanged) &#123; continue; &#125;&#125;&#125; getEvents()函数中与INotify相关的代码共有三处： · 识别表示mINotifyFd可读的Epoll事件，并通过设置mPendingINotify为true以标记有INotify事件待处理。getEvents()并没有立刻处理INotify事件，因为此时进行设备的加载与卸载是不安全的。其他Epoll事件可能包含了来自即将被卸载的设备的输入事件，因此需要将所有Epoll事件都处理完毕后再进行加载与卸载操作。 · 当epoll_wait()所返回的Epoll事件都处理完毕后，调用readNotifyLocked()函数读取mINotifyFd中的事件，并进行设备的加载与卸载操作。 · 完成设备的动态加载与卸载后，需要返回到循环最开始处，以便设备增删事件处理代码生成设备的增删事件。 其中第一部分与第三部分比较容易理解。接下来看一下readNotifyLocked()是如何工作的。 [EventHub.cpp–&gt;EventHub::readNotifyLocked()] 123456789101112131415161718status_t EventHub::readNotifyLocked() &#123; ...... // 从mINotifyFd中读取INotify事件列表 res =read(mINotifyFd, event_buf, sizeof(event_buf)); ...... // 逐个处理列表中的事件 while(res &gt;= (int)sizeof(*event)) &#123; strcpy(filename, event-&gt;name); // 从事件中获取设备节点路径 if(event-&gt;mask &amp; IN_CREATE) &#123; openDeviceLocked(devname); // 如果事件类型为IN_CREATE，则加载对应设备 &#125;else &#123; closeDeviceByPathLocked(devname); // 否则卸载对应设备 &#125; ......// 移动到列表中的下一个事件 &#125; return0; &#125; （5）、EventHub设备管理总结至此，EventHub的设备管理相关的知识便讨论完毕了。在这里进行一下总结： · EventHub通过Device结构体描述输入设备的各种信息。 · EventHub在getEvents()函数中进行设备的加载与卸载操作。设备的加载与卸载分为按需加载或卸载以及通过INotify动态加载或卸载特定设备两种方式。 · getEvents()函数进行了设备的加载与卸载操作后，会生成DEVICE_ADDED、DEVICE_REMOVED以及FINISHED_DEVICE_SCAN三种设备增删事件，并且设备增删事件拥有高于Epoll事件的优先级。 4．原始输入事件的监听与读取 本节将讨论EventHub另一个核心的功能，监听与读取原始输入事件。 回忆一下输入设备的加载过程，当设备加载时，openDeviceLocked()会打开设备节点的文件描述符，并将其可读事件注册进Epoll中。于是当设备的原始输入事件到来时，getEvents()函数将会获得一条Epoll事件，然后根据此Epoll事件读取文件描述符中的原始输入事件，将其填充到RawEvents结构体并放入buffer中被调用者取走。openDeviceLocked()注册了设备节点的EPOLLIN和EPOLLHUP两个事件，分别表示可读与被挂起（不可用），因此getEvents()需要分别处理这两种事件。 看一下getEvents()函数中的相关代码： [EventHub.cpp–&gt;EventHub::getEvents()] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253size_t EventHub::getEvents(int timeoutMillis,RawEvent* buffer, size_t bufferSize) &#123;for (;;)&#123; ...... // 设备增删事件处理 while(mPendingEventIndex &lt; mPendingEventCount) &#123; const struct epoll_event&amp; eventItem = mPendingEventItems[mPendingEventIndex++]; ...... // INotify与wakeFd的Epoll事件处理 /* ① 通过Epoll的data.u32字段获取设备Id，进而获取对应的Device对象。如果无法找到 对应的Device对象，说明此Epoll事件并不表示原始输入事件的到来，忽略之 */ ssize_t deviceIndex = mDevices.indexOfKey(eventItem.data.u32); Device* device = mDevices.valueAt(deviceIndex); ...... if (eventItem.events &amp; EPOLLIN) &#123; /* ② 如果Epoll事件为EPOLLIN，表示设备节点有原始输入事件可读。此时可以从描述符 中读取。读取结果作为input_event结构体并存储在readBuffer中，注意事件的个数 受到capacity的限制*/ int32_t readSize = read(device-&gt;fd, readBuffer, sizeof(structinput_event) * capacity); if (......) &#123; ......// 一些错误处理 &#125; else &#123; size_t count = size_t(readSize) / sizeof(struct input_event); /* ② 将读取到的每一个input_event结构体中的数据转换为一个RawEvent对象， 并存储在buffer参数中以返回给调用者 */ for (size_t i = 0; i &lt; count; i++) &#123; const structinput_event&amp; iev = readBuffer[i]; ...... event-&gt;when = now; event-&gt;deviceId =deviceId; event-&gt;type =iev.type; event-&gt;code =iev.code; event-&gt;value =iev.value; event += 1; // 移动到buffer的下一个可用元素 &#125; /* 接下来的一个细节需要注意，因为buffer的容量限制，可能无法完全读取设备节点 中存储的原始事件。一旦buffer满了则需要立刻返回给调用者。设备节点中剩余的 输入事件将在下次getEvents()调用时继续读取，也就是说，当前的Epoll事件 并未处理完毕。mPendingEventIndex -= 1的目的就是使下次getEvents()调用 能够继续处理这个Epoll事件 */ capacity -= count; if (capacity == 0) &#123; mPendingEventIndex -=1; break; &#125; &#125; &#125; else if (eventItem.events &amp; EPOLLHUP) &#123; deviceChanged = true; // 如果设备节点的文件描述符被挂起则卸载此设备 closeDeviceLocked(device); &#125; else &#123; ...... &#125; &#125; ...... // 读取并处理INotify事件 ......// 等待新的Epoll事件&#125;return event – buffer；&#125; getEvents()通过Epoll事件的data.u32字段在mDevices列表中查找已加载的设备，并从设备的文件描述符中读取原始输入事件列表。从文件描述符中读取的原始输入事件存储在input_event结构体中，这个结构体的四个字段存储了事件的事件戳、类型、代码与值四个元素。然后逐一将input_event的数据转存到RawEvent中并保存至buffer以返回给调用者。 注意为了叙述简单，上述代码使用了调用getEvents()的时间作为输入事件的时间戳。由于调用getEvents()函数的时机与用户操作的时间差的存在，会使得此时间戳与事件的真实时间有所偏差。从设备节点中读取的input_event中也包含了一个时间戳，这个时间戳消除了getEvents()调用所带来的时间差，因此可以获得更精确的时间控制。可以通过打开HAVE_POSIX_CLOCKS宏以使用input_event中的时间而不是将getEvents()调用的时间作为输入事件的时间戳。 需要注意的是，由于Epoll事件的处理优先级低于设备增删事件，因此当发生设备加载与卸载动作时，不会产生设备输入事件。另外还需注意，在一个监听周期中，getEvents()在将一个设备节点中的所有原始输入事件读取完毕之前，不会读取其他设备节点中的事件。 5、EventHub总结本节针对EventHub的设备管理与原始输入事件的监听读取两个核心内容介绍了EventHub的工作原理。EventHub作为直接操作设备节点的输入系统组件，隐藏了INotify与Epoll以及设备节点读取等底层操作，通过一个简单的接口getEvents()向使用者提供抽取设备事件与原始输入事件的功能。EventHub的核心功能都在getEvents()函数中完成，因此深入理解getEvents()的工作原理对于深入理解EventHub至关重要。 getEvents()函数的本质是通过epoll_wait()获取Epoll事件到事件池，并对事件池中的事件进行消费的过程。从epoll_wait()的调用开始到事件池中最后一个事件被消费完毕的过程称之为EventHub的一个监听周期。由于buffer参数的尺寸限制，一个监听周期可能包含多个getEvents()调用。周期中的第一个getEvents()调用一定会因事件池的枯竭而直接进行epoll_wait()，而周期中的最后一个getEvents()一定会将事件池中的最后一条事件消费完毕并将事件返回给调用者。前文所讨论的事件优先级都是在同一个监听周期内而言的。 在本节中出现了很多种事件，有原始输入事件、设备增删事件、Epoll事件、INotify事件等，存储事件的结构体有RawEvent、epoll_event、inotify_event、input_event等。图5-6可以帮助读者理清这些事件之间的关系。 图 5-6 EventHub的事件关联 另外，getEvents()函数返回的事件列表依照事件的优先级拥有特定的顺序。并且在一个监听周期中，同一输入设备的输入事件在列表中是相邻的。 至此，相信读者对EventHub的工作原理，以及EventHub的事件监听与读取机制有了深入的了解。接下来的内容将讨论EventHub所提供的原始输入事件如何被加工为Android输入事件，这个加工者就是Reader子系统中的另一员大将：InputReader。 五、Input Reader根据第四节的分析。输入设备扫描完成，并加入epoll中，监听事件。从前面的getEvents函数分析得知，当按键事件发生后，getEvents函数返回。 这里再贴一下Input 处理时间流程图，然后按步骤详细分析。 以一次键盘按键为例，得到下面的6个事件 123456EventHub: /dev/input/event2 got: time=4383.680195, type=4, code=4, value=458792EventHub: /dev/input/event2 got: time=4383.680195, type=1, code=28, value=1EventHub: /dev/input/event2 got: time=4383.680195, type=0, code=0, value=0EventHub: /dev/input/event2 got: time=4383.760186, type=4, code=4, value=458792EventHub: /dev/input/event2 got: time=4383.760186, type=1, code=28, value=0EventHub: /dev/input/event2 got: time=4383.760186, type=0, code=0, value=0 上面的type是linux的输入系统里的事件，具体的值可以查看 查看input.h 123456789101112131415/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */#define EV_SYN 0x00 同步事件#define EV_KEY 0x01 按键事件#define EV_REL 0x02 相对坐标#define EV_ABS 0x03 绝对坐标/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */#define EV_MSC 0x04 其它#define EV_SW 0x05 #define EV_LED 0x11 LED#define EV_SND 0x12 声音/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */#define EV_REP 0x14 Repeat#define EV_FF 0x15 力反馈#define EV_PWR 0x16 电源#define EV_FF_STATUS 0x17 状态 上面6个事件，只有两个type为1的事件，是我们需要处理的按键事件，一个down，一个up Step 1、 InputReader::loopOnce()返回到InputReader的loopOnce函数 1234567891011121314[-&gt;frameworks/native/services/inputflinger/InputReader.cpp]void InputReader::loopOnce() &#123; size_t count = mEventHub-&gt;getEvents(timeoutMillis, mEventBuffer, EVENT_BUFFER_SIZE); &#123; // acquire lock AutoMutex _l(mLock); mReaderIsAliveCondition.broadcast(); //当有按键事件发生时，count将不为0，以一次按键为例，这里应该是6个事件 if (count) &#123; processEventsLocked(mEventBuffer, count); &#125; &#125; // release lock &#125; 当有按键事件发生时，count将不为0，之后会调用processEventsLocked来处理RawEvent。 Step 2、InputReader.processEventsLocked()123456789101112131415[-&gt;frameworks/native/services/inputflinger/InputReader.cpp]void InputReader::processEventsLocked(const RawEvent* rawEvents, size_t count) &#123; for (const RawEvent* rawEvent = rawEvents; count;) &#123; int32_t type = rawEvent-&gt;type; size_t batchSize = 1; if (type &lt; EventHubInterface::FIRST_SYNTHETIC_EVENT) &#123; int32_t deviceId = rawEvent-&gt;deviceId; //依次处理rawEvent processEventsForDeviceLocked(deviceId, rawEvent, batchSize); &#125; count -= batchSize; rawEvent += batchSize; &#125;&#125; 该函数调用processEventsForDeviceLocked依次处理rawEvent Step 3、InputReader.processEventsForDeviceLocked()123456789[-&gt;frameworks/native/services/inputflinger/InputReader.cpp]void InputReader::processEventsForDeviceLocked(int32_t deviceId, const RawEvent* rawEvents, size_t count) &#123; ssize_t deviceIndex = mDevices.indexOfKey(deviceId); InputDevice* device = mDevices.valueAt(deviceIndex); //调用InputDevice的process函数 device-&gt;process(rawEvents, count);&#125; 这里根据deviceId获取到InputDevice，然后调用InputDevice的process函数 Step 4、InputDevice.process()1234567891011121314151617181920[-&gt;frameworks/native/services/inputflinger/InputReader.cpp]void InputDevice::process(const RawEvent* rawEvents, size_t count) &#123;size_t numMappers = mMappers.size();for (const RawEvent* rawEvent = rawEvents; count--; rawEvent++) &#123; if (mDropUntilNextSync) &#123; if (rawEvent-&gt;type == EV_SYN &amp;&amp; rawEvent-&gt;code == SYN_REPORT) &#123; &#125; else if (rawEvent-&gt;type == EV_SYN &amp;&amp; rawEvent-&gt;code == SYN_DROPPED) &#123; &#125; else &#123; for (size_t i = 0; i &lt; numMappers; i++) &#123; InputMapper* mapper = mMappers[i]; // InputMapper是做什么的呢，它是用于解析原始输入事件的。比如back, home等VirtualKey， // 传上来时是个Touch事件，这里要根据坐标转化为相应的按键事件。再比如多点触摸时，需要计算 // 每个触摸点分别属于哪条轨迹，安卓系统中每种输入设备都对应了一种Mapper,比如 // SwitchInputMapper, VibratorInputMapper,KeyBoardInputMapper mapper-&gt;process(rawEvent); &#125; &#125; &#125;&#125; 这里的mMappers成员变量保存了一系列输入设备事件处理对象，例如负责处理键盘事件的KeyboardKeyMapper对象以及负责处理触摸屏事件的TouchInputMapper对象， 它们是在InputReader类的成员函数createDeviceLocked中创建的。这里查询每一个InputMapper对象是否要对当前发生的事件进行处理。由于发生的是键盘事件，真正会对该事件进行处理的只有KeyboardKeyMapper对象。 Step 5、KeyboardInputMapper.process()123456789101112131415161718192021222324252627282930313233343536373839[-&gt;frameworks/native/services/inputflinger/InputReader.cpp]void KeyboardInputMapper::process(const RawEvent* rawEvent) &#123; switch (rawEvent-&gt;type) &#123; case EV_KEY: &#123; int32_t scanCode = rawEvent-&gt;code; int32_t usageCode = mCurrentHidUsage; mCurrentHidUsage = 0; if (isKeyboardOrGamepadKey(scanCode)) &#123; int32_t keyCode; uint32_t flags; // 调用EventHub中的mapKey函数进行转化 // 传入参数 // scanCode：驱动程序上报的扫描码；keyCode：转化之后的Android使用的按键值 if (getEventHub()-&gt;mapKey(getDeviceId(), scanCode, usageCode, &amp;keyCode, &amp;flags)) &#123; keyCode = AKEYCODE_UNKNOWN; flags = 0; &#125; //映射成功之后，处理该按键 processKey(rawEvent-&gt;when, rawEvent-&gt;value != 0, keyCode, scanCode, flags); &#125; break; &#125; case EV_MSC: &#123; if (rawEvent-&gt;code == MSC_SCAN) &#123; mCurrentHidUsage = rawEvent-&gt;value; &#125; break; &#125; case EV_SYN: &#123; if (rawEvent-&gt;code == SYN_REPORT) &#123; mCurrentHidUsage = 0; &#125; &#125; &#125;&#125; 函数首先调用isKeyboardOrGamepadKey来判断键盘扫描码是否正确，如果正确则调用processKey来进一步处理 Step 6、KeyboardInputMapper.processKey()123456789101112131415[-&gt;frameworks/native/services/inputflinger/InputReader.cpp]void KeyboardInputMapper::processKey(nsecs_t when, bool down, int32_t keyCode, int32_t scanCode, uint32_t policyFlags) &#123; // 根据扫描码scanCode、按键码keyCode、newMetaState、downTime按下的时间进行处理 // NotifyKeyArgs args(when, getDeviceId(), mSource, policyFlags, ...... NotifyKeyArgs args(when, getDeviceId(), mSource, policyFlags, down ? AKEY_EVENT_ACTION_DOWN : AKEY_EVENT_ACTION_UP, AKEY_EVENT_FLAG_FROM_SYSTEM, keyCode, scanCode, newMetaState, downTime); // 通知Listener处理，Dispatch线程会监听该事件，并处理，下次博文会具体分析 getListener()-&gt;notifyKey(&amp;args);&#125; 这个函数首先对按键作一些处理，根据扫描码scanCode、按键码keyCode、newMetaState、downTime按下的时间进行处理 最后函数会调用： 12345NotifyKeyArgs args(when, getDeviceId(), mSource, policyFlags, down ? AKEY_EVENT_ACTION_DOWN : AKEY_EVENT_ACTION_UP, AKEY_EVENT_FLAG_FROM_SYSTEM, keyCode, scanCode, newMetaState, downTime);getListener()-&gt;notifyKey(&amp;args); 这里getListener是InputReader初始化时传入的对象，即QueuedInputListener，则会调用QueuedInputListener的notifyKey函数 123void QueuedInputListener::notifyKey(const NotifyKeyArgs* args) &#123;mArgsQueue.push(new NotifyKeyArgs(*args));&#125; InputReader的loopOnce()的结尾会调用QueuedInputListener::flush()统一回调缓冲队列中各元素的notify()接口： 123456789void QueuedInputListener::flush() &#123;size_t count = mArgsQueue.size();for (size_t i = 0; i &lt; count; i++) &#123; NotifyArgs* args = mArgsQueue[i]; args-&gt;notify(mInnerListener); delete args;&#125;mArgsQueue.clear();&#125; 进一步调用： 123void NotifyConfigurationChangedArgs::notify(const sp&lt;InputListenerInterface&gt;&amp; listener) const &#123;listener-&gt;notifyConfigurationChanged(this);&#125; 以按键事件为例，由于InputDispatcher 实现了InputListenerInterface接口的notifyConfigurationChanged()函数，所以最后会调用到InputDispatcher的notifyKey()函数中。 Step 7、 InputDispatcher.notifyKey()12345678910111213141516171819202122232425262728293031[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::notifyKey(const NotifyKeyArgs* args) &#123;......// 构造一个KeyEvent对象KeyEvent event;event.initialize(args-&gt;deviceId, args-&gt;source, args-&gt;action, flags, keyCode, args-&gt;scanCode, metaState, 0, args-&gt;downTime, args-&gt;eventTime);// 调用mPolicy的interceptKeyBeforeQueueing函数，该函数最后会调用到java层的PhoneWindowManagerService函数mPolicy-&gt;interceptKeyBeforeQueueing(&amp;event, /*byref*/ policyFlags);bool needWake;&#123; // acquire lock mLock.lock(); ...... //构造一个KeyEntry对象，调用enqueueInboundEventLocked函数将按键事件加入队列 KeyEntry* newEntry = new KeyEntry(args-&gt;eventTime, args-&gt;deviceId, args-&gt;source, policyFlags, args-&gt;action, flags, keyCode, args-&gt;scanCode, metaState, repeatCount, args-&gt;downTime); needWake = enqueueInboundEventLocked(newEntry); mLock.unlock();&#125; // release lockif (needWake) &#123; mLooper-&gt;wake();&#125;&#125; 该函数首先调用validateKeyEvent来判断是否是有效按键事件，实际判断是否是UP/DOWN事件 然后构造一个KeyEvent对象，调用mPolicy的interceptKeyBeforeQueueing函数，该函数最后会调用到java层的PhoneWindowManagerService函数 123456KeyEvent event; event.initialize(args-&gt;deviceId, args-&gt;source, args-&gt;action, flags, keyCode, args-&gt;scanCode, metaState, 0, args-&gt;downTime, args-&gt;eventTime); mPolicy-&gt;interceptKeyBeforeQueueing(&amp;event, /*byref*/ policyFlags); 之后会调用构造一个KeyEntry对象，调用enqueueInboundEventLocked函数将按键事件加入队列，如果返回true，则调用mLooper.wake函数唤醒等待的InputDispatcher，进行按键分发。 123456KeyEntry* newEntry = new KeyEntry(args-&gt;eventTime, args-&gt;deviceId, args-&gt;source, policyFlags, args-&gt;action, flags, keyCode, args-&gt;scanCode, metaState, repeatCount, args-&gt;downTime);needWake = enqueueInboundEventLocked(newEntry); Step 8、InputDispatcher.enqueueInboundEventLocked()123456789101112131415161718192021222324252627[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]bool InputDispatcher::enqueueInboundEventLocked(EventEntry* entry) &#123; bool needWake = mInboundQueue.isEmpty(); mInboundQueue.enqueueAtTail(entry); traceInboundQueueLengthLocked(); switch (entry-&gt;type) &#123; case EventEntry::TYPE_KEY: &#123; // ...... KeyEntry* keyEntry = static_cast&lt;KeyEntry*&gt;(entry); if (isAppSwitchKeyEventLocked(keyEntry)) &#123; if (keyEntry-&gt;action == AKEY_EVENT_ACTION_DOWN) &#123; mAppSwitchSawKeyDown = true; &#125; else if (keyEntry-&gt;action == AKEY_EVENT_ACTION_UP) &#123; if (mAppSwitchSawKeyDown) &#123; mAppSwitchDueTime = keyEntry-&gt;eventTime + APP_SWITCH_TIMEOUT; mAppSwitchSawKeyDown = false; needWake = true; &#125; &#125; &#125; break; &#125; &#125; return needWake;&#125; 将EventEntry加入到mInboundQueue中，该函数两种情况下会返回true,一是当加入该键盘事件到mInboundQueue之前，mInboundQueue为空，这表示InputDispatc herThread线程正在睡眠等待InputReaderThread线程的唤醒，因此，它返回true表示要唤醒InputDispatccherThread线程；二是加入该键盘事件到mInboundQueue之前，mInboundQueue不为空，但是此时用户按下的是Home键等需要切换APP的按键，我们知道，在切换App时，新的App会把它的键盘消息接收通道注册到InputDispatcher中去，并且会等待InputReader的唤醒，因此，在这种情况下，也需要返回true，表示要唤醒InputDispatccherThread线程。如果不是这两种情况，那么就说明InputDispatccherThread线程现在正在处理前面的键盘事件，不需要唤醒它。 至此，InputDispatcherThread被唤醒，开始进行按键分发。 总结：InputReaderThread不断调用InputReader的pollOnce()-&gt;getEvents()函数来得到事件，这些事件可以是输入事件，也可以是由inotify监测到设备增减变更所触发的事件。第一次进入时会扫描/dev/input目录建立设备列表，存在mDevice成员变量中(EventHub中有设备列表KeyedVector mDevices；对应的，InputReader中也有设备列表KeyedVector mDevices。这里先添加到前者，然后会在InputReader::addDeviceLocked()中添加到后者。)，同时将增加的fd加到epoll的等待集合中。在接下来的epoll_wait()等待时，如果有事件就会返回，同时返回可读事件数量。在这里，从Input driver读出的事件从原始的input_event结构转为RawEvent结构，放到getEvents()的输出参数buffer中。getEvents()返回后，InputReader调用processEventsLocked()处理事件，对于设备改变，会根据实际情况调用addDeviceLocked(), removeDeviceLocked()和handleConfigurationChangedLocked()。对于其它设备中来的输入事件，会调用processEventsForDeviceLocked()进一步处理。其中会根据当时注册的InputMapper对事件进行处理，然后将事件处理请求放入缓冲队列（QueuedInputListener中的mArgsQueue）。 InputMapper是做什么的呢，它是用于解析原始输入事件的。比如back, home等VirtualKey，传上来时是个Touch事件，这里要根据坐标转化为相应的按键事件。再比如多点触摸时，需要计算每个触摸点分别属于哪条轨迹，这本质上是个二分图匹配问题，这也是在InputMapper中完成的。回到流程主线上，在InputReader的loopOnce()的结尾会调用QueuedInputListener::flush()统一回调缓冲队列中各元素的notify()接口。 以按键事件为例，最后会调用到InputDispatcher的notifyKey()函数中。这里先将参数封装成KeyEvent： 然后把它作为参数调用NativeInputManager的interceptKeyBeforeQueueing()函数。顾名思义，就是在放到待处理队列前看看是不是需要系统处理的系统按键，它会通过JNI调回Java世界，最终调到PhoneWindowManager的interceptKeyBeforeQueueing()。然后，基于输入事件信息创建KeyEntry对象，调用enqueueInboundEventLocked()将之放入队列等待InputDiaptcherThread线程拿出处理。 六、Input DispatcherInputDisptacher的主要任务是把前面收到的输入事件发送到PWM及App端的焦点窗口。前面提到InputReaderThread中收到事件后会调用notifyKey()来通知InputDispatcher，也就是放在mInboundQueue中，在InputDispatcher的dispatchOnce()函数中，会从这个队列拿出处理。 其中dispatchOnceInnerLocked()会根据拿出的EventEntry类型调用相应的处理函数，以Key事件为例会调用dispatchKeyLocked() 它会找到目标窗口，然后通过之前和App间建立的连接发送事件。如果是个需要系统处理的Key事件，这里会封装成CommandEntry插入到mCommandQueue队列中，后面的runCommandLockedInterruptible()函数中会调用doInterceptKeyBeforeDispatchingLockedInterruptible()来让PWM有机会进行处理。最后dispatchOnce()调用pollOnce()从和App的连接上接收处理完成消息。那么，InputDispatcher是怎么确定要往哪个窗口中发事件呢？这里的成员变量mFocusedWindowHandle指示了焦点窗口，然后findFocusedWindowTargetsLocked()会调用一系列函数（handleTargetsNotReadyLocked(), checkInjectionPermission(), checkWindowReadyForMoreInputLocked()等）检查mFocusedWindowHandle是否能接收输入事件。如果可以，将之以InputTarget的形式加到目标窗口数组中。然后就会调用dispatchEventLocked()进行发送。那么，这个mFocusedWindowHandle是如何维护的呢？为了更好地理解，这里回头分析下窗口连接的管理及焦点窗口的管理。 总体流程图： 再贴一张详细的总体流程图，然后根据步骤详细分析； Step 1、InputDispatcher.dispatchOnce()1234567891011121314151617181920212223242526[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::dispatchOnce() &#123;nsecs_t nextWakeupTime = LONG_LONG_MAX;&#123; // acquire lock AutoMutex _l(mLock); mDispatcherIsAliveCondition.broadcast(); // Run a dispatch loop if there are no pending commands. // The dispatch loop might enqueue commands to run afterwards. if (!haveCommandsLocked()) &#123; dispatchOnceInnerLocked(&amp;nextWakeupTime); &#125; // Run all pending commands if there are any. // If any commands were run then force the next poll to wake up immediately. if (runCommandsLockedInterruptible()) &#123; nextWakeupTime = LONG_LONG_MIN; &#125;&#125; // release lock// Wait for callback or timeout or wake. (make sure we round up, not down)nsecs_t currentTime = now();int timeoutMillis = toMillisecondTimeoutDelay(currentTime, nextWakeupTime);mLooper-&gt;pollOnce(timeoutMillis);&#125; 上述函数主要是调用dispatchOnceInnerLocked来进行一次按键分发，当没有按键消息时会走到mLooper-&gt;pollOnce(timeoutMillis);这个函数会进入睡眠状态，当有按键消息发生时该函数会返回，然后走到dispatchOnceInnerLocked函数。 Step 2、InputDispatcher.dispatchOnceInnerLocked()1234567891011121314151617181920212223242526272829303132333435363738394041[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::dispatchOnceInnerLocked(nsecs_t* nextWakeupTime) &#123; nsecs_t currentTime = now(); ... // Ready to start a new event. // If we don't already have a pending event, go grab one. if (! mPendingEvent) &#123; //当InputReader往队列中插入了一个读取的键盘消息后，此处的mInboundQueue就不为空 if (mInboundQueue.isEmpty()) &#123; ... &#125; else &#123; // Inbound queue has at least one entry. mPendingEvent = mInboundQueue.dequeueAtHead(); ... &#125; ... &#125; ... switch (mPendingEvent-&gt;type) &#123; ... case EventEntry::TYPE_KEY: &#123; KeyEntry* typedEntry = static_cast&lt;KeyEntry*&gt;(mPendingEvent); ... done = dispatchKeyLocked(currentTime, typedEntry, &amp;dropReason, nextWakeupTime); break; &#125; ... &#125; if (done) &#123; if (dropReason != DROP_REASON_NOT_DROPPED) &#123; dropInboundEventLocked(mPendingEvent, dropReason); &#125; mLastDropReason = dropReason; releasePendingEventLocked(); *nextWakeupTime = LONG_LONG_MIN; // force next poll to wake up immediately &#125;&#125; 从前文InputReader读取键盘消息过程分析 InputReader读取到一个消息后会调用KeyboardInputMapper的processKey，该函数会调用InputDispatcher的notifyKey函数，然后InputDispatcher会调用enqueueInboundEventLocked函数，将EventEntry加入到mInboundQueue中，然后调用mLooper-&gt;wake函数会唤醒InputDispatcherThread线程，InputDispatcher中把队列的第一个事件取出来，因为这里是键盘事件，所以mPendingEvent-&gt;type是EventEntry::TYPE_KEY，然后调用dispatchKeyLocked函数 惯例先贴出序列图，按步骤一步步介绍。 Step 3、InputDispatcher.dispatchKeyLocked()12345678910111213141516171819202122232425262728293031[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp ]bool InputDispatcher::dispatchKeyLocked(nsecs_t currentTime, KeyEntry* entry, DropReason* dropReason, nsecs_t* nextWakeupTime) &#123; ... // Give the policy a chance to intercept the key. if (entry-&gt;interceptKeyResult == KeyEntry::INTERCEPT_KEY_RESULT_UNKNOWN) &#123; if (entry-&gt;policyFlags &amp; POLICY_FLAG_PASS_TO_USER) &#123; CommandEntry* commandEntry = postCommandLocked( &amp; InputDispatcher::doInterceptKeyBeforeDispatchingLockedInterruptible); if (mFocusedWindowHandle != NULL) &#123; commandEntry-&gt;inputWindowHandle = mFocusedWindowHandle; &#125; ...... &#125; else &#123; entry-&gt;interceptKeyResult = KeyEntry::INTERCEPT_KEY_RESULT_CONTINUE; &#125; &#125; ...... // Identify targets. Vector&lt;InputTarget&gt; inputTargets; int32_t injectionResult = findFocusedWindowTargetsLocked(currentTime, entry, inputTargets, nextWakeupTime); ...... setInjectionResultLocked(entry, injectionResult); ...... addMonitoringTargetsLocked(inputTargets); // Dispatch the key. dispatchEventLocked(currentTime, entry, inputTargets); return true;&#125; 这个函数主要做了下面三件事 A. 如果按键是第一次分发，则将命令封装为CommandEntry加入队列，后续执行doInterceptKeyBeforeDispatchingLockedInterruptible，以给java层拦截按键的机会 B. 找到当前激活的Window窗口，并将其加入到Vector中，Android ANR就是在findFocusedWindowTargetsLocked()检测的 C. 找到需要主动监听按键的InputChannel,封装成InputTarget，加入到Vector中 D. 将按键分发到上面的Vector中的InputChannel中，这里存在多个 下面先分析如果将按键分发给InputChannel 123456789101112131415[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::dispatchEventLocked(nsecs_t currentTime, EventEntry* eventEntry, const Vector&lt;InputTarget&gt;&amp; inputTargets) &#123;......for (size_t i = 0; i &lt; inputTargets.size(); i++) &#123; const InputTarget&amp; inputTarget = inputTargets.itemAt(i); ssize_t connectionIndex = getConnectionIndexLocked(inputTarget.inputChannel); if (connectionIndex &gt;= 0) &#123; sp&lt;Connection&gt; connection = mConnectionsByFd.valueAt(connectionIndex); prepareDispatchCycleLocked(currentTime, connection, eventEntry, &amp;inputTarget); &#125; ...... &#125; &#125;&#125; Step 5、InputDispatcher.prepareDispatchCycleLocked()12345678[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::prepareDispatchCycleLocked(nsecs_t currentTime, const sp&lt;Connection&gt;&amp; connection, EventEntry* eventEntry, const InputTarget* inputTarget) &#123; ... // Not splitting. Enqueue dispatch entries for the event as is. enqueueDispatchEntriesLocked(currentTime, connection, eventEntry, inputTarget); &#125; 函数前面还有一些状态检查，这里默认都是通过的。最后enqueueDispatchEntriesLocked函数进行将connection分装成DispatchEntry，加入到connection-&gt;outboundQueue的队列中 Step 6. InputDispatcher::enqueueDispatchEntriesLocked()123456789101112131415161718[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::enqueueDispatchEntriesLocked(nsecs_t currentTime, const sp&lt;Connection&gt;&amp; connection, EventEntry* eventEntry, const InputTarget* inputTarget) &#123; bool wasEmpty = connection-&gt;outboundQueue.isEmpty(); // Enqueue dispatch entries for the requested modes. enqueueDispatchEntryLocked(connection, eventEntry, inputTarget, InputTarget::FLAG_DISPATCH_AS_HOVER_EXIT); ...... enqueueDispatchEntryLocked(connection, eventEntry, inputTarget, InputTarget::FLAG_DISPATCH_AS_SLIPPERY_ENTER); // If the outbound queue was previously empty, start the dispatch cycle going. if (wasEmpty &amp;&amp; !connection-&gt;outboundQueue.isEmpty()) &#123; startDispatchCycleLocked(currentTime, connection); &#125;&#125; 这个函数首先获取以前的connection的outboundQueue是否为空，然后将该事件调用enqueueDispatchEntryLocked将事件加入到outboundQueue中，如果以前为空，现在不为空，则调用startDispatchCycleLocked开始分发，如果以前的outboundQueue不为空，说明当前的Activity正在处理前面的按键，则不需要再调用startDispatchCycleLocked，因为只要开始处理，会等到队列为空才会停止。 Step 7、InputDispatcher.startDispatchCycleLocked()12345678910111213141516171819202122232425262728293031323334353637[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::startDispatchCycleLocked(nsecs_t currentTime, const sp&lt;Connection&gt;&amp; connection) &#123; while (connection-&gt;status == Connection::STATUS_NORMAL &amp;&amp; !connection-&gt;outboundQueue.isEmpty()) &#123; DispatchEntry* dispatchEntry = connection-&gt;outboundQueue.head; dispatchEntry-&gt;deliveryTime = currentTime; // Publish the event. status_t status; EventEntry* eventEntry = dispatchEntry-&gt;eventEntry; switch (eventEntry-&gt;type) &#123; case EventEntry::TYPE_KEY: &#123; KeyEntry* keyEntry = static_cast&lt;KeyEntry*&gt;(eventEntry); // Publish the key event. status = connection-&gt;inputPublisher.publishKeyEvent(dispatchEntry-&gt;seq, keyEntry-&gt;deviceId, keyEntry-&gt;source, dispatchEntry-&gt;resolvedAction, dispatchEntry-&gt;resolvedFlags, keyEntry-&gt;keyCode, keyEntry-&gt;scanCode, keyEntry-&gt;metaState, keyEntry-&gt;repeatCount, keyEntry-&gt;downTime, keyEntry-&gt;eventTime); break; &#125; ...... &#125; ...... // Re-enqueue the event on the wait queue. connection-&gt;outboundQueue.dequeue(dispatchEntry); traceOutboundQueueLengthLocked(connection); connection-&gt;waitQueue.enqueueAtTail(dispatchEntry); traceWaitQueueLengthLocked(connection); &#125;//end of while&#125; 该函数从outboundQueue中取出需要处理的键盘事件，交给connection的inputPublisher去分发，之后将事件加入到connection的waitQueue中。分发事件是通过InputPublisher的publishKeyEvent来完成的。 Step 8、InputPublisher.publishKeyEvent123456789101112131415[-&gt;frameworks/native/libs/input/InputTransport.cpp]status_t InputPublisher::publishKeyEvent( uint32_t seq,int32_t deviceId,int32_t source, int32_t action,int32_t flags,int32_t keyCode, int32_t scanCode,int32_t metaState,int32_t repeatCount, nsecs_t downTime,nsecs_t eventTime) &#123; InputMessage msg; msg.header.type = InputMessage::TYPE_KEY; ...... msg.body.key.eventTime = eventTime; return mChannel-&gt;sendMessage(&amp;msg);&#125; 该函数主要是将各个参数封装到InputMessage中，然后交给mChannel对象去分发 mChannel其实是socketpair的server端，其实就是创建的服务器InputChannel，其创建过程稍后详细分析。 Step 9、InputChannel.sendMessage()12345678910[-&gt;frameworks/native/libs/input/InputTransport.cpp]status_t InputChannel::sendMessage(const InputMessage* msg) &#123; size_t msgLength = msg-&gt;size(); ssize_t nWrite; do &#123; nWrite = ::send(mFd, msg, msgLength, MSG_DONTWAIT | MSG_NOSIGNAL); &#125; while (nWrite == -1 &amp;&amp; errno == EINTR); return OK;&#125; 该函数主要是通过send函数往socketpair的server端写入InputMessage对象，应用程序这一侧正睡眠在client端的fd上，此时client端就会收到该InputMessage，client会进行按键按键分发，应用程序这一侧的按键分发请看下一节。 七、App注册消息监听过程分析总体流程图 InputDispatcher会找到目标窗口，然后通过之前和App间建立的连接发送事件。如果是个需要系统处理的Key事件，这里会封装成CommandEntry插入到mCommandQueue队列中，后面的runCommandLockedInterruptible()函数中会调用doInterceptKeyBeforeDispatchingLockedInterruptible()来让PWM有机会进行处理。最后dispatchOnce()调用pollOnce()从和App的连接上接收处理完成消息。那么，InputDispatcher是怎么确定要往哪个窗口中发事件呢？这里的成员变量mFocusedWindowHandle指示了焦点窗口，然后findFocusedWindowTargetsLocked()会调用一系列函数（handleTargetsNotReadyLocked(), checkInjectionPermission(), checkWindowReadyForMoreInputLocked()等）检查mFocusedWindowHandle是否能接收输入事件。如果可以，将之以InputTarget的形式加到目标窗口数组中。然后就会调用dispatchEventLocked()进行发送。那么，这个mFocusedWindowHandle是如何维护的呢？为了更好地理解，这里回头分析下窗口连接的管理及焦点窗口的管理。 在App端，新的顶层窗口需要被注册到WMS中，这是在ViewRootImpl::setView()中做的。 Step 1、ViewRootImpl.setView()1234567891011121314151617181920212223242526272829303132333435[-&gt;frameworks/base/core/java/android/view/ViewRootImpl.java]public void setView(View view, WindowManager.LayoutParams attrs, View panelParentView) &#123; // 调用requestLayout来通知InputManagerService当前的窗口是激活的窗口 requestLayout(); if ((mWindowAttributes.inputFeatures &amp; WindowManager.LayoutParams.INPUT_FEATURE_NO_INPUT_CHANNEL) == 0) &#123; // 如果该窗口没有指定INPUT_FEATURE_NO_INPUT_CHANNEL属性，则创建消息接收通道InputChannel mInputChannel = new InputChannel(); &#125; try &#123; // 通过binder调用，调用server端的Session对象来跟WindowManagerService通信，该函数最后会调 // 用到WindowManagerService的addWindow函数，函数中会创建一对InputChannel(server/client)， // 这样在函数调用结束后，mInputChannel就变成了client端的对象。在 // frameworks/base/core/java/android/view/IWindowSession.aidl的 // addToDisplay函数的声明中，InputChannel指定的数据流的流向是out，因此 // WindowManagerService修改了mInputChannel,客户端就能拿到这个对象的数据了。 res = mWindowSession.addToDisplay(mWindow, mSeq, mWindowAttributes, getHostVisibility(), mDisplay.getDisplayId(), mAttachInfo.mContentInsets, mAttachInfo.mStableInsets, mAttachInfo.mOutsets, mInputChannel); &#125; catch (Exception e) &#123; ... &#125; if (mInputChannel != null) &#123; if (mInputQueueCallback != null) &#123; mInputQueue = new InputQueue(); mInputQueueCallback.onInputQueueCreated(mInputQueue); // 初始化WindowInputEventReceiver，按键消息会从native层传到该对象的onInputEvent函数 // 中，onInputEvent函数是按键在应用端java层分发的起始端。 mInputEventReceiver = new WindowInputEventReceiver(mInputChannel, Looper.myLooper()); &#125; &#125;&#125; 这个函数与注册键盘消息通道的相关主要有三个功能： 一是调用requestLayout函数来通知InputManagerService，这个Activity窗口是当前被激活的窗口,同时将所有的窗口注册到InputDispatcher中 二是调用mWindowSession的add成员函数来把键盘消息接收通道的server端注册端注册到CPP层的InputManagerService中，client端注册到本应用程序的消息循环Looper中，这样当InputManagerService监控到有键盘消息的时候，就会找到当前被激活的窗口，然后找到其在InputManagerService中对应的键盘消息接收通道(InputChannel)，通过这个通道在InputManagerService的server端来通知应用程序消息循环的client端，这样就把键盘消息分发给当前激活的Activity窗口了 三是应用程序这一侧注册消息接收通道 Step 2、ViewRootImpl.requestLayout()12345678910[-&gt;frameworks/base/core/java/android/view/ViewRootImpl.java]Overridepublic void requestLayout() &#123; if (!mHandlingLayoutInLayoutRequest) &#123; checkThread(); mLayoutRequested = true; scheduleTraversals(); &#125; &#125; 这里调用了scheduleTraversals函数来做进一步的操作，该函数调用mChoreographer来post一个Runnable到Looper中，之后Vsycn信号到来会执行mTraversalRunnable中的run方法，即调用doTraversal函数 参考文档：【Android 7.1.2(Android N) Activity-Window加载显示流程】 Step 3、ViewRootImpl.doTraversal()123456789[-&gt;frameworks/base/core/java/android/view/ViewRootImpl.java]void doTraversal() &#123; if (mTraversalScheduled) &#123; mTraversalScheduled = false; mHandler.getLooper().getQueue().removeSyncBarrier(mTraversalBarrier); performTraversals(); &#125;&#125; 该函数主要是执行performTraversals()函数，进而调用relayoutWindow函数，在该函数中又会调用mWindowSession的relayout进入到java层的WindowManagerService的relayoutWindow函数，该函数会调用mInputMonitor.updateInputWindowsLw(true /force/);mInputMonitor是InputMonitor对象。 Step 4、InputMonitor.updateInputWindowsLw()123456789101112131415161718192021[-&gt;frameworks/base/services/core/java/com/android/server/wm/InputMonitor.java]public void updateInputWindowsLw(boolean force) &#123;boolean addInputConsumerHandle = mService.mInputConsumer != null;// Add all windows on the default display.final int numDisplays = mService.mDisplayContents.size();for (int displayNdx = 0; displayNdx &lt; numDisplays; ++displayNdx) &#123; WindowList windows = mService.mDisplayContents.valueAt(displayNdx).getWindowList(); for (int winNdx = windows.size() - 1; winNdx &gt;= 0; --winNdx) &#123; final WindowState child = windows.get(winNdx); final InputChannel inputChannel = child.mInputChannel; final InputWindowHandle inputWindowHandle = child.mInputWindowHandle; ...... addInputWindowHandleLw(inputWindowHandle, child, flags, type, isVisible, hasFocus, hasWallpaper); &#125;&#125;// Send windows to native code.mService.mInputManager.setInputWindows(mInputWindowHandles);&#125; 这个函数将当前系统中带有InputChannel的Activity窗口都设置为InputManagerService的输入窗口，但是后面我们会看到，只有当前激活的窗口才会响应键盘消息。 Step 5、InputManagerService.setInputWindows()12345[-&gt;frameworks/base/services/core/java/com/android/server/input/InputManagerService.java]public void setInputWindows(InputWindowHandle[] windowHandles) &#123; nativeSetInputWindows(mPtr, windowHandles);&#125; 这个函数调用了本地方法nativeSetInputWindows来进一步执行操作,mPtr是native层NativeInputManager实例，在调用InputManagerService.nativeInit函数时会在native层构造NativeInputManager对象并将其保存在mPtr中。nativeSetInputWindows会调用NativeInputManager的setInputWindows函数 Step 6、NativeInputManager.setInputWindows()12345678910111213141516171819202122232425262728293031[-&gt;frameworks/base/services/core/jni/com_android_server_input_InputManagerService.cpp]void NativeInputManager::setInputWindows(JNIEnv* env, jobjectArray windowHandleObjArray) &#123; Vector&lt;sp&lt;InputWindowHandle&gt; &gt; windowHandles; if (windowHandleObjArray) &#123; jsize length = env-&gt;GetArrayLength(windowHandleObjArray); for (jsize i = 0; i &lt; length; i++) &#123; jobject windowHandleObj = env-&gt;GetObjectArrayElement(windowHandleObjArray, i); ...... sp&lt;InputWindowHandle&gt; windowHandle = android_server_InputWindowHandle_getHandle(env, windowHandleObj); if (windowHandle != NULL) &#123; windowHandles.push(windowHandle); &#125; env-&gt;DeleteLocalRef(windowHandleObj); &#125; &#125; mInputManager-&gt;getDispatcher()-&gt;setInputWindows(windowHandles); // Do this after the dispatcher has updated the window handle state. bool newPointerGesturesEnabled = true; size_t numWindows = windowHandles.size(); for (size_t i = 0; i &lt; numWindows; i++) &#123; const sp&lt;InputWindowHandle&gt;&amp; windowHandle = windowHandles.itemAt(i); const InputWindowInfo* windowInfo = windowHandle-&gt;getInfo(); if (windowInfo &amp;&amp; windowInfo-&gt;hasFocus &amp;&amp; (windowInfo-&gt;inputFeatures &amp; InputWindowInfo::INPUT_FEATURE_DISABLE_TOUCH_PAD_GESTURES)) &#123; newPointerGesturesEnabled = false; &#125; &#125;&#125; 这个函数首先将Java层的InputWindowHandle转换成C++层的NativeInputWindowHandle，然后放在windowHandles向量中，最后将这些输入窗口设置到InputDispatcher中去。 Step 7、InputDispatcher.setInputWindows()123456789101112131415161718192021222324252627282930[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]void InputDispatcher::setInputWindows(const Vector&lt;sp&lt;InputWindowHandle&gt; &gt;&amp; inputWindowHandles) &#123; &#123; // acquire lock AutoMutex _l(mLock); Vector&lt;sp&lt;InputWindowHandle&gt; &gt; oldWindowHandles = mWindowHandles; mWindowHandles = inputWindowHandles; sp&lt;InputWindowHandle&gt; newFocusedWindowHandle; bool foundHoveredWindow = false; for (size_t i = 0; i &lt; mWindowHandles.size(); i++) &#123; const sp&lt;InputWindowHandle&gt;&amp; windowHandle = mWindowHandles.itemAt(i); if (!windowHandle-&gt;updateInfo() || windowHandle-&gt;getInputChannel() == NULL) &#123; mWindowHandles.removeAt(i--); continue; &#125; if (windowHandle-&gt;getInfo()-&gt;hasFocus) &#123; newFocusedWindowHandle = windowHandle; &#125; &#125; if (mFocusedWindowHandle != newFocusedWindowHandle) &#123; mFocusedWindowHandle = newFocusedWindowHandle; &#125; &#125; // release lock // Wake up poll loop since it may need to make new input dispatching choices. mLooper-&gt;wake();&#125; 这里InputDispatcher的成员变量mFocusedWindowHandle 就代表当前激活的窗口的。这个函数遍历inputWindowHandles，获取获得焦点的窗口，并赋值给mFocusedWindowHandle 这样，InputManagerService就把当前激活的窗口保存在InputDispatcher中了，后面就可以把键盘消息分发给它来处理。 Step 8、mWindowSession.addToDisplay()123456789101112if ((mWindowAttributes.inputFeatures &amp; WindowManager.LayoutParams.INPUT_FEATURE_NO_INPUT_CHANNEL) == 0) &#123; mInputChannel = new InputChannel(); &#125; try &#123; res = mWindowSession.addToDisplay(mWindow, mSeq, mWindowAttributes, getHostVisibility(), mDisplay.getDisplayId(), mAttachInfo.mContentInsets, mAttachInfo.mStableInsets, mAttachInfo.mOutsets, mInputChannel); &#125; catch (Exception e) &#123; ... &#125; 这里会调用到WindowManagerService的addWindow接口 1234567891011121314frameworks/base/services/core/java/com/android/server/wm/WindowManagerService.javapublic int addWindow(Session session, IWindow client, int seq,WindowManager.LayoutParams attrs, int viewVisibility, int displayId,Rect outContentInsets, Rect outStableInsets, Rect outOutsets,InputChannel outInputChannel) &#123; ...... final boolean openInputChannels = (outInputChannel != null &amp;&amp; (attrs.inputFeatures &amp; INPUT_FEATURE_NO_INPUT_CHANNEL) == 0); if (openInputChannels) &#123; win.openInputChannel(outInputChannel); &#125; ...... &#125; 接着会调用WindowState的 openInputChannel()方法。 123456789101112131415161718192021frameworks/base/services/core/java/com/android/server/wm/WindowState.java void openInputChannel(InputChannel outInputChannel) &#123; String name = makeInputChannelName(); InputChannel[] inputChannels = InputChannel.openInputChannelPair(name); mInputChannel = inputChannels[0]; mClientChannel = inputChannels[1]; mInputWindowHandle.inputChannel = inputChannels[0]; if (outInputChannel != null) &#123; mClientChannel.transferTo(outInputChannel); mClientChannel.dispose(); mClientChannel = null; &#125; else &#123; // If the window died visible, we setup a dummy input channel, so that taps // can still detected by input monitor channel, and we can relaunch the app. // Create dummy event receiver that simply reports all events as handled. mDeadWindowEventReceiver = new DeadWindowEventReceiver(mClientChannel); &#125; mService.mInputManager.registerInputChannel(mInputChannel, mInputWindowHandle);&#125; 这里的outInputChannel即为前面创建的InputChannel，它不为NULL，因此，这里会通过InputChannel.openInputChannelPair函数来创建一对输入通道，其中一个位于WindowManagerService中，另外一个通过outInputChannel参数返回到应用程序中。 WindowManagerService会为每个窗口创建一个WindowState对象，然后将该InputChannel对的service端保存到WindowState中 Step 10、InputChannel.openInputChannelPair()12345[-&gt;frameworks/base/core/java/android/view/InputChannel.java]public static InputChannel[] openInputChannelPair(String name) &#123; return nativeOpenInputChannelPair(name);&#125; 调用了nativeOpenInputChannelPair函数，在native创建一个InputChannel对 Step 11、InputChannel.nativeOpenInputChannelPair()1234567891011121314[-&gt;frameworks/base/core/jni/android_view_InputChannel.cpp]static jobjectArray android_view_InputChannel_nativeOpenInputChannelPair(JNIEnv* env, jclass clazz, jstring nameObj) &#123; const char* nameChars = env-&gt;GetStringUTFChars(nameObj, NULL); String8 name(nameChars); env-&gt;ReleaseStringUTFChars(nameObj, nameChars); sp&lt;InputChannel&gt; serverChannel; sp&lt;InputChannel&gt; clientChannel; status_t result = InputChannel::openInputChannelPair(name, serverChannel, clientChannel); ... return channelPair;&#125; nativeOpenInputChannelPair函数调用InputChannel的openInputChannelPair函数创建一对InputChannel,该对象是Native层的InputChannel,跟java层是一一对应的。 Step 12、InputChannel.openInputChannelPair()123456789101112131415161718192021222324252627282930[-&gt;frameworks/native/libs/input/InputTransport.cpp]status_t InputChannel::openInputChannelPair(const String8&amp; name, sp&lt;InputChannel&gt;&amp; outServerChannel, sp&lt;InputChannel&gt;&amp; outClientChannel) &#123; int sockets[2]; if (socketpair(AF_UNIX, SOCK_SEQPACKET, 0, sockets)) &#123; status_t result = -errno; ALOGE(\"channel '%s' ~ Could not create socket pair. errno=%d\", name.string(), errno); outServerChannel.clear(); outClientChannel.clear(); return result; &#125; int bufferSize = SOCKET_BUFFER_SIZE; //设置server端和client端的接收缓冲区和发送缓冲区 setsockopt(sockets[0], SOL_SOCKET, SO_SNDBUF, &amp;bufferSize, sizeof(bufferSize)); setsockopt(sockets[0], SOL_SOCKET, SO_RCVBUF, &amp;bufferSize, sizeof(bufferSize)); setsockopt(sockets[1], SOL_SOCKET, SO_SNDBUF, &amp;bufferSize, sizeof(bufferSize)); setsockopt(sockets[1], SOL_SOCKET, SO_RCVBUF, &amp;bufferSize, sizeof(bufferSize)); String8 serverChannelName = name; serverChannelName.append(\" (server)\"); outServerChannel = new InputChannel(serverChannelName, sockets[0]); String8 clientChannelName = name; clientChannelName.append(\" (client)\"); outClientChannel = new InputChannel(clientChannelName, sockets[1]); return OK;&#125; 这里调用了socketpair系统调用创建了一对已经连接的UNIX租socket,这里可以把这一对socket当成pipe返回的文件描述符一样使用，pipe返回的管道是单向管道，即只能从一端写入，一端读出，但是socketpair是创建的管道是全双工的，可读可写。 创建好了server端和client端socketpair通道后，在WindowState.openInputChannel()方法中，一方面它把刚才创建的Client端的输入通道通过outInputChannel参数返回到应用程序中： 1inputChannels[1].transferTo(outInputChannel); WindowSession.addToDisplay()通过Binder通信与WMS通信。IWindowSession.java为编译Android 7.1.2源码得到。 在此看一下通信详细过程,可以看到outInputChannel通过_arg8.writeToParcel()写入，然后通过跨进程方式传输，App端就可以得到Client端的InputChannel 了。 [-&gt;IWindowSession.java$ Stub] 12345678910111213141516 case TRANSACTION_addToDisplay:&#123;......android.view.InputChannel _arg8;_arg8 = new android.view.InputChannel();int _result = this.addToDisplay(_arg0, _arg1, _arg2, _arg3, _arg4, _arg5, _arg6, _arg7, _arg8);reply.writeNoException();reply.writeInt(_result);......if ((_arg8!=null)) &#123;reply.writeInt(1);_arg8.writeToParcel(reply, android.os.Parcelable.PARCELABLE_WRITE_RETURN_VALUE);&#125;......return true;&#125; [-&gt;IWindowSession.java$ Proxy] 123456789101112131415//android/out/target/common/obj/JAVA_LIBRARIES/framework_intermediates/src/core/java/android/view/IWindowSession.java@Override public int addToDisplay(android.view.IWindow window, int seq, android.view.WindowManager.LayoutParams attrs, int viewVisibility, int layerStackId, android.graphics.Rect outContentInsets, android.graphics.Rect outStableInsets, android.graphics.Rect outOutsets, android.view.InputChannel outInputChannel) throws android.os.RemoteException&#123;android.os.Parcel _data = android.os.Parcel.obtain();android.os.Parcel _reply = android.os.Parcel.obtain();......mRemote.transact(Stub.TRANSACTION_addToDisplay, _data, _reply, 0);_result = _reply.readInt();....if ((0!=_reply.readInt())) &#123;outInputChannel.readFromParcel(_reply);&#125;return _result;&#125; 另外还需要把server端的InputChannel注册到InputManagerService中： 1mInputManager.registerInputChannel(win.mInputChannel, win.mInputWindowHandle); Step 13、InputManagerService.registerInputChannel()123456[-&gt;frameworks/base/services/core/java/com/android/server/input/InputManagerService.java]public void registerInputChannel(InputChannel inputChannel, InputWindowHandle inputWindowHandle) &#123; nativeRegisterInputChannel(mPtr, inputChannel, inputWindowHandle, false); &#125; 通过调用nativeRegisterInputChannel来将InputChannel注册到native层 Step 14、InputManagerService.nativeRegisterInputChannel()123456789101112131415[-&gt;frameworks/base/services/core/jni/com_android_server_input_InputManagerService.cpp]static void nativeRegisterInputChannel(JNIEnv* env, jclass /* clazz */, jlong ptr, jobject inputChannelObj, jobject inputWindowHandleObj, jboolean monitor) &#123; NativeInputManager* im = reinterpret_cast&lt;NativeInputManager*&gt;(ptr); sp&lt;InputChannel&gt; inputChannel = android_view_InputChannel_getInputChannel(env, inputChannelObj); sp&lt;InputWindowHandle&gt; inputWindowHandle = android_server_InputWindowHandle_getHandle(env, inputWindowHandleObj); status_t status = im-&gt;registerInputChannel( env, inputChannel, inputWindowHandle, monitor);&#125; 根据java层的InputWindowHandle获得native层的InputWindowHandle对象，根据java层的InputChannel获得native层的InputChannel对象，然后调用NativeInputManager的resgiterInputChannel，该函数又调用了InputDispatcher的registerInputChannel Step 15、InputDispatcher.registerInputChannel()1234567891011121314151617181920212223[-&gt;frameworks/native/services/inputflinger/InputDispatcher.cpp]status_t InputDispatcher::registerInputChannel(const sp&lt;InputChannel&gt;&amp; inputChannel, const sp&lt;InputWindowHandle&gt;&amp; inputWindowHandle, bool monitor) &#123; &#123; // acquire lock AutoMutex _l(mLock); sp&lt;Connection&gt; connection = new Connection(inputChannel, inputWindowHandle, monitor); int fd = inputChannel-&gt;getFd(); mConnectionsByFd.add(fd, connection); if (monitor) &#123; mMonitoringChannels.push(inputChannel); &#125; mLooper-&gt;addFd(fd, 0, ALOOPER_EVENT_INPUT, handleReceiveCallback, this); &#125; // release lock // Wake the looper because some connections have changed. mLooper-&gt;wake(); return OK;&#125; 创建Connection，可以看到用inputChannel初始化了inputPublisher(inputChannel)，这就是之前Input dispatcher小节 Step 8. InputPublisher.publishKeyEvent()方法中的那个mChannel。 1234567// --- InputDispatcher::Connection ---InputDispatcher::Connection::Connection(const sp&lt;InputChannel&gt;&amp; inputChannel, const sp&lt;InputWindowHandle&gt;&amp; inputWindowHandle, bool monitor) : status(STATUS_NORMAL), inputChannel(inputChannel), inputWindowHandle(inputWindowHandle), monitor(monitor), inputPublisher(inputChannel), inputPublisherBlocked(false) &#123; &#125; 将InputWindowHandle, InputChanel封装成Connection对象，然后fd作为key，Connection作为Value，保存在mConnectionsByFd中，如果传入的monitor是true，则需要将InputChannel放到mMonitoringChannels中,从上面的InputManagerService的registerInputChannel函数里传入的monitor是false，所以这里不加入到mMonitoringChannels。同时把fd加入到mLooper的监听中，并指定当该fd有内容可读时，Looper就会调用handleReceiveCallback函数。至此server端的InputChannel注册完成，InputDispatcher睡眠在监听的fds上，当有按键事件发生时，InputDispatcher就会往这些fd写入InputMessage对象，进而回调handleReceiveCallback函数。 至此，server端的InputChannel就注册完成了，再回到前面的WindowManagerService.addWindow上的第二步inputChannels[1].transferTo(outInputChannel);，这个是将创建的一对InputChannel的client端复制到传入的参数InputChannel上，当addWindow返回时，就回到ViewRootImpl.setView()方法中，执行应用程序这一侧的键盘消息接收通道。 1234if (mInputChannel != null) &#123; mInputEventReceiver = new WindowInputEventReceiver(mInputChannel, Looper.myLooper());&#125; WindowInputEventReceiver继承自InputEventReceiver类。 Step 16、InputEventReceiver()1234567891011[-&gt;frameworks/base/core/java/android/view/InputEventReceiver.java]public InputEventReceiver(InputChannel inputChannel, Looper looper) &#123; mInputChannel = inputChannel; mMessageQueue = looper.getQueue(); mReceiverPtr = nativeInit(new WeakReference&lt;InputEventReceiver&gt;(this), inputChannel, mMessageQueue); mCloseGuard.open(\"dispose\");&#125; 调用了nativeInit执行native层的初始化 Step 17.、InputEventReceiver.nativeInit()12345678910111213141516[-&gt;frameworks/base/core/jni/android_view_InputEventReceiver.cpp]static jlong nativeInit(JNIEnv* env, jclass clazz, jobject receiverWeak, jobject inputChannelObj, jobject messageQueueObj) &#123; sp&lt;InputChannel&gt; inputChannel = android_view_InputChannel_getInputChannel(env, inputChannelObj); sp&lt;MessageQueue&gt; messageQueue = android_os_MessageQueue_getMessageQueue(env, messageQueueObj); sp&lt;NativeInputEventReceiver&gt; receiver = new NativeInputEventReceiver(env, receiverWeak, inputChannel, messageQueue); status_t status = receiver-&gt;initialize(); receiver-&gt;incStrong(gInputEventReceiverClassInfo.clazz); // retain a reference for the object return reinterpret_cast&lt;jlong&gt;(receiver.get());&#125; 函数创建了一个NativeInputEventReceiver对象，并调用其initialize函数 Step 18.、NativeInputEventReceiver.initialize()123456[-&gt;frameworks/base/core/jni/android_view_InputEventReceiver.cpp]status_t NativeInputEventReceiver::initialize() &#123; setFdEvents(ALOOPER_EVENT_INPUT); return OK;&#125; 调用setFdEvents函数 Step 19、NativeInputEventReceiver.setFdEvents()12345678910111213frameworks/base/core/jni/android_view_InputEventReceiver.cppvoid NativeInputEventReceiver::setFdEvents(int events) &#123; if (mFdEvents != events) &#123; mFdEvents = events; int fd = mInputConsumer.getChannel()-&gt;getFd(); if (events) &#123; mMessageQueue-&gt;getLooper()-&gt;addFd(fd, 0, events, this, NULL); &#125; else &#123; mMessageQueue-&gt;getLooper()-&gt;removeFd(fd); &#125; &#125;&#125; 这里调用传入的MessageQueue获取Looper对象，如果events是0，则表示要移除监听fd，如果events不为0，表示要监听fd，这个fd是前面WindowManagerService创建的一对InputChannel的client端，这样当Server端写入事件时，client端的looper就能被唤醒，并调用handleEvent函数（Looper::addFd函数可以指定LooperCallback对象，当fd可读时，会调用LooperCallback的handleEvent，而NativeInputEventReceiver继承自LooperCallback，所以这里会调用NativeInputEventReceiver的handleEvent函数） 贴上事件处理序列图。 Step 20、NativeInputEventReceiver.handleEvent()12345678910[-&gt;frameworks/base/core/jni/android_view_InputEventReceiver.cpp]int NativeInputEventReceiver::handleEvent(int receiveFd, int events, void* data) &#123; if (events &amp; ALOOPER_EVENT_INPUT) &#123; JNIEnv* env = AndroidRuntime::getJNIEnv(); status_t status = consumeEvents(env, false /*consumeBatches*/, -1, NULL); mMessageQueue-&gt;raiseAndClearException(env, \"handleReceiveCallback\"); return status == OK || status == NO_MEMORY ? 1 : 0; &#125;&#125; 该函数调用consumeEvents函数来处理接收一个按键事件 Step 21、NativeInputEventReceiver.consumeEvents()12345678910111213141516171819202122232425262728293031323334[-&gt;frameworks/base/core/jni/android_view_InputEventReceiver.cpp]status_t NativeInputEventReceiver::consumeEvents(JNIEnv* env, bool consumeBatches, nsecs_t frameTime, bool* outConsumedBatch) &#123; ...... ScopedLocalRef&lt;jobject&gt; receiverObj(env, NULL); bool skipCallbacks = false; for (;;) &#123; uint32_t seq; InputEvent* inputEvent; // 处理接收一个按键事件 status_t status = mInputConsumer.consume(&amp;mInputEventFactory, consumeBatches, frameTime, &amp;seq, &amp;inputEvent); if (!skipCallbacks) &#123; jobject inputEventObj; switch (inputEvent-&gt;getType()) &#123; case AINPUT_EVENT_TYPE_KEY: inputEventObj = android_view_KeyEvent_fromNative(env, static_cast&lt;KeyEvent*&gt;(inputEvent)); break; &#125; if (inputEventObj) &#123; env-&gt;CallVoidMethod(receiverObj.get(), gInputEventReceiverClassInfo.dispatchInputEvent, seq, inputEventObj); env-&gt;DeleteLocalRef(inputEventObj); &#125; &#125; if (skipCallbacks) &#123; mInputConsumer.sendFinishedSignal(seq, false); &#125; &#125;&#125; 函数首先调用mInputConsumer.consume接收一个InputEvent对象,mInputConsumer在NativeInputEventReceiver构造函数中初始化 Step 22、InputConsumer.consume()12345678910111213141516171819202122232425262728293031323334353637383940414243[-&gt;frameworks/native/libs/input/InputTransport.cpp]status_t InputConsumer::consume(InputEventFactoryInterface* factory, bool consumeBatches, nsecs_t frameTime, uint32_t* outSeq, InputEvent** outEvent) &#123; *outSeq = 0; *outEvent = NULL; // Fetch the next input message. // Loop until an event can be returned or no additional events are received. while (!*outEvent) &#123; if (mMsgDeferred) &#123; // mMsg contains a valid input message from the previous call to consume // that has not yet been processed. mMsgDeferred = false; &#125; else &#123; // Receive a fresh message. status_t result = mChannel-&gt;receiveMessage(&amp;mMsg); if (result) &#123; // Consume the next batched event unless batches are being held for later. if (consumeBatches || result != WOULD_BLOCK) &#123; result = consumeBatch(factory, frameTime, outSeq, outEvent); if (*outEvent) &#123; break; &#125; &#125; return result; &#125; &#125; switch (mMsg.header.type) &#123; case InputMessage::TYPE_KEY: &#123; KeyEvent* keyEvent = factory-&gt;createKeyEvent(); if (!keyEvent) return NO_MEMORY; initializeKeyEvent(keyEvent, &amp;mMsg); *outSeq = mMsg.body.key.seq; *outEvent = keyEvent; break; &#125; &#125; return OK;&#125; 函数首先调用InputChannel的receiveMessage函数接收InputMessage对象，然后根据InputMessage对象调用initializeKeyEvent来构造KeyEvent对象。拿到可KeyEvent对象后，再对到consumeEvents中调用java层的InputEventReceiver.java的dispatchInputEvent函数 12env-&gt;CallVoidMethod(receiverObj.get(), gInputEventReceiverClassInfo.dispatchInputEvent, seq, inputEventObj); Step 23、InputEventReceiver.dispatchInputEvent()12345678[-&gt;frameworks/base/core/java/android/view/InputEventReceiver.java]// Called from native code.SuppressWarnings(\"unused\")private void dispatchInputEvent(int seq, InputEvent event) &#123; mSeqMap.put(event.getSequenceNumber(), seq); onInputEvent(event);&#125; 进而调用onInputEvent函数。至此按键就开始了java层的分发(下一节详细介绍)。 回到主线，故事来没讲完。当App这端处理完输入事件调用ViewRootImpl.finishInputEvent() 12345678910 [-&gt;/frameworks/base/core/java/android/view/ViewRootImpl.java] private void finishInputEvent(QueuedInputEvent q) &#123; ...... if (q.mReceiver != null) &#123; boolean handled = (q.mFlags &amp; QueuedInputEvent.FLAG_FINISHED_HANDLED) != 0; q.mReceiver.finishInputEvent(q.mEvent, handled); &#125;...... recycleQueuedInputEvent(q);&#125; Java层InputEventReceiver.nativeFinishInputEvent() 通过JNI 调用android_view_InputEventReceiver.finishInputEvent() 12345678[-&gt;frameworks/base/core/jni/android_view_InputEventReceiver.cpp]status_t NativeInputEventReceiver::finishInputEvent(uint32_t seq, bool handled) &#123; if (kDebugDispatchCycle) &#123; ALOGD(\"channel '%s' ~ Finished input event.\", getInputChannelName()); &#125; status_t status = mInputConsumer.sendFinishedSignal(seq, handled); &#125; 层层跳转最后会调用到InputConsumer.sendUnchainedFinishedSignal()发送一个InputMessage::TYPE_FINISHED消息。 123456789[-&gt;/frameworks/native/libs/input/InputTransport.cpp]status_t InputConsumer::sendUnchainedFinishedSignal(uint32_t seq, bool handled) &#123;InputMessage msg;msg.header.type = InputMessage::TYPE_FINISHED;msg.body.finished.seq = seq;msg.body.finished.handled = handled;return mChannel-&gt;sendMessage(&amp;msg);&#125; 在InputDispatcher.registerInputChannel()中添加了一个 handleReceiveCallback回调。 1mLooper-&gt;addFd(fd, 0, ALOOPER_EVENT_INPUT, handleReceiveCallback, this); 然后通过和IMS中InputDispacher的通信管道InputChannel发了处理完成通知。那InputDispatcher这边收到后如何处理呢？ 由前面分析 InputDispatcher会调用handleReceiveCallback()来处理TYPE_FINISHED信号。这里先是往Command队列里放一个处理事务执行doDispatchCycleFinishedLockedInterruptible()，后面在runCommandsLockedInterruptible()中会取出执行。在doDispatchCycleFinishedLockedInterruptible()函数中，会先调用afterKeyEventLockedInterruptible()。Android中可以定义一些Fallback键，即如果一个Key事件App没有处理，可以Fallback成另外默认的Key事件，这是在这里的dispatchUnhandledKey()函数中进行处理的。接着InputDispatcher会将该收到完成信号的事件项从等待队列中移除。同时由于上一个事件已被App处理完，就可以调用startDispatchCycleLocked()来进行下一轮事件的处理了。 12345678[-&gt;/frameworks/native/services/inputflinger/InputDispatcher.cpp]if (dispatchEntry == connection-&gt;findWaitQueueEntry(seq)) &#123; connection-&gt;waitQueue.dequeue(dispatchEntry); ... &#125; // Start the next dispatch cycle for this connection. startDispatchCycleLocked(now(), connection); startDispatchCycleLocked函数会检查相应连接的输出缓冲中(connection-&gt;outboundQueue)是否有事件要发送的，有的话会通过InputChannel发送出去。 总结： 8、Android Input子系统之java层按键传递Android开发中在自定义Activity以及View时经常会重写onKeyDown,onKeyUp,dispatchKeyEvent，同时View还有setOnKeyListener等，当一个按键事件发生时，这些方法将会被回调，但是到底哪个先回调，哪个后回调呢，一直不是特别清楚，只知道个大概，下面将详细讲述按键在java层的分发过程，其中重点关注按键事件在View层次中的分发 java层的按键分发从ViewRootImpl.java的WindowInputEventReceiver中的onInputEvent开始，从前面的应用程序注册消息监听过程分析和Input Dispatcher分析，InputDispatcher在处理按键事件时，会通过InputChannel::sendMessage函数将按键消息从server端写入，这里的InputChannel是当前获取焦点的窗口的InputChannel对的server端，这样应用程序端就可以收到该消息，然后调用NativeInputEventReceiver的handleEvent,最后调用到InputEventReceiver的onInputEvent函数（具体的可以看应用程序注册消息监听过程分析 的Step20-Step23） 序列图： Step 1、WindowInputEventReceiver.onInputEvent()12345678910111213[-&gt;frameworks/base/core/java/android/view/ViewRootImpl.java]final class WindowInputEventReceiver extends InputEventReceiver &#123; public WindowInputEventReceiver(InputChannel inputChannel, Looper looper) &#123; super(inputChannel, looper); &#125; @Override public void onInputEvent(InputEvent event) &#123; enqueueInputEvent(event, this, 0, true); &#125; ...&#125; 这里只列出部分代码，当一个按键按下时onInputEvent方法就会被回调，其中调用了ViewRootImpl::enqueueInputEvent(event, this, 0, true); Step 2、ViewRootImpl.enqueueInputEvent()12345678910111213[-&gt;frameworks/base/core/java/android/view/ViewRootImpl.java]void enqueueInputEvent(InputEvent event, InputEventReceiver receiver, int flags, boolean processImmediately) &#123; // 从队列中获取一个QueuedInputEvent，这里的flags传入的是0 QueuedInputEvent q = obtainQueuedInputEvent(event, receiver, flags); ... if (processImmediately) &#123; doProcessInputEvents();//这里传入的processImmediately是true，所以调用doProcessInputEvents &#125; else &#123; scheduleProcessInputEvents(); &#125;&#125; 从前面的参数可知，这里表示要立即处理，所以调用doProcessInputEvents函数. Step 3、ViewRootImpl.doProcessInputEvents()1234567891011121314151617frameworks/base/core/java/android/view/ViewRootImpl.javavoid doProcessInputEvents() &#123; // Deliver all pending input events in the queue. while (mPendingInputEventHead != null) &#123; QueuedInputEvent q = mPendingInputEventHead; mPendingInputEventHead = q.mNext; if (mPendingInputEventHead == null) &#123; mPendingInputEventTail = null; &#125; q.mNext = null; mPendingInputEventCount -= 1; ... // 分发按键事件 deliverInputEvent(q); &#125;&#125; 在deliverInputEvent函数中实际做按键的分发 Step 4、ViewRootImpl.deliverInputEvent()1234567891011121314151617181920212223[-&gt;frameworks/base/core/java/android/view/ViewRootImpl.java]private void deliverInputEvent(QueuedInputEvent q) &#123; Trace.asyncTraceBegin(Trace.TRACE_TAG_VIEW, \"deliverInputEvent\", q.mEvent.getSequenceNumber()); if (mInputEventConsistencyVerifier != null) &#123; mInputEventConsistencyVerifier.onInputEvent(q.mEvent, 0); &#125; InputStage stage; if (q.shouldSendToSynthesizer()) &#123; stage = mSyntheticInputStage; &#125; else &#123; //选择责任链的模式的入口，如果InputEvent需要跳过IME处理，则从mFirstPostImeInputStage（EarlyPostImeInputStage）开始,否则从mFirstInputStage(NativePreImeInputStage)开始分发 stage = q.shouldSkipIme() ? mFirstPostImeInputStage : mFirstInputStage; &#125; if (stage != null) &#123; stage.deliver(q); &#125; else &#123; finishInputEvent(q); &#125;&#125; 这里调用了InputStage的deliver方法分发，这里的InputStage代表了输入事件的处理阶段，是一种责任链模式 InputStage将输入事件的处理分成若干个阶段（Stage）, 如果当前有输入法窗口，则事件处理从 NativePreIme 开始，否则的话，从EarlyPostIme 开始。事件会依次经过每个Stage，如果该事件没有被标识为 “Finished”， 该Stage就会处理它，然后返回处理结果，Forward 或 Finish， Forward 运行下一Stage继续处理，而Finished事件将会简单的Forward到下一级，直到最后一级 Synthetic InputStage。流程图和每个阶段完成的事情如下图所示 责任链模式： 责任链模式（Chain of Responsibility）的目标是使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递请求，直到有一个对象处理它为止。 按键分发： 在ViewRootImpl的setView函数中会构造一个如图所示的InputStage的链，按键会从入口阶段，进入责任链，顺序处理，入口阶段根据QueuedInputEvent的状态来决定。q.shouldSendToSynthesizer() 这里一般是false，因此主要看stage = q.shouldSkipIme() ? mFirstPostImeInputStage : mFirstInputStage; 这里的shouldSkipIme其实是一个flag在构造QueuedInputEvent时传入的，从前面的onInputEvent调用的enqueueInputEvent(event, this, 0, true);可知，这里传入的flags是第三个参数0，那这里的shouldSkipIme就是false，那么按键会从mFirstPostImeInputStage 开始分发，就是图中的NativePreImeInputStage分发。 下面只从跟本文前面提到的Activity，View的按键分发流程相关的InputStage（ViewPostImeInputStage）开始分析 Step 5、ViewPostImeInputStage.processKeyEvent()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[-&gt;frameworks/base/core/java/android/view/ViewRootImpl.java]private int processKeyEvent(QueuedInputEvent q) &#123; final KeyEvent event = (KeyEvent)q.mEvent; ... // Deliver the key to the view hierarchy. // 调用成员变量mView的dispatchKeyEvent函数，这里mView是PhoneWindow.DecorView对象 if (mView.dispatchKeyEvent(event)) &#123; return FINISH_HANDLED; &#125; ... // 如果按键是四向键或者是TAB键，则移动焦点 // Handle automatic focus changes. if (event.getAction() == KeyEvent.ACTION_DOWN) &#123; int direction = 0; switch (event.getKeyCode()) &#123; case KeyEvent.KEYCODE_DPAD_LEFT: if (event.hasNoModifiers()) &#123; direction = View.FOCUS_LEFT; &#125; break; ...... case KeyEvent.KEYCODE_TAB: if (event.hasNoModifiers()) &#123; direction = View.FOCUS_FORWARD; &#125; else if (event.hasModifiers(KeyEvent.META_SHIFT_ON)) &#123; direction = View.FOCUS_BACKWARD; &#125; break; &#125; if (direction != 0) &#123; View focused = mView.findFocus(); if (focused != null) &#123; View v = focused.focusSearch(direction); if (v != null &amp;&amp; v != focused) &#123; ...... focused.getFocusedRect(mTempRect); if (mView instanceof ViewGroup) &#123; ((ViewGroup) mView).offsetDescendantRectToMyCoords( focused, mTempRect); ((ViewGroup) mView).offsetRectIntoDescendantCoords( v, mTempRect); &#125; if (v.requestFocus(direction, mTempRect)) &#123; playSoundEffect(SoundEffectConstants .getContantForFocusDirection(direction)); return FINISH_HANDLED; &#125; &#125; // Give the focused view a last chance to handle the dpad key. if (mView.dispatchUnhandledMove(focused, direction)) &#123; return FINISH_HANDLED; &#125; &#125; else &#123; // find the best view to give focus to in this non-touch-mode with no-focus View v = focusSearch(null, direction); if (v != null &amp;&amp; v.requestFocus(direction)) &#123; return FINISH_HANDLED; &#125; &#125; &#125; &#125; return FORWARD;&#125; 上述主要分两步： 第一步是调用PhoneWindow.DecorView的dispatchKeyEvent函数，DecorView是View层次结构的根节点，按键从根节点开始根据Focuse view的path自上而下的分发。 第二步是判断按键是否是四向键，或者是TAB键，如果是则需要移动焦点 Step 6、mView.dispatchKeyEvent()1234567891011121314public boolean dispatchKeyEvent(KeyEvent event) &#123; ... if (!isDestroyed()) &#123; final Callback cb = getCallback(); final boolean handled = cb != null &amp;&amp; mFeatureId &lt; 0 ? cb.dispatchKeyEvent(event) : super.dispatchKeyEvent(event); if (handled) &#123; return true; &#125; &#125; return isDown ? PhoneWindow.this.onKeyDown(mFeatureId, event.getKeyCode(), event) : PhoneWindow.this.onKeyUp(mFeatureId, event.getKeyCode(), event);&#125; 主要的分发在下面开始，如果cb不为空并且mFeatureId小于0，则调用cb.dispatchKeyEvent开始分发，否则会调用DecorView的父类（View）的dispatchKeyEvent函数。cb是Window.Callback类型，Activity实现了Window.Callback接口，在attach函数中，会调用Window.setCallback函数将自己注册进PhoneWindow中，所以cb不为空。在PhoneWindow初始化时会调用installDecor函数生成DecorView对象，该函数中传入的mFeatureId是-1，所以mFeatureId也小于0。因此此处会调用Activity的dispatchKeyEvent函数，开始在View中分发按键。 下面来分析按键在View的层次结构中是如何分发的 DecorView的按键分发 接下来来看这里先看看Activity(Callback)的dispatchKeyEvent实现： Step 7、Activity.dispatchKeyEvent()1234567891011121314151617[frameworks/base/core/java/android/app/Activity.java]public boolean dispatchKeyEvent(KeyEvent event) &#123; //调用自定义的onUserInteraction onUserInteraction(); Window win = getWindow(); //调用PhoneWindow的superDispatchKeyEvent,实际调用DecorView的superDispatchKeyEvent，从DecorView开始从顶层View往子视图传递 if (win.superDispatchKeyEvent(event)) &#123; return true; &#125; View decor = mDecor; if (decor == null) decor = win.getDecorView(); //到这里如果view层次结构没有返回true则交给KeyEvent本身的dispatch方法，Activity的onKeyDown/onKeyUp/onKeyMultiple就会被触发 return event.dispatch(this, decor != null ? decor.getKeyDispatcherState() : null, this);&#125; 接着看下PhoneWindow的superDispatchKeyEvent Step 8、PhoneWindow.superDispatchKeyEvent()12345678910111213141516171819202122&lt;!-- PhoneWindow.java --&gt;Overridepublic boolean superDispatchKeyEvent(KeyEvent event) &#123; return mDecor.superDispatchKeyEvent(event);&#125;&lt;!-- PhoneWindow.DecorView --&gt;public boolean superDispatchKeyEvent(KeyEvent event) &#123; // Give priority to closing action modes if applicable. if (event.getKeyCode() == KeyEvent.KEYCODE_BACK) &#123; final int action = event.getAction(); // Back cancels action modes first. if (mPrimaryActionMode != null) &#123; if (action == KeyEvent.ACTION_UP) &#123; mPrimaryActionMode.finish(); &#125; return true; &#125; &#125; //进入View的层次结构，调用ViewGroup.dispatchKeyEvent return super.dispatchKeyEvent(event);&#125; 再看ViewGroup的dispatchKeyEvent函数 Step 9、ViewGroup.dispatchKeyEvent()1234567891011121314151617181920[-&gt;frameworks/base/core/java/android/view/ViewGroup.java]public boolean dispatchKeyEvent(KeyEvent event) &#123; ... if ((mPrivateFlags &amp; (PFLAG_FOCUSED | PFLAG_HAS_BOUNDS)) == (PFLAG_FOCUSED | PFLAG_HAS_BOUNDS)) &#123; //如果此ViewGroup是focused并且具体的大小被设置了（有边界），则交给它处理，即调用View的实现 if (super.dispatchKeyEvent(event)) &#123; return true; &#125; &#125; else if (mFocused != null &amp;&amp; (mFocused.mPrivateFlags &amp; PFLAG_HAS_BOUNDS) == PFLAG_HAS_BOUNDS) &#123; //否则，如果此ViewGroup中有focused的child，且child有具体的大小，则交给mFocused处理 if (mFocused.dispatchKeyEvent(event)) &#123; return true; &#125; &#125; ... return false;&#125; 这里可以看出如果ViewGroup满足条件，则优先处理事件而不发给子视图去处理。 下面看下View的dispatchKeyEvent实现 Step 10、View.dispatchKeyEvent()1234567891011121314151617181920[-&gt;frameworks/base/core/java/android/view/View.java]public boolean dispatchKeyEvent(KeyEvent event) &#123; ... // Give any attached key listener a first crack at the event. //noinspection SimplifiableIfStatement ListenerInfo li = mListenerInfo; //调用onKeyListener，如果注册了OnKeyListener,并且View属于Enable状态，则触发 if (li != null &amp;&amp; li.mOnKeyListener != null &amp;&amp; (mViewFlags &amp; ENABLED_MASK) == ENABLED &amp;&amp; li.mOnKeyListener.onKey(this, event.getKeyCode(), event)) &#123; return true; &#125; //调用KeyEvent.dispatch方法，并将view作为参数传递进去，实际会回调View的onKeyUp/onKeyDown等方法 if (event.dispatch(this, mAttachInfo != null ? mAttachInfo.mKeyDispatchState : null, this)) &#123; return true; &#125; ... return false;&#125; Step 11、View.onKeyDown/View.onKeyUp123456789101112131415161718192021222324252627282930313233343536373839404142[-&gt;frameworks/base/core/java/android/view/View.java]public boolean onKeyDown(int keyCode, KeyEvent event) &#123; boolean result = false; //处理KEYCODE_DPAD_CENTER、KEYCODE_ENTER按键 if (KeyEvent.isConfirmKey(keyCode)) &#123; if ((mViewFlags &amp; ENABLED_MASK) == DISABLED) &#123; //disabled的view直接返回true，不再继续分发,即Activity的onKeyDown和onKeyUp无法收到KEYCODE_DPAD_CENTER、KEYCODE_ENTER事件 return true; &#125; // Long clickable items don't necessarily have to be clickable if (((mViewFlags &amp; CLICKABLE) == CLICKABLE || (mViewFlags &amp; LONG_CLICKABLE) == LONG_CLICKABLE) &amp;&amp; (event.getRepeatCount() == 0)) &#123;// clickable或者long_clickable且是第一次down事件 setPressed(true);// 标记pressed，你可能设置了View不同的background，这时候就会有所体现（比如高亮效果） checkForLongClick(0); return true; &#125; &#125; return result;&#125;public boolean onKeyUp(int keyCode, KeyEvent event) &#123; //处理KEYCODE_DPAD_CENTER、KEYCODE_ENTER按键 if (KeyEvent.isConfirmKey(keyCode)) &#123; if ((mViewFlags &amp; ENABLED_MASK) == DISABLED) &#123; //disabled的view直接返回true，不再继续分发,即Activity的onKeyDown和onKeyUp无法收到KEYCODE_DPAD_CENTER、KEYCODE_ENTER事件 return true; &#125; if ((mViewFlags &amp; CLICKABLE) == CLICKABLE &amp;&amp; isPressed()) &#123; setPressed(false); if (!mHasPerformedLongPress) &#123; // This is a tap, so remove the longpress check removeLongPressCallback(); return performClick(); &#125; &#125; &#125; return false;&#125; Step 12、Activity.onKeyDown/onKeyUp12345678910111213141516171819202122232425262728[-&gt;frameworks/base/core/java/android/app/Activity.java]public boolean onKeyDown(int keyCode, KeyEvent event) &#123; //如果是back键则启动追踪 if (keyCode == KeyEvent.KEYCODE_BACK) &#123; if (getApplicationInfo().targetSdkVersion &gt;= Build.VERSION_CODES.ECLAIR) &#123; event.startTracking(); &#125; else &#123; onBackPressed(); &#125; return true; &#125; ...&#125;public boolean onKeyUp(int keyCode, KeyEvent event) &#123; if (getApplicationInfo().targetSdkVersion &gt;= Build.VERSION_CODES.ECLAIR) &#123; if (keyCode == KeyEvent.KEYCODE_BACK &amp;&amp; event.isTracking() &amp;&amp; !event.isCanceled()) &#123; //如果是back键并且正在追踪该Event，则调用onBackPressed onBackPressed(); return true; &#125; &#125; return false;&#125; 而Android常见Touch事件是通过dispatchPointerEvent(MotionEvent event)分发的，主要跟底层传上来的 输入事件相关，不同类型事件分别处理。 具体Touch事件分发机制可参考博客： Android事件分发机制完全解析，带你从源码的角度彻底理解(上) Android事件分发机制完全解析，带你从源码的角度彻底理解(下) Android触摸屏事件派发机制详解与源码分析一(View篇) Android触摸屏事件派发机制详解与源码分析二(ViewGroup篇) Android触摸屏事件派发机制详解与源码分析三(Activity篇) Android Deeper(00) - Touch事件分发响应机制 九、总结：再贴一下Input system总体框架图： （一）、IMS初始化&amp;&amp; IMS与App建立通信： SystemServer初始化过程中，创建InputManagerService，IMS第一件事情就是初始化Native层，包括EventHub, InputReader 和 InputDispatcher IMS以及其他的System Service 初始化完成之后，应用程序就开始启动。如果一个应用程序有Activity（只有Activit能够接受用户输入），它要将自己的Window(ViewRootImpl)通过setView()注册到WindowManagerService 中 用户输入的捕捉和处理发生在不同的进程里（生产者：Input Reader 和 Input Dispatcher 在System Server 进程里，而消费者，应用程序运行在自己的进程里），因此用户输入事件（Event)的传递需要跨进程。在这里，Android使用了Socket + Binder来完成。OpenInputChannelPair 生成了两个Socket的FD， 代表一个双向通道的两端，向一端写入数据，另外一端便可以读出，反之依然，如果一端没有写入数据，另外一端去读，则陷入阻塞等待。OpenInputChannelPair() 发生在WindowManager Service.addWindow()中 通过RegisterInputChannel, WindowManagerService 将刚刚创建的一个Socket FD，封装在InputWindowHandle(代表一个WindowState) 里传给InputManagerService InputManagerService 通过JNI（NativeInputManager）最终调用到了InputDispatcher 的 RegisterInputChannel()方法，这里，一个Connection 对象被创建出来，代表与远端某个窗口(InputWindowHandle)的一条用户输入数据通道。一个Dispatcher可能有多个Connection（多个Window）同时存在。为了监听来自于Window的消息，InputDispatcher 通过AddFd 将这些个FD 加入到Looper中，这样，只要某个Window在Socket的另一端写入数据，Looper就会马上从睡眠中醒来，进行处理。 到这里，ViewRootImpl mWindowSession.addToDisplay返回，WMS 将SocketPair的另外一个FD 放在返回参数 OutputChannel 里，即返回给APP进程。 接着ViewRootImpl 创建了WindowInputEventReceiver 用于接受InputDispatchor 传过来的事件，App进程同样通过AddFd() 将读端的Socket FD 加入到Looper中，这样一旦InputDispatchor发送Event，Looper就会立即醒来处理。 （二）、Eventhub 和 Input Reader NativeInputManager的构造函数里第一件事情就是创建一个EventHub对象，EventHub构造函数里主要生成并初始化几个控制的FD mINotifyFd: 用来监控””/dev/input”目录下是否有文件生成，有的话说明有新的输入设备接入，EventHub将从epool_wait中唤醒，来打开新加入的设备 mWakeReaderFD， mWakeWriterFD： 一个Pipe的两端，当往mWakeWriteFD 写入数据的时候，等待在mWakeReaderFD的线程被唤醒，这里用来给上层应用提供唤醒等待线程，比如说，当上层应用改变输入属性需要EventHub进行相应更新时 mEpollFD，用于epoll_wait()的阻塞等待，这里通过epoll_ctrl(EPOLL_ADD_FD, fd) 可以等待多个fd的事件，包括上面提到的mINotifyFD, mWakeReaderFD, 以及输入设备的FD。 InputManagerService启动InputReader 线程，进入无限的循环，每次循环调用loopOnce(). 第一次循环，会主动扫描 “/dev/input/“ 目录，并打开下面的所有文件，通过ioctl()从底层驱动获取设备信息，并判断它的设备类型。这里处理的设备类型有：INPUT_DEVICE_CLASS_KEYBOARD， INPUT_DEVICE_CLASS_TOUCH， INPUT_DEVICE_CLASS_DPAD，INPUT_DEVICE_CLASS_JOYSTICK 等。 找到每个设备对应的键值映射文件，读取并生产一个KeyMap 对象。一般来说，设备对应的键值映射文件是 “/system/usr/keylayout/Generic.kl”. 将刚才扫描到的/dev/input 下所有文件的FD 加到epool等待队列中，调用epool_wait() 开始等待事件的发生。 某个时间发生，可能是用户按键输入，也可能是某个设备插入，亦或用户调整了设备属性，epoll_wait() 返回，将发生的Event 存放在mPendingEventItems 里。如果这是一个用户输入，系统调用Read() 从驱动读到这个按键的信息，存放在rawEvents里。 EventHub-&gt;getEvents() 返回,代表有新的input事件到来，进入InputReader的processEventLocked函数。 通过rawEvent 找到产生时间的Device，再找到这个Device对应的InputMapper对象，最终生成一个NotifyArgs对象，将其放到NotifyArgs的队列中。 调用NotifyArgs里面的Notify()方法，最终调用到InputDispatchor 对应的Notify接口（比如NotifyKey) 将接下来的处理交给InputDispatchor，EventHub 和 InputReader 工作结束，但马上又开始新的一轮等待，重复6～9的循环。 （三）、Input Dispatcher 接上节的最后一步，NotifyKey() 的实现在Input Dispatcher 内部，他首先做简单的校验，对于按键事件，只有Action 是 AKEY_EVENT_ACTION_DOWN 和 AKEY_EVENT_ACTION_UP，即按下和弹起这两个Event别接受。 Input Reader 传给Input Dispather的数据类型是 NotifyKeyArgs， 后者在这里将其转换为 KeyEvent, 然后交由 Policy 来进行第一步的解析和过滤，interceptKeyBeforeDispatching(), 对于手机产品，这个工作是在PhoneWindowManager 里完成，（不同类型的产品可以定义不同的WindowManager, 比如GoogleTV 里用到的是TVWindowManager)。KeyEvent 在这里将会被分为三类： System Key: 比如说 音量键，Power键，以及一些特殊的组合键，如用于截屏的音量+Power，等等。部分System Key 会在这里立即处理，比如说电话键，但有一些会放到后面去做处理，比如说音量键，但不管怎样，这些键不会传给应用程序，所以称为系统键。 Global Key：最终产品中可能会有一些特殊的按键，它不属于某个特定的应用，在所有应用中的行为都是一样，但也不包含在Andrioid的系统键中，比如说GoogleTV 里会有一个”TV” 按键，按它会直接呼起”TV”应用然后收看电视直播，这类按键在Android定义为Global Key. User Key：除此之外的按键就是User Key, 它最终会传递到当前的应用窗口。 此时，InputDispather 还在Looper中睡眠等待，mLooper-&gt;wake();将其唤醒，然后进入Input Dispatcher 线程。 InputDispatcher 大部分的工作在 dispatcherOnce 里完成。首先从mInBoundQueue 中读出队列头部的事件 mPendingEvent, 然后调用 pokeUserActivityLocked()。 poke的英文意思是”搓一下, 捅一下”， 这个函数的目的也就是”捅一下”PowerManagerService 提醒它”别睡眠啊，我还活着呢”，最终调用到PowerManagerService 的 updatePowerStateLocked()，防止手机进入休眠状态。需要注意的是，上述动作不会马上执行，而是存储在命令队列，mCommandQueue里，这里面的命令会在后面依次被执行。 接下来是dispatchOnceInnerLocked()-&gt;dispatchKeyLocked() 第一次进去这个函数的时候，先检查Event是否已经过处理（doInterceptKeyBeforeDispatchingLockedInterruptible), 如果没有，则生成一个命令，同样放入mCommandQueue里。 runCommandsLockedInterruptible() 依次执行mCommandQueue 里的命令，前面说过，pokeUserActivity 会调用PowerManagerService 的 updatePowerStateLocked(), 而 interceptKeyBeforeDispatching() 则最终调用到PhoneWindowManager的同名函数。我们在interceptBeforeQueuing 里面提到的一些系统按键在这个被执行，比如 HOME/MENU/SEARCH 等。 命令运行完之后，退出 dispatchOnce， 然后调用pollOnce 进入下一轮等待。但这里不会被阻塞，因为timeout值被设成了0. 第二次进入dispatchKeyLocked(), 这是Event的状态已经设为”已处理”，这时候才真正进入了发射阶段。 接下来调用 findFocusedWindowTargetLocked() 获取当前的焦点窗口，这里面会做一件非常重要的事情，就是检测目标应用是否有ANR发生，如果下诉条件满足，则说明可能发生了ANR： 目标应用不会空，而目标窗口为空。说明应用程序在启动过程中出现了问题。 目标 Activity 的状态是Pause，即不再是Focused的应用。 目标窗口还在处理上一个事件。这个我们下面会说到。 如果目标窗口处于正常状态，调用dispatchEventLocked() 进入真正的发送程序。 然后调用prepareDispatchCycleLocked() ,这里事件换了一件马甲，从EventEntry 变成 DispatchEntry, 并送人mOutBoundQueue。然后调用startDispatchCycleLocked() 开始发送。 最终的发送发生在InputChannel的sendMessage()。这里就用到了我们前面提到的SocketPair, 一旦sendMessage() 执行，目标窗口所在进程的Looper线程就会被唤醒，然后读取键值并进行处理。 乖乖，还没走完啊？是的，工作还差最后一步，Input Dispatcher给这个窗口发送下一个命令之前，必须等待该窗口的回复，如果超过5s没有收到，就会通过Input Manager Service 向Activity Manager 汇报，后者会弹出我们熟知的 “Application No Response” 窗口。所以，事件会放入mWaitQueue进行暂存。如果窗口一切正常，完成按键处理后它会调用InputConsumer的sendFinishedSignal() 往SocketPair 里写入完成信号，Input Dispatcher 从 Loop中醒来，并从Socket中读取该信号，然后从mWaitQueue 里清除该事件标志其处理完毕。 （四）、Key Processing略、请参考： 图解Android - Android GUI 系统 (5) - Android的Event Input System - 漫天尘沙 - 博客园 参考文档(特别感谢)：韦东山第4期Android驱动深度开发视频源码-GitHub韦东山第4期Android驱动深度开发视频-输入系统-100ask.orgAndroid输入子系统-ChenWeiaiYanYan《深入理解Android 卷III》第五章 深入理解Android输入系统 - CSDN博客图解Android - Android GUI 系统 (5) - Android的Event Input System - 漫天尘沙 - 博客园Android 5.0(Lollipop)事件输入系统(Input System) - 世事难料，保持低调 - CSDN博客【Android】Android输入子系统 - Leo.cheng - 博客园Android(Linux) 输入子系统解析 | Andy.Lee’s BlogINPUT事件的读取和分发：INPUTREADER、INPUTDISPATCHERAndroid 触摸事件分发机制深入理解Android之Touch事件的分发","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android 7.1.2 (Android N) Activity - Window 加载显示流程（AMS && WMS）分析","slug":"Android-7-1-2-Android-N-Activity-Window加载显示流程","date":"2017-10-31T16:00:00.000Z","updated":"2018-04-19T14:29:48.415Z","comments":true,"path":"2017/11/01/Android-7-1-2-Android-N-Activity-Window加载显示流程/","link":"","permalink":"http://zhoujinjian.cc/2017/11/01/Android-7-1-2-Android-N-Activity-Window加载显示流程/","excerpt":"Activity-Window加载显示流程概述： Android系统中图形系统是相当复杂的，包括WindowManager，SurfaceFlinger,Open GL,GPU等模块。 其中SurfaceFlinger作为负责绘制应用UI的核心，从名字可以看出其功能是将所有Surface合成工作。 不论使用什么渲染API, 所有的东西最终都是渲染到”surface”. surface代表BufferQueue的生产者端, 并且 由SurfaceFlinger所消费, 这便是基本的生产者-消费者模式. Android平台所创建的Window都由surface所支持, 所有可见的surface渲染到显示设备都是通过SurfaceFlinger来完成的. 本文详细分析Android Window加载显示流程，并通过SurfaceFlinger渲染合成输出到屏幕的过程。","text":"Activity-Window加载显示流程概述： Android系统中图形系统是相当复杂的，包括WindowManager，SurfaceFlinger,Open GL,GPU等模块。 其中SurfaceFlinger作为负责绘制应用UI的核心，从名字可以看出其功能是将所有Surface合成工作。 不论使用什么渲染API, 所有的东西最终都是渲染到”surface”. surface代表BufferQueue的生产者端, 并且 由SurfaceFlinger所消费, 这便是基本的生产者-消费者模式. Android平台所创建的Window都由surface所支持, 所有可见的surface渲染到显示设备都是通过SurfaceFlinger来完成的. 本文详细分析Android Window加载显示流程，并通过SurfaceFlinger渲染合成输出到屏幕的过程。 一、Activity启动流程概述基于Android 7.1.2的源码剖析， 分析Activity-Window加载显示流程，相关源码： frameworks/base/core/java/android/app/● Activity.java● ActivityThread.java● Instrumentation.java frameworks/base/core/jni/● android_view_DisplayEventReceiver.cpp● android_view_SurfaceControl.cpp● android_view_Surface.cpp● android_view_SurfaceSession.cpp frameworks/native/include/gui/● SurfaceComposerClient.cpp● SurfaceComposerClient.h frameworks/native/services/surfaceflinger/● SurfaceFlinger.cpp● Client.cpp frameworks/base/core/java/android/view/● WindowManagerImpl.java● ViewManager.java● WindowManagerGlobal.java● ViewRootImpl.java● Choreographer.java● IWindowSession.aidl● DisplayEventReceiver.java● SurfaceControl.java● Surface.java● SurfaceSession.java frameworks/base/core/java/com/android/internal/policy/● PhoneWindow.java frameworks/base/services/core/java/com/android/server/wm/● WindowManagerService.java● Session.java● WindowState.java● WindowStateAnimator.java● WindowSurfaceController.java 博客原图链接在前面文章（Android 7.1.2(Android N) Activity启动流程分析）中详细分析了Activity启动流程，这里回顾一下总体流程。 Activity启动流程图： 启动流程： ● 点击桌面App图标，Launcher进程采用Binder IPC向system_server进程发起startActivity请求；● system_server进程接收到请求后，向zygote进程发送创建进程的请求；● Zygote进程fork出新的子进程，即App进程；● App进程，通过Binder IPC向sytem_server进程发起attachApplication请求；● system_server进程在收到请求后，进行一系列准备工作后，再通过binder IPC向App进程发送scheduleLaunchActivity请求；● App进程的binder线程（ApplicationThread）在收到请求后，通过handler向主线程发送LAUNCH_ACTIVITY消息；● 主线程在收到Message后，通过发射机制创建目标Activity，并回调Activity.onCreate()等方法。 App正式启动后，开始进入Activity生命周期，执行完onCreate/onStart/onResume方法，UI渲染结束后便可以看到App的主界面， 接下来分析UI渲染流程。 二、Window加载显示流程2.1、ActivityThread.handleLaunchActivity()接着从ActivityThread的handleLaunchActivity方法： [-&gt;ActivityThread.java] 12345678910111213141516 private void handleLaunchActivity(ActivityClientRecord r, Intent customIntent, String reason)&#123; ...... // Initialize before creating the activity WindowManagerGlobal.initialize(); //创建Activity Activity a = performLaunchActivity(r, customIntent); if (a != null) &#123; ...... //启动Activity handleResumeActivity(r.token, false, r.isForward, !r.activity.mFinished &amp;&amp; !r.startsNotResumed, r.lastProcessedSeq, reason); ...... &#125;&#125; 应用程序进程通过performLaunchActivity函数将即将要启动的Activity加载到当前进程空间来，同时为启动Activity做准备。 [ActivityThread.java #performLaunchActivity()] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768private Activity performLaunchActivity(ActivityClientRecord r, Intent customIntent) &#123; //通过Activity所在的应用程序信息及该Activity对应的CompatibilityInfo信息从PMS服务中查询当前Activity的包信息 ActivityInfo aInfo = r.activityInfo; if (r.packageInfo == null) &#123; r.packageInfo = getPackageInfo(aInfo.applicationInfo, r.compatInfo, Context.CONTEXT_INCLUDE_CODE); &#125; //获取当前Activity的组件信息 ComponentName component = r.intent.getComponent(); if (component == null) &#123; component = r.intent.resolveActivity( mInitialApplication.getPackageManager()); r.intent.setComponent(component); &#125; //packageName为启动Activity的包名，targetActivity为Activity的类名 if (r.activityInfo.targetActivity != null) &#123; component = new ComponentName(r.activityInfo.packageName, r.activityInfo.targetActivity); &#125; //通过类反射方式加载即将启动的Activity Activity activity = null; try &#123; java.lang.ClassLoader cl = r.packageInfo.getClassLoader(); activity = mInstrumentation.newActivity( cl, component.getClassName(), r.intent); StrictMode.incrementExpectedActivityCount(activity.getClass()); r.intent.setExtrasClassLoader(cl); r.intent.prepareToEnterProcess(); if (r.state != null) &#123; r.state.setClassLoader(cl); &#125; &#125; ...... try &#123; //通过单例模式为应用程序进程创建Application对象 Application app = r.packageInfo.makeApplication(false, mInstrumentation); ...... if (activity != null) &#123; //为当前Activity创建上下文对象ContextImpl Context appContext = createBaseContextForActivity(r, activity); ...... //将当前启动的Activity和上下文ContextImpl、Application绑定 activity.attach(appContext, this, getInstrumentation(), r.token, r.ident, app, r.intent, r.activityInfo, title, r.parent, r.embeddedID, r.lastNonConfigurationInstances, config, r.referrer, r.voiceInteractor, window); ...... //将Activity保存到ActivityClientRecord中，ActivityClientRecord为Activity在应用程序进程中的描述符 r.activity = activity; activity.mCalled = false; if (r.isPersistable()) &#123; mInstrumentation.callActivityOnCreate(activity, r.state, r.persistentState); &#125; else &#123; mInstrumentation.callActivityOnCreate(activity, r.state); &#125; ...... //生命周期onStart、onresume if (!r.activity.mFinished) &#123; activity.performStart(); r.stopped = false; &#125; //ActivityThread的成员变量mActivities保存了当前应用程序进程中的所有Activity的描述符 mActivities.put(r.token, r); ...... return activity;&#125; 在该函数中，首先通过PMS服务查找到即将启动的Activity的包名信息，然后通过类反射方式创建一个该Activity实例，同时为应用程序启动的每一个Activity创建一个LoadedApk实例对象，应用程序进程中创建的所有LoadedApk对象保存在ActivityThread的成员变量mPackages中。接着通过LoadedApk对象的makeApplication函数，使用单例模式创建Application对象，因此在android应用程序进程中有且只有一个Application实例。然后为当前启动的Activity创建一个ContextImpl上下文对象，并初始化该上下文，到此我们可以知道，启动一个Activity需要以下对象： 1) Activity对象，需要启动的Activity； 2) LoadedApk对象，每个启动的Activity都拥有属于自身的LoadedApk对象； 3) ContextImpl对象，每个启动的Activity都拥有属于自身的ContextImpl对象； 4) Application对象，应用程序进程中有且只有一个实例，和Activity是一对多的关系； 2.2、Activity对象Attach过程Activity所需要的对象都创建好了，就需要将Activity和Application对象、ContextImpl对象绑定在一起。 参数： context：Activity的上下文对象，就是前面创建的ContextImpl对象； aThread：Activity运行所在的主线程描述符ActivityThread； instr：用于监控Activity运行状态的Instrumentation对象； token：用于和AMS服务通信的IApplicationToken.Proxy代理对象； application：Activity运行所在进程的Application对象； parent：启动当前Activity的Activity； [-&gt;Activity.java] 1234567891011121314151617181920212223242526272829 final void attach(Context context, ActivityThread aThread, Instrumentation instr, IBinder token, int ident, Application application, Intent intent, ActivityInfo info, CharSequence title, Activity parent, String id, NonConfigurationInstances lastNonConfigurationInstances, Configuration config, String referrer, IVoiceInteractor voiceInteractor, Window window) &#123; //将上下文对象ContextImpl保存到Activity的成员变量中 attachBaseContext(context); ...... mWindow = new PhoneWindow(this, window); ...... //记录应用程序的UI线程 mUiThread = Thread.currentThread(); //记录应用程序的ActivityThread对象 mMainThread = aThread; ...... //为Activity所在的窗口创建窗口管理器 mWindow.setWindowManager( (WindowManager)context.getSystemService(Context.WINDOW_SERVICE), mToken, mComponent.flattenToString(), (info.flags &amp; ActivityInfo.FLAG_HARDWARE_ACCELERATED) != 0); if (mParent != null) &#123; mWindow.setContainer(mParent.getWindow()); &#125; mWindowManager = mWindow.getWindowManager(); mCurrentConfig = config;&#125; 在该attach函数中主要做了以下几件事： 1) 根据参数初始化Activity的成员变量； 2) 为Activity创建窗口Window对象； 3) 为Window创建窗口管理器； 2.3、Activity视图对象的创建过程-Activity.setContentView() ActivityThread.performLaunchActivity()–&gt; Instrumentation.callActivityOnCreate()——&gt;Activity.performCreate()——&gt;Activity.onCreate()–&gt;Activity.performStart()——&gt;Instrumentation.callActivityOnStart()——&gt;Activity.onStart() 1234 public void setContentView(@LayoutRes int layoutResID) &#123; getWindow().setContentView(layoutResID); initWindowDecorActionBar();&#125; getWindow()函数得到前面创建的窗口对象PhoneWindow，通过PhoneWindow来设置Activity的视图。 [-&gt;PhoneWindow.java] 1234567891011121314151617181920 public void setContentView(View view, ViewGroup.LayoutParams params) &#123; ...... //如果窗口顶级视图对象为空，则创建窗口视图对象 if (mContentParent == null) &#123; installDecor(); &#125; ...... if (hasFeature(FEATURE_CONTENT_TRANSITIONS)) &#123; ...... &#125; else &#123;//加载布局文件，并将布局文件中的所有视图对象添加到mContentParent容器中,PhoneWindow的成员变量mContentParent的类型为ViewGroup，是窗口内容存放的地方 mLayoutInflater.inflate(layoutResID, mContentParent); &#125; mContentParent.requestApplyInsets(); final Callback cb = getCallback(); if (cb != null &amp;&amp; !isDestroyed()) &#123; cb.onContentChanged(); &#125; mContentParentExplicitlySet = true;&#125; Activity.onCreate()会调用setContentView(),整个过程主要是Activity的布局文件或View添加至窗口里， 详细过程不再赘述，详细加载过程请参考： Android应用setContentView与LayoutInflater加载解析机制源码分析 重点概括为： 1234● 创建一个DecorView的对象mDecor，该mDecor对象将作为整个应用窗口的根视图。● 依据Feature等style theme创建不同的窗口修饰布局文件，并且通过findViewById获取Activity布局文件该存放的地方（窗口修饰布局文件中id为content的FrameLayout）。● 将Activity的布局文件添加至id为content的FrameLayout内。● 当setContentView设置显示OK以后会回调Activity的onContentChanged方法。Activity的各种View的findViewById()方法等都可以放到该方法中，系统会帮忙回调。 2.4、ActivityThread.handleResumeActivity()回到我们刚刚的handleLaunchActivity()方法，在调用完performLaunchActivity()方法之后，其有掉用了handleResumeActivity()法。 performLaunchActivity()方法完成了两件事： 1) Activity窗口对象的创建，通过attach函数来完成； 2) Activity视图对象的创建，通过setContentView函数来完成； 这些准备工作完成后，就可以显示该Activity了，应用程序进程通过调用handleResumeActivity函数来启动Activity的显示过程。 [-&gt;ActivityThread.java] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 final void handleResumeActivity(IBinder token, boolean clearHide, boolean isForward, boolean reallyResume, int seq, String reason) &#123; ActivityClientRecord r = mActivities.get(token); ...... // r = performResumeActivity(token, clearHide, reason); ...... if (r.window == null &amp;&amp; !a.mFinished &amp;&amp; willBeVisible) &#123; //获得为当前Activity创建的窗口PhoneWindow对象 r.window = r.activity.getWindow(); //获取为窗口创建的视图DecorView对象 View decor = r.window.getDecorView(); decor.setVisibility(View.INVISIBLE); //在attach函数中就为当前Activity创建了WindowManager对象 ViewManager wm = a.getWindowManager(); //得到该视图对象的布局参数 WindowManager.LayoutParams l = r.window.getAttributes(); //将视图对象保存到Activity的成员变量mDecor中 a.mDecor = decor; l.type = WindowManager.LayoutParams.TYPE_BASE_APPLICATION; l.softInputMode |= forwardBit; ...... if (a.mVisibleFromClient &amp;&amp; !a.mWindowAdded) &#123; a.mWindowAdded = true; //将创建的视图对象DecorView添加到Activity的窗口管理器中 wm.addView(decor, l); &#125; ...... if (r.activity.mVisibleFromClient) &#123; r.activity.makeVisible(); &#125; &#125; if (!r.onlyLocalRequest) &#123; //onStop()...... Looper.myQueue().addIdleHandler(new Idler()); &#125; r.onlyLocalRequest = false; if (reallyResume) &#123; try &#123; ActivityManagerNative.getDefault().activityResumed(token); &#125; &#125; ...... &#125;&#125; 我们知道，在前面的performLaunchActivity函数中完成Activity的创建后，会将当前当前创建的Activity在应用程序进程端的描述符ActivityClientRecord以键值对的形式保存到ActivityThread的成员变量mActivities中：mActivities.put(r.token, r)，r.token就是Activity的身份证，即是IApplicationToken.Proxy代理对象，也用于与AMS通信。上面的函数首先通过performResumeActivity从mActivities变量中取出Activity的应用程序端描述符ActivityClientRecord，然后取出前面为Activity创建的视图对象DecorView和窗口管理器WindowManager，最后将视图对象添加到窗口管理器中。 ViewManager.addView()真正实现的的地方在WindowManagerImpl.java中。 123456public interface ViewManager&#123;public void addView(View view, ViewGroup.LayoutParams params);public void updateViewLayout(View view, ViewGroup.LayoutParams params);public void removeView(View view);&#125; [-&gt;WindowManagerImpl.java] 12345@Overridepublic void addView(@NonNull View view, @NonNull ViewGroup.LayoutParams params) &#123; ...... mGlobal.addView(view, params, mContext.getDisplay(), mParentWindow);&#125; [-&gt;WindowManagerGlobal.java] 123456789101112131415161718 public void addView(View view, ViewGroup.LayoutParams params, Display display, Window parentWindow) &#123; ...... ViewRootImpl root; View panelParentView = null; synchronized (mLock) &#123; ...... root = new ViewRootImpl(view.getContext(), display); view.setLayoutParams(wparams); mViews.add(view); mRoots.add(root); mParams.add(wparams); &#125; try &#123; root.setView(view, wparams, panelParentView); &#125; ......&#125; 2.5、ViewRootImpl()构造过程：[ViewRootImpl.java # ViewRootImpl()] 1234567891011121314151617181920 final W mWindow; final Surface mSurface = new Surface(); final ViewRootHandler mHandler = new ViewRootHandler(); ...... public ViewRootImpl(Context context, Display display) &#123; mContext = context; mWindowSession = WindowManagerGlobal.getWindowSession();//IWindowSession的代理对象，该对象用于和WMS通信。 mDisplay = display; ...... mWindow = new W(this);//创建了一个W本地Binder对象，用于WMS通知应用程序进程 ...... mAttachInfo = new View.AttachInfo(mWindowSession, mWindow, display, this, mHandler, this); ...... mViewConfiguration = ViewConfiguration.get(context); mDensity = context.getResources().getDisplayMetrics().densityDpi; mNoncompatDensity = context.getResources().getDisplayMetrics().noncompatDensityDpi; mFallbackEventHandler = new PhoneFallbackEventHandler(context); mChoreographer = Choreographer.getInstance();//Choreographer对象 ......&#125; 在ViewRootImpl的构造函数中初始化了一些成员变量，ViewRootImpl创建了以下几个主要对象： (1) 通过WindowManagerGlobal.getWindowSession()得到IWindowSession的代理对象，该对象用于和WMS通信。 (2) 创建了一个W本地Binder对象，用于WMS通知应用程序进程。 (3) 采用单例模式创建了一个Choreographer对象，用于统一调度窗口绘图。 (4) 创建ViewRootHandler对象，用于处理当前视图消息。 (5) 构造一个AttachInfo对象； (6) 创建Surface对象，用于绘制当前视图，当然该Surface对象的真正创建是由WMS来完成的，只不过是WMS传递给应用程序进程的。 2.6、IWindowSession代理获取过程[-&gt;WindowManagerGlobal.java] 12345678910111213141516171819202122232425 private static IWindowSession sWindowSession; public static IWindowSession getWindowSession() &#123; synchronized (WindowManagerGlobal.class) &#123; if (sWindowSession == null) &#123; try &#123; //获取输入法管理器 InputMethodManager imm = InputMethodManager.getInstance(); //获取窗口管理器 IWindowManager windowManager = getWindowManagerService(); //得到IWindowSession代理对象 sWindowSession = windowManager.openSession( new IWindowSessionCallback.Stub() &#123; @Override public void onAnimatorScaleChanged(float scale) &#123; ValueAnimator.setDurationScale(scale); &#125; &#125;, imm.getClient(), imm.getInputContext()); &#125; catch (RemoteException e) &#123; throw e.rethrowFromSystemServer(); &#125; &#125; return sWindowSession; &#125;&#125; 以上函数通过WMS的openSession函数创建应用程序与WMS之间的连接通道，即获取IWindowSession代理对象，并将该代理对象保存到ViewRootImpl的静态成员变量sWindowSession中,因此在应用程序进程中有且只有一个IWindowSession代理对象。 [-&gt;WindowManagerService.java] 12345678@Overridepublic IWindowSession openSession(IWindowSessionCallback callback, IInputMethodClient client, IInputContext inputContext) &#123; if (client == null) throw new IllegalArgumentException(\"null client\"); if (inputContext == null) throw new IllegalArgumentException(\"null inputContext\"); Session session = new Session(this, callback, client, inputContext); return session;&#125; 在WMS服务端构造了一个Session实例对象。ViewRootImpl 是一很重要的类，类似 ActivityThread 负责跟AmS通信一样，ViewRootImpl 的一个重要职责就是跟 WmS 通信，它通静态变量 sWindowSession（IWindowSession实例）与 WmS 进行通信。每个应用进程，仅有一个 sWindowSession 对象，它对应了 WmS 中的 Session 子类，WmS 为每一个应用进程分配一个 Session 对象。WindowState 类有一个 IWindow mClient 参数，是在构造方法中赋值的，是由 Session 调用 addWindow 传递过来了，对应了 ViewRootImpl 中的 W 类的实例。 2.7、AttachInfo构造过程12345678910 AttachInfo(IWindowSession session, IWindow window, Display display, ViewRootImpl viewRootImpl, Handler handler, Callbacks effectPlayer) &#123; mSession = session;//IWindowSession代理对象，用于与WMS通信 mWindow = window;//W对象 mWindowToken = window.asBinder();//W本地Binder对象 mDisplay = display; mViewRootImpl = viewRootImpl;//ViewRootImpl实例 mHandler = handler;//ViewRootHandler对象 mRootCallbacks = effectPlayer;&#125; 2.8、创建Choreographer对象[-&gt;Choreographer.java] 123456789101112131415 // Thread local storage for the choreographer.private static final ThreadLocal&lt;Choreographer&gt; sThreadInstance = new ThreadLocal&lt;Choreographer&gt;() &#123; @Override protected Choreographer initialValue() &#123; Looper looper = Looper.myLooper(); if (looper == null) &#123; throw new IllegalStateException(\"The current thread must have a looper!\"); &#125; return new Choreographer(looper); &#125;&#125;;public static Choreographer getInstance() &#123; return sThreadInstance.get();&#125; 为调用线程创建一个Choreographer实例，调用线程必须具备消息循环功能，因为ViewRootImpl对象的构造是在应用程序进程的UI主线程中执行的，因此创建的Choreographer对象将使用UI线程消息队列。 [-&gt;Choreographer.java] 12345678910111213141516 private Choreographer(Looper looper) &#123; mLooper = looper; //创建消息处理Handler mHandler = new FrameHandler(looper); //如果系统使用了Vsync机制，则注册一个FrameDisplayEventReceiver接收器 mDisplayEventReceiver = USE_VSYNC ? new FrameDisplayEventReceiver(looper) : null; mLastFrameTimeNanos = Long.MIN_VALUE; //屏幕刷新周期 mFrameIntervalNanos = (long)(1000000000 / getRefreshRate()); //创建回调数组 mCallbackQueues = new CallbackQueue[CALLBACK_LAST + 1]; //初始化数组 for (int i = 0; i &lt;= CALLBACK_LAST; i++) &#123; mCallbackQueues[i] = new CallbackQueue(); &#125;&#125; FrameDisplayEventReceiver详细过程以后再Android 7.1.2(Android N) Choreographer机制实现过程分析。 2.9、视图View添加过程窗口管理器WindowManagerImpl为当前添加的窗口创建好各种对象后，调用ViewRootImpl的setView函数向WMS服务添加一个窗口对象。 [-&gt;ViewRootImpl.java] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 public void setView(View view, WindowManager.LayoutParams attrs, View panelParentView) &#123; synchronized (this) &#123; if (mView == null) &#123; ////将DecorView保存到ViewRootImpl的成员变量mView中 mView = view; ...... mSoftInputMode = attrs.softInputMode; mWindowAttributesChanged = true; mWindowAttributesChangesFlag = WindowManager.LayoutParams.EVERYTHING_CHANGED; //同时将DecorView保存到mAttachInfo中 mAttachInfo.mRootView = view; mAttachInfo.mScalingRequired = mTranslator != null; mAttachInfo.mApplicationScale = mTranslator == null ? 1.0f : mTranslator.applicationScale; if (panelParentView != null) &#123; mAttachInfo.mPanelParentWindowToken = panelParentView.getApplicationWindowToken(); &#125; mAdded = true; int res; /* = WindowManagerImpl.ADD_OKAY; */ //1）在添加窗口前进行UI布局 requestLayout(); if ((mWindowAttributes.inputFeatures &amp; WindowManager.LayoutParams.INPUT_FEATURE_NO_INPUT_CHANNEL) == 0) &#123; mInputChannel = new InputChannel(); &#125; mForceDecorViewVisibility = (mWindowAttributes.privateFlags &amp; PRIVATE_FLAG_FORCE_DECOR_VIEW_VISIBILITY) != 0; try &#123; mOrigWindowType = mWindowAttributes.type; mAttachInfo.mRecomputeGlobalAttributes = true; collectViewAttributes(); //2)将窗口添加到WMS服务中，mWindow为W本地Binder对象，通过Binder传输到WMS服务端后，变为IWindow代理对象 res = mWindowSession.addToDisplay(mWindow, mSeq, mWindowAttributes, getHostVisibility(), mDisplay.getDisplayId(), mAttachInfo.mContentInsets, mAttachInfo.mStableInsets, mAttachInfo.mOutsets, mInputChannel); &#125; ...... //3)建立窗口消息通道 if (mInputChannel != null) &#123; if (mInputQueueCallback != null) &#123; mInputQueue = new InputQueue(); mInputQueueCallback.onInputQueueCreated(mInputQueue); &#125; mInputEventReceiver = new WindowInputEventReceiver(mInputChannel, Looper.myLooper()); &#125; ...... &#125; &#125;&#125; 通过前面的分析可以知道，用户自定义的UI作为一个子View被添加到DecorView中，然后将顶级视图DecorView添加到应用程序进程的窗口管理器中，窗口管理器首先为当前添加的View创建一个ViewRootImpl对象、一个布局参数对象ViewGroup.LayoutParams，然后将这三个对象分别保存到当前应用程序进程的窗口管理器WindowManagerImpl中，最后通过ViewRootImpl对象将当前视图对象注册到WMS服务中。 ViewRootImpl的setView函数向WMS服务添加一个窗口对象过程： (1) requestLayout()在应用程序进程中进行窗口UI布局； (2) WindowSession.addToDisplay()向WMS服务注册一个窗口对象； (3) 注册应用程序进程端的消息接收通道； (1)、requestLayout()在应用程序进程中进行窗口UI布局；2.10、窗口UI布局过程requestLayout函数调用里面使用了Hanlder的一个小手段，那就是利用postSyncBarrier添加了一个Barrier（挡板），这个挡板的作用是阻塞普通的同步消息的执行，在挡板被撤销之前，只会执行异步消息，而requestLayout先添加了一个挡板Barrier，之后自己插入了一个异步任务mTraversalRunnable，其主要作用就是保证mTraversalRunnable在所有同步Message之前被执行，保证View绘制的最高优先级。具体实现如下： 123456789101112131415161718192021@Overridepublic void requestLayout() &#123; if (!mHandlingLayoutInLayoutRequest) &#123; checkThread(); mLayoutRequested = true; scheduleTraversals(); &#125;&#125; void scheduleTraversals() &#123; if (!mTraversalScheduled) &#123; mTraversalScheduled = true; mTraversalBarrier = mHandler.getLooper().getQueue().postSyncBarrier(); mChoreographer.postCallback( Choreographer.CALLBACK_TRAVERSAL, mTraversalRunnable, null); if (!mUnbufferedInputDispatch) &#123; scheduleConsumeBatchedInput(); &#125; notifyRendererOfFramePending(); pokeDrawLockIfNeeded(); &#125;&#125; 2.10.1、添加回调过程[-&gt;Choreographer.java] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public void postCallback(int callbackType, Runnable action, Object token) &#123; postCallbackDelayed(callbackType, action, token, 0);&#125;public void postCallbackDelayed(int callbackType, Runnable action, Object token, long delayMillis) &#123; ...... postCallbackDelayedInternal(callbackType, action, token, delayMillis);&#125;private void postCallbackDelayedInternal(int callbackType, Object action, Object token, long delayMillis) &#123; ...... synchronized (mLock) &#123; final long now = SystemClock.uptimeMillis(); final long dueTime = now + delayMillis; //将要执行的回调封装成CallbackRecord对象，保存到mCallbackQueues数组中 mCallbackQueues[callbackType].addCallbackLocked(dueTime, action, token); if (dueTime &lt;= now) &#123; scheduleFrameLocked(now); &#125; else &#123; Message msg = mHandler.obtainMessage(MSG_DO_SCHEDULE_CALLBACK, action); msg.arg1 = callbackType; msg.setAsynchronous(true); mHandler.sendMessageAtTime(msg, dueTime); &#125; &#125;&#125; private void scheduleFrameLocked(long now) &#123; if (!mFrameScheduled) &#123; mFrameScheduled = true; if (USE_VSYNC) &#123; if (isRunningOnLooperThreadLocked()) &#123; scheduleVsyncLocked(); &#125; else &#123; Message msg = mHandler.obtainMessage(MSG_DO_SCHEDULE_VSYNC); msg.setAsynchronous(true); mHandler.sendMessageAtFrontOfQueue(msg); &#125; &#125; else &#123; final long nextFrameTime = Math.max( mLastFrameTimeNanos / TimeUtils.NANOS_PER_MS + sFrameDelay, now); Message msg = mHandler.obtainMessage(MSG_DO_FRAME); msg.setAsynchronous(true); mHandler.sendMessageAtTime(msg, nextFrameTime); &#125; &#125;&#125; 消息处理： 12345678910111213141516171819202122232425 private final class FrameHandler extends Handler &#123; public FrameHandler(Looper looper) &#123; super(looper); &#125; @Override public void handleMessage(Message msg) &#123; switch (msg.what) &#123; case MSG_DO_SCHEDULE_VSYNC: doScheduleVsync(); break; &#125; &#125;&#125; void doScheduleVsync() &#123; synchronized (mLock) &#123; if (mFrameScheduled) &#123; scheduleVsyncLocked(); &#125; &#125;&#125;private void scheduleVsyncLocked() &#123; //申请Vsync信号 mDisplayEventReceiver.scheduleVsync(); &#125; 在该函数中考虑了两种情况，一种是系统没有使用Vsync机制，在这种情况下，首先根据屏幕刷新频率计算下一次刷新时间，通过异步消息方式延时执行doFrame()函数实现屏幕刷新。如果系统使用了Vsync机制，并且当前线程具备消息循环，则直接请求Vsync信号，否则就通过主线程来请求Vsync信号。 2.10.2、Vsync请求过程我们知道在Choreographer构造函数中，构造了一个FrameDisplayEventReceiver对象，用于请求并接收Vsync信号，Vsync信号请求过程如下： 1234private void scheduleVsyncLocked() &#123; //申请Vsync信号 mDisplayEventReceiver.scheduleVsync(); &#125; [-&gt;DisplayEventReceiver.java] 12345678public void scheduleVsync() &#123; if (mReceiverPtr == 0) &#123; Log.w(TAG, \"Attempted to schedule a vertical sync pulse but the display event \" + \"receiver has already been disposed.\"); &#125; else &#123; nativeScheduleVsync(mReceiverPtr); &#125;&#125; [-&gt;android_view_DisplayEventReceiver.cpp ] 12345678910static void nativeScheduleVsync(JNIEnv* env, jclass clazz, jlong receiverPtr) &#123;sp&lt;NativeDisplayEventReceiver&gt; receiver = reinterpret_cast&lt;NativeDisplayEventReceiver*&gt;(receiverPtr);status_t status = receiver-&gt;scheduleVsync();if (status) &#123; String8 message; message.appendFormat(\"Failed to schedule next vertical sync pulse. status=%d\", status); jniThrowRuntimeException(env, message.string());&#125;&#125; VSync请求过程又转交给了DisplayEventReceiver： [-&gt;DisplayEventReceiver.cpp] 1234567status_t DisplayEventReceiver::requestNextVsync() &#123;if (mEventConnection != NULL) &#123; mEventConnection-&gt;requestNextVsync(); return NO_ERROR;&#125;return NO_INIT;&#125; 这里又通过IDisplayEventConnection接口来请求Vsync信号，IDisplayEventConnection实现了Binder通信框架，可以跨进程调用，因为Vsync信号请求进程和Vsync产生进程有可能不在同一个进程空间，因此这里就借助IDisplayEventConnection接口来实现。下面通过图来梳理Vsync请求的调用流程： 需要说明的是/Vsync/之间的代码此时其实还未执行，call requestNextVsync()来告诉系统我要在下一个VSYNC需要被trigger. 继续ViewRootImpl的setView函数中的WindowSession.addToDisplay()。 (2) 、mWindowSession.addToDisplay()向WMS服务注册一个窗口对象；[Session.java] 1234567@Overridepublic int addToDisplay(IWindow window, int seq, WindowManager.LayoutParams attrs, int viewVisibility, int displayId, Rect outContentInsets, Rect outStableInsets, Rect outOutsets, InputChannel outInputChannel) &#123; return mService.addWindow(this, window, seq, attrs, viewVisibility, displayId, outContentInsets, outStableInsets, outOutsets, outInputChannel);&#125; [WindowManagerService.java] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990 public int addWindow(Session session, IWindow client, int seq, WindowManager.LayoutParams attrs, int viewVisibility, int displayId, Rect outContentInsets, Rect outStableInsets, Rect outOutsets, InputChannel outInputChannel) &#123; ...... boolean reportNewConfig = false; WindowState attachedWindow = null; long origId; final int callingUid = Binder.getCallingUid(); final int type = attrs.type; synchronized(mWindowMap) &#123; ...... final DisplayContent displayContent = getDisplayContentLocked(displayId); ...... boolean addToken = false; WindowToken token = mTokenMap.get(attrs.token); AppWindowToken atoken = null; boolean addToastWindowRequiresToken = false; ...... WindowState win = new WindowState(this, session, client, token, attachedWindow, appOp[0], seq, attrs, viewVisibility, displayContent); return WindowManagerGlobal.ADD_APP_EXITING; &#125; ...... mPolicy.adjustWindowParamsLw(win.mAttrs); win.setShowToOwnerOnlyLocked(mPolicy.checkShowToOwnerOnly(attrs)); res = mPolicy.prepareAddWindowLw(win, attrs); ...... final boolean openInputChannels = (outInputChannel != null &amp;&amp; (attrs.inputFeatures &amp; INPUT_FEATURE_NO_INPUT_CHANNEL) == 0); if (openInputChannels) &#123; win.openInputChannel(outInputChannel); &#125; ...... if (addToken) &#123; mTokenMap.put(attrs.token, token); &#125; win.attach(); mWindowMap.put(client.asBinder(), win); ...... &#125; boolean imMayMove = true; if (type == TYPE_INPUT_METHOD) &#123; win.mGivenInsetsPending = true; mInputMethodWindow = win; addInputMethodWindowToListLocked(win); imMayMove = false; &#125; else if (type == TYPE_INPUT_METHOD_DIALOG) &#123; mInputMethodDialogs.add(win); addWindowToListInOrderLocked(win, true); moveInputMethodDialogsLocked(findDesiredInputMethodWindowIndexLocked(true)); imMayMove = false; &#125; else &#123; addWindowToListInOrderLocked(win, true); if (type == TYPE_WALLPAPER) &#123; mWallpaperControllerLocked.clearLastWallpaperTimeoutTime(); displayContent.pendingLayoutChanges |= FINISH_LAYOUT_REDO_WALLPAPER; &#125; else if ((attrs.flags&amp;FLAG_SHOW_WALLPAPER) != 0) &#123; displayContent.pendingLayoutChanges |= FINISH_LAYOUT_REDO_WALLPAPER; &#125; else if (mWallpaperControllerLocked.isBelowWallpaperTarget(win)) &#123; displayContent.pendingLayoutChanges |= FINISH_LAYOUT_REDO_WALLPAPER; &#125; &#125; ...... mInputMonitor.setUpdateInputWindowsNeededLw(); boolean focusChanged = false; if (win.canReceiveKeys()) &#123; focusChanged = updateFocusedWindowLocked(UPDATE_FOCUS_WILL_ASSIGN_LAYERS, false /*updateInputWindows*/); if (focusChanged) &#123; imMayMove = false; &#125; &#125; ...... mLayersController.assignLayersLocked(displayContent.getWindowList()); // Don't do layout here, the window must call // relayout to be displayed, so we'll do it there. if (focusChanged) &#123; mInputMonitor.setInputFocusLw(mCurrentFocus, false /*updateInputWindows*/); &#125; mInputMonitor.updateInputWindowsLw(false /*force*/); ...... &#125; if (reportNewConfig) &#123; sendNewConfiguration(); &#125; return res;&#125; 我们知道当应用程序进程添加一个DecorView到窗口管理器时，会为当前添加的窗口创建ViewRootImpl对象，同时构造了一个W本地Binder对象，无论是窗口视图对象DecorView还是ViewRootImpl对象，都只是存在于应用程序进程中，在添加窗口过程中仅仅将该窗口的W对象传递给WMS服务，经过Binder传输后，到达WMS服务端进程后变为IWindow.Proxy代理对象，因此该函数的参数client的类型为IWindow.Proxy。参数attrs的类型为WindowManager.LayoutParams，在应用程序进程启动Activity时，handleResumeActivity()函数通过WindowManager.LayoutParams l = r.window.getAttributes();来得到应用程序窗口布局参数，由于WindowManager.LayoutParams实现了Parcelable接口，因此WindowManager.LayoutParams对象可以跨进程传输，WMS服务的addWindow函数中的attrs参数就是应用程序进程发送过来的窗口布局参数。在WindowManagerImpl的addView函数中为窗口布局参数设置了相应的token，如果是应用程序窗口，则布局参数的token设为W本地Binder对象。如果不是应用程序窗口，同时当前窗口没有父窗口，则设置token为当前窗口的IApplicationToken.Proxy代理对象，否则设置为父窗口的IApplicationToken.Proxy代理对象，由于应用程序和WMS分属于两个不同的进程空间，因此经过Binder传输后，布局参数的令牌attrs.token就转变为IWindow.Proxy或者Token。以上函数首先根据布局参数的token等信息构造一个WindowToken对象，然后在构造一个WindowState对象，并将添加的窗口信息记录到mTokenMap和mWindowMap哈希表中。 在WMS服务端创建了所需对象后，接着调用了WindowState的attach()来进一步完成窗口添加。 [WindowState.java] 123456void attach() &#123; if (WindowManagerService.localLOGV) Slog.v( TAG, \"Attaching \" + this + \" token=\" + mToken + \", list=\" + mToken.windows); mSession.windowAddedLocked();&#125; [Session.java] 1234567891011121314 void windowAddedLocked() &#123; if (mSurfaceSession == null) &#123; if (WindowManagerService.localLOGV) Slog.v( TAG_WM, \"First window added to \" + this + \", creating SurfaceSession\"); mSurfaceSession = new SurfaceSession(); if (SHOW_TRANSACTIONS) Slog.i( TAG_WM, \" NEW SURFACE SESSION \" + mSurfaceSession); mService.mSessions.add(this); if (mLastReportedAnimatorScale != mService.getCurrentAnimatorScale()) &#123; mService.dispatchNewAnimatorScaleLocked(this); &#125; &#125; mNumWindow++;&#125; SurfaceSession建立过程SurfaceSession对象承担了应用程序与SurfaceFlinger之间的通信过程，每一个需要与SurfaceFlinger进程交互的应用程序端都需要创建一个SurfaceSession对象。 客户端请求 [SurfaceSession.java] 123public SurfaceSession() &#123; mNativeClient = nativeCreate();&#125; Java层的SurfaceSession对象构造过程会通过JNI在native层创建一个SurfaceComposerClient对象。 [android_view_SurfaceSession.cpp] 12345static jlong nativeCreate(JNIEnv* env, jclass clazz) &#123;SurfaceComposerClient* client = new SurfaceComposerClient();client-&gt;incStrong((void*)nativeCreate);return reinterpret_cast&lt;jlong&gt;(client);&#125; Java层的SurfaceSession对象与C++层的SurfaceComposerClient对象之间是一对一关系。 [SurfaceComposerClient.cpp] 12345678910111213SurfaceComposerClient::SurfaceComposerClient(): mStatus(NO_INIT), mComposer(Composer::getInstance())&#123;&#125;void SurfaceComposerClient::onFirstRef() &#123;//得到SurfaceFlinger的代理对象BpSurfaceComposer sp&lt;ISurfaceComposer&gt; sm(ComposerService::getComposerService());if (sm != 0) &#123; sp&lt;ISurfaceComposerClient&gt; conn = sm-&gt;createConnection(); if (conn != 0) &#123; mClient = conn; mStatus = NO_ERROR; &#125;&#125;&#125; SurfaceComposerClient继承于RefBase类，当第一次被强引用时，onFirstRef函数被回调，在该函数中SurfaceComposerClient会请求SurfaceFlinger为当前应用程序创建一个Client对象，专门接收该应用程序的请求，在SurfaceFlinger端创建好Client本地Binder对象后，将该Binder代理对象返回给应用程序端，并保存在SurfaceComposerClient的成员变量mClient中。 服务端处理 在SurfaceFlinger服务端为应用程序创建交互的Client对象 [SurfaceFlinger.cpp] 12345678910sp&lt;ISurfaceComposerClient&gt; SurfaceFlinger::createConnection()&#123;sp&lt;ISurfaceComposerClient&gt; bclient;sp&lt;Client&gt; client(new Client(this));status_t err = client-&gt;initCheck();if (err == NO_ERROR) &#123; bclient = client;&#125;return bclient;&#125; /**************Vsync**************/ Vsync信号处理以上是请求过程，FrameDisplayEventReceiver对象用于请求并接收Vsync信号，当Vsync信号到来时，系统会自动调用其onVsync()函数，后面会回调到FrameDisplayEventReceiver.run方法（Why “同步分割栏”？再分析），再回调函数中执行doFrame()实现屏幕刷新。 当VSYNC信号到达时，Choreographer doFrame()函数被调用 [-&gt;Choreographer.java] 1234567891011121314151617181920212223242526272829303132333435363738void doFrame(long frameTimeNanos, int frame) &#123; final long startNanos; ...... long intendedFrameTimeNanos = frameTimeNanos; //保存起始时间 startNanos = System.nanoTime(); //由于Vsync事件处理采用的是异步方式，因此这里计算消息发送与函数调用开始之间所花费的时间 final long jitterNanos = startNanos - frameTimeNanos; //计算函数调用期间所错过的帧数 if (jitterNanos &gt;= mFrameIntervalNanos) &#123; final long skippedFrames = jitterNanos / mFrameIntervalNanos; if (skippedFrames &gt;= SKIPPED_FRAME_WARNING_LIMIT) &#123; Log.i(TAG, \"Skipped \" + skippedFrames + \" frames! \" + \"The application may be doing too much work on its main thread.\"); &#125; final long lastFrameOffset = jitterNanos % mFrameIntervalNanos; frameTimeNanos = startNanos - lastFrameOffset; &#125; //如果frameTimeNanos小于一个屏幕刷新周期，则重新请求VSync信号 if (frameTimeNanos &lt; mLastFrameTimeNanos) &#123; scheduleVsyncLocked(); return; &#125; mFrameInfo.setVsync(intendedFrameTimeNanos, frameTimeNanos); mFrameScheduled = false; mLastFrameTimeNanos = frameTimeNanos; &#125; try &#123; //分别回调CALLBACK_INPUT、CALLBACK_ANIMATION、CALLBACK_TRAVERSAL、CALLBACK_COMMIT事件 doCallbacks(Choreographer.CALLBACK_INPUT, frameTimeNanos); doCallbacks(Choreographer.CALLBACK_ANIMATION, frameTimeNanos); doCallbacks(Choreographer.CALLBACK_TRAVERSAL, frameTimeNanos); doCallbacks(Choreographer.CALLBACK_COMMIT, frameTimeNanos); &#125;&#125; Choreographer类中分别定义了CallbackRecord、CallbackQueue内部类，CallbackQueue是一个按时间先后顺序保存CallbackRecord的单向循环链表。 在Choreographer中定义了三个CallbackQueue队列，用数组mCallbackQueues表示，用于分别保存CALLBACK_INPUT、CALLBACK_ANIMATION、CALLBACK_TRAVERSAL这三种类型的Callback，当调用Choreographer类的postCallback()函数时，就是往指定类型的CallbackQueue队列中通过addCallbackLocked()函数添加一个CallbackRecord项：首先构造一个CallbackRecord对象，然后按时间先后顺序插入到CallbackQueue链表中。从代码注释中，我们可以知道CALLBACK_INPUT是指输入回调，该回调优先级最高，首先得到执行，而CALLBACK_TRAVERSAL是指处理布局和绘图的回调，只有在所有异步消息都执行完后才得到执行，CALLBACK_ANIMATION是指动画回调，比CALLBACK_TRAVERSAL优先执行，从doFrame()函数中的doCallbacks调用就能印证这点。 当Vsync事件到来时，顺序执行CALLBACK_INPUT、CALLBACK_ANIMATION、CALLBACK_TRAVERSAL 和CALLBACK_COMMIT 对应CallbackQueue队列中注册的回调。 [-&gt;Choreographer.java] 12345678910111213141516171819202122232425262728293031323334353637 void doCallbacks(int callbackType, long frameTimeNanos) &#123; CallbackRecord callbacks; synchronized (mLock) &#123; final long now = System.nanoTime(); //从指定类型的CallbackQueue队列中查找执行时间到的CallbackRecord callbacks = mCallbackQueues[callbackType].extractDueCallbacksLocked( now / TimeUtils.NANOS_PER_MS); ...... if (callbackType == Choreographer.CALLBACK_COMMIT) &#123; final long jitterNanos = now - frameTimeNanos; if (jitterNanos &gt;= 2 * mFrameIntervalNanos) &#123; final long lastFrameOffset = jitterNanos % mFrameIntervalNanos + mFrameIntervalNanos; mDebugPrintNextFrameTimeDelta = true; &#125; frameTimeNanos = now - lastFrameOffset; mLastFrameTimeNanos = frameTimeNanos; &#125; &#125; &#125; try &#123; for (CallbackRecord c = callbacks; c != null; c = c.next) &#123; //由于CallbackQueues是按时间先后顺序排序的，因此遍历执行所有时间到的CallbackRecord c.run(frameTimeNanos); &#125; &#125; finally &#123; synchronized (mLock) &#123; mCallbacksRunning = false; do &#123; final CallbackRecord next = callbacks.next; recycleCallbackLocked(callbacks); callbacks = next; &#125; while (callbacks != null); &#125; &#125;&#125; 我们知道Choreographer对外提供了两个接口函数用于注册指定的Callback，postCallback()用于注册Runnable对象，而postFrameCallback()函数用于注册FrameCallback对象，无论注册的是Runnable对象还是FrameCallback对象，在CallbackRecord对象中统一装箱为Object类型。在执行其回调函数时，就需要区别这两种对象类型，如果注册的是Runnable对象，则调用其run()函数，如果注册的是FrameCallback对象，则调用它的doFrame()函数。 2.11、视图View添加过程 关于Choreographer的postCallback()用法在前面进行了详细的介绍，当Vsync事件到来时，mTraversalRunnable对象的run()函数将被调用。 mTraversalRunnable对象的类型为TraversalRunnable，该类实现了Runnable接口，在其run()函数中调用了doTraversal()函数来完成窗口布局。 [-&gt;ViewRootImpl.java] 123456789101112131415161718192021222324 final class TraversalRunnable implements Runnable &#123; @Override public void run() &#123; doTraversal(); &#125;&#125;final TraversalRunnable mTraversalRunnable = new TraversalRunnable();void doTraversal() &#123; if (mTraversalScheduled) &#123; mTraversalScheduled = false; mHandler.getLooper().getQueue().removeSyncBarrier(mTraversalBarrier); if (mProfile) &#123; Debug.startMethodTracing(\"ViewAncestor\"); &#125; performTraversals(); if (mProfile) &#123; Debug.stopMethodTracing(); mProfile = false; &#125; &#125;&#125; performTraversals函数相当复杂，其主要实现以下几个重要步骤： 1.执行窗口测量； 2.执行窗口注册； 3.执行窗口布局； 4.执行窗口绘图； [-&gt;ViewRootImpl.java] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143 private void performTraversals() &#123; ...... final View host = mView; mIsInTraversal = true; mWillDrawSoon = true; boolean windowSizeMayChange = false; boolean newSurface = false; boolean surfaceChanged = false; WindowManager.LayoutParams lp = mWindowAttributes; int desiredWindowWidth; int desiredWindowHeight; final int viewVisibility = getHostVisibility(); final boolean viewVisibilityChanged = !mFirst &amp;&amp; (mViewVisibility != viewVisibility || mNewSurfaceNeeded); final boolean viewUserVisibilityChanged = !mFirst &amp;&amp; ((mViewVisibility == View.VISIBLE) != (viewVisibility == View.VISIBLE)); WindowManager.LayoutParams params = null; if (mWindowAttributesChanged) &#123; mWindowAttributesChanged = false; surfaceChanged = true; params = lp; &#125; ...... /****************执行窗口测量******************/ boolean layoutRequested = mLayoutRequested &amp;&amp; (!mStopped || mReportNextDraw); if (layoutRequested) &#123; windowSizeMayChange |= measureHierarchy(host, lp, res, desiredWindowWidth, desiredWindowHeight); &#125; ...... /****************向WMS服务添加窗口******************/ if (mFirst || windowShouldResize || insetsChanged || viewVisibilityChanged || params != null || mForceNextWindowRelayout) &#123; ...... try &#123; ...... relayoutResult = relayoutWindow(params, viewVisibility, insetsPending); ...... &#125; catch (RemoteException e) &#123; &#125; ...... if (!mStopped || mReportNextDraw) &#123; ...... // Ask host how big it wants to be performMeasure(childWidthMeasureSpec, childHeightMeasureSpec); ...... &#125; &#125; &#125; else &#123; ...... &#125; /****************执行窗口布局******************/ final boolean didLayout = layoutRequested &amp;&amp; (!mStopped || mReportNextDraw); boolean triggerGlobalLayoutListener = didLayout || mAttachInfo.mRecomputeGlobalAttributes; if (didLayout) &#123; performLayout(lp, mWidth, mHeight); ...... &#125; /****************查找窗口焦点******************/ if (mFirst) &#123; ...... if (mView != null) &#123; if (!mView.hasFocus()) &#123; mView.requestFocus(View.FOCUS_FORWARD); &#125; else &#123;&#125; &#125; &#125; final boolean changedVisibility = (viewVisibilityChanged || mFirst) &amp;&amp; isViewVisible; final boolean hasWindowFocus = mAttachInfo.mHasWindowFocus &amp;&amp; isViewVisible; final boolean regainedFocus = hasWindowFocus &amp;&amp; mLostWindowFocus; if (regainedFocus) &#123; mLostWindowFocus = false; &#125; else if (!hasWindowFocus &amp;&amp; mHadWindowFocus) &#123; mLostWindowFocus = true; &#125; if (changedVisibility || regainedFocus) &#123; // Toasts are presented as notifications - don't present them as windows as well boolean isToast = (mWindowAttributes == null) ? false : (mWindowAttributes.type == WindowManager.LayoutParams.TYPE_TOAST); if (!isToast) &#123; host.sendAccessibilityEvent(AccessibilityEvent.TYPE_WINDOW_STATE_CHANGED); &#125; &#125; mFirst = false; mWillDrawSoon = false; mNewSurfaceNeeded = false; mActivityRelaunched = false; mViewVisibility = viewVisibility; mHadWindowFocus = hasWindowFocus; if (hasWindowFocus &amp;&amp; !isInLocalFocusMode()) &#123; final boolean imTarget = WindowManager.LayoutParams .mayUseInputMethod(mWindowAttributes.flags); if (imTarget != mLastWasImTarget) &#123; mLastWasImTarget = imTarget; InputMethodManager imm = InputMethodManager.peekInstance(); if (imm != null &amp;&amp; imTarget) &#123; imm.onPreWindowFocus(mView, hasWindowFocus); imm.onPostWindowFocus(mView, mView.findFocus(), mWindowAttributes.softInputMode, !mHasHadWindowFocus, mWindowAttributes.flags); &#125; &#125; &#125; // Remember if we must report the next draw. if ((relayoutResult &amp; WindowManagerGlobal.RELAYOUT_RES_FIRST_TIME) != 0) &#123; mReportNextDraw = true; &#125; /****************执行窗口绘制******************/ boolean cancelDraw = mAttachInfo.mTreeObserver.dispatchOnPreDraw() || !isViewVisible; if (!cancelDraw &amp;&amp; !newSurface) &#123; if (mPendingTransitions != null &amp;&amp; mPendingTransitions.size() &gt; 0) &#123; for (int i = 0; i &lt; mPendingTransitions.size(); ++i) &#123; mPendingTransitions.get(i).startChangingAnimations(); &#125; mPendingTransitions.clear(); &#125; performDraw(); &#125; else &#123; if (isViewVisible) &#123; // Try again scheduleTraversals(); &#125; else if (mPendingTransitions != null &amp;&amp; mPendingTransitions.size() &gt; 0) &#123; for (int i = 0; i &lt; mPendingTransitions.size(); ++i) &#123; mPendingTransitions.get(i).endChangingAnimations(); &#125; mPendingTransitions.clear(); &#125; &#125; mIsInTraversal = false;&#125; 1、执行窗口测量performMeasure()[-&gt;ViewRootImpl.java] 12345678 private void performMeasure(int childWidthMeasureSpec, int childHeightMeasureSpec) &#123; Trace.traceBegin(Trace.TRACE_TAG_VIEW, \"measure\"); try &#123; mView.measure(childWidthMeasureSpec, childHeightMeasureSpec); &#125; finally &#123; Trace.traceEnd(Trace.TRACE_TAG_VIEW); &#125;&#125; 2、执行窗口注册relayoutWindow；[-&gt;ViewRootImpl.java] 12345678910111213141516 private int relayoutWindow(WindowManager.LayoutParams params, int viewVisibility, boolean insetsPending) throws RemoteException &#123; ...... int relayoutResult = mWindowSession.relayout( mWindow, mSeq, params, (int) (mView.getMeasuredWidth() * appScale + 0.5f), (int) (mView.getMeasuredHeight() * appScale + 0.5f), viewVisibility, insetsPending ? WindowManagerGlobal.RELAYOUT_INSETS_PENDING : 0, mWinFrame, mPendingOverscanInsets, mPendingContentInsets, mPendingVisibleInsets, mPendingStableInsets, mPendingOutsets, mPendingBackDropFrame, mPendingConfiguration, mSurface); ...... return relayoutResult;&#125; 这里通过前面获取的IWindowSession代理对象请求WMS服务执行窗口布局，mSurface是ViewRootImpl的成员变量 [-&gt;ViewRootImpl.java] 1final Surface mSurface = new Surface(); [-&gt;Surface.java] 123456/** * Create an empty surface, which will later be filled in by readFromParcel(). * @hide */public Surface() &#123;&#125; 该Surface构造函数仅仅创建了一个空Surface对象，并没有对该Surface进程native层的初始化，到此我们知道应用程序进程为每个窗口对象都创建了一个Surface对象。并且将该Surface通过跨进程方式传输给WMS服务进程，我们知道，在Android系统中，如果一个对象需要在不同进程间传输，必须实现Parcelable接口，Surface类正好实现了Parcelable接口。ViewRootImpl通过IWindowSession接口请求WMS的完整过程如下： [-&gt;IWindowSession.java$ Proxy] 123456789101112131415161718192021/** This file is auto-generated. DO NOT MODIFY* * Original file: frameworks/base/core/java/android/view/IWindowSession.aidl*/@Override public int relayout(android.view.IWindow window, int seq, android.view.WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewVisibility, int flags, android.graphics.Rect outFrame, android.graphics.Rect outOverscanInsets, android.graphics.Rect outContentInsets, android.graphics.Rect outVisibleInsets, android.graphics.Rect outStableInsets, android.graphics.Rect outOutsets, android.graphics.Rect outBackdropFrame, android.content.res.Configuration outConfig, android.view.Surface outSurface) throws android.os.RemoteException &#123;android.os.Parcel _data = android.os.Parcel.obtain();android.os.Parcel _reply = android.os.Parcel.obtain();int _result;try &#123; ...... mRemote.transact(Stub.TRANSACTION_relayout, _data, _reply, 0); ...... if ((0 != _reply.readInt())) &#123; outSurface.readFromParcel(_reply); &#125;&#125; finally &#123; ......&#125;return _result;&#125; 从该函数的实现可以看出，应用程序进程中创建的Surface对象并没有传递到WMS服务进程，只是读取WMS服务进程返回来的Surface。那么WMS服务进程是如何响应应用程序进程布局请求的呢？ [-&gt;IWindowSession.java$ Stub] 1234567891011121314151617181920@Override public boolean onTransact(int code, android.os.Parcel data, android.os.Parcel reply, int flags) throws android.os.RemoteException&#123;switch (code)&#123;case TRANSACTION_relayout: &#123; ...... android.view.Surface _arg15; _arg15 = new android.view.Surface(); int _result = this.relayout(_arg0, _arg1, _arg2, _arg3, _arg4, _arg5, _arg6, _arg7, _arg8, _arg9, _arg10, _arg11, _arg12, _arg13, _arg14, _arg15); reply.writeNoException(); reply.writeInt(_result); ...... if ((_arg15!=null)) &#123; reply.writeInt(1); _arg15.writeToParcel(reply, android.os.Parcelable.PARCELABLE_WRITE_RETURN_VALUE); &#125; return true; &#125;&#125; 该函数可以看出，WMS服务在响应应用程序进程请求添加窗口时，首先在当前进程空间创建一个Surface对象，然后调用Session的relayout()函数进一步完成窗口添加过程，最后将WMS服务中创建的Surface返回给应用程序进程。 到目前为止，在应用程序进程和WMS服务进程分别创建了一个Surface对象，但是他们调用的都是Surface的无参构造函数，在该构造函数中并未真正初始化native层的Surface，那native层的Surface是在那里创建的呢？ [-&gt;Session.java] 1234567891011 public int relayout(IWindow window, int seq, WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewFlags, int flags, Rect outFrame, Rect outOverscanInsets, Rect outContentInsets, Rect outVisibleInsets, Rect outStableInsets, Rect outsets, Rect outBackdropFrame, Configuration outConfig, Surface outSurface) &#123; int res = mService.relayoutWindow(this, window, seq, attrs, requestedWidth, requestedHeight, viewFlags, flags, outFrame, outOverscanInsets, outContentInsets, outVisibleInsets, outStableInsets, outsets, outBackdropFrame, outConfig, outSurface); return res;&#125; [-&gt;WindowManagerService.java] 1234567891011121314151617181920212223242526 public int relayoutWindow(Session session, IWindow client, int seq, WindowManager.LayoutParams attrs, int requestedWidth, int requestedHeight, int viewVisibility, int flags, Rect outFrame, Rect outOverscanInsets, Rect outContentInsets, Rect outVisibleInsets, Rect outStableInsets, Rect outOutsets, Rect outBackdropFrame, Configuration outConfig, Surface outSurface) &#123; int result = 0; ...... if (viewVisibility == View.VISIBLE &amp;&amp; (win.mAppToken == null || !win.mAppToken.clientHidden)) &#123; result = relayoutVisibleWindow(outConfig, result, win, winAnimator, attrChanges, oldVisibility); try &#123; result = createSurfaceControl(outSurface, result, win, winAnimator); &#125; catch (Exception e) &#123; ...... return 0; &#125; ...... &#125; else &#123; ...... &#125; ...... return result;&#125; [-&gt;WindowManagerService.java] 12345678910111213 private int createSurfaceControl(Surface outSurface, int result, WindowState win, WindowStateAnimator winAnimator) &#123; if (!win.mHasSurface) &#123; result |= RELAYOUT_RES_SURFACE_CHANGED; &#125; WindowSurfaceController surfaceController = winAnimator.createSurfaceLocked(); if (surfaceController != null) &#123; surfaceController.getSurface(outSurface); &#125; else &#123; outSurface.release(); &#125; return result;&#125; [-&gt;WindowSurfaceController.java] 123 void getSurface(Surface outSurface) &#123; outSurface.copyFrom(mSurfaceControl);&#125; [-&gt;WindowStateAnimator.java] 123456789101112 WindowSurfaceController createSurfaceLocked() &#123; ...... try &#123; ...... mSurfaceController = new WindowSurfaceController(mSession.mSurfaceSession, attrs.getTitle().toString(), width, height, format, flags, this); w.setHasSurface(true); &#125; ...... return mSurfaceController;&#125; [-&gt;WindowSurfaceController.java] 123456789101112131415public WindowSurfaceController(SurfaceSession s, String name, int w, int h, int format, int flags, WindowStateAnimator animator) &#123; mAnimator = animator; mSurfaceW = w; mSurfaceH = h; ...... if (animator.mWin.isChildWindow() &amp;&amp; animator.mWin.mSubLayer &lt; 0 &amp;&amp; animator.mWin.mAppToken != null) &#123; ...... &#125; else &#123; mSurfaceControl = new SurfaceControl( s, name, w, h, format, flags); &#125;&#125; 2.12、Surface创建过程[-&gt;SurfaceControl.java] 1234567 public SurfaceControl(SurfaceSession session, String name, int w, int h, int format, int flags) throws OutOfResourcesException &#123; ...... mNativeObject = nativeCreate(session, name, w, h, format, flags); ......&#125; [-&gt;android_view_SurfaceControl.cpp] 12345678910111213static jlong nativeCreate(JNIEnv* env, jclass clazz, jobject sessionObj, jstring nameStr, jint w, jint h, jint format, jint flags) &#123;ScopedUtfChars name(env, nameStr);sp&lt;SurfaceComposerClient&gt; client(android_view_SurfaceSession_getClient(env, sessionObj));sp&lt;SurfaceControl&gt; surface = client-&gt;createSurface( String8(name.c_str()), w, h, format, flags);if (surface == NULL) &#123; jniThrowException(env, OutOfResourcesException, NULL); return 0;&#125;surface-&gt;incStrong((void *)nativeCreate);return reinterpret_cast&lt;jlong&gt;(surface.get());&#125; 该函数首先得到前面创建好的SurfaceComposerClient对象，通过该对象向SurfaceFlinger端的Client对象发送创建Surface的请求，最后得到一个SurfaceControl对象。 [-&gt;SurfaceComposerClient.cpp] 1234567891011121314151617181920sp&lt;SurfaceControl&gt; SurfaceComposerClient::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags) &#123;sp&lt;SurfaceControl&gt; sur;if (mStatus == NO_ERROR) &#123; sp&lt;IBinder&gt; handle; sp&lt;IGraphicBufferProducer&gt; gbp; status_t err = mClient-&gt;createSurface(name, w, h, format, flags, &amp;handle, &amp;gbp); ALOGE_IF(err, \"SurfaceComposerClient::createSurface error %s\", strerror(-err)); if (err == NO_ERROR) &#123; sur = new SurfaceControl(this, handle, gbp); &#125;&#125;return sur;&#125; SurfaceComposerClient将Surface创建请求转交给保存在其成员变量中的Bp SurfaceComposerClient对象来完成，在SurfaceFlinger端的Client本地对象会返回一个ISurface代理对象给应用程序，通过该代理对象为应用程序当前创建的Surface创建一个SurfaceControl对象。 [ISurfaceComposerClient.cpp] 1234567891011 virtual status_t createSurface(const String8&amp; name, uint32_t width, uint32_t height, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp) &#123; Parcel data, reply; ...... remote()-&gt;transact(CREATE_SURFACE, data, &amp;reply); *handle = reply.readStrongBinder(); *gbp = interface_cast&lt;IGraphicBufferProducer&gt;(reply.readStrongBinder()); return reply.readInt32();&#125; [Client.cpp] MessageCreateSurface消息是专门为应用程序请求创建Surface而定义的一种消息类型： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 status_t Client::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp)&#123; /* * createSurface must be called from the GL thread so that it can * have access to the GL context. */ class MessageCreateLayer : public MessageBase &#123; SurfaceFlinger* flinger; Client* client; sp&lt;IBinder&gt;* handle; sp&lt;IGraphicBufferProducer&gt;* gbp; status_t result; const String8&amp; name; uint32_t w, h; PixelFormat format; uint32_t flags; public: MessageCreateLayer(SurfaceFlinger* flinger, const String8&amp; name, Client* client, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp) : flinger(flinger), client(client), handle(handle), gbp(gbp), result(NO_ERROR), name(name), w(w), h(h), format(format), flags(flags) &#123; &#125; status_t getResult() const &#123; return result; &#125; virtual bool handler() &#123; result = flinger-&gt;createLayer(name, client, w, h, format, flags, handle, gbp); return true; &#125; &#125;; sp&lt;MessageBase&gt; msg = new MessageCreateLayer(mFlinger.get(), name, this, w, h, format, flags, handle, gbp); mFlinger-&gt;postMessageSync(msg); return static_cast&lt;MessageCreateLayer*&gt;( msg.get() )-&gt;getResult(); &#125;Client将应用程序创建Surface的请求转换为异步消息投递到SurfaceFlinger的消息队列中，将创建Surface的任务转交给SurfaceFlinger。[-&gt;SurfaceFlinger.cpp] status_t SurfaceFlinger::createLayer( const String8&amp; name, const sp&lt;Client&gt;&amp; client, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp)&#123; //ALOGD(\"createLayer for (%d x %d), name=%s\", w, h, name.string()); ...... status_t result = NO_ERROR; sp&lt;Layer&gt; layer; ////根据flags创建不同类型的layer switch (flags &amp; ISurfaceComposerClient::eFXSurfaceMask) &#123; case ISurfaceComposerClient::eFXSurfaceNormal: result = createNormalLayer(client, name, w, h, flags, format, handle, gbp, &amp;layer); break; case ISurfaceComposerClient::eFXSurfaceDim: result = createDimLayer(client, name, w, h, flags, handle, gbp, &amp;layer); break; default: result = BAD_VALUE; break; &#125; if (result != NO_ERROR) &#123; return result; &#125; //将创建好的Layer对象保存在Client中 result = addClientLayer(client, *handle, *gbp, layer); if (result != NO_ERROR) &#123; return result; &#125; setTransactionFlags(eTransactionNeeded); return result; &#125; SurfaceFlinger根据标志位创建对应类型的Surface，当前系统定义了2种类型的Layer: [-&gt;ISurfaceComposerClient.h] 12eFXSurfaceNormal = 0x00000000,eFXSurfaceDim = 0x00020000, [-&gt;SurfaceFlinger.cpp] 123456789101112131415161718192021222324status_t SurfaceFlinger::createNormalLayer(const sp&lt;Client&gt;&amp; client, const String8&amp; name, uint32_t w, uint32_t h, uint32_t flags, PixelFormat&amp; format, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp, sp&lt;Layer&gt;* outLayer)&#123;// initialize the surfacesswitch (format) &#123;case PIXEL_FORMAT_TRANSPARENT:case PIXEL_FORMAT_TRANSLUCENT: format = PIXEL_FORMAT_RGBA_8888; break;case PIXEL_FORMAT_OPAQUE: format = PIXEL_FORMAT_RGBX_8888; break;&#125;//在SurfaceFlinger端为应用程序的Surface创建对应的Layer对象 *outLayer = new Layer(this, client, name, w, h, flags);status_t err = (*outLayer)-&gt;setBuffers(w, h, format, flags);if (err == NO_ERROR) &#123; *handle = (*outLayer)-&gt;getHandle(); *gbp = (*outLayer)-&gt;getProducer();&#125;ALOGE_IF(err, \"createNormalLayer() failed (%s)\", strerror(-err));return err;&#125; 在SurfaceFlinger服务端为应用程序创建的Surface创建对应的Layer对象。应用程序请求创建Surface过程如下： 第一次强引用Layer对象时，onFirstRef()函数被回调 [Layer.cpp] 123456789101112131415161718192021void Layer::onFirstRef() &#123;// Creates a custom BufferQueue for SurfaceFlingerConsumer to usesp&lt;IGraphicBufferProducer&gt; producer;sp&lt;IGraphicBufferConsumer&gt; consumer;//创建BufferQueue对象BufferQueue::createBufferQueue(&amp;producer, &amp;consumer);mProducer = new MonitoredProducer(producer, mFlinger);mSurfaceFlingerConsumer = new SurfaceFlingerConsumer(consumer, mTextureName, this);mSurfaceFlingerConsumer-&gt;setConsumerUsageBits(getEffectiveUsage(0));mSurfaceFlingerConsumer-&gt;setContentsChangedListener(this);mSurfaceFlingerConsumer-&gt;setName(mName);#ifdef TARGET_DISABLE_TRIPLE_BUFFERING#warning \"disabling triple buffering\"#elsemProducer-&gt;setMaxDequeuedBufferCount(2);#endifconst sp&lt;const DisplayDevice&gt; hw(mFlinger-&gt;getDefaultDisplayDevice());updateTransformHint(hw);&#125; 根据buffer可用监听器的注册过程，我们知道，当生产者也就是应用程序填充好图形buffer数据后，通过回调方式通知消费者的 BufferQueue构造过程[-&gt;BufferQueue.cpp] 1234567891011void BufferQueue::createBufferQueue(sp&lt;IGraphicBufferProducer&gt;* outProducer, sp&lt;IGraphicBufferConsumer&gt;* outConsumer, const sp&lt;IGraphicBufferAlloc&gt;&amp; allocator) &#123;......sp&lt;BufferQueueCore&gt; core(new BufferQueueCore(allocator));sp&lt;IGraphicBufferProducer&gt; producer(new BufferQueueProducer(core));sp&lt;IGraphicBufferConsumer&gt; consumer(new BufferQueueConsumer(core));*outProducer = producer;*outConsumer = consumer;&#125; [-&gt;BufferQueueCore.cpp] 所以核心都是这个BufferQueueCore，他是管理图形缓冲区的中枢。这里举一个SurfaceTexture的例子，来看看他们之间的关系： 123456789101112131415161718192021BufferQueueCore::BufferQueueCore(const sp&lt;IGraphicBufferAlloc&gt;&amp; allocator) :mAllocator(allocator),......&#123;if (allocator == NULL) &#123; sp&lt;ISurfaceComposer&gt; composer(ComposerService::getComposerService()); mAllocator = composer-&gt;createGraphicBufferAlloc(); if (mAllocator == NULL) &#123; BQ_LOGE(\"createGraphicBufferAlloc failed\"); &#125;&#125;int numStartingBuffers = getMaxBufferCountLocked();for (int s = 0; s &lt; numStartingBuffers; s++) &#123; mFreeSlots.insert(s);&#125;for (int s = numStartingBuffers; s &lt; BufferQueueDefs::NUM_BUFFER_SLOTS; s++) &#123; mUnusedSlots.push_front(s);&#125;&#125; BufferQueueCore类中定义了一个64项的数据mSlots，是一个容量大小为64的数组，因此BufferQueueCore可以管理最多64块的GraphicBuffer。 [-&gt;ISurfaceComposer.cpp] 1234567 virtual sp&lt;IGraphicBufferAlloc&gt; createGraphicBufferAlloc()&#123; Parcel data, reply; data.writeInterfaceToken(ISurfaceComposer::getInterfaceDescriptor()); remote()-&gt;transact(BnSurfaceComposer::CREATE_GRAPHIC_BUFFER_ALLOC, data, &amp;reply); return interface_cast&lt;IGraphicBufferAlloc&gt;(reply.readStrongBinder());&#125; [-&gt;SurfaceFlinger.cpp] 12345sp&lt;IGraphicBufferAlloc&gt; SurfaceFlinger::createGraphicBufferAlloc()&#123;sp&lt;GraphicBufferAlloc&gt; gba(new GraphicBufferAlloc());return gba;&#125; GraphicBufferAlloc构造过程[-&gt;GraphicBufferAlloc.cpp] 123456789sp&lt;GraphicBuffer&gt; GraphicBufferAlloc::createGraphicBuffer(uint32_t width, uint32_t height, PixelFormat format, uint32_t usage, std::string requestorName, status_t* error) &#123;sp&lt;GraphicBuffer&gt; graphicBuffer(new GraphicBuffer( width, height, format, usage, std::move(requestorName)));status_t err = graphicBuffer-&gt;initCheck();......return graphicBuffer;&#125; 图形缓冲区创建过程[-&gt;GraphicBuffer.cpp] 1234567891011121314GraphicBuffer::GraphicBuffer(uint32_t inWidth, uint32_t inHeight, PixelFormat inFormat, uint32_t inUsage, std::string requestorName): BASE(), mOwner(ownData), mBufferMapper(GraphicBufferMapper::get()), mInitCheck(NO_ERROR), mId(getUniqueId()), mGenerationNumber(0) &#123;width =height =stride =format =usage = 0;handle = NULL;mInitCheck = initSize(inWidth, inHeight, inFormat, inUsage, std::move(requestorName)); &#125; 根据图形buffer的宽高、格式等信息为图形缓冲区分配存储空间。 使用GraphicBufferAllocator对象来为图形缓冲区分配内存空间，GraphicBufferAllocator是对Gralloc模块中的gpu设备的封装类。关于GraphicBufferAllocator内存分配过程请查看Android图形缓冲区分配过程源码分析图形缓冲区分配完成后，还会映射到SurfaceFlinger服务进程的虚拟地址空间。 Android图形缓冲区分配过程源码分析[-&gt;Layer.cpp] 123456789101112131415Layer::Layer(SurfaceFlinger* flinger, const sp&lt;Client&gt;&amp; client, const String8&amp; name, uint32_t w, uint32_t h, uint32_t flags): contentDirty(false), sequence(uint32_t(android_atomic_inc(&amp;sSequence))), mFlinger(flinger), mTextureName(-1U), mPremultipliedAlpha(true), mName(\"unnamed\"), mFormat(PIXEL_FORMAT_NONE), ......&#123;mCurrentCrop.makeInvalid();mFlinger-&gt;getRenderEngine().genTextures(1, &amp;mTextureName);mTexture.init(Texture::TEXTURE_EXTERNAL, mTextureName);......&#125; 到此才算真正创建了一个可用于绘图的Surface (Layer)，从上面的分析我们可以看出，在WMS服务进程端，其实创建了两个Java层的Surface对象，第一个Surface使用了无参构造函数，仅仅构造一个Surface对象而已，而第二个Surface却使用了有参构造函数，参数指定了图象宽高等信息，这个Java层Surface对象还会在native层请求SurfaceFlinger创建一个真正能用于绘制图象的native层Surface。最后通过浅拷贝的方式将第二个Surface复制到第一个Surface中，最后通过writeToParcel方式写回到应用程序进程。 12345678910111213 public void copyFrom(SurfaceControl other) &#123; ...... long surfaceControlPtr = other.mNativeObject; ...... long newNativeObject = nativeCreateFromSurfaceControl(surfaceControlPtr); synchronized (mLock) &#123; if (mNativeObject != 0) &#123; nativeRelease(mNativeObject); &#125; setNativeObjectLocked(newNativeObject); &#125;&#125; [android_view_Surface.cpp] 123456789101112131415static jlong nativeCreateFromSurfaceControl(JNIEnv* env, jclass clazz, jlong surfaceControlNativeObj) &#123;/* * This is used by the WindowManagerService just after constructing * a Surface and is necessary for returning the Surface reference to * the caller. At this point, we should only have a SurfaceControl. */sp&lt;SurfaceControl&gt; ctrl(reinterpret_cast&lt;SurfaceControl *&gt;(surfaceControlNativeObj));sp&lt;Surface&gt; surface(ctrl-&gt;getSurface());if (surface != NULL) &#123; surface-&gt;incStrong(&amp;sRefBaseOwner);&#125;return reinterpret_cast&lt;jlong&gt;(surface.get());&#125; 2号Surface引用到了3号Surface的SurfaceControl对象后，通过writeToParcel()函数写会到应用程序进程。 [Surface.java] 12345678910111213141516 @Overridepublic void writeToParcel(Parcel dest, int flags) &#123; if (dest == null) &#123; throw new IllegalArgumentException(\"dest must not be null\"); &#125; synchronized (mLock) &#123; // NOTE: This must be kept synchronized with the native parceling code // in frameworks/native/libs/Surface.cpp dest.writeString(mName); dest.writeInt(mIsSingleBuffered ? 1 : 0); nativeWriteToParcel(mNativeObject, dest); &#125; if ((flags &amp; Parcelable.PARCELABLE_WRITE_RETURN_VALUE) != 0) &#123; release(); &#125;&#125; [android_view_Surface.cpp] 12345678910111213141516static void nativeWriteToParcel(JNIEnv* env, jclass clazz, jlong nativeObject, jobject parcelObj) &#123;Parcel* parcel = parcelForJavaObject(env, parcelObj);if (parcel == NULL) &#123; doThrowNPE(env); return;&#125;sp&lt;Surface&gt; self(reinterpret_cast&lt;Surface *&gt;(nativeObject));android::view::Surface surfaceShim;if (self != nullptr) &#123; surfaceShim.graphicBufferProducer = self-&gt;getIGraphicBufferProducer();&#125;// Calling code in Surface.java has already written the name of the Surface// to the ParcelsurfaceShim.writeToParcel(parcel, /*nameAlreadyWritten*/true);&#125; 应用程序进程中的1号Surface按相反顺序读取WMS服务端返回过来的Binder对象等数据，并构造一个native层的Surface对象。 1234567891011121314151617 public void readFromParcel(Parcel source) &#123; if (source == null) &#123; throw new IllegalArgumentException(\"source must not be null\"); &#125; synchronized (mLock) &#123; // nativeReadFromParcel() will either return mNativeObject, or // create a new native Surface and return it after reducing // the reference count on mNativeObject. Either way, it is // not necessary to call nativeRelease() here. // NOTE: This must be kept synchronized with the native parceling code // in frameworks/native/libs/Surface.cpp mName = source.readString(); mIsSingleBuffered = source.readInt() != 0; setNativeObjectLocked(nativeReadFromParcel(mNativeObject, source)); &#125;&#125; 应用程序进程中的1号Surface按相反顺序读取WMS服务端返回过来的Binder对象等数据，并构造一个native层的Surface对象。 123456789101112131415static jlong nativeCreateFromSurfaceControl(JNIEnv* env, jclass clazz, jlong surfaceControlNativeObj) &#123;/* * This is used by the WindowManagerService just after constructing * a Surface and is necessary for returning the Surface reference to * the caller. At this point, we should only have a SurfaceControl. */sp&lt;SurfaceControl&gt; ctrl(reinterpret_cast&lt;SurfaceControl *&gt;(surfaceControlNativeObj));sp&lt;Surface&gt; surface(ctrl-&gt;getSurface());if (surface != NULL) &#123; surface-&gt;incStrong(&amp;sRefBaseOwner);&#125;return reinterpret_cast&lt;jlong&gt;(surface.get());&#125; 每个Activity可以有一个或多个Surface，默认情况下一个Activity只有一个Surface，当Activity中使用SurfaceView时，就存在多个Surface。Activity默认surface是在relayoutWindow过程中由WMS服务创建的，然后回传给应用程序进程，我们知道一个Surface其实就是应用程序端的本地窗口，关于Surface的初始化过程这里就不在介绍。 应用程序本地窗口Surface创建过程从前面分析可知，SurfaceFlinger在处理应用程序请求创建Surface中，在SurfaceFlinger服务端仅仅创建了Layer对象，那么应用程序本地窗口Surface在什么时候、什么地方创建呢？ 为应用程序创建好了Layer对象并返回ISurface的代理对象给应用程序，应用程序通过该代理对象创建了一个SurfaceControl对象，Java层Surface需要通过android_view_Surface.cpp中的JNI函数来操作native层的Surface，在操作native层Surface前，首先需要获取到native的Surface，应用程序本地窗口Surface就是在这个时候创建的。 [-&gt;SurfaceControl.cpp] 12345678910sp&lt;Surface&gt; SurfaceControl::getSurface() const&#123;Mutex::Autolock _l(mLock);if (mSurfaceData == 0) &#123; // This surface is always consumed by SurfaceFlinger, so the // producerControlledByApp value doesn't matter; using false. mSurfaceData = new Surface(mGraphicBufferProducer, false);&#125;return mSurfaceData;&#125; [Surface.cpp] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Surface::Surface( const sp&lt;IGraphicBufferProducer&gt;&amp; bufferProducer, bool controlledByApp): mGraphicBufferProducer(bufferProducer), mCrop(Rect::EMPTY_RECT), mGenerationNumber(0), mSharedBufferMode(false), mAutoRefresh(false), mSharedBufferSlot(BufferItem::INVALID_BUFFER_SLOT), mSharedBufferHasBeenQueued(false), mNextFrameNumber(1) &#123;// Initialize the ANativeWindow function pointers.ANativeWindow::setSwapInterval = hook_setSwapInterval;ANativeWindow::dequeueBuffer = hook_dequeueBuffer;ANativeWindow::cancelBuffer = hook_cancelBuffer;ANativeWindow::queueBuffer = hook_queueBuffer;ANativeWindow::query = hook_query;ANativeWindow::perform = hook_perform;ANativeWindow::dequeueBuffer_DEPRECATED = hook_dequeueBuffer_DEPRECATED;ANativeWindow::cancelBuffer_DEPRECATED = hook_cancelBuffer_DEPRECATED;ANativeWindow::lockBuffer_DEPRECATED = hook_lockBuffer_DEPRECATED;ANativeWindow::queueBuffer_DEPRECATED = hook_queueBuffer_DEPRECATED;const_cast&lt;int&amp;&gt;(ANativeWindow::minSwapInterval) = 0;const_cast&lt;int&amp;&gt;(ANativeWindow::maxSwapInterval) = 1;mReqWidth = 0;mReqHeight = 0;mReqFormat = 0;mReqUsage = 0;mTimestamp = NATIVE_WINDOW_TIMESTAMP_AUTO;mDataSpace = HAL_DATASPACE_UNKNOWN;mScalingMode = NATIVE_WINDOW_SCALING_MODE_FREEZE;mTransform = 0;mStickyTransform = 0;mDefaultWidth = 0;mDefaultHeight = 0;mUserWidth = 0;mUserHeight = 0;mTransformHint = 0;mConsumerRunningBehind = false;mConnectedToCpu = false;mProducerControlledByApp = controlledByApp;mSwapIntervalZero = false;&#125; 在创建完应用程序本地窗口Surface后，想要在该Surface上绘图，首先需要为该Surface分配图形buffer。我们前面介绍了Android应用程序图形缓冲区的分配都是由SurfaceFlinger服务进程来完成，在请求创建Surface时，在服务端创建了一个BufferQueue本地Binder对象，该对象负责管理应用程序一个本地窗口Surface的图形缓冲区。 3、执行窗口布局performLayout()[-&gt;ViewRootImpl.java] 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 private void performLayout(WindowManager.LayoutParams lp, int desiredWindowWidth, int desiredWindowHeight) &#123; mLayoutRequested = false; mScrollMayChange = true; mInLayout = true; final View host = mView; try &#123; host.layout(0, 0, host.getMeasuredWidth(), host.getMeasuredHeight()); mInLayout = false; int numViewsRequestingLayout = mLayoutRequesters.size(); if (numViewsRequestingLayout &gt; 0) &#123; // requestLayout() was called during layout. // If no layout-request flags are set on the requesting views, there is no problem. // If some requests are still pending, then we need to clear those flags and do // a full request/measure/layout pass to handle this situation. ArrayList&lt;View&gt; validLayoutRequesters = getValidLayoutRequesters(mLayoutRequesters, false); if (validLayoutRequesters != null) &#123; // Set this flag to indicate that any further requests are happening during // the second pass, which may result in posting those requests to the next // frame instead mHandlingLayoutInLayoutRequest = true; // Process fresh layout requests, then measure and layout int numValidRequests = validLayoutRequesters.size(); for (int i = 0; i &lt; numValidRequests; ++i) &#123; final View view = validLayoutRequesters.get(i); Log.w(\"View\", \"requestLayout() improperly called by \" + view + \" during layout: running second layout pass\"); view.requestLayout(); &#125; measureHierarchy(host, lp, mView.getContext().getResources(), desiredWindowWidth, desiredWindowHeight); mInLayout = true; host.layout(0, 0, host.getMeasuredWidth(), host.getMeasuredHeight()); mHandlingLayoutInLayoutRequest = false; // Check the valid requests again, this time without checking/clearing the // layout flags, since requests happening during the second pass get noop'd validLayoutRequesters = getValidLayoutRequesters(mLayoutRequesters, true); if (validLayoutRequesters != null) &#123; final ArrayList&lt;View&gt; finalRequesters = validLayoutRequesters; // Post second-pass requests to the next frame getRunQueue().post(new Runnable() &#123; @Override public void run() &#123; int numValidRequests = finalRequesters.size(); for (int i = 0; i &lt; numValidRequests; ++i) &#123; final View view = finalRequesters.get(i); view.requestLayout(); &#125; &#125; &#125;);&#125; &#125; &#125; &#125; finally &#123; Trace.traceEnd(Trace.TRACE_TAG_VIEW); &#125; mInLayout = false;&#125; 4.执行窗口绘制performDraw()12345678910111213[-&gt;ViewRootImpl.java] private void performDraw() &#123; ...... try &#123; draw(fullRedrawNeeded); &#125; finally &#123; mIsDrawing = false; Trace.traceEnd(Trace.TRACE_TAG_VIEW); &#125; ...... &#125; &#125; Android是怎样将View画出来的？ [-&gt;ViewRootImpl.java] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384 private void draw(boolean fullRedrawNeeded) &#123; Surface surface = mSurface; ...... mAttachInfo.mTreeObserver.dispatchOnDraw(); int xOffset = -mCanvasOffsetX; int yOffset = -mCanvasOffsetY + curScrollY; final WindowManager.LayoutParams params = mWindowAttributes; final Rect surfaceInsets = params != null ? params.surfaceInsets : null; if (surfaceInsets != null) &#123; xOffset -= surfaceInsets.left; yOffset -= surfaceInsets.top; // Offset dirty rect for surface insets. dirty.offset(surfaceInsets.left, surfaceInsets.right); &#125; boolean accessibilityFocusDirty = false; final Drawable drawable = mAttachInfo.mAccessibilityFocusDrawable; if (drawable != null) &#123; final Rect bounds = mAttachInfo.mTmpInvalRect; final boolean hasFocus = getAccessibilityFocusedRect(bounds); if (!hasFocus) &#123; bounds.setEmpty(); &#125; if (!bounds.equals(drawable.getBounds())) &#123; accessibilityFocusDirty = true; &#125; &#125; mAttachInfo.mDrawingTime = mChoreographer.getFrameTimeNanos() / TimeUtils.NANOS_PER_MS; if (!dirty.isEmpty() || mIsAnimating || accessibilityFocusDirty) &#123; if (mAttachInfo.mHardwareRenderer != null &amp;&amp; mAttachInfo.mHardwareRenderer.isEnabled()) &#123; // If accessibility focus moved, always invalidate the root. boolean invalidateRoot = accessibilityFocusDirty || mInvalidateRootRequested; mInvalidateRootRequested = false; // Draw with hardware renderer. mIsAnimating = false; if (mHardwareYOffset != yOffset || mHardwareXOffset != xOffset) &#123; mHardwareYOffset = yOffset; mHardwareXOffset = xOffset; invalidateRoot = true; &#125; if (invalidateRoot) &#123; mAttachInfo.mHardwareRenderer.invalidateRoot(); &#125; ...... if (updated) &#123; requestDrawWindow(); &#125; mAttachInfo.mHardwareRenderer.draw(mView, mAttachInfo, this); &#125; else &#123; if (mAttachInfo.mHardwareRenderer != null &amp;&amp; !mAttachInfo.mHardwareRenderer.isEnabled() &amp;&amp; mAttachInfo.mHardwareRenderer.isRequested()) &#123; try &#123; mAttachInfo.mHardwareRenderer.initializeIfNeeded( mWidth, mHeight, mAttachInfo, mSurface, surfaceInsets); &#125; catch (OutOfResourcesException e) &#123; handleOutOfResourcesException(e); return; &#125; mFullRedrawNeeded = true; scheduleTraversals(); return; &#125; if (!drawSoftware(surface, mAttachInfo, xOffset, yOffset, scalingRequired, dirty)) &#123; return; &#125; &#125; &#125; if (animating) &#123; mFullRedrawNeeded = true; scheduleTraversals(); &#125;&#125; 关于绘制这个流程很复杂，我们后续章节再分析。 参考博客：Android应用程序UI硬件加速渲染技术简要介绍和学习计划 这里我们因为要分析Surface机制，所以只分析ViewRootImpl的draw流程。（如果开启了硬件加速功能，则会使用hwui硬件绘制功能，这里我们忽略这个，使用默认的软件绘制流程drawSoftware）。 [-&gt;ViewRootImpl.java] 1234567891011121314151617181920212223242526272829303132 private boolean drawSoftware(Surface surface, AttachInfo attachInfo, int xoff, int yoff, boolean scalingRequired, Rect dirty) &#123; // Draw with software renderer. final Canvas canvas; try &#123; ...... canvas = mSurface.lockCanvas(dirty); ...... &#125; ...... try &#123; canvas.translate(-xoff, -yoff); if (mTranslator != null) &#123; mTranslator.translateCanvas(canvas); &#125; canvas.setScreenDensity(scalingRequired ? mNoncompatDensity : 0); attachInfo.mSetIgnoreDirtyState = false; mView.draw(canvas); drawAccessibilityFocusedDrawableIfNeeded(canvas); &#125;...... &#125; finally &#123; try &#123; surface.unlockCanvasAndPost(canvas); &#125; catch (IllegalArgumentException e) &#123; ...... return false; &#125; &#125; return true;&#125; 先看看Surface的lockCanvas方法： [-&gt;Surface.java] 1234567891011//mCanvas 变量直接赋值private final Canvas mCanvas = new CompatibleCanvas();public Canvas lockCanvas(Rect inOutDirty) throws Surface.OutOfResourcesException, IllegalArgumentException &#123;synchronized (mLock) &#123; checkNotReleasedLocked(); ...... mLockedObject = nativeLockCanvas(mNativeObject, mCanvas, inOutDirty); return mCanvas;&#125;&#125; [-&gt;android_view_Surface.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263static jlong nativeLockCanvas(JNIEnv* env, jclass clazz, jlong nativeObject, jobject canvasObj, jobject dirtyRectObj) &#123; //获取java层的Surface保存的long型句柄sp&lt;Surface&gt; surface(reinterpret_cast&lt;Surface *&gt;(nativeObject));if (!isSurfaceValid(surface)) &#123; doThrowIAE(env); return 0;&#125;Rect dirtyRect(Rect::EMPTY_RECT);Rect* dirtyRectPtr = NULL;//获取java层dirty Rect的位置大小信息if (dirtyRectObj) &#123; dirtyRect.left = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.left); dirtyRect.top = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.top); dirtyRect.right = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.right); dirtyRect.bottom = env-&gt;GetIntField(dirtyRectObj, gRectClassInfo.bottom); dirtyRectPtr = &amp;dirtyRect;&#125;ANativeWindow_Buffer outBuffer; //调用Surface的lock方法,将申请的图形缓冲区赋给outBufferstatus_t err = surface-&gt;lock(&amp;outBuffer, dirtyRectPtr);......SkImageInfo info = SkImageInfo::Make(outBuffer.width, outBuffer.height, convertPixelFormat(outBuffer.format), outBuffer.format == PIXEL_FORMAT_RGBX_8888 ? kOpaque_SkAlphaType : kPremul_SkAlphaType);SkBitmap bitmap;//创建一个SkBitmap//图形缓冲区每一行像素大小ssize_t bpr = outBuffer.stride * bytesPerPixel(outBuffer.format);bitmap.setInfo(info, bpr);if (outBuffer.width &gt; 0 &amp;&amp; outBuffer.height &gt; 0) &#123; bitmap.setPixels(outBuffer.bits);&#125; else &#123; // be safe with an empty bitmap. bitmap.setPixels(NULL);&#125;Canvas* nativeCanvas = GraphicsJNI::getNativeCanvas(env, canvasObj);nativeCanvas-&gt;setBitmap(bitmap);if (dirtyRectPtr) &#123; nativeCanvas-&gt;clipRect(dirtyRect.left, dirtyRect.top, dirtyRect.right, dirtyRect.bottom);&#125;if (dirtyRectObj) &#123; env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.left, dirtyRect.left); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.top, dirtyRect.top); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.right, dirtyRect.right); env-&gt;SetIntField(dirtyRectObj, gRectClassInfo.bottom, dirtyRect.bottom);&#125;......sp&lt;Surface&gt; lockedSurface(surface);lockedSurface-&gt;incStrong(&amp;sRefBaseOwner);return (jlong) lockedSurface.get();&#125; 12345这段代码逻辑主要如下： 1）获取java层dirty 的Rect大小和位置信息； 2）调用Surface的lock方法,将申请的图形缓冲区赋给outBuffer； 3）创建一个Skbitmap，填充它用来保存申请的图形缓冲区，并赋值给Java层的Canvas对象； 4）将剪裁位置大小信息赋给java层Canvas对象。 unlockCanvasAndPost()Surface绘制完毕后，unlockCanvasAndPost操作。 [-&gt;android_view_Surface.cpp] 1234567891011121314151617static void nativeUnlockCanvasAndPost(JNIEnv* env, jclass clazz, jlong nativeObject, jobject canvasObj) &#123;sp&lt;Surface&gt; surface(reinterpret_cast&lt;Surface *&gt;(nativeObject));if (!isSurfaceValid(surface)) &#123; return;&#125;// detach the canvas from the surfaceCanvas* nativeCanvas = GraphicsJNI::getNativeCanvas(env, canvasObj);nativeCanvas-&gt;setBitmap(SkBitmap());// unlock surfacestatus_t err = surface-&gt;unlockAndPost();if (err &lt; 0) &#123; doThrowIAE(env);&#125;&#125; Surface管理图形缓冲区我们上边分析到了申请图形缓冲区，用到了Surface的lock函数，我们继续查看。 [-&gt;Surface.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384status_t Surface::lock( ANativeWindow_Buffer* outBuffer, ARect* inOutDirtyBounds) &#123;......ANativeWindowBuffer* out;int fenceFd = -1;//调用dequeueBuffer函数，申请图形缓冲区status_t err = dequeueBuffer(&amp;out, &amp;fenceFd);ALOGE_IF(err, \"dequeueBuffer failed (%s)\", strerror(-err));if (err == NO_ERROR) &#123; //获取图形缓冲区区域大小,赋给后备缓冲区变量backBuffer sp&lt;GraphicBuffer&gt; backBuffer(GraphicBuffer::getSelf(out)); const Rect bounds(backBuffer-&gt;width, backBuffer-&gt;height); Region newDirtyRegion; if (inOutDirtyBounds) &#123; //如果上层指定乐刷新脏矩形区域，则用这个区域和缓冲区区域求交集， //然后将交集的结果设给需要去刷新的新区域 newDirtyRegion.set(static_cast&lt;Rect const&amp;&gt;(*inOutDirtyBounds)); newDirtyRegion.andSelf(bounds); &#125; else &#123; /如果上层没有指定脏矩形区域，所以刷新整个图形缓冲区 newDirtyRegion.set(bounds); &#125; // figure out if we can copy the frontbuffer back //上一次绘制的信息保存在mPostedBuffer中，而这个mPostedBuffer则要在unLockAndPost函数中设置 int backBufferSlot(getSlotFromBufferLocked(backBuffer.get())); const sp&lt;GraphicBuffer&gt;&amp; frontBuffer(mPostedBuffer); const bool canCopyBack = (frontBuffer != 0 &amp;&amp; backBuffer-&gt;width == frontBuffer-&gt;width &amp;&amp; backBuffer-&gt;height == frontBuffer-&gt;height &amp;&amp; backBuffer-&gt;format == frontBuffer-&gt;format); if (canCopyBack) &#123; Mutex::Autolock lock(mMutex); Region oldDirtyRegion; if(mSlots[backBufferSlot].dirtyRegion.isEmpty()) &#123; oldDirtyRegion.set(bounds); &#125; else &#123; for(int i = 0 ; i &lt; NUM_BUFFER_SLOTS; i++ ) &#123; if(i != backBufferSlot &amp;&amp; !mSlots[i].dirtyRegion.isEmpty()) oldDirtyRegion.orSelf(mSlots[i].dirtyRegion); &#125; &#125; const Region copyback(oldDirtyRegion.subtract(newDirtyRegion)); if (!copyback.isEmpty()) //这里把mPostedBuffer中的旧数据拷贝到BackBuffer中。 //后续的绘画只要更新脏区域就可以了，这会节约不少资源 copyBlt(backBuffer, frontBuffer, copyback); &#125; else &#123; // if we can't copy-back anything, modify the user's dirty // region to make sure they redraw the whole buffer //如果两次图形缓冲区大小不一致，我们就要修改用户指定的dirty区域大小为整个缓冲区大小， //然后去更新整个缓冲区 newDirtyRegion.set(bounds); Mutex::Autolock lock(mMutex); for (size_t i=0 ; i&lt;NUM_BUFFER_SLOTS ; i++) &#123; mSlots[i].dirtyRegion.clear(); &#125; &#125; &#123; // scope for the lock Mutex::Autolock lock(mMutex); //将新的dirty赋给这个bufferslot mSlots[backBufferSlot].dirtyRegion = newDirtyRegion; &#125; if (inOutDirtyBounds) &#123; *inOutDirtyBounds = newDirtyRegion.getBounds(); &#125; void* vaddr; //lock和unlock分别用来锁定和解锁一个指定的图形缓冲区，在访问一块图形缓冲区的时候， //例如，向一块图形缓冲写入内容的时候，需要将该图形缓冲区锁定，用来避免访问冲突, //锁定之后，就可以获得由参数参数l、t、w和h所圈定的一块缓冲区的起始地址，保存在输出参数vaddr中 status_t res = backBuffer-&gt;lockAsync( GRALLOC_USAGE_SW_READ_OFTEN | GRALLOC_USAGE_SW_WRITE_OFTEN, newDirtyRegion.bounds(), &amp;vaddr, fenceFd); ......&#125;return err;&#125; Surface的lock函数用来申请图形缓冲区和一些操作，方法不长，大概工作有： 1）调用connect函数完成一些初始化； 2）调用dequeueBuffer函数，申请图形缓冲区； 3）计算需要绘制的新的dirty区域，旧的区域原样copy数据。 [-&gt;BufferQueueProducer.cpp] 123456789101112131415161718192021222324252627int Surface::dequeueBuffer(android_native_buffer_t** buffer, int* fenceFd) &#123;uint32_t reqWidth;uint32_t reqHeight;PixelFormat reqFormat;uint32_t reqUsage;&#123; ......//申请图形缓冲区status_t result = mGraphicBufferProducer-&gt;dequeueBuffer(&amp;buf, &amp;fence, reqWidth, reqHeight, reqFormat, reqUsage);......//根据index获取缓冲区sp&lt;GraphicBuffer&gt;&amp; gbuf(mSlots[buf].buffer);......if ((result &amp; IGraphicBufferProducer::BUFFER_NEEDS_REALLOCATION) || gbuf == 0) &#123; //由于申请的内存是在surfaceflinger进程中， //BufferQueue中的图形缓冲区也是通过匿名共享内存和binder传递描述符映射过去的， //Surface通过调用requestBuffer将图形缓冲区映射到Surface所在进程 result = mGraphicBufferProducer-&gt;requestBuffer(buf, &amp;gbuf); ......&#125;......//获取这个这个buffer对象的指针内容*buffer = gbuf.get();......return OK;&#125; [-&gt;BufferQueueProducer.cpp] 12345678910status_t BufferQueueProducer::requestBuffer(int slot, sp&lt;GraphicBuffer&gt;* buf) &#123;ATRACE_CALL();Mutex::Autolock lock(mCore-&gt;mMutex);......mSlots[slot].mRequestBufferCalled = true;*buf = mSlots[slot].mGraphicBuffer;return NO_ERROR;&#125; 这个比较简单，还是很好理解的额，就是根据指定index取出mSlots中的slot中的buffer。 图形缓冲区入队我们前面讲了，省略了第二步绘制流程，因此我们这里分析第三部，绘制完毕后再queueBuffer。 同样，调用了Surface的unlockCanvasAndPost函数，我们查看它的实现： [-&gt;Surface.cpp] 123456789101112131415status_t Surface::unlockAndPost()&#123;......int fd = -1;//解锁图形缓冲区，和前面的lockAsync成对出现status_t err = mLockedBuffer-&gt;unlockAsync(&amp;fd);//queueBuffer去归还图形缓冲区err = queueBuffer(mLockedBuffer.get(), fd);mPostedBuffer = mLockedBuffer;mLockedBuffer = 0;return err;&#125; 这里也比较简单，核心也是分两步： 1）解锁图形缓冲区，和前面的lockAsync成对出现； 2）queueBuffer去归还图形缓冲区； 所以我们还是重点分析第二步，查看queueBuffer的实现： [-&gt;Surface.cpp] 1234567int Surface::queueBuffer(android_native_buffer_t* buffer, int fenceFd) &#123;......status_t err = mGraphicBufferProducer-&gt;queueBuffer(i, input, &amp;output);mLastQueueDuration = systemTime() - now;......return err;&#125; 调用BufferQueueProducer的queueBuffer归还缓冲区，将绘制后的图形缓冲区queue回去。 [-&gt;BufferQueueProducer.cpp] 1234567891011121314151617181920status_t BufferQueueProducer::queueBuffer(int slot, const QueueBufferInput &amp;input, QueueBufferOutput *output) &#123;......&#123; // scope for the lock Mutex::Autolock lock(mCallbackMutex); while (callbackTicket != mCurrentCallbackTicket) &#123; mCallbackCondition.wait(mCallbackMutex); &#125; if (frameAvailableListener != NULL) &#123; frameAvailableListener-&gt;onFrameAvailable(item); &#125; else if (frameReplacedListener != NULL) &#123; frameReplacedListener-&gt;onFrameReplaced(item); &#125; ......&#125;......return NO_ERROR;&#125; 总结： 1）从传入的QueueBufferInput ，解析填充一些变量； 2）改变入队Slot的状态为QUEUED，每次推进来，mFrameCounter都加1。这里的slot，上一篇讲分配缓冲区返回最老的FREE状态buffer，就是用这个mFrameCounter最小值判断，就是上一篇LRU算法的判断； 3）创建一个BufferItem来描述GraphicBuffer，用mSlots[slot]中的slot填充BufferItem； 4）将BufferItem塞进mCore的mQueue队列，依照指定规则； 5）然后通知SurfaceFlinger去消费。 上述lockCanvas和unlockCanvasAndPost可以用下图来总结一下： 通知SF消费合成 当绘制完毕的GraphicBuffer入队之后，会通知SurfaceFlinger去消费，就是BufferQueueProducer的queueBuffer函数的最后几行，listener-&gt;onFrameAvailable()。 listener最终通过回调，会回到Layer当中，所以最终调用Layer的onFrameAvailable接口，我们看看它的实现： [Layer.cpp] 123456789101112void Layer::onFrameAvailable(const BufferItem&amp; item) &#123;// Add this buffer from our internal queue tracker&#123; // Autolock scope ...... mQueueItems.push_back(item); android_atomic_inc(&amp;mQueuedFrames); // Wake up any pending callbacks mLastFrameNumberReceived = item.mFrameNumber; mQueueItemCondition.broadcast();&#125;mFlinger-&gt;signalLayerUpdate();&#125; 这里又调用SurfaceFlinger的signalLayerUpdate函数，继续查看： [SurfaceFlinger.cpp] 123void SurfaceFlinger::signalLayerUpdate() &#123;mEventQueue.invalidate();&#125; 这里又调用MessageQueue的invalidate函数： [MessageQueue.cpp] 123void MessageQueue::invalidate() &#123;mEvents-&gt;requestNextVsync();&#125; 贴一下SurfaceFlinger的初始化请求vsync信号流程图： 最终结果会走到SurfaceFlinger的vsync信号接收逻辑，即SurfaceFlinger的onMessageReceived函数： [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627282930void SurfaceFlinger::onMessageReceived(int32_t what) &#123;ATRACE_CALL();switch (what) &#123; case MessageQueue::INVALIDATE: &#123; bool frameMissed = !mHadClientComposition &amp;&amp; mPreviousPresentFence != Fence::NO_FENCE &amp;&amp; mPreviousPresentFence-&gt;getSignalTime() == INT64_MAX; ATRACE_INT(\"FrameMissed\", static_cast&lt;int&gt;(frameMissed)); if (mPropagateBackpressure &amp;&amp; frameMissed) &#123; signalLayerUpdate(); break; &#125; bool refreshNeeded = handleMessageTransaction(); refreshNeeded |= handleMessageInvalidate(); refreshNeeded |= mRepaintEverything; if (refreshNeeded) &#123; // Signal a refresh if a transaction modified the window state, // a new buffer was latched, or if HWC has requested a full // repaint signalRefresh(); &#125; break; &#125; case MessageQueue::REFRESH: &#123; handleMessageRefresh(); break; &#125;&#125;&#125; SurfaceFlinger收到了VSync信号后，调用了handleMessageRefresh函数 [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627void SurfaceFlinger::handleMessageRefresh() &#123;ATRACE_CALL();nsecs_t refreshStartTime = systemTime(SYSTEM_TIME_MONOTONIC);preComposition();rebuildLayerStacks();setUpHWComposer();doDebugFlashRegions();doComposition();postComposition(refreshStartTime);mPreviousPresentFence = mHwc-&gt;getRetireFence(HWC_DISPLAY_PRIMARY);mHadClientComposition = false;for (size_t displayId = 0; displayId &lt; mDisplays.size(); ++displayId) &#123; const sp&lt;DisplayDevice&gt;&amp; displayDevice = mDisplays[displayId]; mHadClientComposition = mHadClientComposition || mHwc-&gt;hasClientComposition(displayDevice-&gt;getHwcDisplayId());&#125;// Release any buffers which were replaced this framefor (auto&amp; layer : mLayersWithQueuedFrames) &#123; layer-&gt;releasePendingBuffer();&#125;mLayersWithQueuedFrames.clear();&#125; 我们主要看下下面几个函数。 [SurfaceFlinger.cpp] 123456preComposition();rebuildLayerStacks();setUpHWComposer();doDebugFlashRegions();doComposition();postComposition(refreshStartTime); 一、preComposition()函数我们先来看第一个函数preComposition() [SurfaceFlinger.cpp] 1234567891011121314void SurfaceFlinger::preComposition()&#123;bool needExtraInvalidate = false;const LayerVector&amp; layers(mDrawingState.layersSortedByZ);const size_t count = layers.size();for (size_t i=0 ; i&lt;count ; i++) &#123; if (layers[i]-&gt;onPreComposition()) &#123; needExtraInvalidate = true; &#125;&#125;if (needExtraInvalidate) &#123; signalLayerUpdate();&#125;&#125; 上面函数先是调用了mDrawingState的layersSortedByZ来得到上次绘图的Layer层列表。并不是所有的Layer都会参与屏幕图像的绘制，因此SurfaceFlinger用state对象来记录参与绘制的Layer对象。 记得我们之前分析过createLayer函数来创建Layer，创建之后会调用addClientLayer函数。 [SurfaceFlinger.cpp] 1234567891011121314151617181920status_t SurfaceFlinger::addClientLayer(const sp&lt;Client&gt;&amp; client, const sp&lt;IBinder&gt;&amp; handle, const sp&lt;IGraphicBufferProducer&gt;&amp; gbc, const sp&lt;Layer&gt;&amp; lbc) &#123;// add this layer to the current state list&#123; Mutex::Autolock _l(mStateLock); if (mCurrentState.layersSortedByZ.size() &gt;= MAX_LAYERS) &#123; return NO_MEMORY; &#125; mCurrentState.layersSortedByZ.add(lbc); mGraphicBufferProducerList.add(IInterface::asBinder(gbc));&#125;// attach this layer to the clientclient-&gt;attachLayer(handle, lbc);return NO_ERROR;&#125; 我们来看下addClientLayer函数，这里会把Layer对象放在mCurrentState的layersSortedByZ对象中。而mDrawingState和mCurrentState什么关系呢？在后面我们会介绍，mDrawingState代表上一次绘图时的状态，处理完之后会把mCurrentState赋给mDrawingState。 回到preComposition函数，遍历所有的Layer对象，调用其onPreComposition函数来检测Layer层中的图像是否有变化。 1.1、每个Layer的onFrameAvailable函数onPreComposition函数来根据mQueuedFrames来判断图像是否发生了变化，或者是mSidebandStreamChanged、mAutoRefresh。 [Layer.cpp] 1234bool Layer::onPreComposition() &#123;mRefreshPending = false;return mQueuedFrames &gt; 0 || mSidebandStreamChanged || mAutoRefresh;&#125; 当Layer所对应的Surface更新图像后，它所对应的Layer对象的onFrameAvailable函数会被调用来通知这种变化。 在SurfaceFlinger的preComposition函数中当有Layer的图像改变了，最后也会调用SurfaceFlinger的signalLayerUpdate函数。 SurfaceFlinger::signalLayerUpdate是调用了MessageQueue的invalidate函数 最后处理还是调用了SurfaceFlinger的onMessageReceived函数。看看SurfaceFlinger的onMessageReceived函数对NVALIDATE的处理 handleMessageInvalidate函数中调用了handlePageFlip函数，这个函数将会处理Layer中的缓冲区，把更新过的图像缓冲区切换到前台，等待VSync信号更新到FrameBuffer。 1.2、绘制流程具体完整的绘制流程如图。 二、handleTransaction handPageFlip更新Layer对象在上一节中的绘图的流程中，我们看到了handleTransaction和handPageFlip这两个函数通常是在用户进程更新Surface图像时会调用，来更新Layer对象。这节就主要讲解这两个函数。 2.1、handleTransaction函数handleTransaction函数的参数是transactionFlags，不过函数中没有使用这个参数，而是通过getTransactionFlags(eTransactionMask)来重新对transactionFlags赋值，然后使用它作为参数来调用函数 handleTransactionLocked。 [SurfaceFlinger.cpp] 123456789101112131415void SurfaceFlinger::handleTransaction(uint32_t transactionFlags)&#123;ATRACE_CALL();Mutex::Autolock _l(mStateLock);const nsecs_t now = systemTime();mDebugInTransaction = now;transactionFlags = getTransactionFlags(eTransactionMask);handleTransactionLocked(transactionFlags);mLastTransactionTime = systemTime() - now;mDebugInTransaction = 0;invalidateHwcGeometry();&#125; getTransactionFlags函数的参数是eTransactionMask只是屏蔽其他位。 handleTransactionLocked函数会调用每个Layer类的doTransaction函数，在分析handleTransactionLocked函数之前，我们先看看Layer类 的doTransaction函数。 2.2、Layer的doTransaction函数下面是Layer的doTransaction函数代码 [Layer.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475uint32_t Layer::doTransaction(uint32_t flags) &#123;ATRACE_CALL();pushPendingState();//上次绘制的State对象 Layer::State c = getCurrentState();//当前使用的State对象const Layer::State&amp; s(getDrawingState());const bool sizeChanged = (c.requested.w != s.requested.w) || (c.requested.h != s.requested.h);if (sizeChanged) &#123; // the size changed, we need to ask our client to request a new buffer //如果Layer的尺寸发生变化，就要改变Surface的缓冲区的尺寸 // record the new size, form this point on, when the client request // a buffer, it'll get the new size. mSurfaceFlingerConsumer-&gt;setDefaultBufferSize( c.requested.w, c.requested.h);&#125;const bool resizePending = (c.requested.w != c.active.w) || (c.requested.h != c.active.h);if (!isFixedSize()) &#123; if (resizePending &amp;&amp; mSidebandStream == NULL) &#123; //如果Layer不是固定尺寸的类型，比较它的实际大小和要求的改变大小 flags |= eDontUpdateGeometryState; &#125;&#125;//如果没有eDontUpdateGeometryState标志，更新active的值为request if (flags &amp; eDontUpdateGeometryState) &#123;&#125; else &#123; Layer::State&amp; editCurrentState(getCurrentState()); if (mFreezePositionUpdates) &#123; float tx = c.active.transform.tx(); float ty = c.active.transform.ty(); c.active = c.requested; c.active.transform.set(tx, ty); editCurrentState.active = c.active; &#125; else &#123; editCurrentState.active = editCurrentState.requested; c.active = c.requested; &#125;&#125;// 如果当前state的active和以前的State的active不等，设置更新标志 if (s.active != c.active) &#123; // invalidate and recompute the visible regions if needed flags |= Layer::eVisibleRegion;&#125;//如果当前state的sequence和以前state的sequence不等，设置更新标志if (c.sequence != s.sequence) &#123; // invalidate and recompute the visible regions if needed flags |= eVisibleRegion; this-&gt;contentDirty = true; // we may use linear filtering, if the matrix scales us const uint8_t type = c.active.transform.getType(); mNeedsFiltering = (!c.active.transform.preserveRects() || (type &gt;= Transform::SCALE));&#125;// If the layer is hidden, signal and clear out all local sync points so// that transactions for layers depending on this layer's frames becoming// visible are not blockedif (c.flags &amp; layer_state_t::eLayerHidden) &#123; Mutex::Autolock lock(mLocalSyncPointMutex); for (auto&amp; point : mLocalSyncPoints) &#123; point-&gt;setFrameAvailable(); &#125; mLocalSyncPoints.clear();&#125;// Commit the transactioncommitTransaction(c);return flags;&#125; Layer类中的两个类型为Layer::State的成员变量mDrawingState、mCurrentState，这里为什么要两个对象呢？Layer对象在绘制图形时，使用的是mDrawingState变量，用户调用接口设置Layer对象属性是，设置的值保存在mCurrentState对象中，这样就不会因为用户的操作而干扰Layer对象的绘制了。 Layer的doTransaction函数据你是比较这两个变量，如果有不同的地方，说明在上次绘制以后，用户改变的Layer的设置，要把这种变化通过flags返回。 State的结构中有两个Geometry字段，active和requested。他们表示layer的尺寸，其中requested保存是用户设置的尺寸，而active保存的值通过计算后的实际尺寸。 State中的z字段的值就是Layer在显示轴的位置，值越小位置越靠下。 layerStack字段是用户指定的一个值，用户可以给DisplayDevice也指定一个layerStack值，只有Layer对象和DisplayDevice对象的layerStack相等，这个Layer才能在这个显示设备上输出，这样的好处是可以让显示设备只显示某个Surface的内容。例如，可以让HDMI显示设备只显示手机上播放视频的Surface窗口，但不显示Activity窗口。 sequence字段是个序列值，每当用户调用了Layer的接口，例如setAlpha、setSize或者setLayer等改变Layer对象属性的哈数，这个值都会加1。因此在doTransaction函数中能通过比较sequence值来判断Layer的属性值有没有变化。 doTransaction函数最后会调用commitTransaction函数，就是把mCurrentState赋值给mDrawingState [Layer.cpp] 123void Layer::commitTransaction(const State&amp; stateToCommit) &#123;mDrawingState = stateToCommit;&#125; 2.3、handleTransactionLocked函数下面我们来分析handleTransactionLocked函数，这个函数比较长，我们分段分析 2.3.1 处理Layer的事务 [SurfaceFlinger.cpp] 123456789101112131415161718192021void SurfaceFlinger::handleTransactionLocked(uint32_t transactionFlags)&#123;const LayerVector&amp; currentLayers(mCurrentState.layersSortedByZ);const size_t count = currentLayers.size();// Notify all layers of available framesfor (size_t i = 0; i &lt; count; ++i) &#123; currentLayers[i]-&gt;notifyAvailableFrames();&#125;if (transactionFlags &amp; eTraversalNeeded) &#123; for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); uint32_t trFlags = layer-&gt;getTransactionFlags(eTransactionNeeded); if (!trFlags) continue; const uint32_t flags = layer-&gt;doTransaction(0); if (flags &amp; Layer::eVisibleRegion) mVisibleRegionsDirty = true; &#125;&#125; 在SurfaceFlinger中也有两个类型为State的变量mCurrentState和mDrawingState，但是和Layer中的不要混起来。它的名字相同而已 1234 struct State &#123; LayerVector layersSortedByZ; DefaultKeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt; displays;&#125;; 结构layersSortedByZ字段保存所有参与绘制的Layer对象，而字段displays保存的是所有输出设备的DisplayDeviceState对象 这里用两个变量的目的是和Layer中使用两个变量是一样的。 上面代码根据eTraversalNeeded标志来决定是否要检查所有的Layer对象。如果某个Layer对象中有eTransactionNeeded标志，将调用它的doTransaction函数。Layer的doTransaction函数返回的flags如果有eVisibleRegion，说明这个Layer需要更新，就把mVisibleRegionsDirty设置为true 2.3.2、处理显示设备的变化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172 if (transactionFlags &amp; eDisplayTransactionNeeded) &#123; // here we take advantage of Vector's copy-on-write semantics to // improve performance by skipping the transaction entirely when // know that the lists are identical const KeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt;&amp; curr(mCurrentState.displays); const KeyedVector&lt; wp&lt;IBinder&gt;, DisplayDeviceState&gt;&amp; draw(mDrawingState.displays); if (!curr.isIdenticalTo(draw)) &#123; mVisibleRegionsDirty = true; const size_t cc = curr.size(); size_t dc = draw.size(); // find the displays that were removed // (ie: in drawing state but not in current state) // also handle displays that changed // (ie: displays that are in both lists) for (size_t i=0 ; i&lt;dc ; i++) &#123; const ssize_t j = curr.indexOfKey(draw.keyAt(i)); if (j &lt; 0) &#123; // in drawing state but not in current state if (!draw[i].isMainDisplay()) &#123; // Call makeCurrent() on the primary display so we can // be sure that nothing associated with this display // is current. const sp&lt;const DisplayDevice&gt; defaultDisplay(getDefaultDisplayDevice()); defaultDisplay-&gt;makeCurrent(mEGLDisplay, mEGLContext); sp&lt;DisplayDevice&gt; hw(getDisplayDevice(draw.keyAt(i))); if (hw != NULL) hw-&gt;disconnect(getHwComposer()); if (draw[i].type &lt; DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES) mEventThread-&gt;onHotplugReceived(draw[i].type, false); mDisplays.removeItem(draw.keyAt(i)); &#125; else &#123; ALOGW(\"trying to remove the main display\"); &#125; &#125; else &#123; // this display is in both lists. see if something changed. const DisplayDeviceState&amp; state(curr[j]); const wp&lt;IBinder&gt;&amp; display(curr.keyAt(j)); const sp&lt;IBinder&gt; state_binder = IInterface::asBinder(state.surface); const sp&lt;IBinder&gt; draw_binder = IInterface::asBinder(draw[i].surface); if (state_binder != draw_binder) &#123; // changing the surface is like destroying and // recreating the DisplayDevice, so we just remove it // from the drawing state, so that it get re-added // below. sp&lt;DisplayDevice&gt; hw(getDisplayDevice(display)); if (hw != NULL) hw-&gt;disconnect(getHwComposer()); mDisplays.removeItem(display); mDrawingState.displays.removeItemsAt(i); dc--; i--; // at this point we must loop to the next item continue; &#125; const sp&lt;DisplayDevice&gt; disp(getDisplayDevice(display)); if (disp != NULL) &#123; if (state.layerStack != draw[i].layerStack) &#123; disp-&gt;setLayerStack(state.layerStack); &#125; if ((state.orientation != draw[i].orientation) || (state.viewport != draw[i].viewport) || (state.frame != draw[i].frame)) &#123; disp-&gt;setProjection(state.orientation, state.viewport, state.frame); &#125; if (state.width != draw[i].width || state.height != draw[i].height) &#123; disp-&gt;setDisplaySize(state.width, state.height); &#125; &#125; &#125; &#125; // find displays that were added // (ie: in current state but not in drawing state) for (size_t i=0 ; i&lt;cc ; i++) &#123; if (draw.indexOfKey(curr.keyAt(i)) &lt; 0) &#123; const DisplayDeviceState&amp; state(curr[i]); sp&lt;DisplaySurface&gt; dispSurface; sp&lt;IGraphicBufferProducer&gt; producer; sp&lt;IGraphicBufferProducer&gt; bqProducer; sp&lt;IGraphicBufferConsumer&gt; bqConsumer; BufferQueue::createBufferQueue(&amp;bqProducer, &amp;bqConsumer, new GraphicBufferAlloc()); int32_t hwcDisplayId = -1; if (state.isVirtualDisplay()) &#123; // Virtual displays without a surface are dormant: // they have external state (layer stack, projection, // etc.) but no internal state (i.e. a DisplayDevice). if (state.surface != NULL) &#123; int width = 0; DisplayUtils* displayUtils = DisplayUtils::getInstance(); int status = state.surface-&gt;query( NATIVE_WINDOW_WIDTH, &amp;width); ALOGE_IF(status != NO_ERROR, \"Unable to query width (%d)\", status); int height = 0; status = state.surface-&gt;query( NATIVE_WINDOW_HEIGHT, &amp;height); ALOGE_IF(status != NO_ERROR, \"Unable to query height (%d)\", status); if (MAX_VIRTUAL_DISPLAY_DIMENSION == 0 || (width &lt;= MAX_VIRTUAL_DISPLAY_DIMENSION &amp;&amp; height &lt;= MAX_VIRTUAL_DISPLAY_DIMENSION)) &#123; int usage = 0; status = state.surface-&gt;query( NATIVE_WINDOW_CONSUMER_USAGE_BITS, &amp;usage); ALOGW_IF(status != NO_ERROR, \"Unable to query usage (%d)\", status); if ( (status == NO_ERROR) &amp;&amp; displayUtils-&gt;canAllocateHwcDisplayIdForVDS(usage)) &#123; hwcDisplayId = allocateHwcDisplayId(state.type); &#125; &#125; displayUtils-&gt;initVDSInstance(mHwc, hwcDisplayId, state.surface, dispSurface, producer, bqProducer, bqConsumer, state.displayName, state.isSecure, state.type); &#125; &#125; else &#123; ALOGE_IF(state.surface!=NULL, \"adding a supported display, but rendering \" \"surface is provided (%p), ignoring it\", state.surface.get()); hwcDisplayId = allocateHwcDisplayId(state.type); // for supported (by hwc) displays we provide our // own rendering surface dispSurface = new FramebufferSurface(*mHwc, state.type, bqConsumer); producer = bqProducer; &#125; const wp&lt;IBinder&gt;&amp; display(curr.keyAt(i)); if (dispSurface != NULL &amp;&amp; producer != NULL) &#123; sp&lt;DisplayDevice&gt; hw = new DisplayDevice(this, state.type, hwcDisplayId, mHwc-&gt;getFormat(hwcDisplayId), state.isSecure, display, dispSurface, producer, mRenderEngine-&gt;getEGLConfig()); hw-&gt;setLayerStack(state.layerStack); hw-&gt;setProjection(state.orientation, state.viewport, state.frame); hw-&gt;setDisplayName(state.displayName); // When a new display device is added update the active // config by querying HWC otherwise the default config // (config 0) will be used. if (hwcDisplayId &gt;= DisplayDevice::DISPLAY_PRIMARY &amp;&amp; hwcDisplayId &lt; DisplayDevice::NUM_BUILTIN_DISPLAY_TYPES) &#123; int activeConfig = mHwc-&gt;getActiveConfig(hwcDisplayId); if (activeConfig &gt;= 0) &#123; hw-&gt;setActiveConfig(activeConfig); &#125; &#125; mDisplays.add(display, hw); if (state.isVirtualDisplay()) &#123; if (hwcDisplayId &gt;= 0) &#123; mHwc-&gt;setVirtualDisplayProperties(hwcDisplayId, hw-&gt;getWidth(), hw-&gt;getHeight(), hw-&gt;getFormat()); &#125; &#125; else &#123; mEventThread-&gt;onHotplugReceived(state.type, true); &#125; &#125; &#125; &#125; &#125;&#125; 这段代码的作用是处理显示设备的变化，分成3种情况： 1.显示设备减少了，需要把显示设备对应的DisplayDevice移除 2.显示设备发生了变化，例如用户设置了Surface、重新设置了layerStack、旋转了屏幕等，这就需要重新设置显示对象的属性 3.显示设备增加了，创建新的DisplayDevice加入系统中。 2.3.3、设置TransfromHit12345678910111213141516171819202122232425262728293031323334353637383940 if (transactionFlags &amp; (eTraversalNeeded|eDisplayTransactionNeeded)) &#123; ...... sp&lt;const DisplayDevice&gt; disp; uint32_t currentlayerStack = 0; for (size_t i=0; i&lt;count; i++) &#123; // NOTE: we rely on the fact that layers are sorted by // layerStack first (so we don't have to traverse the list // of displays for every layer). const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); uint32_t layerStack = layer-&gt;getDrawingState().layerStack; if (i==0 || currentlayerStack != layerStack) &#123; currentlayerStack = layerStack; // figure out if this layerstack is mirrored // (more than one display) if so, pick the default display, // if not, pick the only display it's on. disp.clear(); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); if (hw-&gt;getLayerStack() == currentlayerStack) &#123; if (disp == NULL) &#123; disp = hw; &#125; else &#123; disp = NULL; break; &#125; &#125; &#125; &#125; if (disp == NULL) &#123; // NOTE: TEMPORARY FIX ONLY. Real fix should cause layers to // redraw after transform hint changes. See bug 8508397. // could be null when this layer is using a layerStack // that is not visible on any display. Also can occur at // screen off/on times. disp = getDefaultDisplayDevice(); &#125; layer-&gt;updateTransformHint(disp); &#125;&#125; 这段代码的作用是根据每种显示设备的不同，设置和显示设备关联在一起的Layer（主要看Layer的layerStack是否和DisplayDevice的layerStack）的TransformHint（主要指设备的显示方向orientation）。 2.3.4、处理Layer增加情况123456789101112131415161718192021222324252627282930/* * Perform our own transaction if needed */const LayerVector&amp; layers(mDrawingState.layersSortedByZ);if (currentLayers.size() &gt; layers.size()) &#123; // layers have been added mVisibleRegionsDirty = true;&#125;// some layers might have been removed, so// we need to update the regions they're exposing.if (mLayersRemoved) &#123; mLayersRemoved = false; mVisibleRegionsDirty = true; const size_t count = layers.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); if (currentLayers.indexOf(layer) &lt; 0) &#123; // this layer is not visible anymore // TODO: we could traverse the tree from front to back and // compute the actual visible region // TODO: we could cache the transformed region const Layer::State&amp; s(layer-&gt;getDrawingState()); Region visibleReg = s.active.transform.transform( Region(Rect(s.active.w, s.active.h))); invalidateLayerStack(s.layerStack, visibleReg); &#125; &#125;&#125; 这段代码处理Layer的增加情况，如果Layer增加了，需要重新计算设备的更新区域，因此把mVisibleRegionsDirty设为true，如果Layer删除了，需要把Layer的可见区域加入到系统需要更新的区域中。 2.3.5、设置mDrawingState123 commitTransaction();updateCursorAsync();&#125; 调用commitTransaction和updateCursorAsync函数 commitTransaction函数作用是把mDrawingState的值设置成mCurrentState的值。而updateCursorAsync函数会更新所有显示设备中光标的位置。 2.3.6 小结 handleTransaction函数的作用的就是处理系统在两次刷新期间的各种变化。SurfaceFlinger模块中不管是SurfaceFlinger类还是Layer类，都采用了双缓冲的方式来保存他们的属性，这样的好处是刚改变SurfaceFlinger对象或者Layer类对象的属性是，不需要上锁，大大的提高了系统效率。只有在最后的图像输出是，才进行一次上锁，并进行内存的属性变化处理。正因此，应用进程必须收到VSync信号才开始改变Surface的内容。 2.4、handlePageFlip函数handlePageFlip函数代码如下： [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051bool SurfaceFlinger::handlePageFlip()&#123;Region dirtyRegion;bool visibleRegions = false;const LayerVector&amp; layers(mDrawingState.layersSortedByZ);bool frameQueued = false;// Store the set of layers that need updates. This set must not change as// buffers are being latched, as this could result in a deadlock.// Example: Two producers share the same command stream and:// 1.) Layer 0 is latched// 2.) Layer 0 gets a new frame// 2.) Layer 1 gets a new frame// 3.) Layer 1 is latched.// Display is now waiting on Layer 1's frame, which is behind layer 0's// second frame. But layer 0's second frame could be waiting on display.Vector&lt;Layer*&gt; layersWithQueuedFrames;for (size_t i = 0, count = layers.size(); i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); if (layer-&gt;hasQueuedFrame()) &#123; frameQueued = true; if (layer-&gt;shouldPresentNow(mPrimaryDispSync)) &#123; layersWithQueuedFrames.push_back(layer.get()); &#125; else &#123; layer-&gt;useEmptyDamage(); &#125; &#125; else &#123; layer-&gt;useEmptyDamage(); &#125;&#125;for (size_t i = 0, count = layersWithQueuedFrames.size() ; i&lt;count ; i++) &#123; Layer* layer = layersWithQueuedFrames[i]; const Region dirty(layer-&gt;latchBuffer(visibleRegions)); layer-&gt;useSurfaceDamage(); const Layer::State&amp; s(layer-&gt;getDrawingState()); invalidateLayerStack(s.layerStack, dirty);&#125;mVisibleRegionsDirty |= visibleRegions;// If we will need to wake up at some time in the future to deal with a// queued frame that shouldn't be displayed during this vsync period, wake// up during the next vsync period to check again.if (frameQueued &amp;&amp; layersWithQueuedFrames.empty()) &#123; signalLayerUpdate();&#125;// Only continue with the refresh if there is actually new work to doreturn !layersWithQueuedFrames.empty();&#125; handlePageFlip函数先调用每个Layer对象的hasQueuedFrame函数，确定这个Layer对象是否有需要更新的图层，然后把需要更新的Layer对象放到layersWithQueuedFrames中。 我们先来看Layer的hasQueuedFrame方法就是看其mQueuedFrames是否大于0 和mSidebandStreamChanged。前面小节分析只要Surface有数据写入，就会调用Layer的onFrameAvailable函数，然后mQueuedFrames值加1. 继续看handlePageFlip函数，接着调用需要更新的Layer对象的latchBuffer函数，然后根据返回的更新区域调用invalidateLayerStack函数来设置更新设备对象的更新区域。 下面我们看看latchBuffer函数 LatchBuffer函数调用updateTextImage来得到需要的图像。这里参数r是Reject对象，其作用是判断在缓冲区的尺寸是否符合要求。调用updateTextImage函数如果得到的结果是PRESENT_LATER,表示推迟处理，然后调用signalLayerUpdate函数来发送invalidate消息，这次绘制过程就不处理这个Surface的图像了。 如果不需要推迟处理，把mQueuedFrames的值减1. 最后LatchBuffer函数调用mSurfaceFlingerConsumer的getCurrentBuffer来取回当前的图像缓冲区指针，保存在mActiveBuffer中。 2.5 小结这样经过handleTransaction handlePageFlip两个函数处理，SurfaceFlinger中无论是Layer属性的变化还是图像的变化都处理好了，只等VSync信号到来就可以输出了。 三、rebuildLayerStacks函数前面介绍，VSync信号到来后，先是调用了rebuildLayerStacks函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445void SurfaceFlinger::rebuildLayerStacks() &#123;updateExtendedMode();// rebuild the visible layer list per screenif (CC_UNLIKELY(mVisibleRegionsDirty)) &#123; ATRACE_CALL(); mVisibleRegionsDirty = false; invalidateHwcGeometry(); //计算每个显示设备上可见的Layer const LayerVector&amp; layers(mDrawingState.layersSortedByZ); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; Region opaqueRegion; Region dirtyRegion; Vector&lt; sp&lt;Layer&gt; &gt; layersSortedByZ; const sp&lt;DisplayDevice&gt;&amp; hw(mDisplays[dpy]); const Transform&amp; tr(hw-&gt;getTransform()); const Rect bounds(hw-&gt;getBounds()); if (hw-&gt;isDisplayOn()) &#123; //计算每个layer的可见区域，确定设备需要重新绘制的区域 computeVisibleRegions(hw-&gt;getHwcDisplayId(), layers, hw-&gt;getLayerStack(), dirtyRegion, opaqueRegion); const size_t count = layers.size(); for (size_t i=0 ; i&lt;count ; i++) &#123; const sp&lt;Layer&gt;&amp; layer(layers[i]); &#123; //只需要和显示设备的LayerStack相同的layer Region drawRegion(tr.transform( layer-&gt;visibleNonTransparentRegion)); drawRegion.andSelf(bounds); if (!drawRegion.isEmpty()) &#123; //如果Layer的显示区域和显示设备的窗口有交集 //把Layer加入列表中 layersSortedByZ.add(layer); &#125; &#125; &#125; &#125; //设置显示设备的可见Layer列表 hw-&gt;setVisibleLayersSortedByZ(layersSortedByZ); hw-&gt;undefinedRegion.set(bounds); hw-&gt;undefinedRegion.subtractSelf(tr.transform(opaqueRegion)); hw-&gt;dirtyRegion.orSelf(dirtyRegion); &#125;&#125;&#125; rebuildLayerStacks函数的作用是重建每个显示设备的可见layer对象列表。对于按显示轴（Z轴）排列的Layer对象，排在最前面的当然会优先显示，但是Layer图像可能有透明域，也可能有尺寸没有覆盖整个屏幕，因此下面的layer也有显示的机会。rebuildLayerStacks函数对每个显示设备，先计算和显示设备具有相同layerStack值的Layer对象在该显示设备上的可见区域。然后将可见区域和显示设备的窗口区域有交集的layer组成一个新的列表，最后把这个列表设置到显示设备对象中。 computeVisibleRegions函数首先计算每个Layer在设备上的可见区域visibleRegion。计算方法就是用整个Layer的区域减去上层所有不透明区域aboveOpaqueLayers。而上层所有不透明区域值是一个逐层累计的过程，每层都需要把自己的不透明区域累加到aboveOpaqueLayers中。 而每层的不透明区域的计算方法：如果Layer的alpha的值为255，并且layer的isOpaque函数为true，则本层的不透明区域等于Layer所在区域，否则为0.这样一层层算下来，就很容易得到每层的可见区域大小了。 其次，计算整个显示设备需要更新的区域outDirtyRegion。outDirtyRegion的值也是累计所有层的需要重回的区域得到的。如果Layer中的显示内容发生了变化，则整个可见区域visibleRegion都需要更新，同时还要包括上一次的可见区域，然后在去掉被上层覆盖后的区域得到的就是Layer需要更新的区域。如果Layer显示的内容没有变化，但是考虑到窗口大小的变化或者上层窗口的变化，因此Layer中还是有区域可以需要重绘的地方。这种情况下最简单的算法是用Layer计算出可见区域减去以前的可见区域就可以了。但是在computeVisibleRegions函数还引入了被覆盖区域，通常被覆盖区域和可见区域并不重复，因此函数中计算暴露区域是用可见区域减去被覆盖区域的。 四、setUpHWComposer函数setUpHWComposer函数的作用是更新HWComposer对象中图层对象列表以及图层属性。 [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102void SurfaceFlinger::setUpHWComposer() &#123;for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; bool dirty = !mDisplays[dpy]-&gt;getDirtyRegion(false).isEmpty(); bool empty = mDisplays[dpy]-&gt;getVisibleLayersSortedByZ().size() == 0; bool wasEmpty = !mDisplays[dpy]-&gt;lastCompositionHadVisibleLayers; ...... bool mustRecompose = dirty &amp;&amp; !(empty &amp;&amp; wasEmpty); ...... mDisplays[dpy]-&gt;beginFrame(mustRecompose); if (mustRecompose) &#123; mDisplays[dpy]-&gt;lastCompositionHadVisibleLayers = !empty; &#125;&#125;//得到系统HWComposer对象 HWComposer&amp; hwc(getHwComposer());if (hwc.initCheck() == NO_ERROR) &#123; // build the h/w work list if (CC_UNLIKELY(mHwWorkListDirty)) &#123; mHwWorkListDirty = false; for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;= 0) &#123; const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers( hw-&gt;getVisibleLayersSortedByZ()); const size_t count = currentLayers.size(); //根据Layer数量在HWComposer中创建hwc_layer_list_t列表 if (hwc.createWorkList(id, count) == NO_ERROR) &#123; HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i=0 ; cur!=end &amp;&amp; i&lt;count ; ++i, ++cur) &#123; const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); layer-&gt;setGeometry(hw, *cur); if (mDebugDisableHWC || mDebugRegion || mDaltonize || mHasColorMatrix) &#123; cur-&gt;setSkip(true); &#125; &#125; &#125; &#125; &#125; &#125; // set the per-frame data for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;= 0) &#123; bool freezeSurfacePresent = false; isfreezeSurfacePresent(freezeSurfacePresent, hw, id); const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers( hw-&gt;getVisibleLayersSortedByZ()); const size_t count = currentLayers.size(); HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i=0 ; cur!=end &amp;&amp; i&lt;count ; ++i, ++cur) &#123; /* * update the per-frame h/w composer data for each layer * and build the transparent region of the FB */ const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); //将Layer的mActiveBuffer设置到HWComposer中 layer-&gt;setPerFrameData(hw, *cur); setOrientationEventControl(freezeSurfacePresent,id); &#125; &#125; &#125; // If possible, attempt to use the cursor overlay on each display. for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;= 0) &#123; const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers( hw-&gt;getVisibleLayersSortedByZ()); const size_t count = currentLayers.size(); HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i=0 ; cur!=end &amp;&amp; i&lt;count ; ++i, ++cur) &#123; const sp&lt;Layer&gt;&amp; layer(currentLayers[i]); if (layer-&gt;isPotentialCursor()) &#123; cur-&gt;setIsCursorLayerHint(); break; &#125; &#125; &#125; &#125; dumpDrawCycle(true); status_t err = hwc.prepare(); ALOGE_IF(err, \"HWComposer::prepare failed (%s)\", strerror(-err)); for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); hw-&gt;prepareFrame(hwc); &#125;&#125;&#125; HWComposer中有一个类型为DisplayData结构的数组mDisplayData，它维护着每个显示设备的信息。DisplayData结构中有一个类型为hwc_display_contents_l字段list，这个字段又有一个hwc_layer_l类型的数组hwLayers，记录该显示设备所有需要输出的Layer信息。 setUpHWComposer函数调用HWComposer的createWorkList函数就是根据每种显示设备的Layer数量，创建和初始化hwc_display_contents_l对象和hwc_layer_l数组 创建完HWComposer中的列表后，接下来是对每个Layer对象调用它的setPerFrameData函数，参数是HWComposer和HWCLayerInterface。setPerFrameData函数将Layer对象的当前图像缓冲区mActiveBuffer设置到HWCLayerInterface对象对应的hwc_layer_l对象中。 HWComposer类中除了前面介绍的Gralloc还管理着Composer模块，这个模块实现了硬件的图像合成功能。setUpHWComposer函数接下来调用HWComposer类的prepare函数，而prepare函数会调用Composer模块的prepare接口。最后到各个厂家的实现hwc_prepare函数将每种HWComposer中的所有图层的类型都设置为HWC_FRAMEBUFFER就结束了。 五、合成所有层的图像 （doComposition()函数）doComposition函数是合成所有层的图像，代码如下： [SurfaceFlinger.cpp] 123456789101112131415161718192021void SurfaceFlinger::doComposition() &#123;ATRACE_CALL();const bool repaintEverything = android_atomic_and(0, &amp;mRepaintEverything);for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; const sp&lt;DisplayDevice&gt;&amp; hw(mDisplays[dpy]); if (hw-&gt;isDisplayOn()) &#123; // transform the dirty region into this screen's coordinate space const Region dirtyRegion(hw-&gt;getDirtyRegion(repaintEverything)); // repaint the framebuffer (if needed) doDisplayComposition(hw, dirtyRegion); hw-&gt;dirtyRegion.clear(); hw-&gt;flip(hw-&gt;swapRegion); hw-&gt;swapRegion.clear(); &#125; // inform the h/w that we're done compositing hw-&gt;compositionComplete();&#125;postFramebuffer();&#125; doComposition函数针对每种显示设备调用doDisplayComposition函数来合成，合成后调用postFramebuffer函数，我们先来看看doDisplayComposition函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950void SurfaceFlinger::doDisplayComposition(const sp&lt;const DisplayDevice&gt;&amp; hw, const Region&amp; inDirtyRegion) &#123;// We only need to actually compose the display if:// 1) It is being handled by hardware composer, which may need this to// keep its virtual display state machine in sync, or// 2) There is work to be done (the dirty region isn't empty)bool isHwcDisplay = hw-&gt;getHwcDisplayId() &gt;= 0;if (!isHwcDisplay &amp;&amp; inDirtyRegion.isEmpty()) &#123; ALOGV(\"Skipping display composition\"); return;&#125;ALOGV(\"doDisplayComposition\");Region dirtyRegion(inDirtyRegion);// compute the invalid region//swapRegion设置为需要更新的区域 hw-&gt;swapRegion.orSelf(dirtyRegion);uint32_t flags = hw-&gt;getFlags();//获得显示设备支持的更新方式标志 if (flags &amp; DisplayDevice::SWAP_RECTANGLE) &#123; // we can redraw only what's dirty, but since SWAP_RECTANGLE only // takes a rectangle, we must make sure to update that whole // rectangle in that case dirtyRegion.set(hw-&gt;swapRegion.bounds());&#125; else &#123; if (flags &amp; DisplayDevice::PARTIAL_UPDATES) &#123;//支持部分更新 // We need to redraw the rectangle that will be updated // (pushed to the framebuffer). // This is needed because PARTIAL_UPDATES only takes one // rectangle instead of a region (see DisplayDevice::flip()) //将更新区域调整为整个窗口大小 dirtyRegion.set(hw-&gt;swapRegion.bounds()); &#125; else &#123; // we need to redraw everything (the whole screen) dirtyRegion.set(hw-&gt;bounds()); hw-&gt;swapRegion = dirtyRegion; &#125;&#125;//合成 if (!doComposeSurfaces(hw, dirtyRegion)) return;// update the swap region and clear the dirty regionhw-&gt;swapRegion.orSelf(dirtyRegion);//没有硬件composer的情况，输出图像// swap buffers (presentation)hw-&gt;swapBuffers(getHwComposer());&#125; doDisplayComposition函数根据显示设备支持的更新方式，重新设置需要更新区域的大小。 真正的合成工作是在doComposerSurfaces函数中完成，这个函数在layer的类型为HWC_FRAMEBUFFER,或者不支持硬件的composer的情况下，调用layer的draw函数来一层一层低合成最后的图像。 合成完后，doDisplayComposition函数调用了hw的swapBuffers函数，这个函数前面介绍过了，它将在系统不支持硬件的composer情况下调用eglSwapBuffers来输出图像到显示设备。 六、postFramebuffer函数上一节的doComposition函数最后调用了postFramebuffer函数，代码如下： [SurfaceFlinger.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051void SurfaceFlinger::postFramebuffer()&#123;ATRACE_CALL();const nsecs_t now = systemTime();mDebugInSwapBuffers = now;HWComposer&amp; hwc(getHwComposer());if (hwc.initCheck() == NO_ERROR) &#123; if (!hwc.supportsFramebufferTarget()) &#123; // EGL spec says: // \"surface must be bound to the calling thread's current context, // for the current rendering API.\" getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext); &#125; hwc.commit();&#125;// make the default display current because the VirtualDisplayDevice code cannot// deal with dequeueBuffer() being called outside of the composition loop; however// the code below can call glFlush() which is allowed (and does in some case) call// dequeueBuffer().getDefaultDisplayDevice()-&gt;makeCurrent(mEGLDisplay, mEGLContext);for (size_t dpy=0 ; dpy&lt;mDisplays.size() ; dpy++) &#123; sp&lt;const DisplayDevice&gt; hw(mDisplays[dpy]); const Vector&lt; sp&lt;Layer&gt; &gt;&amp; currentLayers(hw-&gt;getVisibleLayersSortedByZ()); hw-&gt;onSwapBuffersCompleted(hwc); const size_t count = currentLayers.size(); int32_t id = hw-&gt;getHwcDisplayId(); if (id &gt;=0 &amp;&amp; hwc.initCheck() == NO_ERROR) &#123; HWComposer::LayerListIterator cur = hwc.begin(id); const HWComposer::LayerListIterator end = hwc.end(id); for (size_t i = 0; cur != end &amp;&amp; i &lt; count; ++i, ++cur) &#123; currentLayers[i]-&gt;onLayerDisplayed(hw, &amp;*cur); &#125; &#125; else &#123; for (size_t i = 0; i &lt; count; i++) &#123; currentLayers[i]-&gt;onLayerDisplayed(hw, NULL); &#125; &#125;&#125;mLastSwapBufferTime = systemTime() - now;mDebugInSwapBuffers = 0;uint32_t flipCount = getDefaultDisplayDevice()-&gt;getPageFlipCount();if (flipCount % LOG_FRAME_STATS_PERIOD == 0) &#123; logFrameStats();&#125;&#125; postFramebuffer先判断系统是否支持composer，如果不支持，我们知道图像已经在doComposition函数时调用hw-&gt;swapBuffers输出了，就返回了。如果支持硬件composer，postFramebuffer函数将调用HWComposer的commit函数继续执行。 [HWComposer.cpp] 12345678910111213141516171819202122232425262728293031323334353637status_t HWComposer::commit() &#123;int err = NO_ERROR;if (mHwc) &#123; if (!hwcHasApiVersion(mHwc, HWC_DEVICE_API_VERSION_1_1)) &#123; // On version 1.0, the OpenGL ES target surface is communicated // by the (dpy, sur) fields and we are guaranteed to have only // a single display. mLists[0]-&gt;dpy = eglGetCurrentDisplay(); mLists[0]-&gt;sur = eglGetCurrentSurface(EGL_DRAW); &#125; for (size_t i=VIRTUAL_DISPLAY_ID_BASE; i&lt;mNumDisplays; i++) &#123; DisplayData&amp; disp(mDisplayData[i]); if (disp.outbufHandle) &#123; mLists[i]-&gt;outbuf = disp.outbufHandle; mLists[i]-&gt;outbufAcquireFenceFd = disp.outbufAcquireFence-&gt;dup(); &#125; &#125; err = mHwc-&gt;set(mHwc, mNumDisplays, mLists); for (size_t i=0 ; i&lt;mNumDisplays ; i++) &#123; DisplayData&amp; disp(mDisplayData[i]); disp.lastDisplayFence = disp.lastRetireFence; disp.lastRetireFence = Fence::NO_FENCE; if (disp.list) &#123; if (disp.list-&gt;retireFenceFd != -1) &#123; disp.lastRetireFence = new Fence(disp.list-&gt;retireFenceFd); disp.list-&gt;retireFenceFd = -1; &#125; disp.list-&gt;flags &amp;= ~HWC_GEOMETRY_CHANGED; &#125; &#125;&#125;return (status_t)err;&#125; /**************Vsync**************/ 参考文档（特别感谢）：Android6.0 显示系统（六） 图像的输出过程 - kc58236582的博客 - CSDN博客startActivity启动过程分析浅析Android的窗口 - DEV CLUBAndroid源码解析之（十四）–&gt;Activity启动流程Android应用setContentView与LayoutInflater加载解析机制源码分析Android应用程序窗口设计框架介绍 - 深入剖析Android系统 - CSDN博客Android显示系统设计框架介绍 - 深入剖析Android系统 - CSDN博客图解Android - Android GUI 系统 (2) - 窗口管理 (View, Canvas, Window Manager) - 漫天尘沙 - 博客园Android SurfaceFlinger 学习之路(五)—-VSync 工作原理 | April is your lieAndroid graphics 学习－生产者、消费者、BufferQueue介绍 - armwind的专栏 - CSDN博客 Graphics DEMO BingoAndroid 图形系统概述","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android 7.1.2 (Android N) Activity 启动流程 （AMS）分析","slug":"Android-7-1-2-Android-N-Activity启动流程分析","date":"2017-09-30T16:00:00.000Z","updated":"2018-04-19T14:29:42.809Z","comments":true,"path":"2017/10/01/Android-7-1-2-Android-N-Activity启动流程分析/","link":"","permalink":"http://zhoujinjian.cc/2017/10/01/Android-7-1-2-Android-N-Activity启动流程分析/","excerpt":"Activity启动流程概述：● 点击桌面App图标，Launcher进程采用Binder IPC向system_server进程发起startActivity请求；● system_server进程接收到请求后，向zygote进程发送创建进程的请求；● Zygote进程fork出新的子进程，即App进程；● App进程，通过Binder IPC向sytem_server进程发起attachApplication请求；● system_server进程在收到请求后，进行一系列准备工作后，再通过binder IPC向App进程发送scheduleLaunchActivity请求；● App进程的binder线程（ApplicationThread）在收到请求后，通过handler向主线程发送LAUNCH_ACTIVITY消息；● 主线程在收到Message后，通过发射机制创建目标Activity，并回调Activity.onCreate()等方法。","text":"Activity启动流程概述：● 点击桌面App图标，Launcher进程采用Binder IPC向system_server进程发起startActivity请求；● system_server进程接收到请求后，向zygote进程发送创建进程的请求；● Zygote进程fork出新的子进程，即App进程；● App进程，通过Binder IPC向sytem_server进程发起attachApplication请求；● system_server进程在收到请求后，进行一系列准备工作后，再通过binder IPC向App进程发送scheduleLaunchActivity请求；● App进程的binder线程（ApplicationThread）在收到请求后，通过handler向主线程发送LAUNCH_ACTIVITY消息；● 主线程在收到Message后，通过发射机制创建目标Activity，并回调Activity.onCreate()等方法。 一、概述基于Android 7.1.2的源码剖析， 分析Android Activity启动流程，相关源码： frameworks/base/services/core/java/com/android/server/am/● ActivityManagerService.java● ActivityStackSupervisor.java● ActivityStack.java● ActivityRecord.java● ProcessRecord.java● TaskRecord.java frameworks/base/services/core/java/com/android/server/pm/● PackageManagerService.java frameworks/base/core/java/android/os/● Process.java frameworks/base/core/java/android/app/● IActivityManager.java● ActivityManagerNative.java (内含AMP)● ActivityManager.java● Activity.java● ActivityThread.java● Instrumentation.java● IApplicationThread.java● ApplicationThreadNative.java (内含ATP)● ActivityThread.java (内含ApplicationThread)● ContextImpl.java 博客原图链接 主要对象功能介绍：● ActivityManagerService，简称AMS，服务端对象，负责系统中所有Activity的生命周期。● ActivityManagerNative继承Java的Binder类，同时实现了IActivityManager接口，即ActivityManagerNative将作为Binder通信的服务端为用户提供支持。● ActivityManagerProxy：在ActivityManagerNative类中定义了内部类ActivityManagerProxy，该类同样实现了IActivityManager接口，将作为客户端使用的服务端代理。● ActivityThread，App的真正入口。当开启App之后，会调用main()开始运行，开启消息循环队列，这就是传说中的UI线程或者叫主线程。与ActivityManagerServices配合，一起完成Activity的管理工作● ApplicationThread，用来实现ActivityManagerService与ActivityThread之间的交互。在ActivityManagerService需要管理相关Application中的Activity的生命周期时，通过ApplicationThread的代理对象与ActivityThread通讯。● ApplicationThreadProxy，是ApplicationThread在服务器端的代理，负责和客户端的ApplicationThread通讯。AMS就是通过该代理与ActivityThread进行通信的。● Instrumentation，每一个应用程序只有一个Instrumentation对象，每个Activity内都有一个对该对象的引用。Instrumentation可以理解为应用进程的管家，ActivityThread要创建或暂停某个Activity时，都需要通过Instrumentation来进行具体的操作。● ActivityStack，Activity在AMS的栈管理，用来记录已经启动的Activity的先后关系，状态信息等。通过ActivityStack决定是否需要启动新的进程。● ActivityRecord，ActivityStack的管理对象，每个Activity在AMS对应一个ActivityRecord，来记录Activity的状态以及其他的管理信息。其实就是服务器端的Activity对象的映像。● TaskRecord，AMS抽象出来的一个”任务”的概念，是记录ActivityRecord的栈，一个”Task”包含若干个ActivityRecord。AMS用TaskRecord确保Activity启动和退出的顺序。如果你清楚Activity的4种launchMode，那么对这个概念应该不陌生。 相关类的类图：（1）IActivityManager相关类（2）IApplicationThread相关类（3）ActivityManagerService相关类 1.1、Task和StackAndroid系统中的每一个Activity都位于一个Task中。一个Task可以包含多个Activity，同一个Activity也可能有多个实例。 在AndroidManifest.xml中，我们可以通过android:launchMode来控制Activity在Task中的实例。 另外，在startActivity的时候，我们也可以通过setFlag 来控制启动的Activity在Task中的实例。 Task管理的意义还在于近期任务列表以及Back栈。 当你通过多任务键（有些设备上是长按Home键，有些设备上是专门提供的多任务键）调出多任务时，其实就是从ActivityManagerService获取了最近启动的Task列表。 Back栈管理了当你在Activity上点击Back键，当前Activity销毁后应该跳转到哪一个Activity的逻辑。关于Task和Back栈，请参见这里：Tasks and Back Stack。 其实在ActivityManagerService与WindowManagerService内部管理中，在Task之外，还有一层容器，这个容器应用开发者和用户可能都不会感觉到或者用到，但它却非常重要，那就是Stack。 下文中，我们将看到，Android系统中的多窗口管理，就是建立在Stack的数据结构上的。 一个Stack中包含了多个Task，一个Task中包含了多个Activity（Window），下图描述了它们的关系： 另外还有一点需要注意的是，ActivityManagerService和WindowManagerService中的Task和Stack结构是一一对应的，对应关系对于如下： ActivityStack &lt;–&gt; TaskStack TaskRecord &lt;–&gt; Task 即，ActivityManagerService中的每一个ActivityStack或者TaskRecord在WindowManagerService中都有对应的TaskStack和Task，这两类对象都有唯一的id（id是int类型），它们通过id进行关联。 1.2、小结： 用户从Launcher程序点击应用图标可启动应用的入口Activity，Activity启动时需要多个进程之间的交互，Android系统中有一个zygote进程专用于孵化Android框架层和应用层程序的进程。还有一个system_server进程，该进程里运行了很多binder service，例如ActivityManagerService，PackageManagerService，WindowManagerService，这些binder service分别运行在不同的线程中，其中ActivityManagerService负责管理Activity栈，应用进程，task。 用户在Launcher程序里点击应用图标时，会通知ActivityManagerService启动应用的入口Activity，ActivityManagerService发现这个应用还未启动，则会通知Zygote进程孵化出应用进程，然后在这个应用进程里执行ActivityThread的main方法。应用进程接下来通知ActivityManagerService应用进程已启动，ActivityManagerService保存应用进程的一个代理对象，这样ActivityManagerService可以通过这个代理对象控制应用进程，然后ActivityManagerService通知应用进程创建入口Activity的实例，并执行它的生命周期方法。 总体启动流程图： 二、 开始请求执行启动Activity Activity.startActivity()Activity.startActivityForResult()Instrumentation.execStartActivity()ActivityManagerProxy.startActivity()ActivityManagerNative.onTransact()ActivityManagerService.startActivity() 2.1、Activity.startActivity()从Launcher启动应用的时候，经过调用会执行Activity中的startActivity。 12345678910111213141516[-&gt; Activity.java]......@Overridepublic void startActivity(Intent intent) &#123; this.startActivity(intent, null);&#125;......@Overridepublic void startActivity(Intent intent, @Nullable Bundle options) &#123; if (options != null) &#123; startActivityForResult(intent, -1, options); &#125; else &#123; ...... startActivityForResult(intent, -1); &#125;&#125; 2.2、Activity.startActivityForResult()1234567891011[-&gt; Activity.java] @Overridepublic void startActivityForResult( String who, Intent intent, int requestCode, @Nullable Bundle options) &#123; ...... Instrumentation.ActivityResult ar = mInstrumentation.execStartActivity( this, mMainThread.getApplicationThread(), mToken, who, intent, requestCode, options); ......&#125; 可以发现execStartActivity方法传递的几个参数： this，为启动Activity的对象； contextThread，为Binder对象，是主进程的context对象； token，也是一个Binder对象，指向了服务端一个ActivityRecord对象； target，为启动的Activity； intent，启动的Intent对象； requestCode，请求码； options，参数； 2.3、Instrumentation.execStartActivity()1234567891011121314151617[-&gt; Instrumentation.java] ......public ActivityResult execStartActivity( Context who, IBinder contextThread, IBinder token, String target, Intent intent, int requestCode, Bundle options) &#123; IApplicationThread whoThread = (IApplicationThread) contextThread; ...... try &#123; intent.migrateExtraStreamToClipData(); intent.prepareToLeaveProcess(who); int result = ActivityManagerNative.getDefault() .startActivity(whoThread, who.getBasePackageName(), intent, intent.resolveTypeIfNeeded(who.getContentResolver()), token, target, requestCode, 0, null, options); checkStartActivityResult(result, intent); &#125; ......&#125; 关于 ActivityManagerNative.getDefault()返回的是? 1234567 [-&gt;ActivityManagerNative.java]/** * Retrieve the system's default/global activity manager. */static public IActivityManager getDefault() &#123; return gDefault.get();&#125; 直接返回的是gDefault.get()，那么gDefault又是什么呢？ 1234567891011121314 [-&gt;ActivityManagerNative.java] private static final Singleton&lt;IActivityManager&gt; gDefault = new Singleton&lt;IActivityManager&gt;() &#123; protected IActivityManager create() &#123; IBinder b = ServiceManager.getService(\"activity\"); if (false) &#123; Log.v(\"ActivityManager\", \"default service binder = \" + b); &#125; IActivityManager am = asInterface(b); if (false) &#123; Log.v(\"ActivityManager\", \"default service = \" + am); &#125; return am; &#125;&#125;; 可以发现启动过asInterface()方法创建，然后我们继续看一下asInterface方法的实现： 123456789101112 static public IActivityManager asInterface(IBinder obj) &#123; if (obj == null) &#123; return null; &#125; IActivityManager in = (IActivityManager)obj.queryLocalInterface(descriptor); if (in != null) &#123; return in; &#125; return new ActivityManagerProxy(obj);&#125; 最后直接返回一个对象， 此处startActivity()的共有10个参数, 下面说说每个参数传递AMP.startActivity()每一项的对应值: caller: 当前应用的ApplicationThread对象mAppThread; callingPackage: 调用当前ContextImpl.getBasePackageName(),获取当前Activity所在包名; intent: 这便是启动Activity时,传递过来的参数; resolvedType: 调用intent.resolveTypeIfNeeded而获取; resultTo: 来自于当前Activity.mToken resultWho: 来自于当前Activity.mEmbeddedID requestCode = -1; startFlags = 0; profilerInfo = null; options = null; 好吧，最后直接返回一个对象，而继承与IActivityManager，到了这里就引出了我们android系统中很重要的一个概念：Binder机制。我们知道应用进程与SystemServer进程属于两个不同的进程，进程之间需要通讯，android系统采取了自身设计的Binder机制，这里的和ActivityManagerNative都是继承与IActivityManager的而SystemServer进程中的ActivityManagerService对象则继承与ActivityManagerNative。简单的表示： Binder接口 –&gt; ActivityManagerNative/ –&gt; ActivityManagerService； 这样，ActivityManagerNative与相当于一个Binder的客户端而ActivityManagerService相当于Binder的服务端，这样当ActivityManagerNative调用接口方法的时候底层通过Binder driver就会将请求数据与请求传递给server端，并在server端执行具体的接口逻辑。需要注意的是Binder机制是单向的，是异步的，也就是说只能通过client端向server端传递数据与请求而不用等待服务端的返回，也无法返回，那如果SystemServer进程想向应用进程传递数据怎么办？这时候就需要重新定义一个Binder请求以SystemServer为client端，以应用进程为server端，这样就是实现了两个进程之间的双向通讯。 好了，说了这么多我们知道这里的ActivityManagerNative是ActivityManagerService在应用进程的一个client就好了，通过它就可以滴啊用ActivityManagerService的方法了。 2.4、ActivityManagerProxy.startActivity()ActivityManagerNative.getDefault()方法会返回一个对象，那么我们看一下对象的startActivity方法： 12345678910111213[-&gt; ActivityManagerNative.java :: ActivityManagerProxy]class ActivityManagerProxy implements IActivityManager&#123;... public int startActivity(IApplicationThread caller, String callingPackage, Intent intent, String resolvedType, IBinder resultTo, String resultWho, int requestCode, int startFlags, ProfilerInfo profilerInfo, Bundle options) throws RemoteException &#123; ...... mRemote.transact(START_ACTIVITY_TRANSACTION, data, reply, 0); ......&#125;...&#125; 2.5、ActivityManagerNative.onTransact()1234567891011121314[-&gt; ActivityManagerNative.java] @Overridepublic boolean onTransact(int code, Parcel data, Parcel reply, int flags) throws RemoteException &#123; switch (code) &#123; case START_ACTIVITY_TRANSACTION: &#123; ...... int result = startActivity(app, callingPackage, intent, resolvedType, resultTo, resultWho, requestCode, startFlags, profilerInfo, options); ...... &#125; ...... &#125; 这里就涉及到了具体的Binder数据传输机制了，我们不做过多的分析，知道通过数据传输之后就会调用SystemServer进程的ActivityManagerService的startActivity就好了。 以上其实都是发生在应用进程中，下面开始调用的ActivityManagerService的执行时发生在SystemServer进程。 三、 ActivityManagerService接收启动Activity的请求 ActivityManagerService.startActivity()ActivityStarter.startActivityMayWait()ActivityStarter.startActivityLocked()ActivityStarter.startActivityUnchecked()ActivityStackSupervisor.resumeFocusedStackTopActivityLocked() ActivityStack.resumeTopActivityUncheckedLocked()ActivityStack.resumeTopActivityInnerLocked()–&gt;ActivityStackSupervisor.pauseBackStacks() [if (mResumedActivity != null)] –&gt;ActivityStackSupervisor.startSpecificActivityLocked() [if (mResumedActivity == null)] 3.1、ActivityManagerService.startActivity() 12345678910111213141516171819202122232425262728293031323334[-&gt; ActivityManagerService.java] public final int startActivity(IApplicationThread caller, String callingPackage, Intent intent, String resolvedType, IBinder resultTo, String resultWho, int requestCode, int startFlags, ProfilerInfo profilerInfo, Bundle bOptions) &#123; return startActivityAsUser(caller, callingPackage, intent, resolvedType, resultTo, resultWho, requestCode, startFlags, profilerInfo, bOptions, UserHandle.getCallingUserId());&#125; @Overridepublic final int startActivityAsCaller(IApplicationThread caller, String callingPackage, Intent intent, String resolvedType, IBinder resultTo, String resultWho, int requestCode, int startFlags, ProfilerInfo profilerInfo, Bundle bOptions, boolean ignoreTargetSecurity, int userId) &#123; ...... try &#123; int ret = mActivityStarter.startActivityMayWait(null, targetUid, targetPackage, intent, resolvedType, null, null, resultTo, resultWho, requestCode, startFlags, null, null, null, bOptions, ignoreTargetSecurity, userId, null, null); return ret; &#125; ......&#125;final int startActivity(Intent intent, ActivityStackSupervisor.ActivityContainer container) &#123; enforceNotIsolatedCaller(\"ActivityContainer.startActivity\"); final int userId = mUserController.handleIncomingUser(Binder.getCallingPid(), Binder.getCallingUid(), mStackSupervisor.mCurrentUser, false, ActivityManagerService.ALLOW_FULL_ONLY, \"ActivityContainer\", null); ...... return mActivityStarter.startActivityMayWait(null, -1, null, intent, mimeType, null, null, null, null, 0, 0, null, null, null, null, false, userId, container, null);&#125; 可以看到这里只是进行了一些关于userid的逻辑判断，然后就调用mStackSupervisor.startActivityMayWait方法，此处mStackSupervisor的数据类型为ActivityStackSupervisor。 当程序运行到这里时, ASS.startActivityMayWait的各个参数取值如下: caller = ApplicationThreadProxy, 用于跟调用者进程ApplicationThread进行通信的binder代理类. callingUid = -1; callingPackage = ContextImpl.getBasePackageName(),获取调用者Activity所在包名 intent: 这是启动Activity时传递过来的参数; resolvedType = intent.resolveTypeIfNeeded voiceSession = null; voiceInteractor = null; resultTo = Activity.mToken, 其中Activity是指调用者所在Activity, mToken对象保存自己所处的ActivityRecord信息 resultWho = Activity.mEmbeddedID, 其中Activity是指调用者所在Activity requestCode = -1; startFlags = 0; profilerInfo = null; outResult = null; config = null; options = null; ignoreTargetSecurity = false; userId = AMS.handleIncomingUser, 当调用者userId跟当前处于同一个userId,则直接返回该userId;当不相等时则根据调用者userId来决定是否需要将callingUserId转换为mCurrentUserId. iContainer = null; inTask = null; 下面我们来看一下这个方法的具体实现： 3.2、ActivityStarter.startActivityMayWait()1234567891011121314151617181920212223242526272829303132333435[-&gt;ActivityStarter.java]final int startActivityMayWait(IApplicationThread caller, int callingUid, String callingPackage, Intent intent, String resolvedType, IVoiceInteractionSession voiceSession, IVoiceInteractor voiceInteractor, IBinder resultTo, String resultWho, int requestCode, int startFlags, ProfilerInfo profilerInfo, IActivityManager.WaitResult outResult, Configuration config, Bundle bOptions, boolean ignoreTargetSecurity, int userId, IActivityContainer iContainer, TaskRecord inTask) &#123; ...... // Save a copy in case ephemeral needs it final Intent ephemeralIntent = new Intent(intent); // Don't modify the client's object! intent = new Intent(intent); ResolveInfo rInfo = mSupervisor.resolveIntent(intent, resolvedType, userId); ...... // Collect information about the target of the Intent. ActivityInfo aInfo = mSupervisor.resolveActivity(intent, rInfo, startFlags, profilerInfo); ActivityOptions options = ActivityOptions.fromBundle(bOptions); ActivityStackSupervisor.ActivityContainer container = (ActivityStackSupervisor.ActivityContainer)iContainer; ...... final ActivityRecord[] outRecord = new ActivityRecord[1]; int res = startActivityLocked(caller, intent, ephemeralIntent, resolvedType, aInfo, rInfo, voiceSession, voiceInteractor, resultTo, resultWho, requestCode, callingPid, callingUid, callingPackage, realCallingPid, realCallingUid, startFlags, options, ignoreTargetSecurity, componentSpecified, outRecord, container, inTask); ...... return res; &#125;&#125; 该过程主要功能：通过resolveActivity来获取ActivityInfo信息, 然后再进入ASS.startActivityLocked().先来看看 3.2.1、ActivityStackSupervisor.resolveActivity()1234567891011121314151617181920[-&gt;ActivityStackSupervisor.java] ResolveInfo resolveIntent(Intent intent, String resolvedType, int userId) &#123; return resolveIntent(intent, resolvedType, userId, 0);&#125;ResolveInfo resolveIntent(Intent intent, String resolvedType, int userId, int flags) &#123; try &#123; return AppGlobals.getPackageManager().resolveIntent(intent, resolvedType, PackageManager.MATCH_DEFAULT_ONLY | flags | ActivityManagerService.STOCK_PM_FLAGS, userId); &#125; catch (RemoteException e) &#123; &#125; return null;&#125;ActivityInfo resolveActivity(Intent intent, String resolvedType, int startFlags, ProfilerInfo profilerInfo, int userId) &#123; final ResolveInfo rInfo = resolveIntent(intent, resolvedType, userId); return resolveActivity(intent, rInfo, startFlags, profilerInfo);&#125; 3.2.2、PackageManagerService.resolveIntent()AppGlobals.getPackageManager()经过函数层层调用，获取的是ApplicationPackageManager对象。经过binder IPC调用，最终会调用PackageManagerService对象。故此时调用方法为PMS.resolveIntent(). 12345678910111213141516171819202122232425262728293031323334353637383940[-&gt; PackageManagerService.java] @Overridepublic ResolveInfo resolveIntent(Intent intent, String resolvedType, int flags, int userId) &#123; try &#123; ...... final List&lt;ResolveInfo&gt; query = queryIntentActivitiesInternal(intent, resolvedType, flags, userId); ...... final ResolveInfo bestChoice = chooseBestActivity(intent, resolvedType, flags, query, userId); return bestChoice; &#125; ......&#125; private @NonNull List&lt;ResolveInfo&gt; queryIntentActivitiesInternal(Intent intent, String resolvedType, int flags, int userId) &#123; ...... ComponentName comp = intent.getComponent(); if (comp == null) &#123; if (intent.getSelector() != null) &#123; intent = intent.getSelector(); comp = intent.getComponent(); &#125; &#125; if (comp != null) &#123; final List&lt;ResolveInfo&gt; list = new ArrayList&lt;ResolveInfo&gt;(1); final ActivityInfo ai = getActivityInfo(comp, flags, userId); if (ai != null) &#123; final ResolveInfo ri = new ResolveInfo(); ri.activityInfo = ai; list.add(ri); &#125; return list; &#125; ......&#125; ActivityStackSupervisor.resolveActivity()方法的核心功能是找到相应的Activity组件，并保存到intent对象。 3.3、ActivityStarter.startActivityLocked()继续ActivityStarter.startActivityMayWait()个方法中执行了启动Activity的一些其他逻辑判断，在经过判断逻辑之后调用startActivityLocked方法： 1234567891011121314151617181920212223242526272829[-&gt;ActivityStarter.java] final int startActivityLocked(IApplicationThread caller, Intent intent, Intent ephemeralIntent, String resolvedType, ActivityInfo aInfo, ResolveInfo rInfo, IVoiceInteractionSession voiceSession, IVoiceInteractor voiceInteractor, IBinder resultTo, String resultWho, int requestCode, int callingPid, int callingUid, String callingPackage, int realCallingPid, int realCallingUid, int startFlags, ActivityOptions options, boolean ignoreTargetSecurity, boolean componentSpecified, ActivityRecord[] outActivity, ActivityStackSupervisor.ActivityContainer container, TaskRecord inTask) &#123; ...... ActivityRecord sourceRecord = null; ActivityRecord resultRecord = null; ...... ActivityRecord r = new ActivityRecord(mService, callerApp, callingUid, callingPackage, intent, resolvedType, aInfo, mService.mConfiguration, resultRecord, resultWho, requestCode, componentSpecified, voiceSession != null, mSupervisor, container, options, sourceRecord); ...... try &#123; mService.mWindowManager.deferSurfaceLayout(); err = startActivityUnchecked(r, sourceRecord, voiceSession, voiceInteractor, startFlags, true, options, inTask); &#125; ...... return err;&#125; 这个方法中主要构造了ActivityManagerService端的Activity对象–&gt;ActivityRecord，并根据Activity的启动模式执行了相关逻辑。然后调用了startActivityUncheckedLocked方法： 3.4、ActivityStarter.startActivityUnchecked()12345678910111213141516171819202122232425262728293031323334353637[-&gt;ActivityStarter.java] private int startActivityUnchecked(final ActivityRecord r, ActivityRecord sourceRecord, IVoiceInteractionSession voiceSession, IVoiceInteractor voiceInteractor, int startFlags, boolean doResume, ActivityOptions options, TaskRecord inTask) &#123; ...... boolean newTask = false; ...... mTargetStack.startActivityLocked(mStartActivity, newTask, mKeepCurTransition, mOptions); if (mDoResume) &#123; if (!mLaunchTaskBehind) &#123; // ...... mService.setFocusedActivityLocked(mStartActivity, \"startedActivity\"); &#125; final ActivityRecord topTaskActivity = mStartActivity.task.topRunningActivityLocked(); if (!mTargetStack.isFocusable() || (topTaskActivity != null &amp;&amp; topTaskActivity.mTaskOverlay &amp;&amp; mStartActivity != topTaskActivity)) &#123; // ...... mTargetStack.ensureActivitiesVisibleLocked(null, 0, !PRESERVE_WINDOWS); // ....... mWindowManager.executeAppTransition(); &#125; else &#123; //3.6 ActivityStackSupervisor.resumeFocusedStackTopActivityLocked() mSupervisor.resumeFocusedStackTopActivityLocked(mTargetStack, mStartActivity, mOptions); &#125; &#125; else &#123; mTargetStack.addRecentActivityLocked(mStartActivity); &#125; mSupervisor.updateUserStackLocked(mStartActivity.userId, mTargetStack); mSupervisor.handleNonResizableTaskIfNeeded( mStartActivity.task, preferredLaunchStackId, mTargetStack.mStackId); return START_SUCCESS;&#125; 找到或创建新的Activit所属于的Task对象，之后调用ActivityStack.startActivityLocked() 3.4.1、ActivityStack.startActivityLocked()1234567891011121314151617181920212223[-&gt; ActivityStack.java] final void startActivityLocked(ActivityRecord r, boolean newTask, boolean keepCurTransition, ActivityOptions options) &#123; ...... &#125; else &#123; // If this is the first activity, don't do any fancy animations, // because there is nothing for it to animate on top of. addConfigOverride(r, task); ...... &#125; ......&#125; void addConfigOverride(ActivityRecord r, TaskRecord task) &#123; final Rect bounds = task.updateOverrideConfigurationFromLaunchBounds(); // TODO: VI deal with activity mWindowManager.addAppToken(task.mActivities.indexOf(r), r.appToken, r.task.taskId, mStackId, r.info.screenOrientation, r.fullscreen, (r.info.flags &amp; FLAG_SHOW_FOR_ALL_USERS) != 0, r.userId, r.info.configChanges, task.voiceSession != null, r.mLaunchTaskBehind, bounds, task.mOverrideConfig, task.mResizeMode, r.isAlwaysFocusable(), task.isHomeTask(), r.appInfo.targetSdkVersion, r.mRotationAnimationHint); r.taskConfigOverride = task.mOverrideConfig;&#125; 3.5、ActivityStackSupervisor.resumeFocusedStackTopActivityLocked()123456789101112 [-&gt;ActivityStackSupervisor.java] boolean resumeFocusedStackTopActivityLocked( ActivityStack targetStack, ActivityRecord target, ActivityOptions targetOptions) &#123; if (targetStack != null &amp;&amp; isFocusedStack(targetStack)) &#123; return targetStack.resumeTopActivityUncheckedLocked(target, targetOptions); &#125; final ActivityRecord r = mFocusedStack.topRunningActivityLocked(); if (r == null || r.state != RESUMED) &#123; mFocusedStack.resumeTopActivityUncheckedLocked(null, null); &#125; return false;&#125; 3.6、ActivityStack.resumeTopActivityUncheckedLocked()inResumeTopActivity用于保证每次只有一个Activity执行resumeTopActivityLocked()操作. 123456789101112131415161718192021[-&gt;ActivityStack.java] boolean resumeTopActivityUncheckedLocked(ActivityRecord prev, ActivityOptions options) &#123; if (mStackSupervisor.inResumeTopActivity) &#123; // Don't even start recursing. return false; &#125; boolean result = false; try &#123; // Protect against recursion. mStackSupervisor.inResumeTopActivity = true; if (mService.mLockScreenShown == ActivityManagerService.LOCK_SCREEN_LEAVING) &#123; mService.mLockScreenShown = ActivityManagerService.LOCK_SCREEN_HIDDEN; mService.updateSleepIfNeededLocked(); &#125; result = resumeTopActivityInnerLocked(prev, options); &#125; finally &#123; mStackSupervisor.inResumeTopActivity = false; &#125; return result;&#125; 3.7、ActivityStack.resumeTopActivityInnerLocked()说明：启动一个新Activity时，如果界面还存在其它的Activity，那么必须先中断其它的Activity。 因此，除了第一个启动的Home界面对应的Activity外，其它的Activity均需要进行此操作，当系统启动第一个Activity，即Home时，mResumedActivity的值才会为null。 经过一系列处理逻辑之后最终调用了startPausingLocked方法，这个方法作用就是让系统中栈中的Activity执行onPause方法。 12345678910111213141516 [-&gt;ActivityStack.java] private boolean resumeTopActivityInnerLocked(ActivityRecord prev, ActivityOptions options) &#123; ...... // We need to start pausing the current activity so the top one can be resumed... final boolean dontWaitForPause = (next.info.flags &amp; FLAG_RESUME_WHILE_PAUSING) != 0; boolean pausing = mStackSupervisor.pauseBackStacks(userLeaving, next, dontWaitForPause); if (mResumedActivity != null) &#123; pausing |= startPausingLocked(userLeaving, false, next, dontWaitForPause); &#125; ...... else &#123; ...... mStackSupervisor.startSpecificActivityLocked(next, true, true); &#125;&#125; 3.8、ActivityStackSupervisor.pauseBackStacks()暂停所有处于后台栈的所有Activity。 12345678910111213141516 [-&gt;ActivityStackSupervisor.java] boolean pauseBackStacks(boolean userLeaving, ActivityRecord resuming, boolean dontWait) &#123; boolean someActivityPaused = false; for (int displayNdx = mActivityDisplays.size() - 1; displayNdx &gt;= 0; --displayNdx) &#123; ArrayList&lt;ActivityStack&gt; stacks = mActivityDisplays.valueAt(displayNdx).mStacks; for (int stackNdx = stacks.size() - 1; stackNdx &gt;= 0; --stackNdx) &#123; final ActivityStack stack = stacks.get(stackNdx); if (!isFocusedStack(stack) &amp;&amp; stack.mResumedActivity != null) &#123; ...... someActivityPaused |= stack.startPausingLocked(userLeaving, false, resuming, dontWait); &#125; &#125; &#125; return someActivityPaused;&#125; 四、执行栈顶Activity的onPause方法 ActivityStack.startPausingLocked()ActivityThread.schedulePauseActivity()ActivityThread.sendMessage()ActivityThread.H.sendMessage()ActivityThread.H.handleMessage()–&gt; -&gt;ActivityThread.handlePauseActivity()–&gt;ActivityThread.performPauseActivity()—-&gt;ActivityThread.performPauseActivityIfNeeded()—-&gt; Instrumentation.callActivityOnPause()—-&gt; Activity.performPause()—-&gt; Activity.onPause()–&gt;ActivityManagerService.activityPaused()—-&gt;ActivityStack.activityPausedLocked()—-&gt;ActivityStack.completePauseLocked()—-&gt;ActivityStackSupervisor.resumeFocusedStackTopActivityLocked() —-&gt;ActivityStack.resumeTopActivityUncheckedLocked()—-&gt;ActivityStack.resumeTopActivityInnerLocked()—-&gt;ActivityStackSupervisor.startSpecificActivityLocked() 4.1、ActivityStack.startPausingLocked()123456789101112 [-&gt;ActivityStack.java] final boolean startPausingLocked(boolean userLeaving, boolean uiSleeping, ActivityRecord resuming, boolean dontWait) &#123; ...... try &#123; ...... mService.updateUsageStats(prev, false); prev.app.thread.schedulePauseActivity(prev.appToken, prev.finishing, userLeaving, prev.configChangeFlags, dontWait); &#125; ......&#125; 可以看到这里执行了pre.app.thread.schedulePauseActivity方法，通过分析不难发现这里的thread是一个IApplicationThread类型的对象，而在ActivityThread中也定义了一个ApplicationThread的类，其继承了IApplicationThread，并且都是Binder对象，不难看出这里的IAppcation是一个Binder的client端而ActivityThread中的ApplicationThread是一个Binder对象的server端，所以通过这里的thread.schedulePauseActivity实际上调用的就是ApplicationThread的schedulePauseActivity方法。 这里的ApplicationThread可以和ActivityManagerNative对于一下： 通过ActivityManagerNative –&gt; ActivityManagerService实现了应用进程与SystemServer进程的通讯 通过AppicationThread &lt;– IApplicationThread实现了SystemServer进程与应用进程的通讯 然后我们继续看一下ActivityThread中schedulePauseActivity的具体实现： 4.2、ActivityThread.schedulePauseActivity()12345678910111213 [-&gt;ActivityThread.java] public final void schedulePauseActivity(IBinder token, boolean finished, boolean userLeaving, int configChanges, boolean dontReport) &#123; int seq = getLifecycleSeq(); if (DEBUG_ORDER) Slog.d(TAG, \"pauseActivity \" + ActivityThread.this + \" operation received seq: \" + seq); sendMessage( finished ? H.PAUSE_ACTIVITY_FINISHING : H.PAUSE_ACTIVITY, token, (userLeaving ? USER_LEAVING : 0) | (dontReport ? DONT_REPORT : 0), configChanges, seq);&#125; 4.3、ActivityThread.sendMessage()12345 [-&gt;ActivityThread.java] private void sendMessage(int what, Object obj, int arg1, int arg2, int seq) &#123; ...... mH.sendMessage(msg);&#125; 最终调用了mH的sendMessage方法，mH是在ActivityThread中定义的一个Handler对象，主要处理SystemServer进程的消息，我们看一下其handleMessge方法的实现： 4.4、[ActivityThread.handleMessage() : mH]12345678910111213[-&gt;ActivityThread.java : mH] public void handleMessage(Message msg) &#123; switch (msg.what) &#123; ...... case PAUSE_ACTIVITY: &#123; Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, \"activityPause\"); SomeArgs args = (SomeArgs) msg.obj; handlePauseActivity((IBinder) args.arg1, false, (args.argi1 &amp; USER_LEAVING) != 0, args.argi2, (args.argi1 &amp; DONT_REPORT) != 0, args.argi3); maybeSnapshot(); Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER); &#125; break; 可以发现其调用了handlePauseActivity方法： 4.5、ActivityThread.handlePauseActivity()123456789101112131415161718[-&gt;ActivityThread.java]private void handlePauseActivity(IBinder token, boolean finished, boolean userLeaving, int configChanges, boolean dontReport, int seq) &#123; ActivityClientRecord r = mActivities.get(token); ...... performPauseActivity(token, finished, r.isPreHoneycomb(), \"handlePauseActivity\"); ...... // Tell the activity manager we have paused. if (!dontReport) &#123; try &#123; ActivityManagerNative.getDefault().activityPaused(token); &#125; catch (RemoteException ex) &#123; throw ex.rethrowFromSystemServer(); &#125; &#125; mSomeActivitiesChanged = true; &#125;&#125; 然后在方法体内部通过调用performPauseActivity方法来实现对栈顶Activity的onPause生命周期方法的回调，可以具体看一下他的实现： 4.6、ActivityThread.performPauseActivity()123456789101112131415 [-&gt;ActivityThread.java] final Bundle performPauseActivity(ActivityClientRecord r, boolean finished, boolean saveState, String reason) &#123; ...... performPauseActivityIfNeeded(r, reason); ......&#125;private void performPauseActivityIfNeeded(ActivityClientRecord r, String reason) &#123; ...... try &#123; r.activity.mCalled = false; mInstrumentation.callActivityOnPause(r.activity); ......&#125; 这样回到了mInstrumentation的callActivityOnPuase方法： 4.7、Instrumentation.callActivityOnPuase()1234 [-&gt;Instrumentation.java]public void callActivityOnPause(Activity activity) &#123; activity.performPause();&#125; 原来最终回调到了Activity的performPause方法： 4.7、Activity.performPause()123456789101112131415[-&gt;Activity.java] final void performPause() &#123; mDoReportFullyDrawn = false; mFragments.dispatchPause(); mCalled = false; onPause(); mResumed = false; if (!mCalled &amp;&amp; getApplicationInfo().targetSdkVersion &gt;= android.os.Build.VERSION_CODES.GINGERBREAD) &#123; throw new SuperNotCalledException( \"Activity \" + mComponent.toShortString() + \" did not call through to super.onPause()\"); &#125; mResumed = false;&#125; 终于，太不容易了，回调到了Activity的onPause方法，哈哈，Activity生命周期中的第一个生命周期方法终于被我们找到了。。。。也就是说我们在启动一个Activity的时候最先被执行的是栈顶的Activity的onPause方法。记住这点吧，面试的时候经常会问到类似的问题。 然后回到我们的handlePauseActivity方法，在该方法的最后面执行了ActivityManagerNative.getDefault().activityPaused(token);方法，这是应用进程告诉服务进程，栈顶Activity已经执行完成onPause方法了，通过前面我们的分析，我们知道这句话最终会被ActivityManagerService的activityPaused方法执行。 123456789101112 [-&gt;ActivityManagerService.java] @Overridepublic final void activityPaused(IBinder token) &#123; final long origId = Binder.clearCallingIdentity(); synchronized(this) &#123; ActivityStack stack = ActivityRecord.getStackLocked(token); if (stack != null) &#123; stack.activityPausedLocked(token, false); &#125; &#125; Binder.restoreCallingIdentity(origId);&#125; 以发现，该方法内部会调用ActivityStack的activityPausedLocked方法，好吧，看一下activityPausedLocked方法的实现，然后执行了completePauseLocked方法： 1234567891011121314151617181920 private void completePauseLocked(boolean resumeNext, ActivityRecord resuming) &#123; ...... if (resumeNext) &#123; final ActivityStack topStack = mStackSupervisor.getFocusedStack(); if (!mService.isSleepingOrShuttingDownLocked()) &#123; mStackSupervisor.resumeFocusedStackTopActivityLocked(topStack, prev, null); &#125; else &#123; mStackSupervisor.checkReadyForSleepLocked(); ActivityRecord top = topStack.topRunningActivityLocked(); if (top == null || (prev != null &amp;&amp; top != prev)) &#123; // If there are no more activities available to run, do resume anyway to start // something. Also if the top activity on the stack is not the just paused // activity, we need to go ahead and resume it to ensure we complete an // in-flight app switch. mStackSupervisor.resumeFocusedStackTopActivityLocked(); &#125; &#125; &#125; ......&#125; 经过了一系列的逻辑之后，又调用了resumeTopActivitiesLocked方法，又回到了第三步中解析的方法中了，这样经过 ActivityStackSupervisor.resumeFocusedStackTopActivityLocked()–&gt; ActivityStack.resumeTopActivityUncheckedLocked()–&gt; ActivityStack.resumeTopActivityInnerLocked()–&gt; ActivityStackSupervisor.startSpecificActivityLocked() 好吧，我们看一下startSpecificActivityLocked的具体实现： 1234567891011121314151617181920212223242526 [-&gt;ActivityStack.java] void startSpecificActivityLocked(ActivityRecord r, boolean andResume, boolean checkConfig) &#123; // Is this activity's application already running? ProcessRecord app = mService.getProcessRecordLocked(r.processName, r.info.applicationInfo.uid, true); r.task.stack.setLaunchTime(r); if (app != null &amp;&amp; app.thread != null) &#123; try &#123; ...... realStartActivityLocked(r, app, andResume, checkConfig); return; &#125; catch (RemoteException e) &#123; Slog.w(TAG, \"Exception when starting activity \" + r.intent.getComponent().flattenToShortString(), e); &#125; // If a dead object exception was thrown -- fall through to // restart the application. &#125; mService.startProcessLocked(r.processName, r.info.applicationInfo, true, 0, \"activity\", r.intent.getComponent(), false, false, true);&#125; 可以发现在这个方法中，首先会判断一下需要启动的Activity所需要的应用进程是否已经启动，若启动的话，则直接调用realStartAtivityLocked方法，否则调用startProcessLocked方法，用于启动应用进程。 这样关于启动Activity时的第三步骤就已经执行完成了，这里主要是实现了对栈顶Activity执行onPause 方法，而这个方法首先判断需要启动的Activity所属的进程是否已经启动，若已经启动则直接调用启动Activity的方法，否则将先启动Activity的应用进程，然后在启动该Activity。 五、创建Activity所属的应用进程 ActivityManagerService.startProcessLocked()-&gt; Process.start()-&gt; 创建进程 Process.startViaZygote()-&gt; 创建进程 Process.zygoteSendArgsAndGetResult(openZygoteSocketIfNeeded(abi), argsForZygote)-&gt; 创建进程 ActivityThread.main()ActivityThread.attach()ActivityManagerProxy.attachApplication()ActivityManagerNative.onTransact()ActivityManagerService.attachApplication()ActivityManagerService.attachApplicationLocked() ApplicationThreadNative.ApplicationThreadProxy.bindApplication()ApplicationThreadNative.onTransact()ActivityThread.ApplicationThread.bindApplication()ActivityThread.sendMessage()ActivityThread.H.sendMessage()ActivityThread.H.handleMessage()ActivityThread.handleBindApplication() 5.1、ActivityManagerService.startProcessLocked()123456789101112131415161718192021222324 [-&gt;ActivityManagerService.java] private final void startProcessLocked(ProcessRecord app, String hostingType, String hostingNameStr) &#123; startProcessLocked(app, hostingType, hostingNameStr, null /* abiOverride */, null /* entryPoint */, null /* entryPointArgs */);&#125;private final void startProcessLocked(ProcessRecord app, String hostingType, String hostingNameStr, String abiOverride, String entryPoint, String[] entryPointArgs) &#123; ...... // Start the process. It will either succeed and return a result containing // the PID of the new process, or else throw a RuntimeException. boolean isActivityProcess = (entryPoint == null); if (entryPoint == null) entryPoint = \"android.app.ActivityThread\"; Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, \"Start proc: \" + app.processName); checkTime(startTime, \"startProcess: asking zygote to start proc\"); Process.ProcessStartResult startResult = Process.start(entryPoint, app.processName, uid, uid, gids, debugFlags, mountExternal, app.info.targetSdkVersion, app.info.seinfo, requiredAbi, instructionSet, app.info.dataDir, entryPointArgs); checkTime(startTime, \"startProcess: returned from zygote!\"); ...... &#125; 可以发现其经过一系列的初始化操作之后调用了Process.start方法，并且传入了启动的类名”android.app.ActivityThread”: 5.2、Process.start()12345678910111213141516171819202122 [-&gt;Process.java] public static final ProcessStartResult start(final String processClass, final String niceName, int uid, int gid, int[] gids, int debugFlags, int mountExternal, int targetSdkVersion, String seInfo, String abi, String instructionSet, String appDataDir, String[] zygoteArgs) &#123; try &#123; return startViaZygote(processClass, niceName, uid, gid, gids, debugFlags, mountExternal, targetSdkVersion, seInfo, abi, instructionSet, appDataDir, zygoteArgs); &#125; catch (ZygoteStartFailedEx ex) &#123; Log.e(LOG_TAG, \"Starting VM process through Zygote failed\"); throw new RuntimeException( \"Starting VM process through Zygote failed\", ex); &#125;&#125; 然后调用了startViaZygote方法： 5.3、Process.startViaZygote()12345678910111213141516171819 [-&gt;Process.java] private static ProcessStartResult startViaZygote(final String processClass, final String niceName, final int uid, final int gid, final int[] gids, int debugFlags, int mountExternal, int targetSdkVersion, String seInfo, String abi, String instructionSet, String appDataDir, String[] extraArgs) throws ZygoteStartFailedEx &#123; synchronized(Process.class) &#123; ...... return zygoteSendArgsAndGetResult(openZygoteSocketIfNeeded(abi), argsForZygote); &#125;&#125; 继续查看一下zygoteSendArgsAndGetResult方法的实现： 5.4、Process.zygoteSendArgsAndGetResult()12345678910111213141516171819 [-&gt;Process.java] private static ProcessStartResult zygoteSendArgsAndGetResult( ZygoteState zygoteState, ArrayList&lt;String&gt; args) throws ZygoteStartFailedEx &#123; ...... // Should there be a timeout on this? ProcessStartResult result = new ProcessStartResult(); // Always read the entire result from the input stream to avoid leaving // bytes in the stream for future process starts to accidentally stumble // upon. //等待socket服务端（即zygote）返回新创建的进程pid; result.pid = inputStream.readInt(); result.usingWrapper = inputStream.readBoolean(); ...... return result; ......&#125; 这个方法的主要功能是通过socket通道向Zygote进程发送一个参数列表，然后进入阻塞等待状态，直到远端的socket服务端发送回来新创建的进程pid才返回。 5.5、Process.openZygoteSocketIfNeeded()12345678910111213141516171819202122232425262728293031 [-&gt;Process.java] private static ZygoteState openZygoteSocketIfNeeded(String abi) throws ZygoteStartFailedEx &#123; if (primaryZygoteState == null || primaryZygoteState.isClosed()) &#123; try &#123; //向主zygote发起connect()操作 primaryZygoteState = ZygoteState.connect(ZYGOTE_SOCKET); &#125; catch (IOException ioe) &#123; throw new ZygoteStartFailedEx(\"Error connecting to primary zygote\", ioe); &#125; &#125; if (primaryZygoteState.matches(abi)) &#123; return primaryZygoteState; &#125; // The primary zygote didn't match. Try the secondary. if (secondaryZygoteState == null || secondaryZygoteState.isClosed()) &#123; try &#123; //当主zygote没能匹配成功，则采用第二个zygote，发起connect()操作 secondaryZygoteState = ZygoteState.connect(SECONDARY_ZYGOTE_SOCKET); &#125; catch (IOException ioe) &#123; throw new ZygoteStartFailedEx(\"Error connecting to secondary zygote\", ioe); &#125; &#125; if (secondaryZygoteState.matches(abi)) &#123; return secondaryZygoteState; &#125; throw new ZygoteStartFailedEx(\"Unsupported zygote ABI: \" + abi);&#125; openZygoteSocketIfNeeded(abi)方法是根据当前的abi来选择与zygote还是zygote64来进行通信。 既然system_server进程的zygoteSendArgsAndGetResult()方法通过socket向Zygote进程发送消息，这是便会唤醒Zygote进程，来响应socket客户端的请求（即system_server端) 具体详细过程可参考大神博客理解Android进程创建流程 和 Android四大组件与进程启动的关系 大神是基于Android M，之后我会跟着大神的脚步站在巨人的肩膀上，完成Android N进程创建流程，估计变化不大加深自己理解。 进程创建流程图： 总结： 可以发现其最终调用了Zygote并通过socket通信的方式让Zygote进程fork除了一个新的进程，并根据我们刚刚传递的”android.app.ActivityThread”字符串，反射出该对象并执行ActivityThread的main方法。这样我们所要启动的应用进程这时候其实已经启动了，但是还没有执行相应的初始化操作。 我们平时App-Crash常见的log就是从ActivityThread.main()抛出异常的,可参考文档：Android 7.1.2(Android N) Android系统启动流程。 java.lang.RuntimeException: Unable to start activity ComponentInfo{……} at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2665) at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2726) at android.app.ActivityThread.-wrap12(ActivityThread.java) at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1477) 1234567&gt; at android.os.Handler.dispatchMessage(Handler.java:102)&gt; at android.os.Looper.loop(Looper.java:154)&gt; at android.app.ActivityThread.main(ActivityThread.java:6119)&gt; at java.lang.reflect.Method.invoke(Native Method)&gt; at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:892)&gt; at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:782)&gt; 为什么我们平时都将ActivityThread称之为ui线程或者是主线程，这里可以看出，应用进程被创建之后首先执行的是ActivityThread的main方法，所以我们将ActivityThread成为主线程。 好了，这时候我们看一下ActivityThread的main方法的实现逻辑。 5.6、ActivityThread.main()123456789101112131415161718192021222324252627 [-&gt;ActivityThread.java] public static void main(String[] args) &#123; Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, \"ActivityThreadMain\"); ...... Process.setArgV0(\"&lt;pre-initialized&gt;\"); Looper.prepareMainLooper(); ActivityThread thread = new ActivityThread(); thread.attach(false); if (sMainThreadHandler == null) &#123; sMainThreadHandler = thread.getHandler(); &#125; if (false) &#123; Looper.myLooper().setMessageLogging(new LogPrinter(Log.DEBUG, \"ActivityThread\")); &#125; // End of event ActivityThreadMain. Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER); Looper.loop(); throw new RuntimeException(\"Main thread loop unexpectedly exited\");&#125;&#125; 在main方法中主要执行了一些初始化的逻辑，并且创建了一个UI线程消息队列，这也就是为什么我们可以在主线程中随意的创建Handler而不会报错的原因，这里提出一个问题，大家可以思考一下：子线程可以创建Handler么？可以的话应该怎么做？ 然后执行了ActivityThread的attach方法，这里我们看一下attach方法执行了那些逻辑操作。 5.7、ActivityThread.attach()12345678910111213141516 private void attach(boolean system) &#123; ...... //此时进程名还是\"&lt;pre-initialized&gt;\" android.ddm.DdmHandleAppName.setAppName(\"&lt;pre-initialized&gt;\", UserHandle.myUserId()); RuntimeInit.setApplicationObject(mAppThread.asBinder()); //创建对象 final IActivityManager mgr = ActivityManagerNative.getDefault(); try &#123; //调用基于IActivityManager接口的Binder通道 mgr.attachApplication(mAppThread); &#125; catch (RemoteException ex) &#123; throw ex.rethrowFromSystemServer(); &#125; ......&#125; 5.8、ActivityManagerProxy .attachApplication()1234567891011[-&gt; ActivityManagerNative.java::ActivityManagerProxy]public void attachApplication(IApplicationThread app) throws RemoteException&#123;Parcel data = Parcel.obtain();Parcel reply = Parcel.obtain();data.writeInterfaceToken(IActivityManager.descriptor);data.writeStrongBinder(app.asBinder());mRemote.transact(ATTACH_APPLICATION_TRANSACTION, data, reply, 0);reply.readException();data.recycle();reply.recycle();&#125; 5.9、ActivityManagerNative.onTransact()123456789101112131415161718[-&gt; ActivityManagerNative.java]public boolean onTransact(int code, Parcel data, Parcel reply, int flags) throws RemoteException &#123;switch (code) &#123;... case ATTACH_APPLICATION_TRANSACTION: &#123; data.enforceInterface(IActivityManager.descriptor); //获取ApplicationThread的binder代理类 ApplicationThreadProxy IApplicationThread app = ApplicationThreadNative.asInterface( data.readStrongBinder()); if (app != null) &#123; attachApplication(app); //此处是ActivityManagerService类中的方法 &#125; reply.writeNoException(); return true;&#125;&#125;&#125; 刚刚我们已经分析过对象是ActivityManagerService的Binder client，所以这里调用了attachApplication实际上就是通过Binder机制调用了ActivityManagerService的attachApplication，具体调用的过程，我们看一下ActivityManagerService是如何实现的： 5.10、ActivityManagerService.attachApplication()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859 [-&gt;ActivityManagerService.java] @Overridepublic final void attachApplication(IApplicationThread thread) &#123; synchronized (this) &#123; int callingPid = Binder.getCallingPid(); final long origId = Binder.clearCallingIdentity(); //此处的thread便是ApplicationThreadProxy对象,用于跟前面通过Process.start()所创建的进程中ApplicationThread对象进行通信. attachApplicationLocked(thread, callingPid); Binder.restoreCallingIdentity(origId); &#125;&#125; private final boolean attachApplicationLocked(IApplicationThread thread, int pid) &#123; // 根据pid获取ProcessRecord ProcessRecord app; if (pid != MY_PID &amp;&amp; pid &gt;= 0) &#123; synchronized (mPidsSelfLocked) &#123; app = mPidsSelfLocked.get(pid); &#125; &#125; ...... //获取应用appInfo ApplicationInfo appInfo = app.instrumentationInfo != null ? app.instrumentationInfo : app.info; app.compat = compatibilityInfoForPackageLocked(appInfo); if (profileFd != null) &#123; profileFd = profileFd.dup(); &#125; ...... //绑定应用 thread.bindApplication(processName, appInfo, providers, app.instrumentationClass, profilerInfo, app.instrumentationArguments, app.instrumentationWatcher, app.instrumentationUiAutomationConnection, testMode, mBinderTransactionTrackingEnabled, enableTrackAllocation, isRestrictedBackupMode || !normalMode, app.persistent, new Configuration(mConfiguration), app.compat, getCommonServicesLocked(app.isolated), mCoreSettingsObserver.getCoreSettingsLocked()); updateLruProcessLocked(app, false, null); &#125; ...... // See if the top visible activity is waiting to run in this process... if (normalMode) &#123; try &#123; if (mStackSupervisor.attachApplicationLocked(app)) &#123; didSomething = true; &#125; &#125; catch (Exception e) &#123; Slog.wtf(TAG, \"Exception thrown launching activities in \" + app, e); badApp = true; &#125; &#125; ...... return true;&#125; 下面,再来说说thread.bindApplication的过程. 5.11、ApplicationThreadProxy.bindApplication()123456789101112131415161718[-&gt; ApplicationThreadNative.java ::ApplicationThreadProxy]class ApplicationThreadProxy implements IApplicationThread &#123;... @Overridepublic final void bindApplication(String packageName, ApplicationInfo info, List&lt;ProviderInfo&gt; providers, ComponentName testName, ProfilerInfo profilerInfo, Bundle testArgs, IInstrumentationWatcher testWatcher, IUiAutomationConnection uiAutomationConnection, int debugMode, boolean enableBinderTracking, boolean trackAllocation, boolean restrictedBackupMode, boolean persistent, Configuration config, CompatibilityInfo compatInfo, Map&lt;String, IBinder&gt; services, Bundle coreSettings) throws RemoteException &#123; ...... mRemote.transact(BIND_APPLICATION_TRANSACTION, data, null, IBinder.FLAG_ONEWAY); data.recycle();&#125;...&#125; 5.12、ApplicationThreadNative.onTransact()12345678910111213141516[-&gt; ApplicationThreadNative.java]public boolean onTransact(int code, Parcel data, Parcel reply, int flags) throws RemoteException &#123;switch (code) &#123;... case BIND_APPLICATION_TRANSACTION: &#123; ...... bindApplication(packageName, info, providers, testName, profilerInfo, testArgs, testWatcher, uiAutomationConnection, testMode, enableBinderTracking, trackAllocation, restrictedBackupMode, persistent, config, compatInfo, services, coreSettings); return true; &#125;...&#125; 5.13、ApplicationThread.bindApplication()[-&gt; ActivityThread.java ::ApplicationThread] 1234567891011121314151617181920 public final void bindApplication(String processName, ApplicationInfo appInfo, List&lt;ProviderInfo&gt; providers, ComponentName instrumentationName, ProfilerInfo profilerInfo, Bundle instrumentationArgs, IInstrumentationWatcher instrumentationWatcher, IUiAutomationConnection instrumentationUiConnection, int debugMode, boolean enableBinderTracking, boolean trackAllocation, boolean isRestrictedBackupMode, boolean persistent, Configuration config, CompatibilityInfo compatInfo, Map&lt;String, IBinder&gt; services, Bundle coreSettings) &#123; if (services != null) &#123; // Setup the service cache in the ServiceManager ServiceManager.initServiceCache(services); &#125; setCoreSettings(coreSettings); AppBindData data = new AppBindData(); ...... sendMessage(H.BIND_APPLICATION, data);&#125; 5.14、ActivityThread.handleBindApplication()当主线程收到H.BIND_APPLICATION,则调用handleBindApplication 12345678910111213141516171819202122232425262728293031323334353637383940414243 [-&gt; ActivityThread.java ::H] private void handleBindApplication(AppBindData data) &#123; ...... mBoundApplication = data; mConfiguration = new Configuration(data.config); mCompatConfiguration = new Configuration(data.config); //设置进程名, 也就是说进程名是在进程真正创建以后的BIND_APPLICATION过程中才取名 // send up app name; do this *before* waiting for debugger Process.setArgV0(data.processName); android.ddm.DdmHandleAppName.setAppName(data.processName, UserHandle.myUserId()); ...... 获取LoadedApk对象 data.info = getPackageInfoNoCheck(data.appInfo, data.compatInfo); ...... //创建ContextImpl上下文 final ContextImpl appContext = ContextImpl.createAppContext(this, data.info); updateLocaleListFromAppContext(appContext, mResourcesManager.getConfiguration().getLocales()); ...... try &#123; // If the app is being launched for full backup or restore, bring it up in // a restricted environment with the base application class. // 此处data.info是指LoadedApk, 通过反射创建目标应用Application对象 Application app = data.info.makeApplication(data.restrictedBackupMode, null); mInitialApplication = app; ...... // Do this after providers, since instrumentation tests generally start their // test thread at this point, and we don't want that racing. try &#123; mInstrumentation.onCreate(data.instrumentationArgs); &#125; ...... try &#123; mInstrumentation.callApplicationOnCreate(app); &#125; ......&#125; 在handleBindApplication()的过程中,会同时设置以下两个值: LoadedApk.mApplication AT.mInitialApplication 图示总结： 六、执行启动Acitivity ActivityStackSupervisor.attachApplicationLocked()ActivityStackSupervisor.realStartActivityLocked()IApplicationThread.scheduleLaunchActivity()ActivityThread.ApplicationThread.scheduleLaunchActivity()ActivityThread.sendMessage()ActivityThread.H.handleMessage()ActivityThread.handleLauncherActivity()ActivityThread.performLauncherActivity()Instrumentation.callActivityOnCreate()-&gt; Activity.performCreate() Activity.onCreate() 在第五节AMS.startProcessLocked()整个过程，创建完新进程后会在新进程中调用AMP.attachApplication ，该方法经过binder ipc后调用到AMS.attachApplicationLocked。该方法执行了一系列的初始化操作，在执行完bindApplication()之后进入ActivityStackSupervisor.attachApplicationLocked()，这样我们整个应用进程已经启动起来了。终于可以开始activity的启动逻辑了。 关系图：ActivityThread简介 首先看一下attachApplicationLocked方法的实现： 6.1、ActivityStackSupervisor.attachApplicationLocked()1234567891011121314151617181920212223242526272829 [-&gt;ActivityStackSupervisor.java] boolean attachApplicationLocked(ProcessRecord app) throws RemoteException &#123; final String processName = app.processName; boolean didSomething = false; for (int displayNdx = mActivityDisplays.size() - 1; displayNdx &gt;= 0; --displayNdx) &#123; ArrayList&lt;ActivityStack&gt; stacks = mActivityDisplays.valueAt(displayNdx).mStacks; for (int stackNdx = stacks.size() - 1; stackNdx &gt;= 0; --stackNdx) &#123; final ActivityStack stack = stacks.get(stackNdx); if (!isFocusedStack(stack)) &#123; continue; &#125; ActivityRecord hr = stack.topRunningActivityLocked(); if (hr != null) &#123; if (hr.app == null &amp;&amp; app.uid == hr.info.applicationInfo.uid &amp;&amp; processName.equals(hr.processName)) &#123; try &#123; if (realStartActivityLocked(hr, app, true, true)) &#123; didSomething = true; &#125; &#125; ...... &#125; &#125; &#125; &#125; if (!didSomething) &#123; ensureActivitiesVisibleLocked(null, 0, !PRESERVE_WINDOWS); &#125; return didSomething;&#125; 可以发现其内部调用了realStartActivityLocked方法，通过名字可以知道这个方法应该就是用来启动Activity的，看一下这个方法的实现逻辑： 6.2、ActivityStackSupervisor.realStartActivityLocked()12345678910111213141516 [-&gt;ActivityStackSupervisor.java] final boolean realStartActivityLocked(ActivityRecord r, ProcessRecord app, boolean andResume, boolean checkConfig) throws RemoteException &#123; ...... app.forceProcessStateUpTo(mService.mTopProcessState); app.thread.scheduleLaunchActivity(new Intent(r.intent), r.appToken, System.identityHashCode(r), r.info, new Configuration(mService.mConfiguration), new Configuration(task.mOverrideConfig), r.compat, r.launchedFromPackage, task.voiceInteractor, app.repProcState, r.icicle, r.persistentState, results, newIntents, !andResume, mService.isNextTransitionForward(), profilerInfo); ...... return true;&#125; 可以发现与第四节执行栈顶Activity onPause时类似，这里也是通过调用IApplicationThread的方法实现的，这里调用的是scheduleLaunchActivity方法，所以真正执行的是ActivityThread中的scheduleLaunchActivity。 6.3、ApplicationThread.scheduleLaunchActivity()1234567891011121314151617[-&gt; ActivityThread.java :ApplicationThread] public final void scheduleLaunchActivity(Intent intent, IBinder token, int ident, ActivityInfo info, Configuration curConfig, Configuration overrideConfig, CompatibilityInfo compatInfo, String referrer, IVoiceInteractor voiceInteractor, int procState, Bundle state, PersistableBundle persistentState, List&lt;ResultInfo&gt; pendingResults, List&lt;ReferrerIntent&gt; pendingNewIntents, boolean notResumed, boolean isForward, ProfilerInfo profilerInfo) &#123; updateProcessState(procState, false); ActivityClientRecord r = new ActivityClientRecord(); ...... updatePendingConfiguration(curConfig); sendMessage(H.LAUNCH_ACTIVITY, r); &#125; 6.4、H.handleMessage12345678910111213[-&gt; ActivityThread.java ::H] public void handleMessage(Message msg) &#123; if (DEBUG_MESSAGES) Slog.v(TAG, \"&gt;&gt;&gt; handling: \" + codeToString(msg.what)); switch (msg.what) &#123; case LAUNCH_ACTIVITY: &#123; final ActivityClientRecord r = (ActivityClientRecord) msg.obj; r.packageInfo = getPackageInfoNoCheck( r.activityInfo.applicationInfo, r.compatInfo); handleLaunchActivity(r, null, \"LAUNCH_ACTIVITY\"); &#125; break; ...... &#125; 6.5、ActivityThread.handleLaunchActivity()ActivityThread接收到SystemServer进程的消息之后会通过其内部的Handler对象分发消息，经过一系列的分发之后调用了ActivityThread的handleLaunchActivity方法： 1234567891011121314151617[-&gt; ActivityThread.java] private void handleLaunchActivity(ActivityClientRecord r, Intent customIntent, String reason) &#123; ...... // Initialize before creating the activity WindowManagerGlobal.initialize(); Activity a = performLaunchActivity(r, customIntent); if (a != null) &#123; r.createdConfig = new Configuration(mConfiguration); reportSizeConfigurations(r); Bundle oldState = r.state; handleResumeActivity(r.token, false, r.isForward, !r.activity.mFinished &amp;&amp; !r.startsNotResumed, r.lastProcessedSeq, reason); &#125; ......&#125; 可以发现这里调用了performLauncherActivity，看名字应该就是执行Activity的启动操作了…… 6.6、ActivityThread.performLaunchActivity()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[-&gt; ActivityThread.java] private Activity performLaunchActivity(ActivityClientRecord r, Intent customIntent) &#123; ActivityInfo aInfo = r.activityInfo; if (r.packageInfo == null) &#123; r.packageInfo = getPackageInfo(aInfo.applicationInfo, r.compatInfo, Context.CONTEXT_INCLUDE_CODE); &#125; ComponentName component = r.intent.getComponent(); if (component == null) &#123; component = r.intent.resolveActivity( mInitialApplication.getPackageManager()); r.intent.setComponent(component); &#125; if (r.activityInfo.targetActivity != null) &#123; component = new ComponentName(r.activityInfo.packageName, r.activityInfo.targetActivity); &#125; Activity activity = null; try &#123; java.lang.ClassLoader cl = r.packageInfo.getClassLoader(); activity = mInstrumentation.newActivity( cl, component.getClassName(), r.intent); StrictMode.incrementExpectedActivityCount(activity.getClass()); r.intent.setExtrasClassLoader(cl); r.intent.prepareToEnterProcess(); if (r.state != null) &#123; r.state.setClassLoader(cl); &#125; &#125; ...... try &#123; Application app = r.packageInfo.makeApplication(false, mInstrumentation); ...... if (activity != null) &#123; Context appContext = createBaseContextForActivity(r, activity); ...... activity.attach(appContext, this, getInstrumentation(), r.token, r.ident, app, r.intent, r.activityInfo, title, r.parent, r.embeddedID, r.lastNonConfigurationInstances, config, r.referrer, r.voiceInteractor, window); ...... activity.mCalled = false; if (r.isPersistable()) &#123; mInstrumentation.callActivityOnCreate(activity, r.state, r.persistentState); &#125; else &#123; mInstrumentation.callActivityOnCreate(activity, r.state); &#125; ...... //生命周期onStart、onresume if (!r.activity.mFinished) &#123; activity.performStart(); r.stopped = false; &#125; ...... return activity;&#125; 可以发现这里我们需要的Activity对象终于是创建出来了，然后在代码中其调用Instrumentation的callActivityOnCreate方法。 6.7、Instrumentation.callActivityOnCreate()1234567 [-&gt;Instrumentation.java] public void callActivityOnCreate(Activity activity, Bundle icicle, PersistableBundle persistentState) &#123; prePerformCreate(activity); activity.performCreate(icicle, persistentState); postPerformCreate(activity);&#125; 然后执行activity的performCreate方法…… 6.8、Activity.performCreate()123456 final void performCreate(Bundle icicle, PersistableBundle persistentState) &#123; restoreHasCurrentPermissionRequest(icicle); onCreate(icicle, persistentState); mActivityTransitionState.readState(icicle); performCreateCommon();&#125; 简要说明剩余生命周期： 回到我们的performLaunchActivity方法，其在调用了mInstrumentation.callActivityOnCreate方法之后又调用了activity.performStart()方法，看一下他的实现方式： 123456789101112131415161718[-&gt;Activity.java]final void performStart() &#123; mActivityTransitionState.setEnterActivityOptions(this, getActivityOptions()); mFragments.noteStateNotSaved(); mCalled = false; mFragments.execPendingActions(); mInstrumentation.callActivityOnStart(this); if (!mCalled) &#123; throw new SuperNotCalledException( \"Activity \" + mComponent.toShortString() + \" did not call through to super.onStart()\"); &#125; mFragments.dispatchStart(); mFragments.reportLoaderStart(); ...... mActivityTransitionState.enterReady(this);&#125; 还是通过Instrumentation调用callActivityOnStart方法： 12345[-&gt;Instrumentation.java]public void callActivityOnStart(Activity activity) &#123; activity.onStart();&#125; 然后是直接调用activity的onStart方法，第三个生命周期方法出现了，O(∩_∩)O 还是回到我们刚刚的handleLaunchActivity方法，在调用完performLaunchActivity方法之后，其有吊用了handleResumeActivity方法，好吧，看名字应该是回调Activity的onResume方法的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[-&gt; ActivityThread.java] final void handleResumeActivity(IBinder token, boolean clearHide, boolean isForward, boolean reallyResume, int seq, String reason) &#123; ActivityClientRecord r = mActivities.get(token); ...... // TODO Push resumeArgs into the activity for consideration r = performResumeActivity(token, clearHide, reason); ..... if (r.window == null &amp;&amp; !a.mFinished &amp;&amp; willBeVisible) &#123; r.window = r.activity.getWindow(); View decor = r.window.getDecorView(); decor.setVisibility(View.INVISIBLE); ViewManager wm = a.getWindowManager(); WindowManager.LayoutParams l = r.window.getAttributes(); a.mDecor = decor; l.type = WindowManager.LayoutParams.TYPE_BASE_APPLICATION; l.softInputMode |= forwardBit; if (r.mPreserveWindow) &#123; a.mWindowAdded = true; r.mPreserveWindow = false; // Normally the ViewRoot sets up callbacks with the Activity // in addView-&gt;ViewRootImpl#setView. If we are instead reusing // the decor view we have to notify the view root that the // callbacks may have changed. ViewRootImpl impl = decor.getViewRootImpl(); if (impl != null) &#123; impl.notifyChildRebuilt(); &#125; &#125; if (a.mVisibleFromClient &amp;&amp; !a.mWindowAdded) &#123; a.mWindowAdded = true; wm.addView(decor, l); &#125; // If the window has already been added, but during resume // we started another activity, then don't yet make the // window visible. &#125; else if (!willBeVisible) &#123; if (localLOGV) Slog.v( TAG, \"Launch \" + r + \" mStartedActivity set\"); r.hideForNow = true; &#125; if (!r.onlyLocalRequest) &#123; r.nextIdle = mNewActivities; mNewActivities = r; if (localLOGV) Slog.v( TAG, \"Scheduling idle handler for \" + r); //线程空闲，也就是activity创建完毕之后，它会执行queueIdle里面的代码。 Looper.myQueue().addIdleHandler(new Idler()); &#125; &#125; 可以发现其resumeActivity的逻辑调用到了performResumeActivity方法，我们来看一下performResumeActivity是如何实现的。 1234567891011121314151617181920212223242526272829[-&gt; ActivityThread.java] public final ActivityClientRecord performResumeActivity(IBinder token, boolean clearHide, String reason) &#123; ActivityClientRecord r = mActivities.get(token); if (localLOGV) Slog.v(TAG, \"Performing resume of \" + r + \" finished=\" + r.activity.mFinished); if (r != null &amp;&amp; !r.activity.mFinished) &#123; ... try &#123; ...... r.activity.performResume(); for (int i = mRelaunchingActivities.size() - 1; i &gt;= 0; i--) &#123; final ActivityClientRecord relaunching = mRelaunchingActivities.get(i); if (relaunching.token == r.token &amp;&amp; relaunching.onlyLocalRequest &amp;&amp; relaunching.startsNotResumed) &#123; relaunching.startsNotResumed = false; &#125; &#125; EventLog.writeEvent(LOG_AM_ON_RESUME_CALLED, UserHandle.myUserId(), r.activity.getComponentName().getClassName(), reason); r.paused = false; r.stopped = false; r.state = null; r.persistentState = null; &#125; ...... &#125; return r; &#125; 在方法体中，最终调用了r.activity.performResume()方法，好吧，这个方法是Activity中定义的方法，我们需要在Activity中查看这个方法的具体实现： 12345678[-&gt; Activity.java]final void performResume() &#123; performRestart(); ... mInstrumentation.callActivityOnResume(this); ... &#125; 可以看到第一个分支走了performRestart()，这个方法即时onRestart()生命周期。 12345678910111213141516171819202122 final void performRestart() &#123; ... mInstrumentation.callActivityOnRestart(this); performStart();&#125; final void performRestart() &#123; mFragments.noteStateNotSaved(); if (mToken != null &amp;&amp; mParent == null) &#123; // No need to check mStopped, the roots will check if they were actually stopped. WindowManagerGlobal.getInstance().setStoppedState(mToken, false /* stopped */); &#125; if (mStopped) &#123; mStopped = false; ...... mCalled = false; mInstrumentation.callActivityOnRestart(this); ...... performStart(); &#125; &#125; 可以看到首先判断当前activity是否为Stopped状态，是才会走OnRestart()-&gt;Onstart()生命周期。 继续看下performResume()第二个分支，又是熟悉的味道，通过Instrumentation来调用了callActivityOnResume方法。。。 12345678910111213141516[-&gt;Instrumentation.java]public void callActivityOnResume(Activity activity) &#123; activity.mResumed = true; activity.onResume(); if (mActivityMonitors != null) &#123; synchronized (mSync) &#123; final int N = mActivityMonitors.size(); for (int i=0; i&lt;N; i++) &#123; final ActivityMonitor am = mActivityMonitors.get(i); am.match(activity, activity, activity.getIntent()); &#125; &#125; &#125; &#125; O(∩_∩)O，第四个生命周期方法出现了，onResume方法。。。 终于回调onResume方法了，这时候我们的界面应该已经展示出来了，照理来说我们的Activity应该已经启动完成了，但是还没有。 有一个问题，Activity a 启动 Activity b 会触发那些生命周期方法？ 你可能会回答？b的onCreate onStart方法，onResume方法 a的onPause方法和onStop方法，onStop方法还没回调，O(∩_∩)O，对了缺少的就是对onStop方法的回调。 七、栈顶Activity执行onStop方法 Looper.myQueue().addIdleHandler(new Idler())-&gt; Idler.queueIdle()ActivityManagerNative.getDefault().activityIdle()ActivityManagerService.activityIdle()ActivityStackSupervisor.activityIdleInternalLocked()ActivityStack.stopActivityLocked()IApplicationThread.scheduleStopActivity()ActivityThread.scheduleStopActivity()-&gt; ActivityThread.sendMessage()ActivityThread.H.sendMessage()-&gt; ActivityThread.H.handleMessage()ActivityThread.handleStopActivity()ActivityThread.performStopActivityInner()ActivityThread.callCallActivityOnSaveInstanceState()Instrumentation.callActivityOnSaveInstanceState()Activity.performSaveInstanceState()-&gt; Activity.onSaveInstanceState()Activity.performStop()-&gt; Instrumentation.callActivityOnStop()Activity.onStop() 回到我们的handleResumeActivity方法，在方法体最后有这样的一代码： 1Looper.myQueue().addIdleHandler(new Idler()); 这段代码是异步消息机制相关的代码，我们可以看一下Idler对象的具体实现： 1234567891011121314151617181920212223242526private class Idler implements MessageQueue.IdleHandler &#123; @Override public final boolean queueIdle() &#123; ActivityClientRecord a = mNewActivities; ..... if (a != null) &#123; mNewActivities = null; IActivityManager am = ActivityManagerNative.getDefault(); ActivityClientRecord prev; do &#123; ...... if (a.activity != null &amp;&amp; !a.activity.mFinished) &#123; try &#123; am.activityIdle(a.token, a.createdConfig, stopProfiling); a.createdConfig = null; &#125; catch (RemoteException ex) &#123; // Ignore &#125; &#125; prev = a; a = a.nextIdle; prev.nextIdle = null; &#125; while (a != null); &#125; &#125; &#125; 这样当Messagequeue执行add方法之后就会回调其queueIdle()方法，我们可以看到在方法体中其调用了ActivityManagerNative.getDefault().activityIdle()，好吧，熟悉了Binder机制以后我们知道这段代码会执行到ActivityManagerService的activityIdle方法： 123456789101112131415161718192021@Overridepublic final void activityIdle(IBinder token, Configuration config, boolean stopProfiling) &#123; final long origId = Binder.clearCallingIdentity(); synchronized (this) &#123; ActivityStack stack = ActivityRecord.getStackLocked(token); if (stack != null) &#123; ActivityRecord r = mStackSupervisor.activityIdleInternalLocked(token, false, config); if (stopProfiling) &#123; if ((mProfileProc == r.app) &amp;&amp; (mProfileFd != null)) &#123; try &#123; mProfileFd.close(); &#125; catch (IOException e) &#123; &#125; clearProfilerLocked(); &#125; &#125; &#125; &#125; Binder.restoreCallingIdentity(origId);&#125; 然后在activityIdle方法中又调用了ActivityStackSupervisor.activityIdleInternalLocked方法： 12345678910111213141516171819[-&gt;ActivityStackSupervisor.java]final ActivityRecord activityIdleInternalLocked(final IBinder token, boolean fromTimeout, Configuration config) &#123; ... for (int i = 0; i &lt; NS; i++) &#123; r = stops.get(i); final ActivityStack stack = r.task.stack; if (stack != null) &#123; if (r.finishing) &#123; stack.finishCurrentActivityLocked(r, ActivityStack.FINISH_IMMEDIATELY, false); &#125; else &#123; stack.stopActivityLocked(r); &#125; &#125; &#125; ... return r; &#125; 可以发现在其中又调用了ActivityStack.stopActivityLocked方法： 12345678final void stopActivityLocked(ActivityRecord r) &#123; if ((r.intent.getFlags()&amp;Intent.FLAG_ACTIVITY_NO_HISTORY) != 0 || (r.info.flags&amp;ActivityInfo.FLAG_NO_HISTORY) != 0) &#123; ... r.app.thread.scheduleStopActivity(r.appToken, r.visible, r.configChangeFlags); ... &#125; &#125; 好吧，又是相同的逻辑通过IApplicationThread.scheduleStopActivity,最终调用了ActivityThread.scheduleStopActivity()方法。。。。 123456public final void scheduleStopActivity(IBinder token, boolean showWindow, int configChanges) &#123; sendMessage( showWindow ? H.STOP_ACTIVITY_SHOW : H.STOP_ACTIVITY_HIDE, token, 0, configChanges); &#125; 然后执行sendMessage方法，最终执行H（Handler）的sendMessage方法，并被H的handleMessge方法接收执行handleStopActivity方法。。。 123456private void handleStopActivity(IBinder token, boolean show, int configChanges) &#123; ... performStopActivityInner(r, info, show, true); ... &#125; 然后我们看一下performStopActivityInner的实现逻辑： 123456789101112131415[-&gt;ActivityThread.java]private void performStopActivityInner(ActivityClientRecord r, StopInfo info, boolean keepShown, boolean saveState) &#123; ... if (!keepShown) &#123; try &#123; r.activity.performStop(); &#125; catch (Exception e) &#123; ...... &#125; r.stopped = true; &#125; &#125; &#125; 我们看一下performStopActivityInner中调用到的Activity方法的performStop方法 1234567891011final void performStop() &#123; if (!mStopped) &#123; ...... mFragments.dispatchStop(); mCalled = false; mInstrumentation.callActivityOnStop(this); ...... mStopped = true; &#125; mResumed = false; &#125; 还是通过Instrumentation来实现的，调用了它的callActivityOnStop方法。。 123public void callActivityOnStop(Activity activity) &#123; activity.onStop(); &#125; 生命周期方法onStop()出来了。 我们来看一下Activity 的生命周期： protected void onCreate();protected void onRestart();protected void onStart();protected void onResume();protected void onPause();protected void onStop();protected void onDestory(); 前面我们分析了onCreate()、onStart()、onRestart() 、onResume()、onPause()、onStop()。 Activity 销毁时的 onDestroy() 回调都与前面的过程大同小异，这里就只列举相应的方法栈，不再继续描述。 Activity.finish()ActivityManagerNative.getDefault().finishActivity()ActivityManagerService.finishActivity()ActivityStack.requestFinishActivityLocked()ActivityStack.finishActivityLocked()ActivityStack.startPausingLocked()参考：Android源码解析之（十五）–&gt;Activity销毁流程 启动流程： 1、点击桌面App图标，Launcher进程采用Binder IPC向system_server进程发起startActivity请求；2、system_server进程接收到请求后，向zygote进程发送创建进程的请求；3、Zygote进程fork出新的子进程，即App进程；4、App进程，通过Binder IPC向sytem_server进程发起attachApplication请求；5、system_server进程在收到请求后，进行一系列准备工作后，再通过binder IPC向App进程发送scheduleLaunchActivity请求；6、App进程的binder线程（ApplicationThread）在收到请求后，通过handler向主线程发送LAUNCH_ACTIVITY消息；7、主线程在收到Message后，通过发射机制创建目标Activity，并回调Activity.onCreate()等方法。 到此，App便正式启动，开始进入Activity生命周期，执行完onCreate/onStart/onResume方法，UI渲染结束后便可以看到App的主界面。 启动Activity较为复杂，后续介绍窗口加载渲染过程，可参考文档：【Android 7.1.2 (Android N) Activity-Window加载显示流程分析】 参考文档(特别感谢)：Android源码解析之（十四）–&gt;Activity启动流程Android Activity启动过程分析Android源码解析之（十五）–&gt;Activity销毁流程凯子哥带你学Framework–Activity启动过程全解析深入理解Activity启动流程(一)–Activity启动的概要流程Android 7.0 ActivityManagerService 启动Activity的过程 系列Activity生命周期的回调，你应该知道得更多！–Android源码剖析（上）Activity生命周期的回调，你应该知道得更多！–Android源码剖析（下）","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android 7.1.2 (Android N) Android 系统启动流程 分析","slug":"Android-7-1-2-Android-N-Android系统启动流程","date":"2017-08-31T16:00:00.000Z","updated":"2018-04-19T14:29:28.620Z","comments":true,"path":"2017/09/01/Android-7-1-2-Android-N-Android系统启动流程/","link":"","permalink":"http://zhoujinjian.cc/2017/09/01/Android-7-1-2-Android-N-Android系统启动流程/","excerpt":"","text":"源码：system/core/rootdir/ init.rc init.zygote64.rc system/core/init/ init.cpp init_parser.cpp signal_handler.cpp frameworks/base/cmds/app_process/ App_main.cpp frameworks/base/core/jni/ AndroidRuntime.cpp frameworks/base/core/java/com/android/internal/os/ ZygoteInit.java Zygote.java ZygoteConnection.java frameworks/base/core/java/com/android/internal/os/ ZygoteInit.java RuntimeInit.java Zygote.java frameworks/base/core/services/java/com/android/server/ SystemServer.java frameworks/base/core/jni/ com_android_internal_os_Zygote.cpp AndroidRuntime.cpp frameworks/base/services/java/com/android/server/ SystemServer.java frameworks/base/services/core/java/com/android/server/ SystemServiceManager.java ServiceThread.java am/ActivityManagerService.java frameworks/base/core/java/android/app/ ActivityThread.java LoadedApk.java ContextImpl.java frameworks/base/core/java/android/app/ ActivityThread.java LoadedApk.java ContextImpl.java frameworks/base/services/java/com/android/server/ SystemServer.java frameworks/base/services/core/java/com/android/server/ SystemServiceManager.java ServiceThread.java pm/Installer.java am/ActivityManagerService.java 博客原图链接一、Android概述Android系统非常庞大，底层是采用Linux作为基底，上层采用带有虚拟机的Java层，通过通过JNI技术，将上下打通，融为一体。下图是Google提供的一张经典的4层架构图，从下往上，依次分为Linux内核，系统库和Android Runtime，应用框架层，应用程序层这4层架构，每一层都包含大量的子模块或子系统。 二、系统启动Google提供的4层架构图，是非常经典，但只是如垒砖般的方式，简单地分层，而不足表达Android整个系统的启动过程，环环相扣的连接关系，本文更多的是以进程的视角，以分层的架构来诠释Android系统的全貌。 系统启动架构图 三、设备启动过程3.1、Bootloader引导Boot ROM: 当手机处于关机状态时，长按Power键开机，引导芯片开始从固化在ROM里的预设出代码开始执行，然后加载引导程序到RAM； Boot Loader：这是启动Android系统之前的引导程序，主要是检查RAM，初始化硬件设备（如CPU、内存、Flash等）并且通过建立内存空间映射，为装载Linux内核准备合适的环境。一旦Linux内核装载完毕，Bootloader将会从内存中清除掉。 如果用户在Bootloader运行期间，按下预定义的组合健，可以进入系统的更新模块。Android的下载更新可以选择进入Fastboot模式或者Recovery模式。 Fastboot是Android设计的一套通过USB来更新手机分区映像的协议，方便开发人员能快速更新指定的手机分区。但是一般的零售机上往往去掉了Fastboot，Google销售的开发机则带有Fastboot模块。 Recovery模式是Android特有的升级系统。利用Recovery模式，手机可以进行恢复出厂设置或进行OTA、补丁和固件升级。进入Recovery模式实际上是启动了一个文本模式的Linux。 3.2、装载和启动Linux内核到这里才刚刚开始进入Android系统. 启动Kernel的0号进程：初始化进程管理、内存管理，加载Display,Camera Driver，Binder Driver等相关工作； 启动kthreadd进程（pid=2）：是Linux系统的内核进程，会创建内核工作线程kworkder，软中断线程ksoftirqd，thermal等内核守护进程。kthreadd进程是所有内核进程的鼻祖。 Android的boot.img存放的就是Linux内核和一个根文件系统。Bootloader会把boot.img映像装载进内存。然后Linux内核会执行整个系统的初始化，完成后装载根文件系统，最后启动Init进程。 3.3、启动Init进程Linux内核加载完毕后，会首先启动Init进程，Init进程是系统的第一个进程。在Init进程的启动过程中，会解析Linux的配置脚本init.rc文件，根据init.rc文件的内容，Init进程会装载Android的文件系统、创建系统目录。初始 化属性系统、启动Android系统重要的守护进程，这些进程包括USB守护进程、adb守护进程、vold守护进程、rild守护进程。 启动init进程(pid=1),是Linux系统的用户进程，init进程是所有用户进程的鼻祖。 init进程启动Media Server(多媒体服务)、servicemanager(binder服务管家)、bootanim(开机动画)等重要服务 init进程还会孵化出installd、ueventd、adbd、等用户守护进程； init进程孵化出Zygote进程，Zygote进程是Android系统的首个Java进程，Zygote是所有Java进程的父进程，Zygote进程本身是由init进程孵化而来的。 Android 7.0 init.rc的一点改变 - 哈哈的个人专栏 - CSDN博客 12345678910111213/system/core/rootdir/init.rc.....service ueventd /sbin/ueventdclass corecriticalseclabel u:r:ueventd:s0service healthd /sbin/healthdclass corecriticalseclabel u:r:healthd:s0group root system wakelock...... 3.4、启动Zygote进程init进程初始化结束时，会启动Zygote进程。Zygote进程负责fork出应用进程，是所有应用进程的父进程。Zygote进程初始化时会创建Dalivik虚拟机、预装系统的资源文件和Java类。所有从Zygote进程fork出的用户进程将继承和共享这些预加载的资源，不用浪费时间重新加载，加快了应用程序的启动过程。启动结束后，Zygote进程也将变成守护进程，负责响应和启动APK应用程序的请求： 1234567891011/system/core/rootdir/init.zygote64.rcservice zygote /system/bin/app_process64 -Xzygote /system/bin --zygote --start-system-serverclass mainsocket zygote stream 660 root systemonrestart write /sys/android_power/request_state wakeonrestart write /sys/power/state ononrestart restart audioserveronrestart restart cameraserveronrestart restart mediaonrestart restart netdwritepid /dev/cpuset/foreground/tasks 3.5、启动SystemServerSystemServer是Zygote进程fork出的第一个进程，也是整个Android系统的核心进程。在SystemServer中运行着系统大部分的Binder服务，SystemServer首先启动本地服务SensorService；接着启动ActivityManagerService、WindowManagerService、PackageManagerService在内的所有Java服务。 Zygote进程fork出System Server进程，System Server是Zygote孵化的第一个进程，地位非常重要； System Server进程：负责启动和管理整个Java framework，包含ActivityManager，PowerManager等服务。 Media Server进程：负责启动和管理整个C++ framework，包含AudioFlinger，Camera Service等服务。 12/frameworks/base/services/java/com/android/server/SystemServer.javaSystemServer().run() 3.6、启动ActivityManagerService3.7、启动Launcher(Activity)SystemServer加载完所有的Java服务后，最后会调用ActivityManagerService的SystemReady()方法，在这个方法的执行中，会发出Intent”android.intent.category.HOME”。凡是响应这个Intent的APK都会运行起来，Launcher应用就是Android系统默认的桌面应用，一般只有它会响应这个Intent，因此，系统开机后，第一个运行的应用就是Launcher。 Zygote进程孵化出的第一个App进程是Launcher； 12/frameworks/base/services/core/java/com/android/server/am/ActivityManagerService.javastartHomeActivityLocked(mCurrentUserId, \"systemReady\"); 四、设备启动过程详细分析（1）、启动Init进程概述： init是Linux系统中用户空间的第一个进程，进程号为1。Kernel启动后，在用户空间，启动init进程，并调用init中的main()方法执行init进程的职责。对于init进程的功能分为4部分： 分析和运行所有的init.rc文件; 生成设备驱动节点; （通过rc文件创建） 处理子进程的终止(signal方式); 提供属性服务。 接下来从main()方法说起。 4.1.1、main()[-&gt; init.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168int main(int argc, char** argv) &#123;if (!strcmp(basename(argv[0]), \"ueventd\")) &#123; return ueventd_main(argc, argv);&#125;if (!strcmp(basename(argv[0]), \"watchdogd\")) &#123; return watchdogd_main(argc, argv);&#125;// Clear the umask.//设置文件属性0777umask(0);add_environment(\"PATH\", _PATH_DEFPATH);bool is_first_stage = (argc == 1) || (strcmp(argv[1], \"--second-stage\") != 0);// Get the basic filesystem setup we need put together in the initramdisk// on / and then we'll let the rc file figure out the rest.//创建文件系统目录并挂载相关的文件系统if (is_first_stage) &#123; mount(\"tmpfs\", \"/dev\", \"tmpfs\", MS_NOSUID, \"mode=0755\"); mkdir(\"/dev/pts\", 0755); mkdir(\"/dev/socket\", 0755); mount(\"devpts\", \"/dev/pts\", \"devpts\", 0, NULL); #define MAKE_STR(x) __STRING(x) mount(\"proc\", \"/proc\", \"proc\", 0, \"hidepid=2,gid=\" MAKE_STR(AID_READPROC)); mount(\"sysfs\", \"/sys\", \"sysfs\", 0, NULL);&#125;// We must have some place other than / to create the device nodes for// kmsg and null, otherwise we won't be able to remount / read-only// later on. Now that tmpfs is mounted on /dev, we can actually talk// to the outside world.//屏蔽标准的输入输出open_devnull_stdio(); //初始化kernel log，位于设备节点/dev/kmsgklog_init(); //设置输出的log级别klog_set_level(KLOG_NOTICE_LEVEL); // 输出init启动阶段的logNOTICE(\"init %s started!\\n\", is_first_stage ? \"first stage\" : \"second stage\");if (!is_first_stage) &#123; // Indicate that booting is in progress to background fw loaders, etc. close(open(\"/dev/.booting\", O_WRONLY | O_CREAT | O_CLOEXEC, 0000)); //创建一块共享的内存空间，用于属性服务 property_init(); // If arguments are passed both on the command line and in DT, // properties set in DT always have priority over the command-line ones. process_kernel_dt(); process_kernel_cmdline(); // Propagate the kernel variables to internal variables // used by init as well as the current required properties. export_kernel_boot_props();&#125;// Set up SELinux, including loading the SELinux policy if we're in the kernel domain.selinux_initialize(is_first_stage);// If we're in the kernel domain, re-exec init to transition to the init domain now// that the SELinux policy has been loaded.if (is_first_stage) &#123; if (restorecon(\"/init\") == -1) &#123; ERROR(\"restorecon failed: %s\\n\", strerror(errno)); security_failure(); &#125; char* path = argv[0]; char* args[] = &#123; path, const_cast&lt;char*&gt;(\"--second-stage\"), nullptr &#125;; if (execv(path, args) == -1) &#123; ERROR(\"execv(\\\"%s\\\") failed: %s\\n\", path, strerror(errno)); security_failure(); &#125;&#125;// These directories were necessarily created before initial policy load// and therefore need their security context restored to the proper value.// This must happen before /dev is populated by ueventd.NOTICE(\"Running restorecon...\\n\");restorecon(\"/dev\");restorecon(\"/dev/socket\");restorecon(\"/dev/__properties__\");restorecon(\"/property_contexts\");restorecon_recursive(\"/sys\");epoll_fd = epoll_create1(EPOLL_CLOEXEC);if (epoll_fd == -1) &#123; ERROR(\"epoll_create1 failed: %s\\n\", strerror(errno)); exit(1);&#125;//初始化子进程退出的信号处理过程signal_handler_init();property_load_boot_defaults();export_oem_lock_status();start_property_service();const BuiltinFunctionMap function_map;Action::set_function_map(&amp;function_map);Parser&amp; parser = Parser::GetInstance();parser.AddSectionParser(\"service\",std::make_unique&lt;ServiceParser&gt;());parser.AddSectionParser(\"on\", std::make_unique&lt;ActionParser&gt;());parser.AddSectionParser(\"import\", std::make_unique&lt;ImportParser&gt;());parser.ParseConfig(\"/init.rc\");ActionManager&amp; am = ActionManager::GetInstance();am.QueueEventTrigger(\"early-init\");// Queue an action that waits for coldboot done so we know ueventd has set up all of /dev...am.QueueBuiltinAction(wait_for_coldboot_done_action, \"wait_for_coldboot_done\");// ... so that we can start queuing up actions that require stuff from /dev.am.QueueBuiltinAction(mix_hwrng_into_linux_rng_action, \"mix_hwrng_into_linux_rng\");am.QueueBuiltinAction(set_mmap_rnd_bits_action, \"set_mmap_rnd_bits\");am.QueueBuiltinAction(keychord_init_action, \"keychord_init\");am.QueueBuiltinAction(console_init_action, \"console_init\");// Trigger all the boot actions to get us started.am.QueueEventTrigger(\"init\");// Repeat mix_hwrng_into_linux_rng in case /dev/hw_random or /dev/random// wasn't ready immediately after wait_for_coldboot_doneam.QueueBuiltinAction(mix_hwrng_into_linux_rng_action, \"mix_hwrng_into_linux_rng\");// Don't mount filesystems or start core system services in charger mode.std::string bootmode = property_get(\"ro.bootmode\");if (bootmode == \"charger\") &#123; am.QueueEventTrigger(\"charger\");&#125; else &#123; am.QueueEventTrigger(\"late-init\");&#125;// Run all property triggers based on current state of the properties.am.QueueBuiltinAction(queue_property_triggers_action, \"queue_property_triggers\");while (true) &#123; if (!waiting_for_exec) &#123; am.ExecuteOneCommand(); restart_processes(); &#125; int timeout = -1; if (process_needs_restart) &#123; timeout = (process_needs_restart - gettime()) * 1000; if (timeout &lt; 0) timeout = 0; &#125; if (am.HasMoreCommands()) &#123; timeout = 0; &#125; bootchart_sample(&amp;timeout); epoll_event ev; int nr = TEMP_FAILURE_RETRY(epoll_wait(epoll_fd, &amp;ev, 1, timeout)); if (nr == -1) &#123; ERROR(\"epoll_wait failed: %s\\n\", strerror(errno)); &#125; else if (nr == 1) &#123; ((void (*)()) ev.data.ptr)(); &#125;&#125;return 0;&#125; 4.1.2、创建文件系统目录并挂载相关的文件系统此时android的log系统还没有启动，采用kernel的log系统，打开的设备节点/dev/kmsg， 那么可通过cat /dev/kmsg来获取内核log。 接下来，设置log的输出级别为KLOG_NOTICE_LEVEL(5)，当log级别小于5时则会输出到kernel log， 默认值为3. 123456789101112131415add_environment(\"PATH\", _PATH_DEFPATH);bool is_first_stage = (argc == 1) || (strcmp(argv[1], \"--second-stage\") != 0);// Get the basic filesystem setup we need put together in the initramdisk// on / and then we'll let the rc file figure out the rest.if (is_first_stage) &#123; mount(\"tmpfs\", \"/dev\", \"tmpfs\", MS_NOSUID, \"mode=0755\"); mkdir(\"/dev/pts\", 0755); mkdir(\"/dev/socket\", 0755); mount(\"devpts\", \"/dev/pts\", \"devpts\", 0, NULL); #define MAKE_STR(x) __STRING(x) mount(\"proc\", \"/proc\", \"proc\", 0, \"hidepid=2,gid=\" MAKE_STR(AID_READPROC)); mount(\"sysfs\", \"/sys\", \"sysfs\", 0, NULL);&#125; 该部分主要用于创建和挂载启动所需的文件目录。 需要注意的是，在编译Android系统源码时，在生成的根文件系统中，并不存在这些目录，它们是系统运行时的目录，即当系统终止时，就会消失。 在init初始化过程中，Android分别挂载了tmpfs，devpts，proc，sysfs这4类文件系统。 tmpfs是一种虚拟内存文件系统，它会将所有的文件存储在虚拟内存中，如果你将tmpfs文件系统卸载后，那么其下的所有的内容将不复存在。 tmpfs既可以使用RAM，也可以使用交换分区，会根据你的实际需要而改变大小。tmpfs的速度非常惊人，毕竟它是驻留在RAM中的，即使用了交换分区，性能仍然非常卓越。 由于tmpfs是驻留在RAM的，因此它的内容是不持久的。断电后，tmpfs的内容就消失了，这也是被称作tmpfs的根本原因。 devpts文件系统为伪终端提供了一个标准接口，它的标准挂接点是/dev/ pts。只要pty的主复合设备/dev/ptmx被打开，就会在/dev/pts下动态的创建一个新的pty设备文件。 proc文件系统是一个非常重要的虚拟文件系统，它可以看作是内核内部数据结构的接口，通过它我们可以获得系统的信息，同时也能够在运行时修改特定的内核参数。 与proc文件系统类似，sysfs文件系统也是一个不占有任何磁盘空间的虚拟文件系统。它通常被挂接在/sys目录下。sysfs文件系统是Linux2.6内核引入的，它把连接在系统上的设备和总线组织成为一个分级的文件，使得它们可以在用户空间存取。 4.1.4、屏蔽标准的输入输出[-&gt; init.cpp] 12345678910111213141516171819202122232425void open_devnull_stdio(void)&#123;// Try to avoid the mknod() call if we can. Since SELinux makes// a /dev/null replacement available for free, let's use it.int fd = open(\"/sys/fs/selinux/null\", O_RDWR);if (fd == -1) &#123; // OOPS, /sys/fs/selinux/null isn't available, likely because // /sys/fs/selinux isn't mounted. Fall back to mknod. static const char *name = \"/dev/__null__\"; if (mknod(name, S_IFCHR | 0600, (1 &lt;&lt; 8) | 3) == 0) &#123; fd = open(name, O_RDWR); unlink(name); &#125; if (fd == -1) &#123; exit(1); &#125;&#125;dup2(fd, 0);dup2(fd, 1);dup2(fd, 2);if (fd &gt; 2) &#123; close(fd);&#125;&#125; 前文生成/dev目录后，init进程将调用open_devnull_stdio函数，屏蔽标准的输入输出。 open_devnull_stdio函数会在/dev目录下生成null设备节点文件，并将标准输入、标准输出、标准错误输出全部重定向到null设备中。 open_devnull_stdio函数定义于system/core/init/util.cpp中。 这里需要说明的是，dup2函数的作用是用来复制一个文件的描述符，通常用来重定向进程的stdin、stdout和stderr。它的函数原形是： int dup2(int oldfd, int targetfd) 该函数执行后，targetfd将变成oldfd的复制品。 因此上述过程其实就是：创建出null设备后，将0、1、2绑定到null设备上。因此init进程调用open_devnull_stdio函数后，通过标准的输入输出无法输出信息。 4.1.5、初始化内核log系统我们继续回到init进程的main函数，init进程通过klog_init函数，提供输出log信息的设备。 [-&gt; init.cpp] 1234567891011121314151617klog_init();klog_set_level(KLOG_NOTICE_LEVEL);void klog_init(void) &#123;if (klog_fd &gt;= 0) return; /* Already initialized */klog_fd = open(\"/dev/kmsg\", O_WRONLY | O_CLOEXEC);if (klog_fd &gt;= 0) &#123; return;&#125;static const char* name = \"/dev/__kmsg__\";if (mknod(name, S_IFCHR | 0600, (1 &lt;&lt; 8) | 11) == 0) &#123; klog_fd = open(name, O_WRONLY | O_CLOEXEC); unlink(name);&#125;&#125; klog_init函数定义于system/core/libcutils/klog.c中。通过klog_init函数，init进程生成kmsg设备节点文件。该设备可以调用内核信息输出函数printk，以输出log信息。 4.1.6、初始化属性域12345if (!is_first_stage) &#123;.......property_init();.......&#125; 调用property_init初始化属性域。在Android平台中，为了让运行中的所有进程共享系统运行时所需要的各种设置值，系统开辟了属性存储区域，并提供了访问该区域的API。 这里存在一个问题是，在init进程中有部分代码块以is_first_stage标志进行区分，决定是否需要进行初始化。 is_first_stage的值，由init进程main函数的入口参数决定，之前不太明白具体的含义。 后来写博客后，有朋友留言，在引入selinux机制后，有些操作必须要在内核态才能完成； 但init进程作为android的第一个进程，又是运行在用户态的。 于是，最终设计为用is_first_stage进行区分init进程的运行状态。init进程在运行的过程中，会完成从内核态到用户态的切换。 123456void property_init() &#123;if (__system_property_area_init()) &#123; ERROR(\"Failed to initialize property area\\n\"); exit(1);&#125;&#125; property_init函数定义于system/core/init/property_service.cpp中，如上面代码所示，最终调用_system_property_area_init函数初始化属性域。 4.1.7、完成SELinux相关工作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// Set up SELinux, including loading the SELinux policy if we're in the kernel domain.selinux_initialize(is_first_stage);static void selinux_initialize(bool in_kernel_domain) &#123;Timer t;selinux_callback cb;//用于打印log的回调函数cb.func_log = selinux_klog_callback;selinux_set_callback(SELINUX_CB_LOG, cb);//用于检查权限的回调函数cb.func_audit = audit_callback;selinux_set_callback(SELINUX_CB_AUDIT, cb);if (in_kernel_domain) &#123; //内核态处理流程 INFO(\"Loading SELinux policy...\\n\"); //用于加载sepolicy文件。该函数最终将sepolicy文件传递给kernel，这样kernel就有了安全策略配置文件，后续的MAC才能开展起来。 if (selinux_android_load_policy() &lt; 0) &#123; ERROR(\"failed to load policy: %s\\n\", strerror(errno)); security_failure(); &#125; //内核中读取的信息 bool kernel_enforcing = (security_getenforce() == 1); //命令行中得到的数据 bool is_enforcing = selinux_is_enforcing(); if (kernel_enforcing != is_enforcing) &#123; //用于设置selinux的工作模式。selinux有两种工作模式： //1、”permissive”，所有的操作都被允许（即没有MAC），但是如果违反权限的话，会记录日志 //2、”enforcing”，所有操作都会进行权限检查。在一般的终端中，应该工作于enforing模式 if (security_setenforce(is_enforcing)) &#123; ERROR(\"security_setenforce(%s) failed: %s\\n\", is_enforcing ? \"true\" : \"false\", strerror(errno)); //将重启进入recovery mode security_failure(); &#125; &#125; if (write_file(\"/sys/fs/selinux/checkreqprot\", \"0\") == -1) &#123; security_failure(); &#125; NOTICE(\"(Initializing SELinux %s took %.2fs.)\\n\", is_enforcing ? \"enforcing\" : \"non-enforcing\", t.duration());&#125; else &#123; selinux_init_all_handles();&#125;&#125; init进程进程调用selinux_initialize启动SELinux。从注释来看，init进程的运行确实是区分用户态和内核态的。 4.1.8、重新设置属性1234567891011121314151617181920212223242526// If we're in the kernel domain, re-exec init to transition to the init domain now// that the SELinux policy has been loaded.if (is_first_stage) &#123;//按selinux policy要求，重新设置init文件属性 if (restorecon(\"/init\") == -1) &#123; ERROR(\"restorecon failed: %s\\n\", strerror(errno)); security_failure(); &#125; char* path = argv[0]; char* args[] = &#123; path, const_cast&lt;char*&gt;(\"--second-stage\"), nullptr &#125;; //这里就是前面所说的，启动用户态的init进程，即second-stage if (execv(path, args) == -1) &#123; ERROR(\"execv(\\\"%s\\\") failed: %s\\n\", path, strerror(errno)); security_failure(); &#125;&#125;// These directories were necessarily created before initial policy load// and therefore need their security context restored to the proper value.// This must happen before /dev is populated by ueventd.NOTICE(\"Running restorecon...\\n\");restorecon(\"/dev\");restorecon(\"/dev/socket\");restorecon(\"/dev/__properties__\");restorecon(\"/property_contexts\");restorecon_recursive(\"/sys\"); 上述文件节点在加载Sepolicy之前已经被创建了，因此在加载完Sepolicy后，需要重新设置相关的属性。 4.1.9、创建epoll句柄如下面代码所示，init进程调用epoll_create1创建epoll句柄。 12345epoll_fd = epoll_create1(EPOLL_CLOEXEC);if (epoll_fd == -1) &#123; ERROR(\"epoll_create1 failed: %s\\n\", strerror(errno)); exit(1);&#125; 在linux的网络编程中，很长的时间都在使用select来做事件触发。在linux新的内核中，有了一种替换它的机制，就是epoll。相比于select，epoll最大的好处在于它不会随着监听fd数目的增长而降低效率。因为在内核中的select实现中，它是采用轮询来处理的，轮询的fd数目越多，自然耗时越多。 epoll机制一般使用epoll_create(int size)函数创建epoll句柄，size用来告诉内核这个句柄可监听的fd的数目。注意这个参数不同于select()中的第一个参数，在select中需给出最大监听数加1的值。 此外，当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，能够看到创建出的fd，因此在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 上述代码使用的epoll_create1(EPOLLCLOEXEC)来创建epoll句柄，该标志位表示生成的epoll fd具有”执行后关闭”特性。 4.1.10、装载子进程信号处理器12345678910111213141516171819202122232425void signal_handler_init() &#123;// Create a signalling mechanism for SIGCHLD.int s[2];//利用socketpair创建出已经连接的两个socket，分别作为信号的读、写端if (socketpair(AF_UNIX, SOCK_STREAM | SOCK_NONBLOCK | SOCK_CLOEXEC, 0, s) == -1) &#123; ERROR(\"socketpair failed: %s\\n\", strerror(errno)); exit(1);&#125;signal_write_fd = s[0];signal_read_fd = s[1];// Write to signal_write_fd if we catch SIGCHLD.struct sigaction act;memset(&amp;act, 0, sizeof(act));//信号处理器为SIGCHLD_handler，其被存在sigaction结构体中，负责处理SIGCHLD消息act.sa_handler = SIGCHLD_handler;act.sa_flags = SA_NOCLDSTOP;//调用信号安装函数sigaction，将监听的信号及对应的信号处理器注册到内核中sigaction(SIGCHLD, &amp;act, 0);//相对于6.0的代码，进一步作了封装，用于终止出现问题的子进程，详细代码于后文分析。ServiceManager::GetInstance().ReapAnyOutstandingChildren();register_epoll_handler(signal_read_fd, handle_signal);&#125; Linux进程通过互相发送接收消息来实现进程间的通信，这些消息被称为”信号”。每个进程在处理其它进程发送的信号时都要注册处理者，处理者被称为信号处理器。 注意到sigaction结构体的sa_flags为SA_NOCLDSTOP。由于系统默认在子进程暂停时也会发送信号SIGCHLD，init需要忽略子进程在暂停时发出的SIGCHLD信号，因此将act.sa_flags 置为SA_NOCLDSTOP，该标志位表示仅当进程终止时才接受SIGCHLD信号。 我们来看看SIGCHLD_handler的具体工作。 12345static void SIGCHLD_handler(int) &#123;if (TEMP_FAILURE_RETRY(write(signal_write_fd, \"1\", 1)) == -1) &#123; ERROR(\"write(signal_write_fd) failed: %s\\n\", strerror(errno));&#125;&#125; 从上面代码我们知道，init进程是所有进程的父进程，当其子进程终止产生SIGCHLD信号时，SIGCHLD_handler对signal_write_fd执行写操作。由于socketpair的绑定关系，这将触发信号对应的signal_read_fd收到数据。 在装载信号监听器的最后，signal_handler_init调用register_epoll_handler，其代码如下所示，传入参数分别为signal_read_fd和handle_signal。 123456789void register_epoll_handler(int fd, void (*fn)()) &#123;epoll_event ev;ev.events = EPOLLIN;ev.data.ptr = reinterpret_cast&lt;void*&gt;(fn);//epoll_fd增加一个监听对象fd,fd上有数据到来时，调用fn处理if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, fd, &amp;ev) == -1) &#123; ERROR(\"epoll_ctl failed: %s\\n\", strerror(errno));&#125;&#125; 根据代码，我们知道：当epoll句柄监听到signal_read_fd中有数据可读时，将调用handle_signal进行处理。 至此，结合上文我们知道：当init进程调用signal_handler_init后，一旦收到子进程终止带来的SIGCHLD消息后，将利用信号处理者SIGCHLD_handler向signal_write_fd写入信息； epoll句柄监听到signal_read_fd收消息后，将调用handle_signal进行处理。整个过程如下图所示。 1234567static void handle_signal() &#123;// Clear outstanding requests.char buf[32];read(signal_read_fd, buf, sizeof(buf));ServiceManager::GetInstance().ReapAnyOutstandingChildren();&#125; 从代码中可以看出，handle_signal只是清空signal_read_fd中的数据，然后调用ServiceManager::GetInstance().ReapAnyOutstandingChildren()。 ServiceManager定义于system/core/init/service.cpp中，是一个单例对象： 1234567891011121314............//C++中默认是private属性ServiceManager::ServiceManager() &#123;&#125;ServiceManager&amp; ServiceManager::GetInstance() &#123;static ServiceManager instance;return instance;&#125;void ServiceManager::ReapAnyOutstandingChildren() &#123;while (ReapOneProcess()) &#123;&#125;&#125;............ ReapAnyOutstandingChildren函数实际上调用了ReapOneProcess。 我们结合代码，看看ReapOneProcess的具体工作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798bool ServiceManager::ReapOneProcess() &#123;int status;//用waitpid函数获取状态发生变化的子进程pid//waitpid的标记为WNOHANG，即非阻塞，返回为正值就说明有进程挂掉了pid_t pid = TEMP_FAILURE_RETRY(waitpid(-1, &amp;status, WNOHANG));if (pid == 0) &#123; return false;&#125; else if (pid == -1) &#123; ERROR(\"waitpid failed: %s\\n\", strerror(errno)); return false;&#125;//利用FindServiceByPid函数，找到pid对应的服务。//FindServiceByPid主要通过轮询解析init.rc生成的service_list，找到pid与参数一致的srvc。Service* svc = FindServiceByPid(pid);//输出服务结束的原因.........if (!svc) &#123; return true;&#125;//结束服务，相对于6.0作了进一步的封装if (svc-&gt;Reap()) &#123; waiting_for_exec = false; //移除服务对应的信息 RemoveService(*svc);&#125;return true;&#125;bool Service::Reap() &#123;//清理未携带SVC_ONESHOT 或 携带了SVC_RESTART标志的srvc的子进程if (!(flags_ &amp; SVC_ONESHOT) || (flags_ &amp; SVC_RESTART)) &#123; NOTICE(\"Service '%s' (pid %d) killing any children in process group\\n\", name_.c_str(), pid_); kill(-pid_, SIGKILL);&#125;//清除srvc中创建出的socketfor (const auto&amp; si : sockets_) &#123; std::string tmp = StringPrintf(ANDROID_SOCKET_DIR \"/%s\", si.name.c_str()); unlink(tmp.c_str());&#125;if (flags_ &amp; SVC_EXEC) &#123; INFO(\"SVC_EXEC pid %d finished...\\n\", pid_); return true;&#125;pid_ = 0;flags_ &amp;= (~SVC_RUNNING);//对于携带了SVC_ONESHOT并且未携带SVC_RESTART的srvc，将这类服务的标志置为SVC_DISABLED，不再启动if ((flags_ &amp; SVC_ONESHOT) &amp;&amp; !(flags_ &amp; SVC_RESTART)) &#123; flags_ |= SVC_DISABLED;&#125;// Disabled and reset processes do not get restarted automatically.if (flags_ &amp; (SVC_DISABLED | SVC_RESET)) &#123; svc-&gt;NotifyStateChange(\"stopped\"); return true;&#125;time_t now = gettime();//未携带SVC_RESTART的关键服务，在规定的间隔内，crash字数过多时，会导致整机重启；if ((flags_ &amp; SVC_CRITICAL) &amp;&amp; !(flags_ &amp; SVC_RESTART)) &#123; if (time_crashed_ + CRITICAL_CRASH_WINDOW &gt;= now) &#123; if (++nr_crashed_ &gt; CRITICAL_CRASH_THRESHOLD) &#123; .......... android_reboot(ANDROID_RB_RESTART2, 0, \"recovery\"); return true; &#125; &#125; else &#123; time_crashed_ = now; nr_crashed_ = 1; &#125;&#125;//将待重启srvc的标志位置为SVC_RESTARTING（init进程将根据该标志位，重启服务）flags_ &amp;= (~SVC_RESTART);flags_ |= SVC_RESTARTING;// Execute all onrestart commands for this service.//重启在init.rc文件中带有onrestart选项的服务，相对于6.0，此处也增加了封装性onrestart_.ExecuteAllCommands();svc-&gt;NotifyStateChange(\"restarting\");return true;&#125;void Action::ExecuteAllCommands() const &#123;for (const auto&amp; c : commands_) &#123; ExecuteCommand(c);&#125;&#125;void Action::ExecuteCommand(const Command&amp; command) const &#123;Timer t;//服务重启时，将执行对应的选项int result = command.InvokeFunc();//打印log........&#125; waitpid的函数原型为: 1pid_t waitpid(pid_t pid, int *status, int options) 其中，第一个参数pid为预等待的子进程的识别码，pid=-1表示等待任何子进程是否发出SIGCHLD。第二个参数status，用于返回子进程的结束状态。第三个参数决定waitpid函数是否处于阻塞处理方式，WNOHANG表示若pid指定的子进程没有结束，则waitpid()函数返回0，不予等待；若子进程结束，则返回子进程的pid。waitpid如果出错，则返回-1。 总结一下：整个signal_handler_init其实就是为了重启子进程用的，上述过程其实最终可以简化为下图： 4.1.11、设置默认系统属性1property_load_boot_defaults(); 接下来，进程调用property_load_boot_defaults进行默认属性配置相关的工作。 12void property_load_boot_defaults() &#123;load_properties_from_file(PROP_PATH_RAMDISK_DEFAULT, NULL); 如代码所示，property_load_boot_defaults实际上就是调用load_properties_from_file解析配置文件；然后根据解析的结果，设置系统属性。该部分功能较为单一，不再深入分析。 4.1.12、配置属性的服务端12345678910111213start_property_service();void start_property_service() &#123;//创建了一个非阻塞socketproperty_set_fd = create_socket(PROP_SERVICE_NAME, SOCK_STREAM | SOCK_CLOEXEC | SOCK_NONBLOCK, 0666, 0, 0, NULL);if (property_set_fd == -1) &#123; ERROR(\"start_property_service socket creation failed: %s\\n\", strerror(errno)); exit(1);&#125;//调用listen函数监听property_set_fd， 于是该socket变成一个serverlisten(property_set_fd, 8);//监听server socket上是否有数据到来register_epoll_handler(property_set_fd, handle_property_set_fd);&#125; 我们知道，在create_socket函数返回套接字property_set_fd时，property_set_fd是一个主动连接的套接字。此时，系统假设用户会对这个套接字调用connect函数，期待它主动与其它进程连接。 由于在服务器编程中，用户希望这个套接字可以接受外来的连接请求，也就是被动等待用户来连接，于是需要调用listen函数使用主动连接套接字变为被连接套接字，使得一个进程可以接受其它进程的请求，从而成为一个服务器进程。 因此，调用listen后，init进程成为一个服务进程，其它进程可以通过property_set_fd连接init进程，提交设置系统属性的申请。 listen函数的第二个参数，涉及到一些网络的细节。 在进程处理一个连接请求的时候，可能还存在其它的连接请求。因为TCP连接是一个过程，所以可能存在一种半连接的状态。有时由于同时尝试连接的用户过多，使得服务器进程无法快速地完成连接请求。 因此，内核会在自己的进程空间里维护一个队列，以跟踪那些已完成连接但服务器进程还没有接手处理的用户，或正在进行的连接的用户。这样的一个队列不可能任意大，所以必须有一个上限。listen的第二个参数就是告诉内核使用这个数值作为上限。因此，init进程作为系统属性设置的服务器，最多可以同时为8个试图设置属性的用户提供服务。 在启动配置属性服务的最后，调用函数register_epoll_handler。根据上文所述，我们知道该函数将利用之前创建出的epoll句柄监听property_set_fd。当property_set_fd中有数据到来时，init进程将利用handle_property_set_fd函数进行处理。 12345678910111213static void handle_property_set_fd() &#123; .......... if ((s = accept(property_set_fd, (struct sockaddr *) &amp;addr, &amp;addr_size)) &lt; 0) &#123; return; &#125; ........ r = TEMP_FAILURE_RETRY(recv(s, &amp;msg, sizeof(msg), MSG_DONTWAIT)); ......... switch(msg.cmd) &#123; ......... &#125; .........&#125; handle_propery_set_fd函数实际上是调用accept函数监听连接请求，接收property_set_fd中到来的数据，然后利用recv函数接受到来的数据，最后根据到来数据的类型，进行设置系统属性等相关操作，在此不做深入分析。 在这一部分的最后，我们简单举例介绍一下，系统属性改变的一些用途。 在init.rc中定义了一些与属性相关的触发器。当某个条件相关的属性被改变时，与该条件相关的触发器就会被触发。举例来说，如下面代码所示，debuggable属性变为1时，将执行启动console进程等操作。 12345on property:ro.debuggable=1# Give writes to anyone for the trace folder on debug builds.# The folder is used to store method traces.chmod 0773 /data/misc/tracestart console 总结一下，其它进程修改系统属性时，大致的流程如下图所示：其它的进程像init进程发送请求后，由init进程检查权限后，修改共享内存区。 4.1.12、解析init.rc文件关于解析init.rc的代码，Android 7.0相对于6.0，作了巨大的修改。 1234567891011121314//这里将Action的function_map_替换为BuiltinFunctionMap//下文将通过BuiltinFuntionMap的map方法，获取keyword对应的处理函数const BuiltinFunctionMap function_map;Action::set_function_map(&amp;function_map);//构造出解析文件用的parser对象Parser&amp; parser = Parser::GetInstance();//为一些类型的关键字，创建特定的parserparser.AddSectionParser(\"service\",std::make_unique&lt;ServiceParser&gt;());parser.AddSectionParser(\"on\", std::make_unique&lt;ActionParser&gt;());parser.AddSectionParser(\"import\", std::make_unique&lt;ImportParser&gt;());//开始实际的解析过程parser.ParseConfig(\"/init.rc\");........ 在解析init.rc文件的过程前，我们先来简单介绍一下init.rc文件。 init.rc文件是在init进程启动后执行的启动脚本，文件中记录着init进程需执行的操作。在Android系统中，使用init.rc和init.{ hardware }.rc两个文件。 其中init.rc文件在Android系统运行过程中用于通用的环境设置与进程相关的定义，init.{hardware}.rc（例如，高通有init.qcom.rc，MTK有init.mediatek.rc）用于定义Android在不同平台下的特定进程和环境设置等。 此处解析函数传入的参数为”/init.rc”，解析的是运行时与init进程同在根目录下的init.rc文件。该文件在编译前，定义于system/core/rootdir/init.rc中（与平台相关的rc文件不在这里加载）。 init.rc文件大致分为两大部分，一部分是以”on”关键字开头的动作列表（action list）： 12345on early-init # Set init and its forked children's oom_adj. write /proc/1/oom_score_adj -1000 ......... start ueventd 另一部分是以”service”关键字开头的服务列表（service list）： 1234service ueventd /sbin/ueventd class core critical seclabel u:r:ueventd:s0 借助系统环境变量或Linux命令，动作列表用于创建所需目录，以及为某些特定文件指定权限，而服务列表用来记录init进程需要启动的一些子进程。如上面代码所示，service关键字后的第一个字符串表示服务（子进程）的名称，第二个字符串表示服务的执行路径。 接下来，我们从ParseConfig函数入手，逐步分析整个解析过程(函数定义于system/core/init/ init_parser.cpp中)： 123456789101112131415161718192021222324bool Parser::ParseConfig(const std::string&amp; path) &#123; if (is_dir(path.c_str())) &#123; //传入参数为目录地址 return ParseConfigDir(path); &#125; //传入参数为文件地址 return ParseConfigFile(path);&#125;bool Parser::ParseConfigDir(const std::string&amp; path) &#123; ........... std::unique_ptr&lt;DIR, int(*)(DIR*)&gt; config_dir(opendir(path.c_str()), closedir); .......... //看起来很复杂，其实就是递归目录 while ((current_file = readdir(config_dir.get()))) &#123; std::string current_path = android::base::StringPrintf(\"%s/%s\", path.c_str(), current_file-&gt;d_name); if (current_file-&gt;d_type == DT_REG) &#123; //最终还是靠ParseConfigFile来解析实际的文件 if (!ParseConfigFile(current_path)) &#123; ............. &#125; &#125; &#125;&#125; 从上面的代码可以看出，解析init.rc文件的函数是ParseConfigFile： 123456789101112bool Parser::ParseConfigFile(const std::string&amp; path) &#123; ........ std::string data; //读取路径指定文件中的内容，保存为字符串形式 if (!read_file(path.c_str(), &amp;data)) &#123; return false; &#125; ......... //解析获取的字符串 ParseData(path, data); .........&#125; ParseData函数定义于system/core/init/init_parser.cpp中，根据关键字解析出服务和动作。动作与服务会以链表节点的形式注册到service_list与action_list中，service_list与action_list是init进程中声明的全局结构体，其中的关键代码下所示。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455void Parser::ParseData(const std::string&amp; filename, const std::string&amp; data) &#123;.......parse_state state;.......std::vector&lt;std::string&gt; args;for (;;) &#123; //next_token以行为单位分割参数传递过来的字符串 //最先走到T_TEXT分支 switch (next_token(&amp;state)) &#123; case T_EOF: if (section_parser) &#123; //EOF,解析结束 section_parser-&gt;EndSection(); &#125; return; case T_NEWLINE: state.line++; if (args.empty()) &#123; break; &#125; //在前文创建parser时，我们为service，on，import定义了对应的parser //这里就是根据第一个参数，判断是否有对应的parser if (section_parsers_.count(args[0])) &#123; if (section_parser) &#123; //结束上一个parser的工作，将构造出的对象加入到对应的service_list与action_list中 section_parser-&gt;EndSection(); &#125; //获取参数对应的parser section_parser = section_parsers_[args[0]].get(); std::string ret_err; //调用实际parser的ParseSection函数 if (!section_parser-&gt;ParseSection(args, &amp;ret_err)) &#123; parse_error(&amp;state, \"%s\\n\", ret_err.c_str()); section_parser = nullptr; &#125; &#125; else if (section_parser) &#123; std::string ret_err; //如果第一个参数不是service，on，import //则调用前一个parser的ParseLineSection函数 //这里相当于解析一个参数块的子项 if (!section_parser-&gt;ParseLineSection(args, state.filename, state.line, &amp;ret_err)) &#123; parse_error(&amp;state, \"%s\\n\", ret_err.c_str()); &#125; &#125; //清空本次解析的数据 args.clear(); break; case T_TEXT: //将本次解析的内容写入到args中 args.emplace_back(state.text); break; &#125;&#125;&#125; 这里的解析看起来比较复杂，在6.0以前的版本中，整个解析是面向过程的。init进程统一调用一个函数来进行解析，然后在该函数中利用switch-case的形式，根据解析的内容进行相应的处理。 在Android 7.0中，为了更好地封装及面向对象，对于不同的关键字定义了不同的parser对象，每个对象通过多态实现自己的解析操作。 我们现在回忆一下init进程main函数中，创建parser的代码： 123456...........Parser&amp; parser = Parser::GetInstance();parser.AddSectionParser(\"service\",std::make_unique&lt;ServiceParser&gt;());parser.AddSectionParser(\"on\", std::make_unique&lt;ActionParser&gt;());parser.AddSectionParser(\"import\", std::make_unique&lt;ImportParser&gt;());........... 看看三个Parser的定义： 123class ServiceParser : public SectionParser &#123;......&#125;class ActionParser : public SectionParser &#123;......&#125;class ImportParser : public SectionParser &#123;.......&#125; 可以看到三个Parser均是继承SectionParser，具体的实现各有不同，我们以比较常用的ServiceParser和ActionParser为例，看看解析的结果如何处理。 4.1.12..1 ServiceParserServiceParser定义于system/core/init/service.cpp中。从前面的代码，我们知道，解析一个service块，首先需要调用ParseSection函数，接着利用ParseLineSection处理子块，解析完所有数据后，调用EndSection。 因此，我们着重看看ServiceParser的这三个函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778 bool ServiceParser::ParseSection(.....) &#123; ....... const std::string&amp; name = args[1]; ....... std::vector&lt;std::string&gt; str_args(args.begin() + 2, args.end()); //主要根据参数，构造出一个service对象 service_ = std::make_unique&lt;Service&gt;(name, \"default\", str_args); return true;&#125;//注意这里已经在解析子项了bool ServiceParser::ParseLineSection(......) const &#123; //调用service对象的HandleLine return service_ ? service_-&gt;HandleLine(args, err) : false;&#125;bool Service::HandleLine(.....) &#123; ........ //OptionHandlerMap继承自keywordMap&lt;OptionHandler&gt; static const OptionHandlerMap handler_map; //根据子项的内容，找到对应的handler函数 //FindFunction定义与keyword模块中,FindFunction方法利用子类生成对应的map中，然后通过通用的查找方法，即比较键值找到对应的处理函数 auto handler = handler_map.FindFunction(args[0], args.size() - 1, err); if (!handler) &#123; return false; &#125; //调用handler函数 return (this-&gt;*handler)(args, err);&#125;class Service::OptionHandlerMap : public KeywordMap&lt;OptionHandler&gt; &#123; ........... Service::OptionHandlerMap::Map&amp; Service::OptionHandlerMap::map() const &#123; constexpr std::size_t kMax = std::numeric_limits&lt;std::size_t&gt;::max(); static const Map option_handlers = &#123; &#123;\"class\", &#123;1, 1, &amp;Service::HandleClass&#125;&#125;, &#123;\"console\", &#123;0, 0, &amp;Service::HandleConsole&#125;&#125;, &#123;\"critical\", &#123;0, 0, &amp;Service::HandleCritical&#125;&#125;, &#123;\"disabled\", &#123;0, 0, &amp;Service::HandleDisabled&#125;&#125;, &#123;\"group\", &#123;1, NR_SVC_SUPP_GIDS + 1, &amp;Service::HandleGroup&#125;&#125;, &#123;\"ioprio\", &#123;2, 2, &amp;Service::HandleIoprio&#125;&#125;, &#123;\"keycodes\", &#123;1, kMax, &amp;Service::HandleKeycodes&#125;&#125;, &#123;\"oneshot\", &#123;0, 0, &amp;Service::HandleOneshot&#125;&#125;, &#123;\"onrestart\", &#123;1, kMax, &amp;Service::HandleOnrestart&#125;&#125;, &#123;\"seclabel\", &#123;1, 1, &amp;Service::HandleSeclabel&#125;&#125;, &#123;\"setenv\", &#123;2, 2, &amp;Service::HandleSetenv&#125;&#125;, &#123;\"socket\", &#123;3, 6, &amp;Service::HandleSocket&#125;&#125;, &#123;\"user\", &#123;1, 1, &amp;Service::HandleUser&#125;&#125;, &#123;\"writepid\", &#123;1, kMax, &amp;Service::HandleWritepid&#125;&#125;, &#125;; return option_handlers;&#125;//以class对应的处理函数为例，可以看出其实就是填充service对象对应的域bool Service::HandleClass(const std::vector&lt;std::string&gt;&amp; args, std::string* err) &#123; classname_ = args[1]; return true;&#125;//注意此时service对象已经处理完毕void ServiceParser::EndSection() &#123; if (service_) &#123; ServiceManager::GetInstance().AddService(std::move(service_)); &#125;&#125;void ServiceManager::AddService(std::unique_ptr&lt;Service&gt; service) &#123; Service* old_service = FindServiceByName(service-&gt;name()); if (old_service) &#123; ERROR(\"ignored duplicate definition of service '%s'\", service-&gt;name().c_str()); return; &#125; //将service对象加入到services_里 //7.0里，services_已经是个vector了 services_.emplace_back(std::move(service));&#125; 从上面的一系列代码，我们可以看出ServiceParser其实就是：首先根据第一行的名字和参数创建出service对象，然后根据选项域的内容填充service对象，最后将创建出的service对象加入到vector类型的service链表中。 4.1.12.2 ActionParserActionParser定义于system/core/init/action.cpp中。Action的解析过程，其实与Service一样，也是先后调用ParseSection， ParseLineSection和EndSection。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051bool ActionParser::ParseSection(....) &#123; ........ //创建出新的action对象 auto action = std::make_unique&lt;Action&gt;(false); //根据参数，填充action的trigger域，不详细分析了 if (!action-&gt;InitTriggers(triggers, err)) &#123; return false; &#125; .........&#125;bool ActionParser::ParseLineSection(.......) const &#123; //构造Action对象的command域 return action_ ? action_-&gt;AddCommand(args, filename, line, err) : false;&#125;bool Action::AddCommand(.....) &#123; ........ //找出action对应的执行函数 auto function = function_map_-&gt;FindFunction(args[0], args.size() - 1, err); ........ //利用所有信息构造出command，加入到action对象中 AddCommand(function, args, filename, line); return true;&#125;void Action::AddCommand(......) &#123; commands_.emplace_back(f, args, filename, line);&#125;void ActionParser::EndSection() &#123; if (action_ &amp;&amp; action_-&gt;NumCommands() &gt; 0) &#123; ActionManager::GetInstance().AddAction(std::move(action_)); &#125;&#125;void ActionManager::AddAction(.....) &#123; ........ auto old_action_it = std::find_if(actions_.begin(), actions_.end(), [&amp;action] (std::unique_ptr&lt;Action&gt;&amp; a) &#123; return action-&gt;TriggersEqual(*a); &#125;); if (old_action_it != actions_.end()) &#123; (*old_action_it)-&gt;CombineAction(*action); &#125; else &#123; //加入到action链表中，类型也是vector，其中装的是指针 actions_.emplace_back(std::move(action)); &#125;&#125; 从上面的代码可以看出，加载action块的逻辑和service一样，不同的是需要填充trigger和command域。当然，最后解析出的action也需要加入到action链表中。 这里最后还剩下一个问题，那就是哪里定义了Action中command对应处理函数？ 实际上，前文已经出现了过了，在init.cpp的main函数中： 1234.......const BuiltinFunctionMap function_map;Action::set_function_map(&amp;function_map);....... 因此，Action中调用functionmap-&gt;FindFunction时，实际上调用的是BuiltinFunctionMap的FindFunction函数。我们已经知道FindFunction是keyword定义的通用函数，重点是重构的map函数。我们看看system/core/init/builtins.cpp： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546BuiltinFunctionMap::Map&amp; BuiltinFunctionMap::map() const &#123; constexpr std::size_t kMax = std::numeric_limits&lt;std::size_t&gt;::max(); static const Map builtin_functions = &#123; &#123;\"bootchart_init\", &#123;0, 0, do_bootchart_init&#125;&#125;, &#123;\"chmod\", &#123;2, 2, do_chmod&#125;&#125;, &#123;\"chown\", &#123;2, 3, do_chown&#125;&#125;, &#123;\"class_reset\", &#123;1, 1, do_class_reset&#125;&#125;, &#123;\"class_start\", &#123;1, 1, do_class_start&#125;&#125;, &#123;\"class_stop\", &#123;1, 1, do_class_stop&#125;&#125;, &#123;\"copy\", &#123;2, 2, do_copy&#125;&#125;, &#123;\"domainname\", &#123;1, 1, do_domainname&#125;&#125;, &#123;\"enable\", &#123;1, 1, do_enable&#125;&#125;, &#123;\"exec\", &#123;1, kMax, do_exec&#125;&#125;, &#123;\"export\", &#123;2, 2, do_export&#125;&#125;, &#123;\"hostname\", &#123;1, 1, do_hostname&#125;&#125;, &#123;\"ifup\", &#123;1, 1, do_ifup&#125;&#125;, &#123;\"init_user0\", &#123;0, 0, do_init_user0&#125;&#125;, &#123;\"insmod\", &#123;1, kMax, do_insmod&#125;&#125;, &#123;\"installkey\", &#123;1, 1, do_installkey&#125;&#125;, &#123;\"load_persist_props\", &#123;0, 0, do_load_persist_props&#125;&#125;, &#123;\"load_system_props\", &#123;0, 0, do_load_system_props&#125;&#125;, &#123;\"loglevel\", &#123;1, 1, do_loglevel&#125;&#125;, &#123;\"mkdir\", &#123;1, 4, do_mkdir&#125;&#125;, &#123;\"mount_all\", &#123;1, kMax, do_mount_all&#125;&#125;, &#123;\"mount\", &#123;3, kMax, do_mount&#125;&#125;, &#123;\"powerctl\", &#123;1, 1, do_powerctl&#125;&#125;, &#123;\"restart\", &#123;1, 1, do_restart&#125;&#125;, &#123;\"restorecon\", &#123;1, kMax, do_restorecon&#125;&#125;, &#123;\"restorecon_recursive\", &#123;1, kMax, do_restorecon_recursive&#125;&#125;, &#123;\"rm\", &#123;1, 1, do_rm&#125;&#125;, &#123;\"rmdir\", &#123;1, 1, do_rmdir&#125;&#125;, &#123;\"setprop\", &#123;2, 2, do_setprop&#125;&#125;, &#123;\"setrlimit\", &#123;3, 3, do_setrlimit&#125;&#125;, &#123;\"start\", &#123;1, 1, do_start&#125;&#125;, &#123;\"stop\", &#123;1, 1, do_stop&#125;&#125;, &#123;\"swapon_all\", &#123;1, 1, do_swapon_all&#125;&#125;, &#123;\"symlink\", &#123;2, 2, do_symlink&#125;&#125;, &#123;\"sysclktz\", &#123;1, 1, do_sysclktz&#125;&#125;, &#123;\"trigger\", &#123;1, 1, do_trigger&#125;&#125;, &#123;\"verity_load_state\", &#123;0, 0, do_verity_load_state&#125;&#125;, &#123;\"verity_update_state\", &#123;0, 0, do_verity_update_state&#125;&#125;, &#123;\"wait\", &#123;1, 2, do_wait&#125;&#125;, &#123;\"write\", &#123;2, 2, do_write&#125;&#125;, &#125;; return builtin_functions;&#125; 上述代码的第四项就是Action每个command对应的执行函数。 4.1.13、向执行队列中添加其它action介绍完init进程解析init.rc文件的过程后，我们继续将视角拉回到init进程的main函数： 1234567891011121314151617181920212223242526272829ActionManager&amp; am = ActionManager::GetInstance();am.QueueEventTrigger(\"early-init\");// Queue an action that waits for coldboot done so we know ueventd has set up all of /dev...m.QueueBuiltinAction(wait_for_coldboot_done_action, \"wait_for_coldboot_done\");// ... so that we can start queuing up actions that require stuff from /dev.am.QueueBuiltinAction(mix_hwrng_into_linux_rng_action, \"mix_hwrng_into_linux_rng\");am.QueueBuiltinAction(set_mmap_rnd_bits_action, \"set_mmap_rnd_bits\");am.QueueBuiltinAction(keychord_init_action, \"keychord_init\");am.QueueBuiltinAction(console_init_action, \"console_init\");// Trigger all the boot actions to get us started.am.QueueEventTrigger(\"init\");// Repeat mix_hwrng_into_linux_rng in case /dev/hw_random or /dev/random// wasn't ready immediately after wait_for_coldboot_doneam.QueueBuiltinAction(mix_hwrng_into_linux_rng_action, \"mix_hwrng_into_linux_rng\");// Don't mount filesystems or start core system services in charger mode.std::string bootmode = property_get(\"ro.bootmode\");if (bootmode == \"charger\") &#123; am.QueueEventTrigger(\"charger\");&#125; else &#123; am.QueueEventTrigger(\"late-init\");&#125;// Run all property triggers based on current state of the properties. am.QueueBuiltinAction(queue_property_triggers_action, \"queue_property_triggers\"); 从上面的代码可以看出，接下来init进程中调用了大量的QueueEventTrigger和QueueBuiltinAction函数。 123void ActionManager::QueueEventTrigger(const std::string&amp; trigger) &#123; trigger_queue_.push(std::make_unique&lt;EventTrigger&gt;(trigger));&#125; 处QueueEventTrigger函数就是利用参数构造EventTrigger，然后加入到triggerqueue中。后续init进程处理trigger事件时，将会触发相应的操作。根据前文的分析，我们知道实际上就是将action_list中，对应trigger与第一个参数匹配的action，加入到运行队列action_queue中。 12345678910111213141516void ActionManager::QueueBuiltinAction(BuiltinFunction func, const std::string&amp; name) &#123; //创建action auto action = std::make_unique&lt;Action&gt;(true); std::vector&lt;std::string&gt; name_vector&#123;name&#125;; //保证唯一性 if (!action-&gt;InitSingleTrigger(name)) &#123; return; &#125; //创建action的cmd，指定执行函数和参数 action-&gt;AddCommand(func, name_vector); trigger_queue_.push(std::make_unique&lt;BuiltinTrigger&gt;(action.get())); actions_.emplace_back(std::move(action));&#125; QueueBuiltinAction函数中构造新的action加入到actions_中，第一个参数作为新建action携带cmd的执行函数；第二个参数既作为action的trigger name，也作为action携带cmd的参数。 4.1.14、处理添加到运行队列的事件12345678910111213141516171819202122232425262728293031323334353637while (true) &#123; //判断是否有事件需要处理 if (!waiting_for_exec) &#123; //依次执行每个action中携带command对应的执行函数 am.ExecuteOneCommand(); //重启一些挂掉的进程 restart_processes(); &#125; //以下决定timeout的时间，将影响while循环的间隔 int timeout = -1; //有进程需要重启时，等待该进程重启 if (process_needs_restart) &#123; timeout = (process_needs_restart - gettime()) * 1000; if (timeout &lt; 0) timeout = 0; &#125; //有action待处理，不等待 if (am.HasMoreCommands()) &#123; timeout = 0; &#125; //bootchart_sample应该是进行性能数据采样 bootchart_sample(&amp;timeout); epoll_event ev; //没有事件到来的话，最多阻塞timeout时间 int nr = TEMP_FAILURE_RETRY(epoll_wait(epoll_fd, &amp;ev, 1, timeout)); if (nr == -1) &#123; ERROR(\"epoll_wait failed: %s\\n\", strerror(errno)); &#125; else if (nr == 1) &#123; //有事件到来，执行对应处理函数 //根据上文知道，epoll句柄（即epoll_fd）主要监听子进程结束，及其它进程设置系统属性的请求。 ((void (*)()) ev.data.ptr)(); &#125;&#125; 从上面代码可以看出，init进程将所有需要操作的action加入运行队列后， 进入无限循环过程，不断处理运行队列中的事件，同时进行重启service等操作。 ExecuteOneCommand中的主要部分如下图所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748void ActionManager::ExecuteOneCommand() &#123; // Loop through the trigger queue until we have an action to execute //当有可执行action或trigger queue为空时结束 while (current_executing_actions_.empty() &amp;&amp; !trigger_queue_.empty()) &#123; //轮询actions链表 for (const auto&amp; action : actions_) &#123; //依次查找trigger表 if (trigger_queue_.front()-&gt;CheckTriggers(*action)) &#123; //当action与trigger对应时，就可以执行当前action //一个trigger可以对应多个action，均加入current_executing_actions_ current_executing_actions_.emplace(action.get()); &#125; &#125; //trigger event出队 trigger_queue_.pop(); &#125; if (current_executing_actions_.empty()) &#123; return; &#125; //每次只执行一个action，下次init进程while循环时，跳过上面的while循环，接着执行 auto action = current_executing_actions_.front(); if (current_command_ == 0) &#123; std::string trigger_name = action-&gt;BuildTriggersString(); INFO(\"processing action (%s)\\n\", trigger_name.c_str()); &#125; //实际的执行过程，此处仅处理当前action中的一个cmd action-&gt;ExecuteOneCommand(current_command_); //适当地清理工作，注意只有当前action中所有的command均执行完毕后，才会将该action从current_executing_actions_移除 // If this was the last command in the current action, then remove // the action from the executing list. // If this action was oneshot, then also remove it from actions_. ++current_command_; if (current_command_ == action-&gt;NumCommands()) &#123; current_executing_actions_.pop(); current_command_ = 0; if (action-&gt;oneshot()) &#123; auto eraser = [&amp;action] (std::unique_ptr&lt;Action&gt;&amp; a) &#123; return a.get() == action; &#125;; actions_.erase(std::remove_if(actions_.begin(), actions_.end(), eraser)); &#125; &#125;&#125; 123456void Action::ExecuteCommand(const Command&amp; command) const &#123; Timer t; //执行该command对应的处理函数 int result = command.InvokeFunc(); ........&#125; 从代码可以看出，当while循环不断调用ExecuteOneCommand函数时，将按照trigger表的顺序，依次取出action链表中与trigger匹配的action。 每次均执行一个action中的一个command对应函数（一个action可能携带多个command）。 当一个action所有的command均执行完毕后，再执行下一个action。 当一个trigger对应的action均执行完毕后，再执行下一个trigger对应action。 restart_processes函数负责按需重启service，代码如下图所示。 12345678static void restart_processes() &#123; process_needs_restart = 0; ServiceManager::GetInstance().ForEachServiceWithFlags( SVC_RESTARTING, [] (Service* s) &#123; s-&gt;RestartIfNeeded(process_needs_restart); &#125;);&#125; 从上面可以看出，该函数轮询service对应的链表，对于有SVC_RESTARING标志的service执行RestartIfNeeded（如上文所述，当子进程终止时，init进程会将可被重启进程的服务标志位置为SVC_RESTARTING）。 如下面代码所示，restart_service_if_needed可以重新启动服务。 12345678910111213141516171819void Service::RestartIfNeeded(time_t&amp; process_needs_restart)(struct service *svc)&#123; time_t next_start_time = svc-&gt;time_started + 5; //两次服务启动时间的间隔要大于5s if (next_start_time &lt;= gettime()) &#123; svc-&gt;flags &amp;= (~SVC_RESTARTING); //满足时间间隔的要求后，重启服务 //Start将会重新fork服务进程，并做相应的配置 Start(svc, NULL); return; &#125; //更新main函数中，while循环需要等待的时间 if ((next_start_time &lt; process_needs_restart) || (process_needs_restart == 0)) &#123; process_needs_restart = next_start_time; &#125;&#125; 查阅资料发现：Bootchart 是一个能对 GNU/Linux boot 过程进行性能分析并把结果直观化的工具。它在 boot 过程中搜集资源利用情况及进程信息然后以PNG, SVG或EPS格式来显示结果。BootChart 包含数据收集工具和图像产生工具。数据收集工具在原始的BootChart中是独立的shell程序，但在Android中，数据收集工具被集成到了init 程序中。资料与代码基本吻合。 （2）、启动Zygote进程4.2.1、概述Zygote是由init进程通过解析init.zygote.rc文件而创建的，zygote所对应的可执行程序app_process，所对应的源文件是App_main.cpp，进程名为zygote。 12345678910111213141516service zygote /system/bin/app_process32 -Xzygote /system/bin --zygote --start-system-server --socket-name=zygote class main socket zygote stream 660 root system onrestart write /sys/android_power/request_state wake onrestart write /sys/power/state on onrestart restart audioserver onrestart restart cameraserver onrestart restart media onrestart restart netd writepid /dev/cpuset/foreground/tasksservice zygote_secondary /system/bin/app_process64 -Xzygote /system/bin --zygote --socket-name=zygote_secondary class main socket zygote_secondary stream 660 root system onrestart restart zygote writepid /dev/cpuset/foreground/tasks Zygote进程能够重启的地方: servicemanager进程被杀; (onresart) surfaceflinger进程被杀; (onresart) Zygote进程自己被杀; (oneshot=false) system_server进程被杀; (waitpid) 从App_main()开始，Zygote启动过程的函数调用类大致流程如下： 4.2.2、Zygote启动过程4.2.2.1、App_main.main()[-&gt; App_main.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113int main(int argc, char* const argv[])&#123; if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) &lt; 0) &#123; // Older kernels don't understand PR_SET_NO_NEW_PRIVS and return // EINVAL. Don't die on such kernels. if (errno != EINVAL) &#123; LOG_ALWAYS_FATAL(\"PR_SET_NO_NEW_PRIVS failed: %s\", strerror(errno)); return 12; &#125; &#125; //传到的参数argv为“-Xzygote /system/bin --zygote --start-system-server” AppRuntime runtime(argv[0], computeArgBlockSize(argc, argv)); // Process command line arguments // ignore argv[0] //忽略第一个参数 argc--; argv++; int i; for (i = 0; i &lt; argc; i++) &#123; if (argv[i][0] != '-') &#123; break; &#125; if (argv[i][1] == '-' &amp;&amp; argv[i][2] == 0) &#123; ++i; // Skip --. break; &#125; runtime.addOption(strdup(argv[i])); &#125; // Parse runtime arguments. Stop at first unrecognized option. //参数解析 bool zygote = false; bool startSystemServer = false; bool application = false; String8 niceName; String8 className; ++i; // Skip unused \"parent dir\" argument. while (i &lt; argc) &#123; const char* arg = argv[i++]; if (strcmp(arg, \"--zygote\") == 0) &#123; zygote = true; niceName = ZYGOTE_NICE_NAME; &#125; else if (strcmp(arg, \"--start-system-server\") == 0) &#123; startSystemServer = true; &#125; else if (strcmp(arg, \"--application\") == 0) &#123; application = true; &#125; else if (strncmp(arg, \"--nice-name=\", 12) == 0) &#123; niceName.setTo(arg + 12); &#125; else if (strncmp(arg, \"--\", 2) != 0) &#123; className.setTo(arg); break; &#125; else &#123; --i; break; &#125; &#125; Vector&lt;String8&gt; args; if (!className.isEmpty()) &#123; // We're not in zygote mode, the only argument we need to pass // to RuntimeInit is the application argument. // // The Remainder of args get passed to startup class main(). Make // copies of them before we overwrite them with the process name. // 运行application或tool程序 args.add(application ? String8(\"application\") : String8(\"tool\")); runtime.setClassNameAndArgs(className, argc - i, argv + i); &#125; else &#123; // We're in zygote mode. //进入zygote模式，创建 /data/dalvik-cache路径 maybeCreateDalvikCache(); if (startSystemServer) &#123; args.add(String8(\"start-system-server\")); &#125; char prop[PROP_VALUE_MAX]; if (property_get(ABI_LIST_PROPERTY, prop, NULL) == 0) &#123; LOG_ALWAYS_FATAL(\"app_process: Unable to determine ABI list from property %s.\", ABI_LIST_PROPERTY); return 11; &#125; String8 abiFlag(\"--abi-list=\"); abiFlag.append(prop); args.add(abiFlag); // In zygote mode, pass all remaining arguments to the zygote // main() method. for (; i &lt; argc; ++i) &#123; args.add(String8(argv[i])); &#125; &#125; //设置进程名 if (!niceName.isEmpty()) &#123; runtime.setArgv0(niceName.string()); set_process_name(niceName.string()); &#125; if (zygote) &#123; // 启动AppRuntime runtime.start(\"com.android.internal.os.ZygoteInit\", args, zygote); &#125; else if (className) &#123; runtime.start(\"com.android.internal.os.RuntimeInit\", args, zygote); &#125; else &#123; //没有指定类名或zygote，参数错误 fprintf(stderr, \"Error: no class name or --zygote supplied.\\n\"); app_usage(); LOG_ALWAYS_FATAL(\"app_process: no class name or --zygote supplied.\"); return 10; &#125;&#125; 4.2.2.2、AndroidRuntime.start()[-&gt; AndroidRuntime.cpp] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110void AndroidRuntime::start(const char* className, const Vector&lt;String8&gt;&amp; options, bool zygote)&#123;ALOGD(\"&gt;&gt;&gt;&gt;&gt;&gt; START %s uid %d &lt;&lt;&lt;&lt;&lt;&lt;\\n\", className != NULL ? className : \"(unknown)\", getuid());static const String8 startSystemServer(\"start-system-server\");/* * 'startSystemServer == true' means runtime is obsolete and not run from * init.rc anymore, so we print out the boot start event here. */for (size_t i = 0; i &lt; options.size(); ++i) &#123; if (options[i] == startSystemServer) &#123; /* track our progress through the boot sequence */ const int LOG_BOOT_PROGRESS_START = 3000; LOG_EVENT_LONG(LOG_BOOT_PROGRESS_START, ns2ms(systemTime(SYSTEM_TIME_MONOTONIC))); &#125;&#125;const char* rootDir = getenv(\"ANDROID_ROOT\");if (rootDir == NULL) &#123; rootDir = \"/system\"; if (!hasDir(\"/system\")) &#123; LOG_FATAL(\"No root directory specified, and /android does not exist.\"); return; &#125; setenv(\"ANDROID_ROOT\", rootDir, 1);&#125;//const char* kernelHack = getenv(\"LD_ASSUME_KERNEL\");//ALOGD(\"Found LD_ASSUME_KERNEL='%s'\\n\", kernelHack);/* start the virtual machine */JniInvocation jni_invocation;jni_invocation.Init(NULL);JNIEnv* env;// 虚拟机创建if (startVm(&amp;mJavaVM, &amp;env, zygote) != 0) &#123; return;&#125;onVmCreated(env);/* * Register android functions. */ // JNI方法注册if (startReg(env) &lt; 0) &#123; ALOGE(\"Unable to register all android natives\\n\"); return;&#125;/* * We want to call main() with a String array with arguments in it. * At present we have two arguments, the class name and an option string. * Create an array to hold them. */jclass stringClass;jobjectArray strArray;jstring classNameStr;//等价 strArray= new String[options.size() + 1];stringClass = env-&gt;FindClass(\"java/lang/String\");assert(stringClass != NULL);strArray = env-&gt;NewObjectArray(options.size() + 1, stringClass, NULL);assert(strArray != NULL);//等价 strArray[0] = \"com.android.internal.os.ZygoteInit\"classNameStr = env-&gt;NewStringUTF(className);assert(classNameStr != NULL);env-&gt;SetObjectArrayElement(strArray, 0, classNameStr);//等价 strArray[1] = \"start-system-server\"；// strArray[2] = \"--abi-list=xxx\"；//其中xxx为系统响应的cpu架构类型，比如arm64-v8a.for (size_t i = 0; i &lt; options.size(); ++i) &#123; jstring optionsStr = env-&gt;NewStringUTF(options.itemAt(i).string()); assert(optionsStr != NULL); env-&gt;SetObjectArrayElement(strArray, i + 1, optionsStr);&#125;/* * Start VM. This thread becomes the main thread of the VM, and will * not return until the VM exits. */ //将\"com.android.internal.os.ZygoteInit\"转换为\"com/android/internal/os/ZygoteInit\"char* slashClassName = toSlashClassName(className);jclass startClass = env-&gt;FindClass(slashClassName);if (startClass == NULL) &#123; ALOGE(\"JavaVM unable to locate class '%s'\\n\", slashClassName); /* keep going */&#125; else &#123; jmethodID startMeth = env-&gt;GetStaticMethodID(startClass, \"main\", \"([Ljava/lang/String;)V\"); if (startMeth == NULL) &#123; ALOGE(\"JavaVM unable to find main() in '%s'\\n\", className); /* keep going */ &#125; else &#123; // 调用ZygoteInit.main()方法 env-&gt;CallStaticVoidMethod(startClass, startMeth, strArray); #if 0 if (env-&gt;ExceptionCheck()) threadExitUncaughtException(env); #endif &#125;&#125;free(slashClassName);ALOGD(\"Shutting down VM\\n\");if (mJavaVM-&gt;DetachCurrentThread() != JNI_OK) ALOGW(\"Warning: unable to detach main thread\\n\");if (mJavaVM-&gt;DestroyJavaVM() != 0) ALOGW(\"Warning: VM did not shut down cleanly\\n\"); &#125; 4.2.2.3、AndroidRuntime.startVm()[–&gt; AndroidRuntime.cpp] 12345678910111213141516171819202122232425262728293031323334353637383940414243int AndroidRuntime::startVm(JavaVM** pJavaVM, JNIEnv** pEnv, bool zygote)&#123;// JNI检测功能，用于native层调用jni函数时进行常规检测，比较弱字符串格式是否符合要求，资源是否正确释放。该功能一般用于早期系统调试或手机Eng版，对于User版往往不会开启，引用该功能比较消耗系统CPU资源，降低系统性能。 bool checkJni = false;property_get(\"dalvik.vm.checkjni\", propBuf, \"\");if (strcmp(propBuf, \"true\") == 0) &#123; checkJni = true;&#125; else if (strcmp(propBuf, \"false\") != 0) &#123; /* property is neither true nor false; fall back on kernel parameter */ property_get(\"ro.kernel.android.checkjni\", propBuf, \"\"); if (propBuf[0] == '1') &#123; checkJni = true; &#125;&#125;ALOGD(\"CheckJNI is %s\\n\", checkJni ? \"ON\" : \"OFF\");if (checkJni) &#123; addOption(\"-Xcheck:jni\")&#125;...... //虚拟机产生的trace文件，主要用于分析系统问题，路径默认为/data/anr/traces.txtparseRuntimeOption(\"dalvik.vm.stack-trace-file\", stackTraceFileBuf, \"-Xstacktracefile:\");//对于不同的软硬件环境，这些参数往往需要调整、优化，从而使系统达到最佳性能parseRuntimeOption(\"dalvik.vm.heapstartsize\", heapstartsizeOptsBuf, \"-Xms\", \"4m\");parseRuntimeOption(\"dalvik.vm.heapsize\", heapsizeOptsBuf, \"-Xmx\", \"16m\");parseRuntimeOption(\"dalvik.vm.heapgrowthlimit\", heapgrowthlimitOptsBuf, \"-XX:HeapGrowthLimit=\");parseRuntimeOption(\"dalvik.vm.heapminfree\", heapminfreeOptsBuf, \"-XX:HeapMinFree=\");parseRuntimeOption(\"dalvik.vm.heapmaxfree\", heapmaxfreeOptsBuf, \"-XX:HeapMaxFree=\");parseRuntimeOption(\"dalvik.vm.heaptargetutilization\", heaptargetutilizationOptsBuf, \"-XX:HeapTargetUtilization=\");...//preloaded-classes文件内容是由WritePreloadedClassFile.java生成的，//在ZygoteInit类中会预加载工作将其中的classes提前加载到内存，以提高系统性能if (!hasFile(\"/system/etc/preloaded-classes\")) &#123; return -1;&#125;//初始化虚拟机if (JNI_CreateJavaVM(pJavaVM, pEnv, &amp;initArgs) &lt; 0) &#123; ALOGE(\"JNI_CreateJavaVM failed\\n\"); return -1;&#125;&#125; 创建Java虚拟机方法的主要篇幅是关于虚拟机参数的设置，下面只列举部分在调试优化过程中常用参数。 4.2.2.4、AndroidRuntime.startReg()1234567891011121314int AndroidRuntime::startReg(JNIEnv* env)&#123; //设置线程创建方法为javaCreateThreadEtc androidSetCreateThreadFunc((android_create_thread_fn) javaCreateThreadEtc); env-&gt;PushLocalFrame(200); //进程NI方法的注册 if (register_jni_procs(gRegJNI, NELEM(gRegJNI), env) &lt; 0) &#123; env-&gt;PopLocalFrame(NULL); return -1; &#125; env-&gt;PopLocalFrame(NULL); return 0;&#125; 4.2.2.4.1、Threads.androidSetCreateThreadFunc()[-&gt; Threads.cpp] 1234void androidSetCreateThreadFunc(android_create_thread_fn func)&#123; gCreateThreadFn = func;&#125; 虚拟机启动后startReg()过程，会设置线程创建函数指针gCreateThreadFn指向javaCreateThreadEtc. 4.2.2.4.2、register_jni_procs()123456789static int register_jni_procs(const RegJNIRec array[], size_t count, JNIEnv* env)&#123; for (size_t i = 0; i &lt; count; i++) &#123; if (array[i].mProc(env) &lt; 0) &#123; return -1; &#125; &#125; return 0;&#125; 4.2.2.4.3、gRegJNI.mProc12345static const RegJNIRec gRegJNI[] = &#123; REG_JNI(register_com_android_internal_os_RuntimeInit), REG_JNI(register_android_os_Binder)， ...&#125;；* array[i]是指gRegJNI数组, 该数组有100多个成员。其中每一项成员都是通过REG_JNI宏定义的： 1234#define REG_JNI(name) &#123; name &#125;struct RegJNIRec &#123; int (*mProc)(JNIEnv*);&#125;; 可见，调用mProc，就等价于调用其参数名所指向的函数。 例如REG_JNI(register_com_android_internal_os_RuntimeInit).mProc也就是指进入register_com_android_internal_os_RuntimeInit方法，接下来就继续以此为例来说明： 12345int register_com_android_internal_os_RuntimeInit(JNIEnv* env)&#123; return jniRegisterNativeMethods(env, \"com/android/internal/os/RuntimeInit\", gMethods, NELEM(gMethods));&#125; //gMethods：java层方法名与jni层的方法的一一映射关系 12345678static JNINativeMethod gMethods[] = &#123; &#123; \"nativeFinishInit\", \"()V\", (void*) com_android_internal_os_RuntimeInit_nativeFinishInit &#125;, &#123; \"nativeZygoteInit\", \"()V\", (void*) com_android_internal_os_RuntimeInit_nativeZygoteInit &#125;, &#123; \"nativeSetExitWithoutCleanup\", \"(Z)V\", (void*) com_android_internal_os_RuntimeInit_nativeSetExitWithoutCleanup &#125;,&#125;; 4.2.2.5、进入Java层AndroidRuntime.start()执行到最后通过反射调用到ZygoteInit.main(),见下文: 4.2.2.5.1、ZygoteInit.main()12345678910111213141516171819202122232425262728293031323334353637383940public static void main(String argv[]) &#123;try &#123;Init\"); //开启DDMS功能 RuntimeInit.enableDdms(); SamplingProfilerIntegration.start(); boolean startSystemServer = false; String socketName = \"zygote\"; String abiList = null; for (int i = 1; i &lt; argv.length; i++) &#123; if (\"start-system-server\".equals(argv[i])) &#123; startSystemServer = true; &#125; else if (argv[i].startsWith(ABI_LIST_ARG)) &#123; abiList = argv[i].substring(ABI_LIST_ARG.length()); &#125; else if (argv[i].startsWith(SOCKET_NAME_ARG)) &#123; socketName = argv[i].substring(SOCKET_NAME_ARG.length()); &#125; else &#123; throw new RuntimeException(\"Unknown command line argument: \" + argv[i]); &#125; &#125; ...... //为Zygote注册socket registerZygoteSocket(socketName); preload();// 预加载类和资源 SamplingProfilerIntegration.writeZygoteSnapshot(); gcAndFinalize();//GC操作 Zygote.nativeUnmountStorageOnInit(); ZygoteHooks.stopZygoteNoThreadCreation(); if (startSystemServer) &#123;//启动system_server startSystemServer(abiList, socketName); &#125; runSelectLoop(abiList);//进入循环模式 closeServerSocket();&#125; catch (MethodAndArgsCaller caller) &#123; caller.run();&#125; catch (Throwable ex) &#123; closeServerSocket(); throw ex;&#125;&#125; 在异常捕获后调用的方法caller.run()，会在后续的system_server文章会讲到。 4.2.2.5.2、ZygoteInit.registerZygoteSocket()1234567891011121314151617181920private static void registerZygoteSocket(String socketName) &#123;if (sServerSocket == null) &#123; int fileDesc; final String fullSocketName = ANDROID_SOCKET_PREFIX + socketName; try &#123; String env = System.getenv(fullSocketName); fileDesc = Integer.parseInt(env); &#125; catch (RuntimeException ex) &#123; ... &#125; try &#123; FileDescriptor fd = new FileDescriptor(); fd.setInt$(fileDesc); //设置文件描述符 sServerSocket = new LocalServerSocket(fd); //创建Socket的本地服务端 &#125; catch (IOException ex) &#123; ... &#125;&#125;&#125; 4.2.2.5.2、ZygoteInit.preload()1234567891011121314151617181920static void preload() &#123; //预加载位于/system/etc/preloaded-classes文件中的类 preloadClasses(); //预加载资源，包含drawable和color资源 preloadResources(); //预加载OpenGL preloadOpenGL(); //通过System.loadLibrary()方法， //预加载\"android\",\"compiler_rt\",\"jnigraphics\"这3个共享库 preloadSharedLibraries(); //预加载 文本连接符资源 preloadTextResources(); //仅用于zygote进程，用于内存共享的进程 WebViewFactory.prepareWebViewInZygote();&#125; 执行Zygote进程的初始化,对于类加载，采用反射机制Class.forName()方法来加载。对于资源加载，主要是 com.android.internal.R.array.preloaded_drawables和com.android.internal.R.array.preloaded_color_state_lists，在应用程序中以com.android.internal.R.xxx开头的资源，便是此时由Zygote加载到内存的。 zygote进程内加载了preload()方法中的所有资源，当需要fork新进程时，采用copy on write技术，如下： 4.2.2.5.3、ZygoteInit.startSystemServer()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556private static boolean startSystemServer(String abiList, String socketName) throws MethodAndArgsCaller, RuntimeException &#123; long capabilities = posixCapabilitiesAsBits( OsConstants.CAP_BLOCK_SUSPEND, OsConstants.CAP_KILL, OsConstants.CAP_NET_ADMIN, OsConstants.CAP_NET_BIND_SERVICE, OsConstants.CAP_NET_BROADCAST, OsConstants.CAP_NET_RAW, OsConstants.CAP_SYS_MODULE, OsConstants.CAP_SYS_NICE, OsConstants.CAP_SYS_RESOURCE, OsConstants.CAP_SYS_TIME, OsConstants.CAP_SYS_TTY_CONFIG ); //参数准备 String args[] = &#123; \"--setuid=1000\", \"--setgid=1000\", \"--setgroups=1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1018,1021,1032,3001,3002,3003,3006,3007\", \"--capabilities=\" + capabilities + \",\" + capabilities, \"--nice-name=system_server\", \"--runtime-args\", \"com.android.server.SystemServer\", &#125;; ZygoteConnection.Arguments parsedArgs = null; int pid; try &#123; //用于解析参数，生成目标格式 parsedArgs = new ZygoteConnection.Arguments(args); ZygoteConnection.applyDebuggerSystemProperty(parsedArgs); ZygoteConnection.applyInvokeWithSystemProperty(parsedArgs); // fork子进程，用于运行system_server pid = Zygote.forkSystemServer( parsedArgs.uid, parsedArgs.gid, parsedArgs.gids, parsedArgs.debugFlags, null, parsedArgs.permittedCapabilities, parsedArgs.effectiveCapabilities); &#125; catch (IllegalArgumentException ex) &#123; throw new RuntimeException(ex); &#125; //进入子进程system_server if (pid == 0) &#123; if (hasSecondZygote(abiList)) &#123; waitForSecondaryZygote(socketName); &#125; // 完成system_server进程剩余的工作 handleSystemServerProcess(parsedArgs); &#125; return true;&#125; 准备参数并fork新进程，从上面可以看出system server进程参数信息为uid=1000,gid=1000,进程名为sytem_server，从zygote进程fork新进程后，需要关闭zygote原有的socket。另外，对于有两个zygote进程情况，需等待第2个zygote创建完成。 4.2.2.5.4、ZygoteInit.runSelectLoop()1234567891011121314151617181920212223242526272829303132333435363738394041424344private static void runSelectLoop(String abiList) throws MethodAndArgsCaller &#123; ArrayList&lt;FileDescriptor&gt; fds = new ArrayList&lt;FileDescriptor&gt;(); ArrayList&lt;ZygoteConnection&gt; peers = new ArrayList&lt;ZygoteConnection&gt;(); //sServerSocket是socket通信中的服务端，即zygote进程。保存到fds[0] fds.add(sServerSocket.getFileDescriptor()); peers.add(null); while (true) &#123; StructPollfd[] pollFds = new StructPollfd[fds.size()]; for (int i = 0; i &lt; pollFds.length; ++i) &#123; pollFds[i] = new StructPollfd(); pollFds[i].fd = fds.get(i); pollFds[i].events = (short) POLLIN; &#125; try &#123; //处理轮询状态，当pollFds有事件到来则往下执行，否则阻塞在这里 Os.poll(pollFds, -1); &#125; catch (ErrnoException ex) &#123; ... &#125; for (int i = pollFds.length - 1; i &gt;= 0; --i) &#123; //采用I/O多路复用机制，当接收到客户端发出连接请求 或者数据处理请求到来，则往下执行； // 否则进入continue，跳出本次循环。 if ((pollFds[i].revents &amp; POLLIN) == 0) &#123; continue; &#125; if (i == 0) &#123; //即fds[0]，代表的是sServerSocket，则意味着有客户端连接请求； // 则创建ZygoteConnection对象,并添加到fds。 ZygoteConnection newPeer = acceptCommandPeer(abiList); peers.add(newPeer); fds.add(newPeer.getFileDesciptor()); //添加到fds. &#125; else &#123; //i&gt;0，则代表通过socket接收来自对端的数据，并执行相应操作 boolean done = peers.get(i).runOnce(); if (done) &#123; peers.remove(i); fds.remove(i); //处理完则从fds中移除该文件描述符 &#125; &#125; &#125; &#125;&#125; Zygote采用高效的I/O多路复用机制，保证在没有客户端连接请求或数据处理时休眠，否则响应客户端的请求。 4.2.2.5.4、ZygoteConnection.runOnce()123456789101112131415161718192021222324252627282930313233343536373839404142434445boolean runOnce() throws ZygoteInit.MethodAndArgsCaller &#123;String args[];Arguments parsedArgs = null;FileDescriptor[] descriptors;try &#123; //读取socket客户端发送过来的参数列表 args = readArgumentList(); descriptors = mSocket.getAncillaryFileDescriptors();&#125; catch (IOException ex) &#123; ... return true;&#125;...try &#123; //将binder客户端传递过来的参数，解析成Arguments对象格式 parsedArgs = new Arguments(args); ... pid = Zygote.forkAndSpecialize(parsedArgs.uid, parsedArgs.gid, parsedArgs.gids, parsedArgs.debugFlags, rlimits, parsedArgs.mountExternal, parsedArgs.seInfo, parsedArgs.niceName, fdsToClose, parsedArgs.instructionSet, parsedArgs.appDataDir);&#125; catch (Exception e) &#123; ...&#125;try &#123; if (pid == 0) &#123; //子进程执行 IoUtils.closeQuietly(serverPipeFd); serverPipeFd = null; //进入子进程流程 handleChildProc(parsedArgs, descriptors, childPipeFd, newStderr); return true; &#125; else &#123; //父进程执行 IoUtils.closeQuietly(childPipeFd); childPipeFd = null; return handleParentProc(pid, descriptors, serverPipeFd, parsedArgs); &#125;&#125; finally &#123; IoUtils.closeQuietly(childPipeFd); IoUtils.closeQuietly(serverPipeFd);&#125; 4.2.2.6、总结Zygote启动过程的调用流程图： 1、解析init.zygote.rc中的参数，创建AppRuntime并调用AppRuntime.start()方法；2、 调用AndroidRuntime的startVM()方法创建虚拟机，再调用startReg()注册JNI函数；3、通过JNI方式调用ZygoteInit.main()，第一次进入Java世界；4、registerZygoteSocket()建立socket通道，zygote作为通信的服务端，用于响应客户端请求；5、preload()预加载通用类、drawable和color资源、openGL以及共享库以及WebView，用于提高app启动效率；6、zygote完毕大部分工作，接下来再通过startSystemServer()，fork得力帮手system_server进程，也是上层framework的运行载体。7、 zygote功成身退，调用runSelectLoop()，随时待命，当接收到请求创建新进程请求时立即唤醒并执行相应工作。 （3）、启动SystemServer上篇4.3.1、启动流程SystemServer的在Android体系中所处的地位，SystemServer由Zygote fork生成的，进程名为system_server，该进程承载着framework的核心服务。 Android系统启动-zygote篇中讲到Zygote启动过程中会调用startSystemServer()，可知startSystemServer()函数是system_server启动流程的起点， 启动流程图如下： 4.3.2、ZygoteInit.startSystemServer()1234567891011121314151617181920212223242526272829303132333435363738394041424344private static boolean startSystemServer(String abiList, String socketName) throws MethodAndArgsCaller, RuntimeException &#123; ... //参数准备 String args[] = &#123; \"--setuid=1000\", \"--setgid=1000\", \"--setgroups=1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1018,1021,1032,3001,3002,3003,3006,3007\", \"--capabilities=\" + capabilities + \",\" + capabilities, \"--nice-name=system_server\", \"--runtime-args\", \"com.android.server.SystemServer\", &#125;; ZygoteConnection.Arguments parsedArgs = null; int pid; try &#123; //用于解析参数，生成目标格式 parsedArgs = new ZygoteConnection.Arguments(args); ZygoteConnection.applyDebuggerSystemProperty(parsedArgs); ZygoteConnection.applyInvokeWithSystemProperty(parsedArgs); // fork子进程，该进程是system_server进程 pid = Zygote.forkSystemServer( parsedArgs.uid, parsedArgs.gid, parsedArgs.gids, parsedArgs.debugFlags, null, parsedArgs.permittedCapabilities, parsedArgs.effectiveCapabilities); &#125; catch (IllegalArgumentException ex) &#123; throw new RuntimeException(ex); &#125; //进入子进程system_server if (pid == 0) &#123; if (hasSecondZygote(abiList)) &#123; waitForSecondaryZygote(socketName); &#125; // 完成system_server进程剩余的工作 handleSystemServerProcess(parsedArgs); &#125; return true;&#125; 准备参数并fork新进程，从上面可以看出system server进程参数信息为uid=1000,gid=1000,进程名为sytem_server，从zygote进程fork新进程后，需要关闭zygote原有的socket。另外，对于有两个zygote进程情况，需等待第2个zygote创建完成。 4.3.3、Zygote. forkSystemServer()123456789101112public static int forkSystemServer(int uid, int gid, int[] gids, int debugFlags, int[][] rlimits, long permittedCapabilities, long effectiveCapabilities) &#123; VM_HOOKS.preFork(); // 调用native方法fork system_server进程 int pid = nativeForkSystemServer( uid, gid, gids, debugFlags, rlimits, permittedCapabilities, effectiveCapabilities); if (pid == 0) &#123; Trace.setTracingEnabled(true); &#125; VM_HOOKS.postForkCommon(); return pid;&#125; nativeForkSystemServer()方法在AndroidRuntime.cpp中注册的，调用com_android_internal_os_Zygote.cpp中的register_com_android_internal_os_Zygote()方法建立native方法的映射关系，所以接下来进入如下方法。 4.3.4、com_android_internal_os_Zygote.nativeForkSystemServer()123456789101112131415161718192021static jint com_android_internal_os_Zygote_nativeForkSystemServer( JNIEnv* env, jclass, uid_t uid, gid_t gid, jintArray gids, jint debug_flags, jobjectArray rlimits, jlong permittedCapabilities, jlong effectiveCapabilities) &#123; //fork子进程， pid_t pid = ForkAndSpecializeCommon(env, uid, gid, gids, debug_flags, rlimits, permittedCapabilities, effectiveCapabilities, MOUNT_EXTERNAL_DEFAULT, NULL, NULL, true, NULL, NULL, NULL); if (pid &gt; 0) &#123; // zygote进程，检测system_server进程是否创建 gSystemServerPid = pid; int status; if (waitpid(pid, &amp;status, WNOHANG) == pid) &#123; //当system_server进程死亡后，重启zygote进程 RuntimeAbort(env); &#125; &#125; return pid;&#125; 当system_server进程创建失败时，将会重启zygote进程。这里需要注意，对于Android 5.0以上系统，有两个zygote进程，分别是zygote、zygote64两个进程，system_server的父进程，一般来说64位系统其父进程是zygote64进程 当kill system_server进程后，只重启zygote64和system_server，不重启zygote; 当kill zygote64进程后，只重启zygote64和system_server，也不重启zygote； 当kill zygote进程，则重启zygote、zygote64以及system_server。 4.3.5、com_android_internal_os_Zygote.ForkAndSpecializeCommon()12345678910111213141516171819202122232425262728293031323334353637383940414243444546static pid_t ForkAndSpecializeCommon(JNIEnv* env, uid_t uid, gid_t gid, jintArray javaGids, jint debug_flags, jobjectArray javaRlimits, jlong permittedCapabilities, jlong effectiveCapabilities, jint mount_external, jstring java_se_info, jstring java_se_name, bool is_system_server, jintArray fdsToClose, jstring instructionSet, jstring dataDir) &#123; SetSigChldHandler(); //设置子进程的signal信号处理函数 pid_t pid = fork(); //fork子进程 if (pid == 0) &#123; //进入子进程 DetachDescriptors(env, fdsToClose); //关闭并清除文件描述符 if (!is_system_server) &#123; //对于非system_server子进程，则创建进程组 int rc = createProcessGroup(uid, getpid()); &#125; SetGids(env, javaGids); //设置设置group SetRLimits(env, javaRlimits); //设置资源limit int rc = setresgid(gid, gid, gid); rc = setresuid(uid, uid, uid); SetCapabilities(env, permittedCapabilities, effectiveCapabilities); SetSchedulerPolicy(env); //设置调度策略 //selinux上下文 rc = selinux_android_setcontext(uid, is_system_server, se_info_c_str, se_name_c_str); if (se_info_c_str == NULL &amp;&amp; is_system_server) &#123; se_name_c_str = \"system_server\"; &#125; if (se_info_c_str != NULL) &#123; SetThreadName(se_name_c_str); //设置线程名为system_server，方便调试 &#125; UnsetSigChldHandler(); //设置子进程的signal信号处理函数为默认函数 //等价于调用zygote.callPostForkChildHooks() env-&gt;CallStaticVoidMethod(gZygoteClass, gCallPostForkChildHooks, debug_flags, is_system_server ? NULL : instructionSet); ... &#125; else if (pid &gt; 0) &#123; //进入父进程，即zygote进程 &#125; return pid;&#125; fork()创建新进程，采用copy on write方式，这是linux创建进程的标准方法，会有两次return,对于pid==0为子进程的返回，对于pid&gt;0为父进程的返回。 到此system_server进程已完成了创建的所有工作，接下来开始了system_server进程的真正工作。在前面startSystemServer()方法中，zygote进程执行完forkSystemServer()后，新创建出来的system_server进程便进入handleSystemServerProcess()方法。 4.3.5、ZygoteInit.handleSystemServerProcess()123456789101112131415161718192021222324252627282930313233343536373839404142434445private static void handleSystemServerProcess( ZygoteConnection.Arguments parsedArgs) throws ZygoteInit.MethodAndArgsCaller &#123; closeServerSocket(); //关闭父进程zygote复制而来的Socket Os.umask(S_IRWXG | S_IRWXO); if (parsedArgs.niceName != null) &#123; Process.setArgV0(parsedArgs.niceName); //设置当前进程名为\"system_server\" &#125; final String systemServerClasspath = Os.getenv(\"SYSTEMSERVERCLASSPATH\"); if (systemServerClasspath != null) &#123; //执行dex优化操作 performSystemServerDexOpt(systemServerClasspath); &#125; if (parsedArgs.invokeWith != null) &#123; String[] args = parsedArgs.remainingArgs; if (systemServerClasspath != null) &#123; String[] amendedArgs = new String[args.length + 2]; amendedArgs[0] = \"-cp\"; amendedArgs[1] = systemServerClasspath; System.arraycopy(parsedArgs.remainingArgs, 0, amendedArgs, 2, parsedArgs.remainingArgs.length); &#125; //启动应用进程 WrapperInit.execApplication(parsedArgs.invokeWith, parsedArgs.niceName, parsedArgs.targetSdkVersion, VMRuntime.getCurrentInstructionSet(), null, args); &#125; else &#123; ClassLoader cl = null; if (systemServerClasspath != null) &#123; 创建类加载器，并赋予当前线程 cl = new PathClassLoader(systemServerClasspath, ClassLoader.getSystemClassLoader()); Thread.currentThread().setContextClassLoader(cl); &#125; //system_server故进入此分支 RuntimeInit.zygoteInit(parsedArgs.targetSdkVersion, parsedArgs.remainingArgs, cl); &#125; /* should never reach here */&#125; 此处systemServerClasspath环境变量主要有/system/framework/目录下的services.jar，ethernet-service.jar, wifi-service.jar这3个文件 4.3.6、ZygoteInit.performSystemServerDexOpt()123456789101112131415161718192021222324private static void performSystemServerDexOpt(String classPath) &#123; final String[] classPathElements = classPath.split(\":\"); //创建一个与installd的建立socket连接 final InstallerConnection installer = new InstallerConnection(); //执行ping操作，直到与installd服务端连通为止 installer.waitForConnection(); final String instructionSet = VMRuntime.getRuntime().vmInstructionSet(); try &#123; for (String classPathElement : classPathElements) &#123; final int dexoptNeeded = DexFile.getDexOptNeeded( classPathElement, \"*\", instructionSet, false /* defer */); if (dexoptNeeded != DexFile.NO_DEXOPT_NEEDED) &#123; //以system权限，执行dex文件优化 installer.dexopt(classPathElement, Process.SYSTEM_UID, false, instructionSet, dexoptNeeded); &#125; &#125; &#125; catch (IOException ioe) &#123; throw new RuntimeException(\"Error starting system_server\", ioe); &#125; finally &#123; installer.disconnect(); //断开与installd的socket连接 &#125;&#125; 将classPath字符串中的apk，分别进行dex优化操作。真正执行优化工作通过socket通信将相应的命令参数，发送给installd来完成。 4.3.7、RuntimeInit.zygoteInit()12345678910public static final void zygoteInit(int targetSdkVersion, String[] argv, ClassLoader classLoader) throws ZygoteInit.MethodAndArgsCaller &#123; Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, \"RuntimeInit\"); redirectLogStreams(); //重定向log输出 commonInit(); // 通用的一些初始化 nativeZygoteInit(); // zygote初始化 applicationInit(targetSdkVersion, argv, classLoader); // 应用初始化&#125; 4.3.8、RuntimeInit.commonInit()123456789101112131415161718192021222324private static final void commonInit() &#123; // 设置默认的未捕捉异常处理方法 Thread.setDefaultUncaughtExceptionHandler(new UncaughtHandler()); // 设置市区，中国时区为\"Asia/Shanghai\" TimezoneGetter.setInstance(new TimezoneGetter() &#123; @Override public String getId() &#123; return SystemProperties.get(\"persist.sys.timezone\"); &#125; &#125;); TimeZone.setDefault(null); //重置log配置 LogManager.getLogManager().reset(); new AndroidConfig(); // 设置默认的HTTP User-agent格式,用于 HttpURLConnection。 String userAgent = getDefaultUserAgent(); System.setProperty(\"http.agent\", userAgent); // 设置socket的tag，用于网络流量统计 NetworkManagementSocketTagger.install();&#125; 默认的HTTP User-agent格式，例如： “Dalvik/1.1.0 (Linux; U; Android 6.0.1；LenovoX3c70 Build/LMY47V)”. 4.3.9、AndroidRuntime.nativeZygoteInit()nativeZygoteInit()方法在AndroidRuntime.cpp中，进行了jni映射，对应下面的方法。 12345static void com_android_internal_os_RuntimeInit_nativeZygoteInit(JNIEnv* env, jobject clazz)&#123; //此处的gCurRuntime为AppRuntime，是在AndroidRuntime.cpp中定义的 gCurRuntime-&gt;onZygoteInit();&#125; 1234567[–&gt;app_main.cpp]virtual void onZygoteInit()&#123; sp&lt;ProcessState&gt; proc = ProcessState::self(); proc-&gt;startThreadPool(); //启动新binder线程&#125; ProcessState::self()是单例模式，主要工作是调用open()打开/dev/binder驱动设备，再利用mmap()映射内核的地址空间，将Binder驱动的fd赋值ProcessState对象中的变量mDriverFD，用于交互操作。startThreadPool()是创建一个新的binder线程，不断进行talkWithDriver()，在binder系列文章中的注册服务(addService)详细这两个方法的执行原理。 4.3.10、RuntimeInit.applicationInit()123456789101112131415161718192021private static void applicationInit(int targetSdkVersion, String[] argv, ClassLoader classLoader) throws ZygoteInit.MethodAndArgsCaller &#123; //true代表应用程序退出时不调用AppRuntime.onExit()，否则会在退出前调用 nativeSetExitWithoutCleanup(true); //设置虚拟机的内存利用率参数值为0.75 VMRuntime.getRuntime().setTargetHeapUtilization(0.75f); VMRuntime.getRuntime().setTargetSdkVersion(targetSdkVersion); final Arguments args; try &#123; args = new Arguments(argv); //解析参数 &#125; catch (IllegalArgumentException ex) &#123; return; &#125; Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER); //调用startClass的static方法 main() invokeStaticMain(args.startClass, args.startArgs, classLoader);&#125; 在startSystemServer()方法中通过硬编码初始化参数，可知此处args.startClass为”com.android.server.SystemServer”。 4.3.11、RuntimeInit.invokeStaticMain()12345678910111213141516171819202122private static void invokeStaticMain(String className, String[] argv, ClassLoader classLoader) throws ZygoteInit.MethodAndArgsCaller &#123; Class&lt;?&gt; cl = Class.forName(className, true, classLoader); ... Method m; try &#123; m = cl.getMethod(\"main\", new Class[] &#123; String[].class &#125;); &#125; catch (NoSuchMethodException ex) &#123; ... &#125; catch (SecurityException ex) &#123; ... &#125; int modifiers = m.getModifiers(); if (! (Modifier.isStatic(modifiers) &amp;&amp; Modifier.isPublic(modifiers))) &#123; ... &#125; //通过抛出异常，回到ZygoteInit.main()。这样做好处是能清空栈帧，提高栈帧利用率。 throw new ZygoteInit.MethodAndArgsCaller(m, argv);&#125; 4.3.12、MethodAndArgsCaller.run()在Zygote中遗留了一个问题没有讲解，如下： [–&gt;ZygoteInit.java] 1234567891011public static void main(String argv[]) &#123; try &#123; startSystemServer(abiList, socketName);//启动system_server .... &#125; catch (MethodAndArgsCaller caller) &#123; caller.run(); &#125; catch (RuntimeException ex) &#123; closeServerSocket(); throw ex; &#125;&#125; 现在已经很明显了，是invokeStaticMain()方法中抛出的异常MethodAndArgsCaller，从而进入caller.run()方法。 [–&gt;ZygoteInit.java] 1234567891011121314151617181920public static class MethodAndArgsCaller extends Exception implements Runnable &#123; public void run() &#123; try &#123; //根据传递过来的参数，可知此处通过反射机制调用的是SystemServer.main()方法 mMethod.invoke(null, new Object[] &#123; mArgs &#125;); &#125; catch (IllegalAccessException ex) &#123; throw new RuntimeException(ex); &#125; catch (InvocationTargetException ex) &#123; Throwable cause = ex.getCause(); if (cause instanceof RuntimeException) &#123; throw (RuntimeException) cause; &#125; else if (cause instanceof Error) &#123; throw (Error) cause; &#125; throw new RuntimeException(ex); &#125; &#125;&#125; 到此，总算是进入到了SystemServer类的main()方法， 在文章Android系统启动-SystemServer下篇中会紧接着这里开始讲述。 （4）、启动SystemServer下篇上篇文章Android系统启动-systemServer上篇 从Zygote一路启动到SystemServer的过程。 简单回顾下，在RuntimeInit.java中invokeStaticMain方法通过创建并抛出异常ZygoteInit.MethodAndArgsCaller，在ZygoteInit.java中的main()方法会捕捉该异常，并调用caller.run()，再通过反射便会调用到SystemServer.main()方法，该方法主要执行流程： 1234567SystemServer.main SystemServer.run createSystemContext startBootstrapServices(); startCoreServices(); startOtherServices(); Looper.loop(); 接下来，从其main方法说起。 4.4.1、SystemServer.main()1234567public final class SystemServer &#123; ... public static void main(String[] args) &#123; //先初始化SystemServer对象，再调用对象的run()方法 new SystemServer().run(); &#125;&#125; 4.4.2、SystemServer.run()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465private void run() &#123; //当系统时间比1970年更早，就设置当前系统时间为1970年 if (System.currentTimeMillis() &lt; EARLIEST_SUPPORTED_TIME) &#123; SystemClock.setCurrentTimeMillis(EARLIEST_SUPPORTED_TIME); &#125; //变更虚拟机的库文件，对于Android 6.0默认采用的是libart.so SystemProperties.set(\"persist.sys.dalvik.vm.lib.2\", VMRuntime.getRuntime().vmLibrary()); if (SamplingProfilerIntegration.isEnabled()) &#123; ... &#125; //清除vm内存增长上限，由于启动过程需要较多的虚拟机内存空间 VMRuntime.getRuntime().clearGrowthLimit(); //设置内存的可能有效使用率为0.8 VMRuntime.getRuntime().setTargetHeapUtilization(0.8f); // 针对部分设备依赖于运行时就产生指纹信息，因此需要在开机完成前已经定义 Build.ensureFingerprintProperty(); //访问环境变量前，需要明确地指定用户 Environment.setUserRequired(true); //确保当前系统进程的binder调用，总是运行在前台优先级(foreground priority) BinderInternal.disableBackgroundScheduling(true); android.os.Process.setThreadPriority(android.os.Process.THREAD_PRIORITY_FOREGROUND); android.os.Process.setCanSelfBackground(false); // 主线程looper就在当前线程运行 Looper.prepareMainLooper(); //加载android_servers.so库，该库包含的源码在frameworks/base/services/目录下 System.loadLibrary(\"android_servers\"); //检测上次关机过程是否失败，该方法可能不会返回 performPendingShutdown(); //初始化系统上下文 createSystemContext(); //创建系统服务管理 mSystemServiceManager = new SystemServiceManager(mSystemContext); //将mSystemServiceManager添加到本地服务的成员sLocalServiceObjects LocalServices.addService(SystemServiceManager.class, mSystemServiceManager); //启动各种系统服务 try &#123; startBootstrapServices(); // 启动引导服务 startCoreServices(); // 启动核心服务 startOtherServices(); // 启动其他服务 &#125; catch (Throwable ex) &#123; Slog.e(\"System\", \"************ Failure starting system services\", ex); throw ex; &#125; //用于debug版本，将log事件不断循环地输出到dropbox（用于分析） if (StrictMode.conditionallyEnableDebugLogging()) &#123; Slog.i(TAG, \"Enabled StrictMode for system server main thread.\"); &#125; //一直循环执行 Looper.loop(); throw new RuntimeException(\"Main thread loop unexpectedly exited\");&#125; LocalServices通过用静态Map变量sLocalServiceObjects，来保存以服务类名为key，以具体服务对象为value的Map结构。 4.4.3、SystemServer.performPendingShutdown()12345678910111213141516private void performPendingShutdown() &#123; final String shutdownAction = SystemProperties.get( ShutdownThread.SHUTDOWN_ACTION_PROPERTY, \"\"); if (shutdownAction != null &amp;&amp; shutdownAction.length() &gt; 0) &#123; boolean reboot = (shutdownAction.charAt(0) == '1'); final String reason; if (shutdownAction.length() &gt; 1) &#123; reason = shutdownAction.substring(1, shutdownAction.length()); &#125; else &#123; reason = null; &#125; // 当\"sys.shutdown.requested\"值不为空,则会重启或者关机 ShutdownThread.rebootOrShutdown(null, reboot, reason); &#125;&#125; 4.4.4、SystemServer.createSystemContext()1234567private void createSystemContext() &#123; //创建system_server进程的上下文信息 ActivityThread activityThread = ActivityThread.systemMain(); mSystemContext = activityThread.getSystemContext(); //设置主题 mSystemContext.setTheme(android.R.style.Theme_DeviceDefault_Light_DarkActionBar);&#125; 理解Application创建过程已介绍过createSystemContext()过程， 该过程会创建对象有ActivityThread，Instrumentation, ContextImpl，LoadedApk，Application。 4.4.5、SystemServer.startBootstrapServices()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950private void startBootstrapServices() &#123; //阻塞等待与installd建立socket通道 Installer installer = mSystemServiceManager.startService(Installer.class); //启动服务ActivityManagerService mActivityManagerService = mSystemServiceManager.startService( ActivityManagerService.Lifecycle.class).getService(); mActivityManagerService.setSystemServiceManager(mSystemServiceManager); mActivityManagerService.setInstaller(installer); //启动服务PowerManagerService mPowerManagerService = mSystemServiceManager.startService(PowerManagerService.class); //初始化power management mActivityManagerService.initPowerManagement(); //启动服务LightsService mSystemServiceManager.startService(LightsService.class); //启动服务DisplayManagerService mDisplayManagerService = mSystemServiceManager.startService(DisplayManagerService.class); //Phase100: 在初始化package manager之前，需要默认的显示. mSystemServiceManager.startBootPhase(SystemService.PHASE_WAIT_FOR_DEFAULT_DISPLAY); //当设备正在加密时，仅运行核心 String cryptState = SystemProperties.get(\"vold.decrypt\"); if (ENCRYPTING_STATE.equals(cryptState)) &#123; mOnlyCore = true; &#125; else if (ENCRYPTED_STATE.equals(cryptState)) &#123; mOnlyCore = true; &#125; //启动服务PackageManagerService mPackageManagerService = PackageManagerService.main(mSystemContext, installer, mFactoryTestMode != FactoryTest.FACTORY_TEST_OFF, mOnlyCore); mFirstBoot = mPackageManagerService.isFirstBoot(); mPackageManager = mSystemContext.getPackageManager(); //启动服务UserManagerService，新建目录/data/user/ ServiceManager.addService(Context.USER_SERVICE, UserManagerService.getInstance()); AttributeCache.init(mSystemContext); //设置AMS mActivityManagerService.setSystemProcess(); //启动传感器服务 startSensorService();&#125; 该方法所创建的服务：ActivityManagerService, PowerManagerService, LightsService, DisplayManagerService， PackageManagerService， UserManagerService， sensor服务. 4.4.5、SystemServer.startCoreServices()1234567891011121314private void startCoreServices() &#123; //启动服务BatteryService，用于统计电池电量，需要LightService. mSystemServiceManager.startService(BatteryService.class); //启动服务UsageStatsService，用于统计应用使用情况 mSystemServiceManager.startService(UsageStatsService.class); mActivityManagerService.setUsageStatsManager( LocalServices.getService(UsageStatsManagerInternal.class)); mPackageManagerService.getUsageStatsIfNoPackageUsageInfo(); //启动服务WebViewUpdateService mSystemServiceManager.startService(WebViewUpdateService.class);&#125; 启动服务BatteryService，UsageStatsService，WebViewUpdateService。 4.4.6 SystemServer.startOtherServices()该方法比较长，有近千行代码，逻辑很简单，主要是启动一系列的服务，这里就不具体列举源码了，在第四节直接对其中的服务进行一个简单分类。 123456789101112131415161718192021222324252627282930313233343536373839404142private void startOtherServices() &#123; ... SystemConfig.getInstance(); mContentResolver = context.getContentResolver(); // resolver ... mActivityManagerService.installSystemProviders(); //provider mSystemServiceManager.startService(AlarmManagerService.class); // alarm // watchdog watchdog.init(context, mActivityManagerService); inputManager = new InputManagerService(context); // input wm = WindowManagerService.main(...); // window inputManager.start(); //启动input mDisplayManagerService.windowManagerAndInputReady(); ... mSystemServiceManager.startService(MOUNT_SERVICE_CLASS); // mount mPackageManagerService.performBootDexOpt(); // dexopt操作 ActivityManagerNative.getDefault().showBootMessage(...); //显示启动界面 ... statusBar = new StatusBarManagerService(context, wm); //statusBar //dropbox ServiceManager.addService(Context.DROPBOX_SERVICE, new DropBoxManagerService(context, new File(\"/data/system/dropbox\"))); mSystemServiceManager.startService(JobSchedulerService.class); //JobScheduler lockSettings.systemReady(); //lockSettings //phase480 和phase500 mSystemServiceManager.startBootPhase(SystemService.PHASE_LOCK_SETTINGS_READY); mSystemServiceManager.startBootPhase(SystemService.PHASE_SYSTEM_SERVICES_READY); ... // 准备好window, power, package, display服务 wm.systemReady(); mPowerManagerService.systemReady(...); mPackageManagerService.systemReady(); mDisplayManagerService.systemReady(...); //重头戏 mActivityManagerService.systemReady(new Runnable() &#123; public void run() &#123; ... &#125; &#125;); &#125; 4.4.7、服务启动阶段SystemServiceManager的startBootPhase()贯穿system_server进程的整个启动过程： 其中PHASE_BOOT_COMPLETED=1000，该阶段是发生在Boot完成和home应用启动完毕。系统服务更倾向于监听该阶段，而不是注册广播ACTION_BOOT_COMPLETED，从而降低系统延迟。 各个启动阶段所在源码的大致位置： 123456789101112131415161718192021222324252627282930313233public final class SystemServer &#123;private void startBootstrapServices() &#123; ... //phase100 mSystemServiceManager.startBootPhase(SystemService.PHASE_WAIT_FOR_DEFAULT_DISPLAY); ...&#125;private void startCoreServices() &#123; ...&#125;private void startOtherServices() &#123; ... //phase480 &amp;&amp; 500 mSystemServiceManager.startBootPhase(SystemService.PHASE_LOCK_SETTINGS_READY); mSystemServiceManager.startBootPhase(SystemService.PHASE_SYSTEM_SERVICES_READY); ... mActivityManagerService.systemReady(new Runnable() &#123; public void run() &#123; //phase550 mSystemServiceManager.startBootPhase( SystemService.PHASE_ACTIVITY_MANAGER_READY); ... //phase600 mSystemServiceManager.startBootPhase( SystemService.PHASE_THIRD_PARTY_APPS_CAN_START); &#125; &#125;&#125;&#125; 接下来再说说简单每个阶段的大概完成的工作： 4.4.7.1、Phase0创建四大引导服务: 1234ActivityManagerServicePowerManagerServiceLightsServiceDisplayManagerService 4.4.7.1.2、Phase100进入阶段PHASE_WAIT_FOR_DEFAULT_DISPLAY=100回调服务 123onBootPhase(100)DisplayManagerService 然后创建大量服务下面列举部分: 12345678PackageManagerServiceWindowManagerServiceInputManagerServiceNetworkManagerServiceDropBoxManagerServiceFingerprintServiceLauncherAppsService… 4.4.7.1.3、Phase480进入阶段PHASE_LOCK_SETTINGS_READY=480回调服务 123onBootPhase(480)DevicePolicyManagerService 阶段480后马上就进入阶段500. 4.4.7.1.4、Phase500PHASE_SYSTEM_SERVICES_READY=500，进入该阶段服务能安全地调用核心系统服务. 1234567891011121314151617onBootPhase(500)AlarmManagerServiceJobSchedulerServiceNotificationManagerServiceBackupManagerServiceUsageStatsServiceDeviceIdleControllerTrustManagerServiceUiModeManagerServiceBluetoothServiceBluetoothManagerServiceEthernetServiceWifiP2pServiceWifiScanningServiceWifiServiceRttService 各大服务执行systemReady(): 1234WindowManagerService.systemReady():PowerManagerService.systemReady():PackageManagerService.systemReady():DisplayManagerService.systemReady(): 接下来就绪AMS.systemReady方法. 4.4.7.1.5、Phase550PHASE_ACTIVITY_MANAGER_READY=550， AMS.mSystemReady=true, 已准备就绪,进入该阶段服务能广播Intent;但是system_server主线程并没有就绪. 12345678onBootPhase(550)MountServiceTelecomLoaderServiceUsbServiceWebViewUpdateServiceDockObserverBatteryService 接下来执行: (AMS启动native crash监控, 加载WebView，启动SystemUI等),如下 12345678910mActivityManagerService.startObservingNativeCrashes();WebViewFactory.prepareWebViewInSystemServer();startSystemUi(context);networkScoreF.systemReady();networkManagementF.systemReady();networkStatsF.systemReady();networkPolicyF.systemReady();connectivityF.systemReady();audioServiceF.systemReady();Watchdog.getInstance().start(); 4.4.7.1.6、Phase600PHASE_THIRD_PARTY_APPS_CAN_START=600 onBootPhase(600) JobSchedulerServiceNotificationManagerServiceBackupManagerServiceAppWidgetServiceGestureLauncherServiceDreamManagerServiceTrustManagerServiceVoiceInteractionManagerService 接下来,各种服务的systemRunning过程: WallpaperManagerService、InputMethodManagerService、LocationManagerService、CountryDetectorService、NetworkTimeUpdateService、CommonTimeManagementService、TextServicesManagerService、AssetAtlasService、InputManagerService、TelephonyRegistry、MediaRouterService、MmsServiceBroker这些服务依次执行其systemRunning()方法。 4.4.7.1.7、Phase1000在经过一系列流程，再调用AMS.finishBooting()时，则进入阶段Phase1000。 到此，系统服务启动阶段完成就绪，system_server进程启动完成则进入Looper.loop()状态，随时待命，等待消息队列MessageQueue中的消息到来，则马上进入执行状态。 4.4.8、服务类别system_server进程，从源码角度划分为引导服务、核心服务、其他服务3类。 以下这些系统服务的注册过程, 见Android系统服务的注册方式 引导服务(7个)：ActivityManagerService、PowerManagerService、LightsService、DisplayManagerService、PackageManagerService、UserManagerService、SensorService； 核心服务(3个)：BatteryService、UsageStatsService、WebViewUpdateService； 其他服务(70个+)：AlarmManagerService、VibratorService等。 合计总大约80个系统服务： ActivityManagerService PackageManagerService WindowManagerService PowerManagerService BatteryService BatteryStatsService DreamManagerService DropBoxManagerService SamplingProfilerService UsageStatsService DiskStatsService DeviceStorageMonitorService SchedulingPolicyService AlarmManagerService DeviceIdleController ThermalObserver JobSchedulerService AccessibilityManagerService DisplayManagerService LightsService GraphicsStatsService StatusBarManagerService NotificationManagerService WallpaperManagerService UiModeManagerService AppWidgetService LauncherAppsService TextServicesManagerService ContentService LockSettingsService InputMethodManagerService InputManagerService MountService FingerprintService TvInputManagerService DockObserver NetworkManagementService NetworkScoreService NetworkStatsService NetworkPolicyManagerService ConnectivityService BluetoothService WifiP2pService WifiService WifiScanningService AudioService MediaRouterService VoiceInteractionManagerService MediaProjectionManagerService MediaSessionServiceDevicePolicyManagerService PrintManagerService BackupManagerService UserManagerService AccountManagerService TrustManagerService SensorService LocationManagerService VibratorService CountryDetectorService GestureLauncherService PersistentDataBlockService EthernetService WebViewUpdateService ClipboardService TelephonyRegistry TelecomLoaderService NsdService UpdateLockService SerialService SearchManagerService CommonTimeManagementService AssetAtlasService ConsumerIrService MidiServiceCameraService TwilightService RestrictionsManagerService MmsServiceBroker RttService UsbService Service类别众多，其中表中加粗项是指博主挑选的较重要或者较常见的Service，并且在本博客中已经展开或者计划展开讲解的Service，当然如果有精力会讲解更多service，后续再更新。 （5）、启动ActivityManagerService4.5.1、概述ActivityManagerService(AMS)是Android中最核心的服务，主要负责系统中四大组件的启动、切换、调度及应用程序的管理和调度等工作。 AMS通信结构如下图所示： 4.5.2、SystemServer.startBootstrapServices()1234567891011121314151617private void startBootstrapServices() &#123;...//启动AMS服务mActivityManagerService = mSystemServiceManager.startService( ActivityManagerService.Lifecycle.class).getService();//设置AMS的系统服务管理器mActivityManagerService.setSystemServiceManager(mSystemServiceManager);//设置AMS的APP安装器mActivityManagerService.setInstaller(installer);//初始化AMS相关的PMSmActivityManagerService.initPowerManagement();...//设置SystemServermActivityManagerService.setSystemProcess();&#125; 4.5.3、启动AMS服务SystemServiceManager.startService(ActivityManagerService.Lifecycle.class) 功能主要： 创建ActivityManagerService.Lifecycle对象； 调用Lifecycle.onStart()方法。 4.5.4、启动AMS服务4.5.4.1 AMS.Lifecycle [-&gt; ActivityManagerService.java] 123456789101112131415161718public static final class Lifecycle extends SystemService &#123;private final ActivityManagerService mService;public Lifecycle(Context context) &#123; super(context); //创建ActivityManagerService mService = new ActivityManagerService(context);&#125;@Overridepublic void onStart() &#123; mService.start(); &#125;public ActivityManagerService getService() &#123; return mService;&#125;&#125; 该过程：创建AMS内部类的Lifecycle，已经创建AMS对象，并调用AMS.start(); 4.5.4.2 AMS创建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public ActivityManagerService(Context systemContext) &#123;mContext = systemContext;mFactoryTest = FactoryTest.getMode();//默认为FACTORY_TEST_OFFmSystemThread = ActivityThread.currentActivityThread();//创建名为\"ActivityManager\"的前台线程，并获取mHandlermHandlerThread = new ServiceThread(TAG, android.os.Process.THREAD_PRIORITY_FOREGROUND, false);mHandlerThread.start();mHandler = new MainHandler(mHandlerThread.getLooper());//通过UiThread类，创建名为\"android.ui\"的线程mUiHandler = new UiHandler();//前台广播接收器，在运行超过10s将放弃执行mFgBroadcastQueue = new BroadcastQueue(this, mHandler, \"foreground\", BROADCAST_FG_TIMEOUT, false);//后台广播接收器，在运行超过60s将放弃执行mBgBroadcastQueue = new BroadcastQueue(this, mHandler, \"background\", BROADCAST_BG_TIMEOUT, true);mBroadcastQueues[0] = mFgBroadcastQueue;mBroadcastQueues[1] = mBgBroadcastQueue;//创建ActiveServices，其中非低内存手机mMaxStartingBackground为8mServices = new ActiveServices(this);mProviderMap = new ProviderMap(this);//创建目录/data/systemFile dataDir = Environment.getDataDirectory();File systemDir = new File(dataDir, \"system\");systemDir.mkdirs();//创建服务BatteryStatsServicemBatteryStatsService = new BatteryStatsService(systemDir, mHandler);mBatteryStatsService.getActiveStatistics().readLocked();...//创建进程统计服务，信息保存在目录/data/system/procstats，mProcessStats = new ProcessStatsService(this, new File(systemDir, \"procstats\"));mAppOpsService = new AppOpsService(new File(systemDir, \"appops.xml\"), mHandler);mGrantFile = new AtomicFile(new File(systemDir, \"urigrants.xml\"));// User 0是第一个，也是唯一的一个开机过程中运行的用户mStartedUsers.put(UserHandle.USER_OWNER, new UserState(UserHandle.OWNER, true));mUserLru.add(UserHandle.USER_OWNER);updateStartedUserArrayLocked();...//CPU使用情况的追踪器执行初始化mProcessCpuTracker.init();...mRecentTasks = new RecentTasks(this);// 创建ActivityStackSupervisor对象mStackSupervisor = new ActivityStackSupervisor(this, mRecentTasks);mTaskPersister = new TaskPersister(systemDir, mStackSupervisor, mRecentTasks);//创建名为\"CpuTracker\"的线程mProcessCpuThread = new Thread(\"CpuTracker\") &#123; public void run() &#123; while (true) &#123; try &#123; try &#123; synchronized(this) &#123; final long now = SystemClock.uptimeMillis(); long nextCpuDelay = (mLastCpuTime.get()+MONITOR_CPU_MAX_TIME)-now; long nextWriteDelay = (mLastWriteTime+BATTERY_STATS_TIME)-now; if (nextWriteDelay &lt; nextCpuDelay) &#123; nextCpuDelay = nextWriteDelay; &#125; if (nextCpuDelay &gt; 0) &#123; mProcessCpuMutexFree.set(true); this.wait(nextCpuDelay); &#125; &#125; &#125; catch (InterruptedException e) &#123; &#125; updateCpuStatsNow(); //更新CPU状态 &#125; catch (Exception e) &#123; &#125; &#125; &#125;&#125;;...&#125; 4.5.4.3、AMS的start函数 123456789101112private void start() &#123; //完成统计前的复位工作 Process.removeAllProcessGroups(); //开始监控进程的CPU使用情况 mProcessCpuThread.start(); //注册服务 mBatteryStatsService.publish(mContext); mAppOpsService.publish(mContext); Slog.d(\"AppOps\", \"AppOpsService published\"); LocalServices.addService(ActivityManagerInternal.class, new LocalService()); AMS的start函数比较简单，主要是： 1、启动CPU监控线程。该线程将会开始统计不同进程使用CPU的情况。 2、发布一些服务，如BatteryStatsService、AppOpsService(权限管理相关)和本地实现的继承ActivityManagerInternal的服务。 4.5.5 AMS.setSystemProcess() 123456789101112131415161718192021222324252627282930313233public void setSystemProcess() &#123; try &#123; ServiceManager.addService(Context.ACTIVITY_SERVICE, this, true); ServiceManager.addService(ProcessStats.SERVICE_NAME, mProcessStats); ServiceManager.addService(\"meminfo\", new MemBinder(this)); ServiceManager.addService(\"gfxinfo\", new GraphicsBinder(this)); ServiceManager.addService(\"dbinfo\", new DbBinder(this)); if (MONITOR_CPU_USAGE) &#123; ServiceManager.addService(\"cpuinfo\", new CpuBinder(this)); &#125; ServiceManager.addService(\"permission\", new PermissionController(this)); ServiceManager.addService(\"processinfo\", new ProcessInfoService(this)); ApplicationInfo info = mContext.getPackageManager().getApplicationInfo( \"android\", STOCK_PM_FLAGS); mSystemThread.installSystemApplicationInfo(info, getClass().getClassLoader()); synchronized (this) &#123; //创建ProcessRecord对象 ProcessRecord app = newProcessRecordLocked(info, info.processName, false, 0); app.persistent = true; //设置为persistent进程 app.pid = MY_PID; app.maxAdj = ProcessList.SYSTEM_ADJ; app.makeActive(mSystemThread.getApplicationThread(), mProcessStats); synchronized (mPidsSelfLocked) &#123; mPidsSelfLocked.put(app.pid, app); &#125; updateLruProcessLocked(app, false, null);//维护进程lru updateOomAdjLocked(); //更新adj &#125; &#125; catch (PackageManager.NameNotFoundException e) &#123; throw new RuntimeException(\"\", e); &#125;&#125; 该方法主要工作是注册各种服务。 4.5.5.1 AT.installSystemApplicationInfo() 12345678public void installSystemApplicationInfo(ApplicationInfo info, ClassLoader classLoader) &#123; synchronized (this) &#123; // getSystemContext().installSystemApplicationInfo(info, classLoader); //创建用于性能统计的Profiler对象 mProfiler = new Profiler(); &#125;&#125; 该方法调用ContextImpl的nstallSystemApplicationInfo()方法，最终调用LoadedApk的installSystemApplicationInfo，加载名为”android”的package 4.5.5.2 installSystemApplicationInfo() [-&gt; LoadedApk.java] 12345void installSystemApplicationInfo(ApplicationInfo info, ClassLoader classLoader) &#123; assert info.packageName.equals(\"android\"); mApplicationInfo = info; //将包名为\"android\"的应用信息保存到mApplicationInfo mClassLoader = classLoader;&#125; 4.5.6 startOtherServices() 123456789101112131415161718192021222324private void startOtherServices() &#123; ... //安装系统Provider mActivityManagerService.installSystemProviders(); ... //phase480 &amp;&amp; 500 mSystemServiceManager.startBootPhase(SystemService.PHASE_LOCK_SETTINGS_READY); mSystemServiceManager.startBootPhase(SystemService.PHASE_SYSTEM_SERVICES_READY); ... mActivityManagerService.systemReady(new Runnable() &#123; public void run() &#123; //phase550 mSystemServiceManager.startBootPhase( SystemService.PHASE_ACTIVITY_MANAGER_READY); ... //phase600 mSystemServiceManager.startBootPhase( SystemService.PHASE_THIRD_PARTY_APPS_CAN_START); ... &#125; &#125;&#125; 4.5.6.1 AMS.installSystemProviders() 1234567891011121314151617181920212223public final void installSystemProviders() &#123; List&lt;ProviderInfo&gt; providers; synchronized (this) &#123; ProcessRecord app = mProcessNames.get(\"system\", Process.SYSTEM_UID); providers = generateApplicationProvidersLocked(app); if (providers != null) &#123; for (int i=providers.size()-1; i&gt;=0; i--) &#123; ProviderInfo pi = (ProviderInfo)providers.get(i); //移除非系统的provider if ((pi.applicationInfo.flags&amp;ApplicationInfo.FLAG_SYSTEM) == 0) &#123; providers.remove(i); &#125; &#125; &#125; &#125; if (providers != null) &#123; //安装所有的系统provider mSystemThread.installSystemProviders(providers); &#125; // 创建核心Settings Observer，用于监控Settings的改变。 mCoreSettingsObserver = new CoreSettingsObserver(this);&#125; 4.5.7、AMS.systemReady()4.5.7.1、阶段一123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public void systemReady(final Runnable goingCallback) &#123; synchronized(this) &#123; .......... //这一部分主要是调用一些关键服务SystemReady相关的函数， //进行一些等待AMS初始完，才能进行的工作 // Make sure we have the current profile info, since it is needed for security checks. mUserController.onSystemReady(); mRecentTasks.onSystemReadyLocked(); mAppOpsService.systemReady(); mSystemReady = true; &#125; ArrayList&lt;ProcessRecord&gt; procsToKill = null; synchronized(mPidsSelfLocked) &#123; //mPidsSelfLocked中保存当前正在运行的所有进程的信息 for (int i=mPidsSelfLocked.size()-1; i&gt;=0; i--) &#123; ProcessRecord proc = mPidsSelfLocked.valueAt(i); //在AMS启动完成前，如果没有FLAG_PERSISTENT标志的进程已经启动了， //就将这个进程加入到procsToKill中 if (!isAllowedWhileBooting(proc.info))&#123; if (procsToKill == null) &#123; procsToKill = new ArrayList&lt;ProcessRecord&gt;(); &#125; procsToKill.add(proc); &#125; &#125; &#125; synchronized(this) &#123; //利用removeProcessLocked关闭procsToKill中的进程 if (procsToKill != null) &#123; for (int i=procsToKill.size()-1; i&gt;=0; i--) &#123; ProcessRecord proc = procsToKill.get(i); Slog.i(TAG, \"Removing system update proc: \" + proc); removeProcessLocked(proc, true, false, \"system update done\"); &#125; &#125; // Now that we have cleaned up any update processes, we // are ready to start launching real processes and know that // we won't trample on them any more. //至此系统准备完毕 mProcessesReady = true; &#125; ............ //根据数据库和资源文件，获取一些配置参数 retrieveSettings(); final int currentUserId; synchronized (this) &#123; //得到当前的用户ID currentUserId = mUserController.getCurrentUserIdLocked(); //读取urigrants.xml，为其中定义的ContentProvider配置对指定Uri数据的访问/修改权限 //原生代码中，似乎没有urigrants.xml文件 //实际使用的grant-uri-permission是分布式定义的 readGrantedUriPermissionsLocked(); &#125; .......... 这一部分的工作主要是调用一些关键服务的初始化函数， 然后杀死那些没有FLAG_PERSISTENT却在AMS启动完成前已经存在的进程， 同时获取一些配置参数。 需要注意的是，由于只有Java进程才会向AMS注册，而一般的Native进程不会向AMS注册，因此此处杀死的进程是Java进程。 4.5.7.2、阶段二1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//1、调用参数传入的runnable对象，SystemServer中有具体的定义if (goingCallback != null) goingCallback.run();..............//调用所有系统服务的onStartUser接口mSystemServiceManager.startUser(currentUserId);.............synchronized (this) &#123; // Only start up encryption-aware persistent apps; once user is // unlocked we'll come back around and start unaware apps 2、启动persistent为1的application所在的进程 startPersistentApps(PackageManager.MATCH_DIRECT_BOOT_AWARE); // Start up initial activity. mBooting = true; // Enable home activity for system user, so that the system can always boot //当isSplitSystemUser返回true时，意味者system user和primary user是分离的 //这里应该是让system user也有启动home activity的权限吧 if (UserManager.isSplitSystemUser()) &#123; ComponentName cName = new ComponentName(mContext, SystemUserHomeActivity.class); try &#123; AppGlobals.getPackageManager().setComponentEnabledSetting(cName, PackageManager.COMPONENT_ENABLED_STATE_ENABLED, 0, UserHandle.USER_SYSTEM); &#125; catch (RemoteException e) &#123; throw e.rethrowAsRuntimeException(); &#125; &#125; //3、启动Home startHomeActivityLocked(currentUserId, \"systemReady\"); try &#123; //发送消息，触发处理Uid错误的Application if (AppGlobals.getPackageManager().hasSystemUidErrors()) &#123; .......... mUiHandler.obtainMessage(SHOW_UID_ERROR_UI_MSG).sendToTarget(); &#125; &#125; catch (RemoteException e) &#123; &#125; //发送一些广播信息 ............ //这里暂时先不深入，等进一步了解Activity的启动过程后，再做了解 mStackSupervisor.resumeFocusedStackTopActivityLocked(); ............&#125;............. 从部分代码来看，主要的工作就是通知一些服务可以进行systemReady相关的工作，并进行启动服务或应用进程的工作。 2.1、调用回调接口回调接口的具体内容定义与SystemServer.java中，其中会调用大量服务的onBootPhase函数、一些对象的systemReady函数或systemRunning函数。 此处，我们仅截取一些比较特别的内容： 123456789101112131415161718192021public void run() &#123; ............ try &#123; //启动NativeCrashListener监听\"/data/system/ndebugsocket\"中的信息 //实际上是监听debuggerd传入的信息 mActivityManagerService.startObservingNativeCrashes(); &#125; catch (Throwable e) &#123; reportWtf(\"observing native crashes\", e); &#125; ............ try &#123; //启动SystemUi startSystemUi(context); &#125; catch (Throwable e) &#123; reportWtf(\"starting System UI\", e); &#125; ............ //这个以前分析过，启动Watchdog Watchdog.getInstance().start(); ....................&#125; 回调接口中的内容较多，不做一一分析。 2.2、启动persistent标志的进程我们看看startPersistentApps对应的内容： 12345678910111213141516171819private void startPersistentApps(int matchFlags) &#123; ............. synchronized (this) &#123; try &#123; //从PKMS中得到persistent为1的ApplicationInfo final List&lt;ApplicationInfo&gt; apps = AppGlobals.getPackageManager() .getPersistentApplications(STOCK_PM_FLAGS | matchFlags).getList(); for (ApplicationInfo app : apps) &#123; //由于framework-res.apk已经由系统启动，所以此处不再启动它 if (!\"android\".equals(app.packageName)) &#123; //addAppLocked中将启动application所在进程 addAppLocked(app, false, null /* ABI override */); &#125; &#125; &#125; catch (RemoteException ex) &#123; &#125; &#125;&#125; 跟进一下addAppLocked函数： 1234567891011121314151617181920212223242526272829303132333435363738394041final ProcessRecord addAppLocked(ApplicationInfo info, boolean isolated, String abiOverride) &#123; //以下是取出或构造出ApplicationInfo对应的ProcessRecord ProcessRecord app; if (!isolated) &#123; app = getProcessRecordLocked(info.processName, info.uid, true); &#125; else &#123; app = null; &#125; if (app == null) &#123; app = newProcessRecordLocked(info, null, isolated, 0); updateLruProcessLocked(app, false, null); updateOomAdjLocked(); &#125; ........... // This package really, really can not be stopped. try &#123; //通过PKMS将package对应数据结构的StoppedState置为fasle AppGlobals.getPackageManager().setPackageStoppedState( info.packageName, false, UserHandle.getUserId(app.uid)); &#125; catch (RemoteException e) &#123; &#125; catch (IllegalArgumentException e) &#123; Slog.w(TAG, \"Failed trying to unstop package \" + info.packageName + \": \" + e); &#125; if ((info.flags &amp; PERSISTENT_MASK) == PERSISTENT_MASK) &#123; app.persistent = true; app.maxAdj = ProcessList.PERSISTENT_PROC_ADJ; &#125; if (app.thread == null &amp;&amp; mPersistentStartingProcesses.indexOf(app) &lt; 0) &#123; mPersistentStartingProcesses.add(app); //启动应用所在进程，将发送消息给zygote，后者fork出进程 startProcessLocked(app, \"added application\", app.processName, abiOverride, null /* entryPoint */, null /* entryPointArgs */); &#125; return app;&#125; 这里最终将通过startProcessLocked函数，启动实际的应用进程。 正如之前分析zygote进程时，提过的一样，zygote中的server socket将接收消息，然后为应用fork出进程。 总结对于整个AMS启动过程而言，博客中涉及的内容可能只是极小的一部分。 但即使我们尽可能的简化，整个过程的内容还是非常多。 不过我们回头看看整个过程，还是能比较清晰地将AMS的启动过程分为四步，如上图所示： 1、创建出SystemServer进程的Android运行环境。 在这一部分，SystemServer进程主要创建出对应的ActivityThread和ContextImpl，构成Android运行环境。 AMS的后续工作依赖于SystemServer在此创建出的运行环境。 2、完成AMS的初始化和启动。 在这一部分，单纯地调用AMS的构造函数和start函数，完成AMS的一些初始化工作。 3、将SystemServer进程纳入到AMS的管理体系中。 AMS作为Java世界的进程管理和调度中心，要对所有Java进程一视同仁，因此SystemServer进程也必须被AMS管理。 在这个过程中，AMS加载了SystemServer中framework-res.apk的信息，并启动和注册了SettingsProvider.apk。 4、开始执行AMS启动完毕后才能进行的工作。 系统中的一些服务和进程，必须等待AMS完成启动后，才能展开后续工作。 在这一部分，AMS通过调用systemReady函数，通知系统中的其它服务和进程，可以进行对应工作了。 在这个过程中，值得我们关注的是：Home Activity被启动了。当该Activity被加载完成后，最终会触发ACTION_BOOT_COMPLETED广播。 （6）、启动Launcher(Activity)看看启动Home Activity对应的startHomeActivityLocked函数： 1234567891011121314151617181920212223boolean startHomeActivityLocked(int userId, String reason) &#123; .............. Intent intent = getHomeIntent(); //根据intent中携带的ComponentName，利用PKMS得到ActivityInfo ActivityInfo aInfo = resolveActivityInfo(intent, STOCK_PM_FLAGS, userId); if (aInfo != null) &#123; intent.setComponent(new ComponentName(aInfo.applicationInfo.packageName, aInfo.name)); aInfo = new ActivityInfo(aInfo); aInfo.applicationInfo = getAppInfoForUser(aInfo.applicationInfo, userId); //此时home对应进程应该还没启动，app为null ProcessRecord app = getProcessRecordLocked(aInfo.processName, aInfo.applicationInfo.uid, true); if (app == null || app.instrumentationClass == null) &#123; intent.setFlags(intent.getFlags() | Intent.FLAG_ACTIVITY_NEW_TASK); //启动home mActivityStarter.startHomeActivityLocked(intent, aInfo, reason); &#125; &#125; else &#123; .......... &#125; return true;&#125; 这里暂时先不深究Home Activity启动的具体过程。 从手头的资料来看，当Home Activity启动后， ActivityStackSupervisor中的activityIdleInternalLocked函数将被调用： 12345678final ActivityRecord activityIdleInternalLocked(final IBinder token, boolean fromTimeout, Configuration config) &#123; ........... if (isFocusedStack(r.task.stack) || fromTimeout) &#123; booting = checkFinishBootingLocked(); &#125; ............&#125; 在checkFinishBootingLocked函数中： 123456789101112131415private boolean checkFinishBootingLocked() &#123; //mService为AMS，mBooting变量在AMS回调SystemServer中定义的Runnable时，置为了true final boolean booting = mService.mBooting; boolean enableScreen = false; mService.mBooting = false; if (!mService.mBooted) &#123; mService.mBooted = true; enableScreen = true; &#125; if (booting || enableScreen) &#123;、 //调用AMS的接口，发送消息 mService.postFinishBooting(booting, enableScreen); &#125; return booting;&#125; 最终，AMS的finishBooting函数将被调用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061final void finishBooting() &#123; ......... //以下是注册广播接收器，用于处理需要重启的package IntentFilter pkgFilter = new IntentFilter(); pkgFilter.addAction(Intent.ACTION_QUERY_PACKAGE_RESTART); pkgFilter.addDataScheme(\"package\"); mContext.registerReceiver(new BroadcastReceiver() &#123; @Override public void onReceive(Context context, Intent intent) &#123; String[] pkgs = intent.getStringArrayExtra(Intent.EXTRA_PACKAGES); if (pkgs != null) &#123; for (String pkg : pkgs) &#123; synchronized (ActivityManagerService.this) &#123; if (forceStopPackageLocked(pkg, -1, false, false, false, false, false, 0, \"query restart\")) &#123; setResultCode(Activity.RESULT_OK); return; &#125; &#125; &#125; &#125; &#125; &#125;, pkgFilter); ........... // Let system services know. mSystemServiceManager.startBootPhase(SystemService.PHASE_BOOT_COMPLETED); //以下是启动那些等待启动的进程 synchronized (this) &#123; // Ensure that any processes we had put on hold are now started // up. final int NP = mProcessesOnHold.size(); if (NP &gt; 0) &#123; ArrayList&lt;ProcessRecord&gt; procs = new ArrayList&lt;ProcessRecord&gt;(mProcessesOnHold); for (int ip=0; ip&lt;NP; ip++) &#123; ................. startProcessLocked(procs.get(ip), \"on-hold\", null); &#125; &#125; &#125; &#125; .............. if (mFactoryTest != FactoryTest.FACTORY_TEST_LOW_LEVEL) &#123; // Start looking for apps that are abusing wake locks. //每15min检查一次系统各应用进程使用电量的情况，如果某个进程使用WakeLock的时间过长 //AMS将关闭该进程 Message nmsg = mHandler.obtainMessage(CHECK_EXCESSIVE_WAKE_LOCKS_MSG); mHandler.sendMessageDelayed(nmsg, POWER_CHECK_DELAY); // Tell anyone interested that we are done booting! SystemProperties.set(\"sys.boot_completed\", \"1\"); ................. //此处从代码来看发送的是ACTION_LOCKED_BOOT_COMPLETED广播 //在进行unlock相关的工作后，mUserController将调用finishUserUnlocking，发送SYSTEM_USER_UNLOCK_MSG消息给AMS //AMS收到消息后，调用mUserController的finishUserUnlocked函数，经过相应的处理后， //在mUserController的finishUserUnlockedCompleted中，最终将会发送ACTION_BOOT_COMPLETED广播 mUserController.sendBootCompletedLocked(.........); ................. &#125;&#125; 最终，当AMS启动Home Activity结束，并发送ACTION_BOOT_COMPLETED广播时，AMS的启动过程告一段落。 具体启动流程请参考：【Android 7.1.2 (Android N) Activity启动流程分析】 参考文档：Android 7.0 ActivityManagerService 1 - 10Android Init进程源码分析(1) - jay_richardAndroid Init进程源码分析(2) - jay_richardAndroid Zygote进程分析 - jay_richard图解Android - Zygote, System Server 启动分析 - 漫天尘沙 - 博客园Android7.0 init进程源码分析 - ZhangJian的博客 - CSDN博客Android系统启动-SystemServer上篇 - Gityuan博客 | 袁辉辉博客Android系统启动-Init篇 - Gityuan博客 | 袁辉辉博客Android系统启动-zygote篇 - Gityuan博客 | 袁辉辉博客Android系统启动-SystemServer下篇 - Gityuan博客 | 袁辉辉博客ActivityManagerService启动过程 - Gityuan博客 | 袁辉辉博客Android Init进程源码分析 - 深入剖析Android系统 - CSDN博客","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Android 7.1.2 (Android N) Android消息处理机制分析（从JAVA层到NATIVE层）– Handler、Looper、Message","slug":"Android-7-1-2-Android-N-Android消息机制–Handler、Looper、Message","date":"2017-07-31T16:00:00.000Z","updated":"2018-04-19T14:29:20.077Z","comments":true,"path":"2017/08/01/Android-7-1-2-Android-N-Android消息机制–Handler、Looper、Message/","link":"","permalink":"http://zhoujinjian.cc/2017/08/01/Android-7-1-2-Android-N-Android消息机制–Handler、Looper、Message/","excerpt":"","text":"● framework/base/core/java/andorid/os/Handler.java ● framework/base/core/java/andorid/os/Looper.java ● framework/base/core/java/andorid/os/Message.java ● framework/base/core/java/andorid/os/MessageQueue.java ● framework/base/core/java/andorid/os/MessageQueue.java ● framework/base/core/jni/android_os_MessageQueue.cpp ● framework/base/core/java/andorid/os/Looper.java (Java层） ● system/core/libutils/Looper.cpp ( Native层) ● framework/base/native/android/looper.cpp (ALoop对象) ● framework/native/include/android/looper.h 博客原图链接Ⅰ、Android消息机制(Java层)一、Android消息机制相关类、概念（Java层）主线程（UI线程） 定义：当程序第一次启动时，Android会同时启动一条主线程（Main Thread） 作用：主线程主要负责处理与UI相关的事件 Message（消息） 定义：Handler接收和处理的消息对象（Bean对象） 作用：通信时相关信息的存放和传递 ThreadLocal 定义：线程内部的数据存储类 作用：负责存储和获取本线程的Looper MessageQueue（消息队列） 定义：采用单链表的数据结构来存储消息列表 作用：用来存放通过Handler发过来的Message，按照先进先出执行 Handler（处理者） 定义：Message的主要处理者 作用：负责发送Message到消息队列&amp;处理Looper分派过来的Message Looper（循环器） 定义：扮演Message Queue和Handler之间桥梁的角色 作用： 消息循环：循环取出Message Queue的Message 消息派发：将取出的Message交付给相应的Handler 二、类关系图（Java层） ● Looper有一个MessageQueue消息队列；● MessageQueue有一组待处理的Message；●Message中有一个用于处理消息的Handler；● Handler中有Looper和MessageQueue。 典型实例:先展示一个典型的关于Handler/Looper的线程 123456789101112131415class LooperThread extends Thread &#123; public Handler mHandler; public void run() &#123; Looper.prepare(); mHandler = new Handler() &#123; public void handleMessage(Message msg) &#123; //TODO 定义消息处理逻辑. &#125; &#125;; Looper.loop(); &#125;&#125; 接下来，围绕着这个实例展开详细分析。 三、Looper源码分析（Java层）对于无参的情况，默认调用prepare(true)，表示的是这个Looper运行退出，而对于false的情况则表示当前Looper不运行退出。 12345678private static void prepare(boolean quitAllowed) &#123; //每个线程只允许执行一次该方法，第二次执行时线程的TLS已有数据，则会抛出异常。 if (sThreadLocal.get() != null) &#123; throw new RuntimeException(\"Only one Looper may be created per thread\"); &#125; //创建Looper对象，并保存到当前线程的TLS区域 sThreadLocal.set(new Looper(quitAllowed));&#125; 3.1 知识：ThreadLocal介绍这里的sThreadLocal是ThreadLocal类型，下面，先说说ThreadLocal。 ThreadLocal： 线程本地存储区（Thread Local Storage，简称为TLS），每个线程都有自己的私有的本地存储区域，不同线程之间彼此不能访问对方的TLS区域。TLS常用的操作方法： 123456789101112ThreadLocal.set(T value)：将value存储到当前线程的TLS区域，源码如下： public void set(T value) &#123; Thread currentThread = Thread.currentThread(); //获取当前线程 Values values = values(currentThread); //查找当前线程的本地储存区 if (values == null) &#123; //当线程本地存储区，尚未存储该线程相关信息时，则创建Values对象 values = initializeValues(currentThread); &#125; //保存数据value到当前线程this values.put(this, value); &#125; ThreadLocal.get()：获取当前线程TLS区域的数据，源码如下： 123456789101112131415public T get() &#123; Thread currentThread = Thread.currentThread(); //获取当前线程 Values values = values(currentThread); //查找当前线程的本地储存区 if (values != null) &#123; Object[] table = values.table; int index = hash &amp; values.mask; if (this.reference == table[index]) &#123; return (T) table[index + 1]; //返回当前线程储存区中的数据 &#125; &#125; else &#123; //创建Values对象 values = initializeValues(currentThread); &#125; return (T) values.getAfterMiss(this); //从目标线程存储区没有查询是则返回null&#125; ThreadLocal的get()和set()方法操作的类型都是泛型，接着回到前面提到的sThreadLocal变量，其定义如下： 1static final ThreadLocal&lt;Looper&gt; sThreadLocal = new ThreadLocal&lt;Looper&gt;() 可见sThreadLocal的get()和set()操作的类型都是Looper类型。 (1) Looper.prepare()Looper.prepare()在每个线程只允许执行一次，该方法会创建Looper对象，Looper的构造方法中会创建一个MessageQueue对象，再将Looper对象保存到当前线程TLS。 对于Looper类型的构造方法如下： 1234private Looper(boolean quitAllowed) &#123; mQueue = new MessageQueue(quitAllowed); //创建MessageQueue对象. mThread = Thread.currentThread(); //记录当前线程.&#125; 另外，与prepare()相近功能的，还有一个prepareMainLooper()方法，该方法主要在ActivityThread类中使用。 12345678910public static void prepareMainLooper() &#123; prepare(false); //设置不允许退出的Looper synchronized (Looper.class) &#123; //将当前的Looper保存为主Looper，每个线程只允许执行一次。 if (sMainLooper != null) &#123; throw new IllegalStateException(\"The main Looper has already been prepared.\"); &#125; sMainLooper = myLooper(); &#125;&#125; Looper.prepare()在每个线程只允许执行一次，该方法会创建Looper对象，Looper的构造方法中会创建一个MessageQueue对象，再将Looper对象保存到当前线程TLS。 对于Looper类型的构造方法如下： 1234private Looper(boolean quitAllowed) &#123; mQueue = new MessageQueue(quitAllowed); //创建MessageQueue对象.稍后详细介绍 mThread = Thread.currentThread(); //记录当前线程.&#125; 另外，与prepare()相近功能的，还有一个prepareMainLooper()方法，该方法主要在主线程（UI线程）ActivityThread类中使用。 12345678910public static void prepareMainLooper() &#123; prepare(false); //设置不允许退出的Looper synchronized (Looper.class) &#123; //将当前的Looper保存为主Looper，每个线程只允许执行一次。 if (sMainLooper != null) &#123; throw new IllegalStateException(\"The main Looper has already been prepared.\"); &#125; sMainLooper = myLooper(); &#125;&#125; （2）创建MessageQueue()MessageQueue是消息机制的Java层和C++层的连接纽带，大部分核心方法都交给native层来处理，其中MessageQueue类中涉及的native方法如下： 123private native static long nativeInit();private native void nativePollOnce(long ptr, int timeoutMillis);private native static void nativeWake(long ptr); 关于这些native方法的介绍，见第二节：Android消息机制(native篇)。 12345MessageQueue(boolean quitAllowed) &#123; mQuitAllowed = quitAllowed; //通过native方法初始化消息队列，其中mPtr是供native代码使用 mPtr = nativeInit();&#125; MessageQueue创建过程总结： 1、Looper的prepare或者prepareMainLooper静态方法被调用，将一个Looper对象保存在ThreadLocal里面。2、Looper对象的初始化方法里，首先会新建一个MessageQueue对象。3、MessageQueue对象的初始化方法通过JNI初始化C++层的NativeMessageQueue对象。4、NativeMessageQueue对象在创建过程中，会初始化一个C++层的Looper对象。5、C++层的Looper对象在创建的过程中，会在内部创建一个管道（pipe），并将这个管道的读写fd都保存在 mWakeReadPipeFd和mWakeWritePipeFd中。然后新建一个epoll实例，并将两个fd注册进去。6、利用epoll的机制，可以做到当管道没有消息时，线程睡眠在读端的fd上，当其他线程往管道写数据时，本线程便会被唤醒以进行消息处理。 (3) Looper.loop()12345678910111213141516171819202122232425262728293031323334public static void loop() &#123; final Looper me = myLooper(); //获取TLS存储的Looper对象 if (me == null) &#123; throw new RuntimeException(\"No Looper; Looper.prepare() wasn't called on this thread.\"); &#125; final MessageQueue queue = me.mQueue; //获取Looper对象中的消息队列 Binder.clearCallingIdentity(); //确保在权限检查时基于本地进程，而不是基于最初调用进程。 final long ident = Binder.clearCallingIdentity(); for (;;) &#123; //进入loop的主循环方法 Message msg = queue.next(); //可能会阻塞 May be block if (msg == null) &#123; //没有消息，则退出循环 return; &#125; Printer logging = me.mLogging; //默认为null，可通过setMessageLogging()方法来指定输出，用于debug功能 if (logging != null) &#123; logging.println(\"&gt;&gt;&gt;&gt;&gt; Dispatching to \" + msg.target + \" \" + msg.callback + \": \" + msg.what); &#125; msg.target.dispatchMessage(msg); //用于分发Message if (logging != null) &#123; logging.println(\"&lt;&lt;&lt;&lt;&lt; Finished to \" + msg.target + \" \" + msg.callback); &#125; final long newIdent = Binder.clearCallingIdentity(); //确保分发过程中identity不会损坏 if (ident != newIdent) &#123; //打印identity改变的log，在分发消息过程中是不希望身份被改变的。 &#125; msg.recycleUnchecked(); //将Message放入消息池 &#125;&#125; loop()进入循环模式，不断重复下面的操作，直到没有消息时退出循环 读取MessageQueue的下一条Message；把Message分发给相应的target； A1：Looper.loop()循环中的msg.target是什么时候被赋值的？handler.sendMessage()最终会进入MessageQueue.enqueueMessage()，就是在这里面复制的。 稍后再handler.sendMessage()详细介绍。 再把分发后的Message回收到消息池，以便重复利用。Looper.loop()是消息处理的核心部分。 3.1 MessageQueue.next()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586Message next() &#123; final long ptr = mPtr; if (ptr == 0) &#123; //当消息循环已经退出，则直接返回 return null; &#125; int pendingIdleHandlerCount = -1; // 循环迭代的首次为-1 int nextPollTimeoutMillis = 0; for (;;) &#123; if (nextPollTimeoutMillis != 0) &#123; Binder.flushPendingCommands(); &#125; //阻塞操作，当等待nextPollTimeoutMillis时长，或者消息队列被唤醒，都会返回 nativePollOnce(ptr, nextPollTimeoutMillis); synchronized (this) &#123; final long now = SystemClock.uptimeMillis(); Message prevMsg = null; Message msg = mMessages; if (msg != null &amp;&amp; msg.target == null) &#123; //当消息Handler为空时，查询MessageQueue中的下一条异步消息msg，则退出循环。 do &#123; prevMsg = msg; msg = msg.next; &#125; while (msg != null &amp;&amp; !msg.isAsynchronous()); &#125; if (msg != null) &#123; if (now &lt; msg.when) &#123; //当异步消息触发时间大于当前时间，则设置下一次轮询的超时时长 nextPollTimeoutMillis = (int) Math.min(msg.when - now, Integer.MAX_VALUE); &#125; else &#123; // 获取一条消息，并返回 mBlocked = false; if (prevMsg != null) &#123; prevMsg.next = msg.next; &#125; else &#123; mMessages = msg.next; &#125; msg.next = null; //设置消息的使用状态，即flags |= FLAG_IN_USE msg.markInUse(); return msg; //成功地获取MessageQueue中的下一条即将要执行的消息 &#125; &#125; else &#123; //没有消息 nextPollTimeoutMillis = -1; &#125; //消息正在退出，返回null if (mQuitting) &#123; dispose(); return null; &#125; //当消息队列为空，或者是消息队列的第一个消息时 if (pendingIdleHandlerCount &lt; 0 &amp;&amp; (mMessages == null || now &lt; mMessages.when)) &#123; pendingIdleHandlerCount = mIdleHandlers.size(); &#125; if (pendingIdleHandlerCount &lt;= 0) &#123; //没有idle handlers 需要运行，则循环并等待。 mBlocked = true; continue; &#125; if (mPendingIdleHandlers == null) &#123; mPendingIdleHandlers = new IdleHandler[Math.max(pendingIdleHandlerCount, 4)]; &#125; mPendingIdleHandlers = mIdleHandlers.toArray(mPendingIdleHandlers); &#125; //只有第一次循环时，会运行idle handlers，执行完成后，重置pendingIdleHandlerCount为0. for (int i = 0; i &lt; pendingIdleHandlerCount; i++) &#123; final IdleHandler idler = mPendingIdleHandlers[i]; mPendingIdleHandlers[i] = null; //去掉handler的引用 boolean keep = false; try &#123; keep = idler.queueIdle(); //idle时执行的方法 &#125; catch (Throwable t) &#123; Log.wtf(TAG, \"IdleHandler threw exception\", t); &#125; if (!keep) &#123; synchronized (this) &#123; mIdleHandlers.remove(idler); &#125; &#125; &#125; //重置idle handler个数为0，以保证不会再次重复运行 pendingIdleHandlerCount = 0; //当调用一个空闲handler时，一个新message能够被分发，因此无需等待可以直接查询pending message. nextPollTimeoutMillis = 0; &#125;&#125; nativePollOnce()在native做了大量的工作，Android消息机制(native篇)稍后详细分析。 消息循环Looper.loop()总结： 首先通过调用Looper的loop方法开始消息监听。loop方法里会调用MessageQueue的next方法。next方法会堵塞线程直到有消息到来为止。 next方法通过调用nativePollOnce方法来监听事件。next方法内部逻辑如下所示(简化)： a.进入死循环，以参数timout=0调用nativePollOnce方法。 b.如果消息队列中有消息，nativePollOnce方法会将消息保存在mMessage成员中。nativePollOnce方法返回后立刻检查mMessage成员是否为空。 c.如果mMessage不为空，那么检查它指定的运行时间。如果比当前时间要前，那么马上返回这个mMessage，否则设置&gt; timeout为两者之差，进入下一次循环。 d. 如果mMessage为空，那么设置timeout为-1，即下次循环nativePollOnce永久堵塞。 nativePollOnce方法内部利用epoll机制在之前建立的管道上等待数据写入。接收到数据后马上读取并返回结果。 这里先提一下为什么会阻塞，稍后再Android消息处理(Native层)分析，主要是底层使用了Linux epoll： Linux IO模式及 select、poll、epoll详解 3.2 Looper.quit()1234567public void quit() &#123; mQueue.quit(false); //消息移除&#125;public void quitSafely() &#123; mQueue.quit(true); //安全地消息移除&#125; Looper.quit()方法的实现最终调用的是MessageQueue.quit()方法 123456789101112131415161718192021MessageQueue.quit()void quit(boolean safe) &#123; // 当mQuitAllowed为false，表示不运行退出，强行调用quit()会抛出异常 if (!mQuitAllowed) &#123; throw new IllegalStateException(\"Main thread not allowed to quit.\"); &#125; synchronized (this) &#123; if (mQuitting) &#123; //防止多次执行退出操作 return; &#125; mQuitting = true; if (safe) &#123; removeAllFutureMessagesLocked(); //移除尚未触发的所有消息 &#125; else &#123; removeAllMessagesLocked(); //移除所有的消息 &#125; //mQuitting=false，那么认定为 mPtr != 0 nativeWake(mPtr); &#125; &#125; 消息退出的方式： 当safe =true时，只移除尚未触发的所有消息，对于正在触发的消息并不移除； 当safe =flase时，移除所有的消息 前面构造了Looper 、MessageQueue，假设此时没有message处理，Looper.loop()会阻塞在MessageQueue.next()。 接下来讲一下Handler如何发送和处理消息。 四、异步处理大师 Handler()(1) 构造Handler()1.1 无参构造Handler()12345678910111213141516171819202122public Handler() &#123; this(null, false);&#125;public Handler(Callback callback, boolean async) &#123; //匿名类、内部类或本地类都必须申明为static，否则会警告可能出现内存泄露 if (FIND_POTENTIAL_LEAKS) &#123; final Class&lt;? extends Handler&gt; klass = getClass(); if ((klass.isAnonymousClass() || klass.isMemberClass() || klass.isLocalClass()) &amp;&amp; (klass.getModifiers() &amp; Modifier.STATIC) == 0) &#123; Log.w(TAG, \"The following Handler class should be static or leaks might occur: \" + klass.getCanonicalName()); &#125; &#125; //必须先执行Looper.prepare()，才能获取Looper对象，否则为null. mLooper = Looper.myLooper(); //从当前线程的TLS中获取Looper对象 if (mLooper == null) &#123; throw new RuntimeException(\"\"); &#125; mQueue = mLooper.mQueue; //消息队列，来自Looper对象 mCallback = callback; //回调方法 mAsynchronous = async; //设置消息是否为异步处理方式&#125; 对于Handler的无参构造方法，默认采用当前线程TLS中的Looper对象，并且callback回调方法为null，且消息为同步处理方式。只要执行的Looper.prepare()方法，那么便可以获取有效的Looper对象。 1.2 有参构造Handler()12345678910public Handler(Looper looper) &#123; this(looper, null, false);&#125;public Handler(Looper looper, Callback callback, boolean async) &#123; mLooper = looper; mQueue = looper.mQueue; mCallback = callback; mAsynchronous = async;&#125; Handler类在构造方法中，可指定Looper，Callback回调方法以及消息的处理方式(同步或异步)，对于无参的handler，默认是当前线程的Looper。 (2) 使用 Handler发送消息Handler.sendMessage()、Handler.post(Ruunable r)第一种方式：sendMessage(Message msg) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//从这里开始public final boolean sendEmptyMessage(int what)&#123; return sendEmptyMessageDelayed(what, 0);&#125;//往下追踪public final boolean sendEmptyMessageDelayed(int what, long delayMillis) &#123; Message msg = Message.obtain(); msg.what = what; return sendMessageDelayed(msg, delayMillis);&#125;//往下追踪public final boolean sendMessageDelayed(Message msg, long delayMillis)&#123; if (delayMillis &lt; 0) &#123; delayMillis = 0; &#125; return sendMessageAtTime(msg, SystemClock.uptimeMillis() + delayMillis);&#125;//往下追踪public boolean sendMessageAtTime(Message msg, long uptimeMillis) &#123; //直接获取MessageQueue MessageQueue queue = mQueue; if (queue == null) &#123; RuntimeException e = new RuntimeException( this + \" sendMessageAtTime() called with no mQueue\"); Log.w(\"Looper\", e.getMessage(), e); return false; &#125; return enqueueMessage(queue, msg, uptimeMillis);&#125;//调用sendMessage方法其实最后是调用了enqueueMessage方法private boolean enqueueMessage(MessageQueue queue, Message msg, long uptimeMillis) &#123; //为msg.target赋值为this，也就是把当前的handler作为msg的target属性 //如果大家还记得Looper的loop()方法会取出每个msg然后执行msg.target.dispatchMessage(msg)去处理消息，其实就是派发给相应的Handler msg.target = this; if (mAsynchronous) &#123; msg.setAsynchronous(true); &#125; //最终调用queue的enqueueMessage的方法，也就是说handler发出的消息，最终会保存到消息队列中去 return queue.enqueueMessage(msg, uptimeMillis);&#125; enqueueMessage()方法中将msg.target赋值为this，也就是把当前的handler作为msg的target属性 //如果大家还记得Looper的loop()方法会取出每个msg然后执行msg.target.dispatchMessage(msg)去处理消息，其实就是派发给相应的Handler 2.1 MessageQueue.enqueueMessage() 添加一条消息到消息队列1234567891011121314151617181920212223242526272829303132333435363738394041424344454647boolean enqueueMessage(Message msg, long when) &#123; // 每一个普通Message必须有一个target if (msg.target == null) &#123; throw new IllegalArgumentException(\"Message must have a target.\"); &#125; if (msg.isInUse()) &#123; throw new IllegalStateException(msg + \" This message is already in use.\"); &#125; synchronized (this) &#123; if (mQuitting) &#123; //正在退出时，回收msg，加入到消息池 msg.recycle(); return false; &#125; msg.markInUse(); msg.when = when; Message p = mMessages; boolean needWake; if (p == null || when == 0 || when &lt; p.when) &#123; //p为null(代表MessageQueue没有消息） 或者msg的触发时间是队列中最早的， 则进入该该分支 msg.next = p; mMessages = msg; needWake = mBlocked; //当阻塞时需要唤醒 &#125; else &#123; //将消息按时间顺序插入到MessageQueue。一般地，不需要唤醒事件队列，除非 //消息队头存在barrier，并且同时Message是队列中最早的异步消息。 needWake = mBlocked &amp;&amp; p.target == null &amp;&amp; msg.isAsynchronous(); Message prev; for (;;) &#123; prev = p; p = p.next; if (p == null || when &lt; p.when) &#123; break; &#125; if (needWake &amp;&amp; p.isAsynchronous()) &#123; needWake = false; &#125; &#125; msg.next = p; prev.next = msg; &#125; //消息没有退出，我们认为此时mPtr != 0 if (needWake) &#123; nativeWake(mPtr); &#125; &#125; return true;&#125; MessageQueue是按照Message触发时间的先后顺序排列的，队头的消息是将要最早触发的消息。当有消息需要加入消息队列时，会从队列头开始遍历，直到找到消息应该插入的合适位置，以保证所有消息的时间顺序。 第二种方式：post(Ruunable r) 1234public final boolean post(Runnable r)&#123; return sendMessageDelayed(getPostMessage(r), 0);&#125; 其实post()方法最终也会保存到消息队列中去，和上面不同的是它传进来的一个Runnable对象，执行了getPostMessage()方法，我们往下追踪 12345private static Message getPostMessage(Runnable r) &#123; Message m = Message.obtain(); m.callback = r; return m;&#125; 实质上就是将这个Runnable保存在Message的变量中，这就导致了我们下面处理消息的时候有两种不同方案 Handler发送消息总结： 1、Handler对象在创建时会保存当前线程的looper和MessageQueue，如果传入Callback的话也会保存起来。2、用户调用handler对象的sendMessage方法，传入msg对象。handler通过调用MessageQueue的enqueueMessage方法将消息压入MessageQueue。3、enqueueMessage方法会将传入的消息对象根据触发时间（when）插入到message queue中。然后判断是否要唤醒等待中的队列。a. 如果插在队列中间。说明该消息不需要马上处理，不需要由这个消息来唤醒队列。b. 如果插在队列头部（或者when=0），则表明要马上处理这个消息。如果当前队列正在堵塞，则需要唤醒它进行处理。4、如果需要唤醒队列，则通过nativeWake方法，往前面提到的管道中写入一个”W”字符，令nativePollOnce方法返回。 (4) Handler处理消息在Looper.loop()中，当发现有消息时，调用消息的目标handler，执行dispatchMessage()方法来分发消息。 1234567891011121314151617181920212223public void dispatchMessage(Message msg) &#123; if (msg.callback != null) &#123; //1\\. post()方法的处理方法 handleCallback(msg); &#125; else &#123; if (mCallback != null) &#123; if (mCallback.handleMessage(msg)) &#123; return; &#125; &#125; //2\\. sendMessage()方法的处理方法 handleMessage(msg); &#125;&#125;//1\\. post()方法的最终处理方法private static void handleCallback(Message message) &#123; message.callback.run();&#125;//2\\. sendMessage()方法的最终处理方法public void handleMessage(Message msg) &#123;&#125; 处理消息总结： Looper对象的loop方法里面的queue.next方法如果返回了message，那么handler的dispatchMessage会被调用。a. 如果新建Handler的时候传入了callback实例，那么callback的handleMessage方法会被调用。b.如果是通过post方法向handler传入runnable对象的，那么runnable对象的run方法会被调用。c.其他情况下，handler方法的handleMessage会被调用。 (五)总结（1）简洁总结图示： 图解： ● Handler通过sendMessage()发送Message到MessageQueue队列；● Looper通过loop()，不断提取出达到触发条件的Message，并将Message交给target来处理；● 经过dispatchMessage()后，交回给Handler的handleMessage()来进行相应地处理。● 将Message加入MessageQueue时，处往管道写入字符，可以会唤醒loop线程；如果MessageQueue中没有Message，并处于Idle状态，则会执行IdelHandler接口中的方法，往往用于做一些清理性地工作。 消息分发的优先级： 1、Message的回调方法：message.callback.run()，优先级最高； 2、Handler的回调方法：Handler.mCallback.handleMessage(msg)，优先级仅次于1； 3、Handler的默认方法：Handler.handleMessage(msg)，优先级最低。 详细总结图示： ● Looper调用prepare()进行初始化，创建了一个与当前线程对应的Looper对象（通过ThreadLocal实现），并且初始化了一个与当前Looper对应的MessageQueue对象。 ● Looper调用静态方法loop()开始消息循环，通过MessageQueue.next()方法获取Message对象。 ● 当获取到一个Message对象时，让Message的发送者（target）去处理它。 ● Message对象包括数据，发送者（Handler），可执行代码段（Runnable）三个部分组成。 ● Handler可以在一个已经Looper.prepare()的线程中初始化，如果线程没有初始化Looper，创建Handler对象会失败 ● 一个线程的执行流中可以构造多个Handler对象，它们都往同一个MessageQueue中发消息，消息也只会分发给对应的Handler处理。 ● Handler将消息发送到MessageQueue中，Message的target域会引用自己的发送者，Looper从MessageQueue中取出来后，再交给发送这个Message的Handler去处理。 ● Message可以直接添加一个Runnable对象，当这条消息被处理的时候，直接执行Runnable.run()方法。 Ⅱ、Android消息机制(Native层)在前面讲解了Java层的消息处理机制，其中MessageQueue类里面涉及到多个native方法，除了MessageQueue的native方法，native层本身也有一套完整的消息机制，用于处理native的消息。在整个消息机制中，而MessageQueue是连接Java层和Native层的纽带，换言之，Java层可以向MessageQueue消息队列中添加消息，Native层也可以向MessageQueue消息队列中添加消息。 Native层类的关系图： (一) MessageQueue 初始化（Native 层）接着从Java层MessageQueue初始化开始分析： Step 1：MessageQueue()12345MessageQueue(boolean quitAllowed) &#123; mQuitAllowed = quitAllowed; //通过native方法初始化消息队列，其中mPtr是供native代码使用 mPtr = nativeInit();&#125; Step 2：android_os_MessageQueue_nativeInit()12345678910==&gt; android_os_MessageQueue.cppstatic jlong android_os_MessageQueue_nativeInit(JNIEnv* env, jclass clazz) &#123; NativeMessageQueue* nativeMessageQueue = new NativeMessageQueue(); //初始化native消息队列 【3】 if (!nativeMessageQueue) &#123; jniThrowRuntimeException(env, \"Unable to allocate native queue\"); return 0; &#125; nativeMessageQueue-&gt;incStrong(env); return reinterpret_cast&lt;jlong&gt;(nativeMessageQueue);&#125; 在nativeInit中，new了一个Native层的MessageQueue的对象 Step 3：NativeMessageQueue()123456789==&gt; android_os_MessageQueue.cppNativeMessageQueue::NativeMessageQueue() : mPollEnv(NULL), mPollObj(NULL), mExceptionObj(NULL) &#123; mLooper = Looper::getForThread(); //获取TLS中的Looper对象 if (mLooper == NULL) &#123; mLooper = new Looper(false); //创建native层的Looper Looper::setForThread(mLooper); //保存native层的Looper到TLS中 &#125;&#125; 在NativeMessageQueue的构造函数中获得了一个Native层的Looper对象，Native层的Looper也使用了线程本地存储，注意new Looper时传入了参数false。 Looper::getForThread()，功能类比于Java层的Looper.myLooper(); Looper::setForThread(mLooper)，功能类比于Java层的ThreadLocal.set(); MessageQueue是在Java层与Native层有着紧密的联系，但是此次Native层的Looper与Java层的Looper没有任何的关系，可以发现native基本等价于用C++重写了Java的Looper逻辑，故可以发现很多功能类似的地方。 (二) Looper初始化（Native层）1234567891011121314151617181920212223242526272829303132Looper::Looper(bool allowNonCallbacks) : mAllowNonCallbacks(allowNonCallbacks), mSendingMessage(false), mPolling(false), mEpollFd(-1), mEpollRebuildRequired(false), mNextRequestSeq(0), mResponseIndex(0), mNextMessageUptime(LLONG_MAX) &#123; mWakeEventFd = eventfd(0, EFD_NONBLOCK); //构造唤醒事件的fd AutoMutex _l(mLock); rebuildEpollLocked(); //重建Epoll事件&#125;void Looper::rebuildEpollLocked() &#123; if (mEpollFd &gt;= 0) &#123; close(mEpollFd); //关闭旧的epoll实例 &#125; mEpollFd = epoll_create(EPOLL_SIZE_HINT); //创建新的epoll实例，并注册wake管道 struct epoll_event eventItem; memset(&amp; eventItem, 0, sizeof(epoll_event)); //把未使用的数据区域进行置0操作 eventItem.events = EPOLLIN; //可读事件 eventItem.data.fd = mWakeEventFd; //将唤醒事件(mWakeEventFd)添加到epoll实例(mEpollFd) int result = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, mWakeEventFd, &amp; eventItem); for (size_t i = 0; i &lt; mRequests.size(); i++) &#123; const Request&amp; request = mRequests.valueAt(i); struct epoll_event eventItem; request.initEventItem(&amp;eventItem); //将request队列的事件，分别添加到epoll实例 int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, request.fd, &amp; eventItem); if (epollResult &lt; 0) &#123; ALOGE(\"Error adding epoll events for fd %d while rebuilding epoll set, errno=%d\", request.fd, errno); &#125; &#125;&#125; 通过eventfd创建mWakeEventFd用于线程间通信去唤醒Looper的，当需要唤醒Looper时，就往里面写1 创建用于监听epoll_event的mEpollFd，并初始化mEpollFd要监听的epoll_event类型 通过epoll_ctl将mWakeEventFd注册到mEpollFd中，当mWakeEventFd有事件可读则唤醒Looper 如果mRequests不为空的话，说明前面注册了有要监听的fd，则遍历mRequests中的Request，将它初始化为epoll_event并通过epoll_ctl注册到mEpollFd中，当有可读事件同样唤醒Looper (三) nativePollOnce()我们从前面分析知道，Looper.loop()方法被调用后，会启动一个无限循环，而在这个循环中，调用了MessageQueue的next()方法以获取下一条消息，而next()方法中会首先调用nativePollOnce()方法，这个方法的作用在之前说过是阻塞，达到超时时间或有新的消息到达时得到eventFd的通知再唤醒消息队列，其实这个方法也是native消息处理的开始。 nativePollOnce用于提取消息队列中的消息，提取消息的调用链，如下： 下面来进一步来看看调用链的过程： Step 1：MessageQueue.next()12345678910111213==&gt; MessageQueue.javaMessage next() &#123; final long ptr = mPtr; if (ptr == 0) &#123; return null; &#125; for (;;) &#123; ... nativePollOnce(ptr, nextPollTimeoutMillis); //阻塞操作 ... &#125; Step 2：android_os_MessageQueue_nativePollOnce()1234567==&gt; android_os_MessageQueue.cppstatic void android_os_MessageQueue_nativePollOnce(JNIEnv* env, jobject obj, jlong ptr, jint timeoutMillis) &#123; //将Java层传递下来的mPtr转换为nativeMessageQueue NativeMessageQueue* nativeMessageQueue = reinterpret_cast&lt;NativeMessageQueue*&gt;(ptr); nativeMessageQueue-&gt;pollOnce(env, obj, timeoutMillis);&#125; Step 3：NativeMessageQueue::pollOnce()1234567891011121314==&gt; android_os_MessageQueue.cppvoid NativeMessageQueue::pollOnce(JNIEnv* env, jobject pollObj, int timeoutMillis) &#123; mPollEnv = env; mPollObj = pollObj; mLooper-&gt;pollOnce(timeoutMillis); mPollObj = NULL; mPollEnv = NULL; if (mExceptionObj) &#123; env-&gt;Throw(mExceptionObj); env-&gt;DeleteLocalRef(mExceptionObj); mExceptionObj = NULL; &#125;&#125; Step 4：Looper::pollOnce()1234567891011121314151617181920212223242526272829==&gt; Looper.cppint Looper::pollOnce(int timeoutMillis, int* outFd, int* outEvents, void** outData) &#123; int result = 0; for (;;) &#123; // 先处理没有Callback方法的 Response事件 while (mResponseIndex &lt; mResponses.size()) &#123; const Response&amp; response = mResponses.itemAt(mResponseIndex++); int ident = response.request.ident; if (ident &gt;= 0) &#123; //ident大于0，则表示没有callback, 因为POLL_CALLBACK = -2, int fd = response.request.fd; int events = response.events; void* data = response.request.data; if (outFd != NULL) *outFd = fd; if (outEvents != NULL) *outEvents = events; if (outData != NULL) *outData = data; return ident; &#125; &#125; if (result != 0) &#123; if (outFd != NULL) *outFd = 0; if (outEvents != NULL) *outEvents = 0; if (outData != NULL) *outData = NULL; return result; &#125; // 再处理内部轮询 result = pollInner(timeoutMillis); &#125;&#125; Step 5 ：Looper::pollInner()：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102==&gt; Looper.cppvoid Looper::awoken() &#123; uint64_t counter; //不断读取管道数据，目的就是为了清空管道内容 TEMP_FAILURE_RETRY(read(mWakeEventFd, &amp;counter, sizeof(uint64_t)));&#125;int Looper::pollInner(int timeoutMillis) &#123; ... int result = POLL_WAKE; mResponses.clear(); mResponseIndex = 0; mPolling = true; //即将处于idle状态 struct epoll_event eventItems[EPOLL_MAX_EVENTS]; //fd最大个数为16 //等待事件发生或者超时，在nativeWake()方法，向管道写端写入字符，则该方法会返回； int eventCount = epoll_wait(mEpollFd, eventItems, EPOLL_MAX_EVENTS, timeoutMillis); mPolling = false; //不再处于idle状态 mLock.lock(); //请求锁 if (mEpollRebuildRequired) &#123; mEpollRebuildRequired = false; rebuildEpollLocked(); // epoll重建，直接跳转Done; goto Done; &#125; if (eventCount &lt; 0) &#123; if (errno == EINTR) &#123; goto Done; &#125; result = POLL_ERROR; // epoll事件个数小于0，发生错误，直接跳转Done; goto Done; &#125; if (eventCount == 0) &#123; //epoll事件个数等于0，发生超时，直接跳转Done; result = POLL_TIMEOUT; goto Done; &#125; //循环遍历，处理所有的事件 for (int i = 0; i &lt; eventCount; i++) &#123; int fd = eventItems[i].data.fd; uint32_t epollEvents = eventItems[i].events; if (fd == mWakeEventFd) &#123; if (epollEvents &amp; EPOLLIN) &#123; awoken(); //已经唤醒了，则读取并清空管道数据 &#125; &#125; else &#123; ssize_t requestIndex = mRequests.indexOfKey(fd); if (requestIndex &gt;= 0) &#123; int events = 0; if (epollEvents &amp; EPOLLIN) events |= EVENT_INPUT; if (epollEvents &amp; EPOLLOUT) events |= EVENT_OUTPUT; if (epollEvents &amp; EPOLLERR) events |= EVENT_ERROR; if (epollEvents &amp; EPOLLHUP) events |= EVENT_HANGUP; //处理request，生成对应的reponse对象，push到响应数组 pushResponse(events, mRequests.valueAt(requestIndex)); &#125; &#125; &#125;Done: ; //再处理Native的Message，调用相应回调方法 mNextMessageUptime = LLONG_MAX; while (mMessageEnvelopes.size() != 0) &#123; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); const MessageEnvelope&amp; messageEnvelope = mMessageEnvelopes.itemAt(0); if (messageEnvelope.uptime &lt;= now) &#123; &#123; sp&lt;MessageHandler&gt; handler = messageEnvelope.handler; Message message = messageEnvelope.message; mMessageEnvelopes.removeAt(0); mSendingMessage = true; mLock.unlock(); //释放锁 handler-&gt;handleMessage(message); // 处理消息事件 &#125; mLock.lock(); //请求锁 mSendingMessage = false; result = POLL_CALLBACK; // 发生回调 &#125; else &#123; mNextMessageUptime = messageEnvelope.uptime; break; &#125; &#125; mLock.unlock(); //释放锁 //处理带有Callback()方法的Response事件，执行Reponse相应的回调方法 for (size_t i = 0; i &lt; mResponses.size(); i++) &#123; Response&amp; response = mResponses.editItemAt(i); if (response.request.ident == POLL_CALLBACK) &#123; int fd = response.request.fd; int events = response.events; void* data = response.request.data; // 处理请求的回调方法 int callbackResult = response.request.callback-&gt;handleEvent(fd, events, data); if (callbackResult == 0) &#123; removeFd(fd, response.request.seq); //移除fd &#125; response.request.callback.clear(); //清除reponse引用的回调方法 result = POLL_CALLBACK; // 发生回调 &#125; &#125; return result;&#125; pollInner()方法比较长也是native消息机制的核心，我们拆成几个部分看。 5.1 Request 与 Response12345678910111213141516171819202122232425262728293031323334int result = POLL_WAKE;mResponses.clear();mResponseIndex = 0;mPolling = true;struct epoll_event eventItems[EPOLL_MAX_EVENTS];int eventCount = epoll_wait(mEpollFd, eventItems, EPOLL_MAX_EVENTS, timeoutMillis);// 第7行mPolling = false;mLock.lock();...for (int i = 0; i &lt; eventCount; i++) &#123;//第11行 int fd = eventItems[i].data.fd; uint32_t epollEvents = eventItems[i].events; if (fd == mWakeEventFd) &#123; if (epollEvents &amp; EPOLLIN) &#123; awoken(); &#125; else &#123; ALOGW(\"Ignoring unexpected epoll events 0x%x on wake event fd.\", epollEvents); &#125; &#125; else &#123; ssize_t requestIndex = mRequests.indexOfKey(fd); if (requestIndex &gt;= 0) &#123; int events = 0; if (epollEvents &amp; EPOLLIN) events |= EVENT_INPUT; if (epollEvents &amp; EPOLLOUT) events |= EVENT_OUTPUT; if (epollEvents &amp; EPOLLERR) events |= EVENT_ERROR; if (epollEvents &amp; EPOLLHUP) events |= EVENT_HANGUP; pushResponse(events, mRequests.valueAt(requestIndex));// 第28行 &#125; else &#123; ALOGW(\"Ignoring unexpected epoll events 0x%x on fd %d that is \" \"no longer registered.\", epollEvents, fd); &#125; &#125;&#125; 当第7行系统调用epoll_wait()返回时，说明因注册的fd有消息或达到超时，在第11行就对收到的唤醒events进行遍历，首先判断有消息的fd是不是用于唤醒的mWakeEventFd，如果不是的话，说明是系统调用addFd()方法设置的自定义fd（后面会讲）。那么我们需要对这个事件作出响应。 第21到28行就对这个event做处理，首先，我们以这个fd为key从mRequests中找到他的索引，这个mRequests是我们在addFd()方法一并注册的以fd为key，Request为value的映射表。找到request之后，28行调用pushResponse()方法去建立response： 123456void Looper::pushResponse(int events, const Request&amp; request) &#123; Response response; response.events = events; response.request = request; mResponses.push(response);&#125; 现在我们要处理的任务已经被封装成了一个Response对象，等待被处理，那么真正的处理在哪里呢？ 在上面的代码与处理response的代码中间夹着的是处理MessageEnvelope的代码，我们后面再讲这段，现在到处理response的代码： 1234567891011121314 for (size_t i = 0; i &lt; mResponses.size(); i++) &#123; Response&amp; response = mResponses.editItemAt(i); if (response.request.ident == POLL_CALLBACK) &#123; int fd = response.request.fd; int events = response.events; void* data = response.request.data; int callbackResult = response.request.callback-&gt;handleEvent(fd, events, data); if (callbackResult == 0) &#123; removeFd(fd, response.request.seq); &#125; response.request.callback.clear(); result = POLL_CALLBACK; &#125;&#125; 遍历所有response对象，取出之前注册的request对象的信息，然后调用了request.callback-&gt;handleEvent()方法进行回调，如果该回调返回0，则调用removeFd()方法取消这个fd的注册。 再梳理一遍这个过程：注册的自定义fd被消息唤醒，从mRequests中以fd为key找到对应的注册好的request对象然后生成response对象，在MessageEnvelop处理完毕之后处理response，调用request中的callback的handleEvent()方法。 那么addFd()注册自定义fd与removeFd()取消注册是如何实现的呢？ 5.2 addFd()12345678910111213141516171819202122232425262728293031323334353637383940414243444546int Looper::addFd(int fd, int ident, int events, const sp&lt;LooperCallback&gt;&amp; callback, void* data) &#123;...&#123; // acquire lock AutoMutex _l(mLock); Request request;//第6-13行 request.fd = fd; request.ident = ident; request.events = events; request.seq = mNextRequestSeq++; request.callback = callback; request.data = data; // 第6-13行 end if (mNextRequestSeq == -1) mNextRequestSeq = 0; // reserve sequence number -1 struct epoll_event eventItem; request.initEventItem(&amp;eventItem); ssize_t requestIndex = mRequests.indexOfKey(fd); if (requestIndex &lt; 0) &#123; // 第19行 int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, fd, &amp; eventItem); //第20行 if (epollResult &lt; 0) &#123; ALOGE(\"Error adding epoll events for fd %d: %s\", fd, strerror(errno)); return -1; &#125; mRequests.add(fd, request); //第25行 &#125; else &#123; int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_MOD, fd, &amp; eventItem); // 第27行 if (epollResult &lt; 0) &#123; if (errno == ENOENT) &#123; epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_ADD, fd, &amp; eventItem); if (epollResult &lt; 0) &#123; ALOGE(\"Error modifying or adding epoll events for fd %d: %s\", fd, strerror(errno)); return -1; &#125; scheduleEpollRebuildLocked(); &#125; else &#123; ALOGE(\"Error modifying epoll events for fd %d: %s\", fd, strerror(errno)); return -1; &#125; &#125; mRequests.replaceValueAt(requestIndex, request); &#125;&#125; // release lockreturn 1;&#125; 第6-13行使用传入的参数初始化了request对象，然后16行由request来初始化注册epoll使用的event。19行根据mRequests.indexOfKey()方法取出的值来判断fd是否已经注册，如果未注册，则在20行进行系统调用epoll_ctl()注册新监听并在25行将fd与request存入mRequest，如果已注册，则在27行更新注册并在42行更新request。 这就是自定义fd设置的过程：保存request并使用epoll_ctl系统调用注册fd的监听。 5.3 removeFd()12345678910111213141516171819202122232425int Looper::removeFd(int fd, int seq) &#123; &#123; // acquire lock AutoMutex _l(mLock); ssize_t requestIndex = mRequests.indexOfKey(fd); if (requestIndex &lt; 0) &#123; return 0; &#125; if (seq != -1 &amp;&amp; mRequests.valueAt(requestIndex).seq != seq) &#123; return 0; &#125; mRequests.removeItemsAt(requestIndex); int epollResult = epoll_ctl(mEpollFd, EPOLL_CTL_DEL, fd, NULL); if (epollResult &lt; 0) &#123; if (seq != -1 &amp;&amp; (errno == EBADF || errno == ENOENT)) &#123; scheduleEpollRebuildLocked(); &#125; else &#123; ALOGE(\"Error removing epoll events for fd %d: %s\", fd, strerror(errno)); scheduleEpollRebuildLocked(); return -1; &#125; &#125; &#125; // release lock return 1;&#125; 解除的过程相反，在第11行删除mRequests中的键值对，然后在第13行系统调用epoll_ctl()解除fd的epoll注册。 MessageEnvelope消息处理之前说到，在request生成response到response的处理中间有一段代码执行了MessageEnvelop消息的处理，这个顺序保证了MessageEnvelop优先于fd引起的request的处理。 现在我们来看这段代码： 12345678910111213141516171819202122mNextMessageUptime = LLONG_MAX;while (mMessageEnvelopes.size() != 0) &#123; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); const MessageEnvelope&amp; messageEnvelope = mMessageEnvelopes.itemAt(0); // 第4行 if (messageEnvelope.uptime &lt;= now) &#123; &#123; // obtain handler sp&lt;MessageHandler&gt; handler = messageEnvelope.handler; Message message = messageEnvelope.message; mMessageEnvelopes.removeAt(0); mSendingMessage = true; mLock.unlock(); handler-&gt;handleMessage(message); &#125; // release handler mLock.lock(); mSendingMessage = false; result = POLL_CALLBACK; &#125; else &#123; mNextMessageUptime = messageEnvelope.uptime; break; &#125;&#125; 可以看到mMessageEnvelopes容器中存储了所有的消息，第4行从首位置取出一条消息，随后进行时间判断，如果时间到达，先移出容器，与java层比较相似都是调用了handler的handleMessage()来进行消息的处理。 那么MessageEnvelope是如何添加的呢？ Native Looper提供了一套与java层MessageQueue类似的方法，用于添加MessageEnvelope： 12345678910111213141516171819202122void Looper::sendMessageAtTime(nsecs_t uptime, const sp&lt;MessageHandler&gt;&amp; handler, const Message&amp; message) &#123; size_t i = 0; &#123; // acquire lock AutoMutex _l(mLock); size_t messageCount = mMessageEnvelopes.size(); while (i &lt; messageCount &amp;&amp; uptime &gt;= mMessageEnvelopes.itemAt(i).uptime) &#123; i += 1; &#125; MessageEnvelope messageEnvelope(uptime, handler, message); mMessageEnvelopes.insertAt(messageEnvelope, i, 1); if (mSendingMessage) &#123; return; &#125; &#125; // release lock if (i == 0) &#123; wake(); &#125;&#125; 小结: 现在我们看到，其实Native中的消息机制有两个方面，一方面是通过addFd()注册的自定义fd触发消息处理，通过mRequests保存的request对象中的callback进行消息处理。另一方面是通过与java层类似的MessageEnvelop消息对象进行处理，调用的是该对象handler域的handleMessage()方法，与java层非常类似。优先级是先处理MessageEnvelop再处理request。 一些思考 现在消息机制全部内容分析下来，我们可以看到android的消息机制不算复杂，分为native与java两个部分，这两个部分分别有自己的消息处理机制，其中关键的超时与唤醒部分是借助了linux系统epoll机制来实现的。 连接java与native层消息处理过程的是next()方法中的nativePollOnce()，java层消息循环先调用它，自身阻塞，进入native的消息处理，在native消息处理完毕后返回，再进行java层的消息处理，正是因为如此，如果我们在处理java层消息的时候执行了耗时或阻塞的任务（甚至阻塞了整个主线程），整个java层的消息循环就会阻塞，也无法进一步进入native层的消息处理，也就无法响应例如触摸事件这样的消息，导致ANR的发生。这也就是我们不应在主线程中执行这类任务的原因。 (四) 唤醒 nativeWake()在添加消息到消息队列enqueueMessage(), 或者把消息从消息队列中全部移除quit()，再有需要时都会调用 nativeWake方法。包含唤醒过程的添加消息的调用链，nativeWake用于唤醒功能，如下： 下面来进一步来看看调用链的过程： Step 1 ：MessageQueue.enqueueMessage()12345678==&gt; MessageQueue.javaboolean enqueueMessage(Message msg, long when) &#123; ... //将Message按时间顺序插入MessageQueue if (needWake) &#123; nativeWake(mPtr); &#125;&#125; 往消息队列添加Message时，需要根据mBlocked情况来决定是否需要调用nativeWake。 Step 2 ：android_os_MessageQueue_nativeWake()123456==&gt; android_os_MessageQueue.cppstatic void android_os_MessageQueue_nativeWake(JNIEnv* env, jclass clazz, jlong ptr) &#123; NativeMessageQueue* nativeMessageQueue = reinterpret_cast&lt;NativeMessageQueue*&gt;(ptr); nativeMessageQueue-&gt;wake();&#125; Step 3 ：NativeMessageQueue::wake()12345==&gt; android_os_MessageQueue.cppvoid NativeMessageQueue::wake() &#123; mLooper-&gt;wake();&#125; Step 4 ：Looper::wake()12345678910111213==&gt; Looper.cppvoid Looper::wake() &#123; uint64_t inc = 1; // 向管道mWakeEventFd写入字符1 ssize_t nWrite = TEMP_FAILURE_RETRY(write(mWakeEventFd, &amp;inc, sizeof(uint64_t))); if (nWrite != sizeof(uint64_t)) &#123; if (errno != EAGAIN) &#123; ALOGW(\"Could not write wake signal, errno=%d\", errno); &#125; &#125;&#125;其中TEMP_FAILURE_RETRY 是一个宏定义， 当执行write失败后，会不断重复执行，直到执行成功为止。 (五) 发送消息sendMessage（Native层）在前面Android消息机制(Java层)文中，讲述了Java层如何向MessageQueue类中添加消息，那么接下来讲讲Native层如何向MessageQueue发送消息。 Step 1 ：sendMessage()1234void Looper::sendMessage(const sp&lt;MessageHandler&gt;&amp; handler, const Message&amp; message) &#123; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); sendMessageAtTime(now, handler, message);&#125; Step 2 ：sendMessageDelayed()12345void Looper::sendMessageDelayed(nsecs_t uptimeDelay, const sp&lt;MessageHandler&gt;&amp; handler, const Message&amp; message) &#123; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); sendMessageAtTime(now + uptimeDelay, handler, message);&#125; sendMessage(),sendMessageDelayed() 都是调用sendMessageAtTime()来完成消息插入。 Step 3 ：sendMessageAtTime()12345678910111213141516171819202122void Looper::sendMessageAtTime(nsecs_t uptime, const sp&lt;MessageHandler&gt;&amp; handler, const Message&amp; message) &#123; size_t i = 0; &#123; //请求锁 AutoMutex _l(mLock); size_t messageCount = mMessageEnvelopes.size(); //找到message应该插入的位置i while (i &lt; messageCount &amp;&amp; uptime &gt;= mMessageEnvelopes.itemAt(i).uptime) &#123; i += 1; &#125; MessageEnvelope messageEnvelope(uptime, handler, message); mMessageEnvelopes.insertAt(messageEnvelope, i, 1); //如果当前正在发送消息，那么不再调用wake()，直接返回。 if (mSendingMessage) &#123; return; &#125; &#125; //释放锁 //当把消息加入到消息队列的头部时，需要唤醒poll循环。 if (i == 0) &#123; wake(); &#125;&#125; (六) 处理消息MessageHandler.handleMessage() &amp;&amp; LooperCallback.handleEvent()（Native层）其实Native中的消息机制有两个方面，一方面是通过addFd()注册的自定义fd触发消息处理，通过mRequests保存的request对象中的callback进行消息处理。即调用LooperCallback的handleEvent()处理 另一方面是通过与java层类似的MessageEnvelop消息对象进行处理，调用的是该对象handler域的handleMessage()方法，与java层非常类似。优先级是先处理MessageEnvelop再处理request。即调用MessageHandler类的handleMessage()处理 6.1 MessageHandler类：调用MessageHandler类的handleMessage()处理消息 123456class MessageHandler : public virtual RefBase &#123;protected: virtual ~MessageHandler() &#123; &#125;public: virtual void handleMessage(const Message&amp; message) = 0;&#125;; 6.2 LooperCallback.handleEvent)：用于处理指定的文件描述符的poll事件 12345678LooperCallback类class LooperCallback : public virtual RefBase &#123;protected:virtual ~LooperCallback() &#123; &#125;public://用于处理指定的文件描述符的poll事件virtual int handleEvent(int fd, int events, void* data) = 0;&#125;; (七) Native消息机制使用实例：SurfaceFlinger Native消息处理在 【Android 7.1.2(Android N) Activity-Window加载显示流程】中讲到 App请求创建Surface创建过程中，SurfaceFlinger会处理Native消息，此处便是Native消息机制使用的一个具体实例。 12345678910111213141516171819202122232425262728293031323334353637383940414243status_t Client::createSurface( const String8&amp; name, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp)&#123; /* * createSurface must be called from the GL thread so that it can * have access to the GL context. */ class MessageCreateLayer : public MessageBase &#123; SurfaceFlinger* flinger; Client* client; sp&lt;IBinder&gt;* handle; sp&lt;IGraphicBufferProducer&gt;* gbp; status_t result; const String8&amp; name; uint32_t w, h; PixelFormat format; uint32_t flags; public: MessageCreateLayer(SurfaceFlinger* flinger, const String8&amp; name, Client* client, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags, sp&lt;IBinder&gt;* handle, sp&lt;IGraphicBufferProducer&gt;* gbp) : flinger(flinger), client(client), handle(handle), gbp(gbp), result(NO_ERROR), name(name), w(w), h(h), format(format), flags(flags) &#123; &#125; status_t getResult() const &#123; return result; &#125; virtual bool handler() &#123; result = flinger-&gt;createLayer(name, client, w, h, format, flags, handle, gbp); return true; &#125; &#125;; sp&lt;MessageBase&gt; msg = new MessageCreateLayer(mFlinger.get(), name, this, w, h, format, flags, handle, gbp); mFlinger-&gt;postMessageSync(msg); return static_cast&lt;MessageCreateLayer*&gt;( msg.get() )-&gt;getResult(); &#125; Client将应用程序创建Surface的请求转换为异步消息投递到SurfaceFlinger的消息队列中，将创建Surface的任务转交给SurfaceFlinger。 函数首先将请求创建的Surface参数封装为MessageCreateSurface对象，然后调用SurfaceFlinger的postMessageSync函数往SurfaceFlinger的消息队列中发送一个同步消息，当消息处理完后，通过调用消息msg的getResult()函数来得到创建的Surface。 1234567891011121314151617181920212223status_t SurfaceFlinger::postMessageSync(const sp&lt;MessageBase&gt;&amp; msg, nsecs_t reltime, uint32_t flags) &#123; //往消息队列中发送一个消息 status_t res = mEventQueue.postMessage(msg, reltime); //消息发送成功后，当前线程等待消息处理 if (res == NO_ERROR) &#123; msg-&gt;wait(); &#125; return res; &#125; status_t MessageQueue::postMessage( const sp&lt;MessageBase&gt;&amp; messageHandler, nsecs_t relTime) &#123;const Message dummyMessage;//将messageHandler对象和dummyMessage消息对象发送到消息循环Looper对象中if (relTime &gt; 0) &#123; mLooper-&gt;sendMessageDelayed(relTime, messageHandler, dummyMessage);&#125; else &#123; mLooper-&gt;sendMessage(messageHandler, dummyMessage);&#125;return NO_ERROR;&#125; 关于消息循环Looper对象的消息发送函数sendMessage的调用流程请看前面讲解。 这里再次贴上关于消息插入代码： 1234567891011121314151617181920212223void Looper::sendMessageAtTime(nsecs_t uptime, const sp&lt;MessageHandler&gt;&amp; handler, const Message&amp; message) &#123; size_t i = 0; &#123; // acquire lock AutoMutex _l(mLock); //获取消息队列中保存的消息个数 size_t messageCount = mMessageEnvelopes.size(); //按时间排序，查找当前消息应该插入的位置 while (i &lt; messageCount &amp;&amp; uptime &gt;= mMessageEnvelopes.itemAt(i).uptime) &#123; i += 1; &#125; //将Message消息及消息处理Handler封装为MessageEnvelope对象，并插入到消息队列mMessageEnvelopes中 MessageEnvelope messageEnvelope(uptime, handler, message); mMessageEnvelopes.insertAt(messageEnvelope, i, 1); if (mSendingMessage) &#123; return; &#125; &#125; // release lock //唤醒消息循环线程以及时处理消息 if (i == 0) &#123; wake(); &#125; &#125; 到此消息发送就完成了，由于发送的是一个同步消息，因此消息发送线程此刻进入睡眠等待状态，而消息循环线程被唤醒起来处理消息，消息处理过程如下： 12345678910111213141516171819202122232425262728//所有C++层的消息都封装为MessageEnvelope类型的变量并保存到mMessageEnvelopes链表中 while (mMessageEnvelopes.size() != 0) &#123; nsecs_t now = systemTime(SYSTEM_TIME_MONOTONIC); const MessageEnvelope&amp; messageEnvelope = mMessageEnvelopes.itemAt(0); //处理当前时刻之前的所有消息 if (messageEnvelope.uptime &lt;= now) &#123; &#123; //取出处理该消息的Hanlder sp&lt;MessageHandler&gt; handler = messageEnvelope.handler; //取出该消息描述符 Message message = messageEnvelope.message; //从mMessageEnvelopes链表中移除该消息 mMessageEnvelopes.removeAt(0); //表示当前消息循环线程正在处理消息，处于唤醒状态，因此消息发送线程无需唤醒消息循环线程 mSendingMessage = true; mLock.unlock(); //调用该消息Handler对象的handleMessage函数来处理该消息 handler-&gt;handleMessage(message); &#125; // release handler mLock.lock(); mSendingMessage = false; result = ALOOPER_POLL_CALLBACK; &#125; else &#123; // The last message left at the head of the queue determines the next wakeup time. mNextMessageUptime = messageEnvelope.uptime; break; &#125; &#125; 消息处理过程就是调用该消息的Handler对象的handleMessage函数来完成，由于创建Surface时，往消息队列中发送的Handler对象类型为MessageCreateSurface，因此必定会调用该类的handleMessage函数来处理Surface创建消息。但该类并未实现 handleMessage函数，同时该类继承于MessageBase，由此可见其父类MessageBase必定实现了handleMessage函数： 1234void MessageBase::handleMessage(const Message&amp;) &#123; this-&gt;handler(); barrier.open(); &#125;; 该函数首先调用其子类的handler()函数处理消息，然后唤醒消息发送线程，表明发往消息队列中的消息已得到处理，消息发送线程可以往下执行了。由于MessageCreateSurface是MessageBase的子类，因此该类必定实现了handler()函数来处理Surface创建消息： 123456789101112131415161718192021222324252627class MessageCreateSurface : public MessageBase &#123; sp&lt;ISurface&gt; result; SurfaceFlinger* flinger; ISurfaceComposerClient::surface_data_t* params; Client* client; const String8&amp; name; DisplayID display; uint32_t w, h; PixelFormat format; uint32_t flags; public: MessageCreateSurface(SurfaceFlinger* flinger, ISurfaceComposerClient::surface_data_t* params, const String8&amp; name, Client* client, DisplayID display, uint32_t w, uint32_t h, PixelFormat format, uint32_t flags) : flinger(flinger), params(params), client(client), name(name), display(display), w(w), h(h), format(format), flags(flags) &#123; &#125; sp&lt;ISurface&gt; getResult() const &#123; return result; &#125; virtual bool handler() &#123; result = flinger-&gt;createSurface(params, name, client,display, w, h, format, flags); return true; &#125; &#125;; 这里又调用SurfaceFlinger的createSurface函数来创建Surface。绕了一圈又回到SurfaceFlinger，为什么要这么做呢？因为在同一时刻可以有多个应用程序请求SurfaceFlinger为其创建Surface，通过消息队列可以实现请求排队，然后SurfaceFlinger依次为应用程序创建Surface。 图解： 红色虚线关系：Java层和Native层的MessageQueue通过JNI建立关联，彼此之间能相互调用，搞明白这个互调关系，也就搞明白了Java如何调用C++代码，C++代码又是如何调用Java代码。 蓝色虚线关系：Handler/Looper/Message这三大类Java层与Native层并没有任何的真正关联，只是分别在Java层和Native层的handler消息模型中具有相似的功能。都是彼此独立的，各自实现相应的逻辑。 WeakMessageHandler继承于MessageHandler类，NativeMessageQueue继承于MessageQueue类 另外，消息处理流程是先处理Native Message，再处理Native Request，最后处理Java Message。理解了该流程，也就明白有时上层消息很少，但响应时间却较长的真正原因。 总结： 参考文档（特别感谢）：Android消息处理机制(Handler、Looper、MessageQueue与Message)android的消息处理机制（图+源码分析）—-Looper,Handler,MessageAndroid进阶—-Android消息机制之Looper、Handler、MessageQueenHandler、Looper、Message、MessageQueue 基础流程分析图解Android 中线程间通信原理分析：Looper, MessageQueue, HandlerAndroid消息机制1-Handler(Java层)Android消息机制2-Handler(Native层)ANDROID消息机制，从JAVA层到NATIVE层剖析 Android应用程序消息处理机制Android 消息机制（三）Native层消息机制","categories":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"http://zhoujinjian.cc/tags/Android/"}]},{"title":"Hexo - Hello World","slug":"hello-world","date":"2015-06-30T16:00:00.000Z","updated":"2018-04-19T14:23:49.974Z","comments":true,"path":"2015/07/01/hello-world/","link":"","permalink":"http://zhoujinjian.cc/2015/07/01/hello-world/","excerpt":"hexo-theme-melody hexo-theme-melody-documentation","text":"hexo-theme-melody hexo-theme-melody-documentation Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo Deploy More info: Deployment","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://zhoujinjian.cc/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://zhoujinjian.cc/tags/Hexo/"}]}]}